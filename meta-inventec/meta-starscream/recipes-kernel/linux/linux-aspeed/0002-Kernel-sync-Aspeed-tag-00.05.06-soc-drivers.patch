From e536bc56090e73b5a26e1171263027a29446004b Mon Sep 17 00:00:00 2001
From: link <link@inventec.com>
Date: Tue, 20 Jun 2023 17:45:39 +0800
Subject: [PATCH] Kernel-sync - [Aspeed] tag 00.05.06 soc drivers

Symptom/Reason:
    Sync codebase with AspeedTech-BMC/linux tag 00.05.06

Root Cause:
    N/A

Solution/Change:
    soc drivers
        - aspeed-bmc-dev
        - aspeed-host-bmc-dev
        - aspeed-espi
		- Removed old espi files
		  https://github.com/AspeedTech-BMC/linux/commit/b129e9e2c32eb0f8f68a51f34f220c81217f189b
		- Add new espi files for required funcitons
		  https://github.com/AspeedTech-BMC/linux/commit/b0e588fd52958022013d7a07b8994f72fd3be741

        - aspeed-mctp
        - aspeed-otp
        - aspeed-udma
        - ast_video (is removed, its function is merge to patch 0001)
        https://github.com/AspeedTech-BMC/linux/commit/9a96e350b8c1b033b1c1d017036532fef4e0f48b
        - aspeed-ssp
        - aspeed-lpc-mailbox
        - aspeed-rvas

Entry Test:
    N/A
---
 drivers/soc/Makefile                       |    2 +-
 drivers/soc/aspeed/Kconfig                 |   75 +
 drivers/soc/aspeed/Makefile                |   15 +-
 drivers/soc/aspeed/aspeed-bmc-dev.c        |  528 +++++
 drivers/soc/aspeed/aspeed-espi-comm.h      |  196 ++
 drivers/soc/aspeed/aspeed-host-bmc-dev.c   |  519 +++++
 drivers/soc/aspeed/aspeed-lpc-mbox.c       |  418 ++++
 drivers/soc/aspeed/aspeed-lpc-pcc.c        |  406 ++++
 drivers/soc/aspeed/aspeed-lpc-snoop.c      |   21 +-
 drivers/soc/aspeed/aspeed-mctp.c           | 2349 ++++++++++++++++++++
 drivers/soc/aspeed/aspeed-otp.c            |  639 ++++++
 drivers/soc/aspeed/aspeed-ssp.c            |  208 ++
 drivers/soc/aspeed/aspeed-udma.c           |  464 ++++
 drivers/soc/aspeed/aspeed-usb-ahp.c        |   47 +
 drivers/soc/aspeed/aspeed-usb-phy.c        |   70 +
 drivers/soc/aspeed/ast2500-espi.c          | 1511 +++++++++++++
 drivers/soc/aspeed/ast2500-espi.h          |  211 ++
 drivers/soc/aspeed/ast2600-espi.c          | 1883 ++++++++++++++++
 drivers/soc/aspeed/ast2600-espi.h          |  252 +++
 drivers/soc/aspeed/rvas/Kconfig            |    9 +
 drivers/soc/aspeed/rvas/Makefile           |    3 +
 drivers/soc/aspeed/rvas/hardware_engines.c | 2225 ++++++++++++++++++
 drivers/soc/aspeed/rvas/hardware_engines.h |  500 +++++
 drivers/soc/aspeed/rvas/video.h            |   43 +
 drivers/soc/aspeed/rvas/video_debug.h      |   32 +
 drivers/soc/aspeed/rvas/video_engine.c     | 1205 ++++++++++
 drivers/soc/aspeed/rvas/video_engine.h     |  293 +++
 drivers/soc/aspeed/rvas/video_ioctl.h      |  275 +++
 drivers/soc/aspeed/rvas/video_main.c       | 1622 ++++++++++++++
 include/linux/aspeed-mctp.h                |  155 ++
 include/linux/soc/aspeed/aspeed-udma.h     |   30 +
 include/uapi/linux/aspeed-mctp.h           |  136 ++
 include/uapi/linux/aspeed-otp.h            |   39 +
 33 files changed, 16359 insertions(+), 22 deletions(-)
 create mode 100644 drivers/soc/aspeed/aspeed-bmc-dev.c
 create mode 100644 drivers/soc/aspeed/aspeed-espi-comm.h
 create mode 100644 drivers/soc/aspeed/aspeed-host-bmc-dev.c
 create mode 100644 drivers/soc/aspeed/aspeed-lpc-mbox.c
 create mode 100644 drivers/soc/aspeed/aspeed-lpc-pcc.c
 create mode 100644 drivers/soc/aspeed/aspeed-mctp.c
 create mode 100644 drivers/soc/aspeed/aspeed-otp.c
 create mode 100644 drivers/soc/aspeed/aspeed-ssp.c
 create mode 100644 drivers/soc/aspeed/aspeed-udma.c
 create mode 100644 drivers/soc/aspeed/aspeed-usb-ahp.c
 create mode 100644 drivers/soc/aspeed/aspeed-usb-phy.c
 create mode 100644 drivers/soc/aspeed/ast2500-espi.c
 create mode 100644 drivers/soc/aspeed/ast2500-espi.h
 create mode 100644 drivers/soc/aspeed/ast2600-espi.c
 create mode 100644 drivers/soc/aspeed/ast2600-espi.h
 create mode 100644 drivers/soc/aspeed/rvas/Kconfig
 create mode 100644 drivers/soc/aspeed/rvas/Makefile
 create mode 100644 drivers/soc/aspeed/rvas/hardware_engines.c
 create mode 100644 drivers/soc/aspeed/rvas/hardware_engines.h
 create mode 100644 drivers/soc/aspeed/rvas/video.h
 create mode 100644 drivers/soc/aspeed/rvas/video_debug.h
 create mode 100644 drivers/soc/aspeed/rvas/video_engine.c
 create mode 100644 drivers/soc/aspeed/rvas/video_engine.h
 create mode 100644 drivers/soc/aspeed/rvas/video_ioctl.h
 create mode 100644 drivers/soc/aspeed/rvas/video_main.c
 create mode 100644 include/linux/aspeed-mctp.h
 create mode 100644 include/linux/soc/aspeed/aspeed-udma.h
 create mode 100644 include/uapi/linux/aspeed-mctp.h
 create mode 100644 include/uapi/linux/aspeed-otp.h

diff --git a/drivers/soc/Makefile b/drivers/soc/Makefile
index 69ba6508cf2c..ef29d29d56fa 100644
--- a/drivers/soc/Makefile
+++ b/drivers/soc/Makefile
@@ -24,7 +24,7 @@ obj-y				+= pxa/
 obj-y				+= amlogic/
 obj-y				+= qcom/
 obj-y				+= renesas/
-obj-y				+= rockchip/
+obj-$(CONFIG_ARCH_ROCKCHIP)	+= rockchip/
 obj-$(CONFIG_SOC_SAMSUNG)	+= samsung/
 obj-$(CONFIG_SOC_SIFIVE)	+= sifive/
 obj-y				+= sunxi/
diff --git a/drivers/soc/aspeed/Kconfig b/drivers/soc/aspeed/Kconfig
index aaf4596ae4f9..3f3bf7e9db08 100644
--- a/drivers/soc/aspeed/Kconfig
+++ b/drivers/soc/aspeed/Kconfig
@@ -4,6 +4,12 @@ if ARCH_ASPEED || COMPILE_TEST
 
 menu "ASPEED SoC drivers"
 
+config ASPEED_BMC_DEV
+	tristate "ASPEED BMC Device"
+
+config ASPEED_HOST_BMC_DEV
+	tristate "ASPEED Host BMC Device"
+
 config ASPEED_LPC_CTRL
 	tristate "ASPEED LPC firmware cycle control"
 	select REGMAP
@@ -24,6 +30,20 @@ config ASPEED_LPC_SNOOP
 	  allows the BMC to listen on and save the data written by
 	  the host to an arbitrary LPC I/O port.
 
+config ASPEED_SSP
+	tristate "ASPEED SSP loader"
+	default n
+	help
+	  Driver for loading secondary-service-processor binary
+
+config ASPEED_MCTP
+	tristate "Aspeed ast2600 MCTP Controller support"
+	depends on REGMAP && MFD_SYSCON
+	help
+	  Enable support for ast2600 MCTP Controller.
+	  The MCTP controller allows the BMC to communicate with devices on
+	  the host PCIe network.
+
 config ASPEED_UART_ROUTING
 	tristate "ASPEED uart routing control"
 	select REGMAP
@@ -34,6 +54,16 @@ config ASPEED_UART_ROUTING
 	  users to perform runtime configuration of the RX muxes among
 	  the UART controllers and I/O pins.
 
+config ASPEED_LPC_MAILBOX
+	tristate "ASPEED LPC mailbox support"
+	select REGMAP
+	select MFD_SYSCON
+	default ARCH_ASPEED
+	help
+	  Provides a driver to control the LPC mailbox which possesses
+	  up to 32 data registers for the communication between the Host
+	  and the BMC over LPC.
+
 config ASPEED_P2A_CTRL
 	tristate "ASPEED P2A (VGA MMIO to BMC) bridge control"
 	select REGMAP
@@ -62,6 +92,51 @@ config ASPEED_XDMA
 	  SoCs. The XDMA engine can perform PCIe DMA operations between the BMC
 	  and a host processor.
 
+config ASPEED_ESPI
+	bool "ASPEED eSPI slave driver"
+	select AST2500_ESPI if MACH_ASPEED_G5
+	select AST2600_ESPI if MACH_ASPEED_G6
+	default n
+	help
+	  Enable driver support for the Aspeed eSPI engine. The eSPI engine
+	  plays as a slave device in BMC to communicate with the Host over
+	  the eSPI interface. The four eSPI channels, namely peripheral,
+	  virtual wire, out-of-band, and flash are supported.
+
+config AST2500_ESPI
+	bool
+	depends on ASPEED_ESPI
+	help
+	  Enable driver support for Aspeed AST2500 eSPI engine.
+
+config AST2600_ESPI
+	bool
+	depends on ASPEED_ESPI
+	help
+	  Enable driver support for Aspeed AST2600 eSPI engine.
+
+config ASPEED_UDMA
+	tristate "Aspeed UDMA Engine Driver"
+	depends on ARCH_ASPEED && REGMAP && MFD_SYSCON && HAS_DMA
+	help
+	  Enable support for the Aspeed UDMA Engine found on the Aspeed AST2XXX
+	  SOCs. The UDMA engine can perform UART DMA operations between the memory
+	  buffer and the UART/VUART devices.
+
+config ASPEED_LPC_PCC
+	tristate "Aspeed Post Code Capture support"
+	depends on ARCH_ASPEED && REGMAP && MFD_SYSCON
+	help
+	  Provides a driver to control the LPC PCC interface,
+	  allowing the BMC to snoop data bytes written by the
+	  the host to an arbitrary LPC I/O port.
+
+config ASPEED_OTP
+	tristate "Aspeed OTP Driver"
+	depends on MACH_ASPEED_G6
+
+source "drivers/soc/aspeed/rvas/Kconfig"
+
 config ASPEED_SBC
 	bool "ASPEED Secure Boot Controller driver"
 	default MACH_ASPEED_G6
diff --git a/drivers/soc/aspeed/Makefile b/drivers/soc/aspeed/Makefile
index 5f61bb4cc36c..39753197e272 100644
--- a/drivers/soc/aspeed/Makefile
+++ b/drivers/soc/aspeed/Makefile
@@ -1,8 +1,21 @@
 # SPDX-License-Identifier: GPL-2.0-only
+obj-$(CONFIG_ASPEED_BMC_DEV)		+= aspeed-bmc-dev.o
+obj-$(CONFIG_ASPEED_HOST_BMC_DEV)	+= aspeed-host-bmc-dev.o
 obj-$(CONFIG_ASPEED_LPC_CTRL)		+= aspeed-lpc-ctrl.o
 obj-$(CONFIG_ASPEED_LPC_SNOOP)		+= aspeed-lpc-snoop.o
 obj-$(CONFIG_ASPEED_UART_ROUTING)	+= aspeed-uart-routing.o
+obj-$(CONFIG_ASPEED_SSP)		+= aspeed-ssp.o
 obj-$(CONFIG_ASPEED_P2A_CTRL)		+= aspeed-p2a-ctrl.o
 obj-$(CONFIG_ASPEED_SOCINFO)		+= aspeed-socinfo.o
-obj-$(CONFIG_ASPEED_SBC)		+= aspeed-sbc.o
 obj-$(CONFIG_ASPEED_XDMA)		+= aspeed-xdma.o
+obj-$(CONFIG_AST2500_ESPI)		+= ast2500-espi.o
+obj-$(CONFIG_AST2600_ESPI)		+= ast2600-espi.o
+obj-$(CONFIG_ASPEED_LPC_MAILBOX)	+= aspeed-lpc-mbox.o
+obj-$(CONFIG_ASPEED_UDMA)		+= aspeed-udma.o
+obj-$(CONFIG_ASPEED_LPC_PCC)		+= aspeed-lpc-pcc.o
+obj-$(CONFIG_ASPEED_RVAS)		+= rvas/
+obj-$(CONFIG_ARCH_ASPEED)		+= aspeed-usb-phy.o
+obj-$(CONFIG_ARCH_ASPEED)		+= aspeed-usb-ahp.o
+obj-$(CONFIG_ASPEED_MCTP)		+= aspeed-mctp.o
+obj-$(CONFIG_ASPEED_OTP)		+= aspeed-otp.o
+obj-$(CONFIG_ASPEED_SBC)		+= aspeed-sbc.o
diff --git a/drivers/soc/aspeed/aspeed-bmc-dev.c b/drivers/soc/aspeed/aspeed-bmc-dev.c
new file mode 100644
index 000000000000..0a86d8e110e7
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-bmc-dev.c
@@ -0,0 +1,528 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+// Copyright (C) ASPEED Technology Inc.
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/of_reserved_mem.h>
+#include <linux/platform_device.h>
+
+#include <linux/wait.h>
+#include <linux/workqueue.h>
+
+#include <linux/regmap.h>
+#include <linux/interrupt.h>
+#include <linux/mfd/syscon.h>
+#include <linux/dma-mapping.h>
+#include <linux/miscdevice.h>
+
+#define DEVICE_NAME     "bmc-device"
+#define SCU_TRIGGER_MSI
+
+struct aspeed_bmc_device {
+	unsigned char *host2bmc_base_virt;
+	struct device *dev;
+	struct miscdevice	miscdev;
+	void __iomem	*reg_base;
+	void __iomem	*bmc_mem_virt;
+	dma_addr_t bmc_mem_phy;
+	struct bin_attribute	bin0;
+	struct bin_attribute	bin1;
+
+	/* Queue waiters for idle engine */
+	wait_queue_head_t tx_wait0;
+	wait_queue_head_t tx_wait1;
+	wait_queue_head_t rx_wait0;
+	wait_queue_head_t rx_wait1;
+
+	struct regmap		*scu;
+
+//	phys_addr_t		mem_base;
+//	resource_size_t		mem_size;
+
+	struct kernfs_node	*kn0;
+	struct kernfs_node	*kn1;
+
+	int pcie2lpc;
+	int irq;
+	int pcie_irq;
+};
+
+#define BMC_MEM_BAR_SIZE		0x100000
+/* =================== SCU Define ================================================ */
+#define ASPEED_SCU04				0x04
+#define AST2600A3_SCU04	0x05030303
+#define ASPEED_SCUC20				0xC20
+#define ASPEED_SCUC24				0xC24
+#define MSI_ROUTING_MASK		GENMASK(11, 10)
+#define PCIDEV1_INTX_MSI_HOST2BMC_EN	BIT(18)
+#define MSI_ROUTING_PCIe2LPC_PCIDEV0	(0x1 << 10)
+#define MSI_ROUTING_PCIe2LPC_PCIDEV1	(0x2 << 10)
+/* ================================================================================== */
+#define ASPEED_BMC_MEM_BAR			0xF10
+#define  PCIE2PCI_MEM_BAR_ENABLE		BIT(1)
+#define  HOST2BMC_MEM_BAR_ENABLE		BIT(0)
+#define ASPEED_BMC_MEM_BAR_REMAP	0xF18
+
+#define ASPEED_BMC_SHADOW_CTRL		0xF50
+#define  READ_ONLY_MASK					BIT(31)
+#define  MASK_BAR1						BIT(2)
+#define  MASK_BAR0						BIT(1)
+#define  SHADOW_CFG						BIT(0)
+
+#define ASPEED_BMC_HOST2BMC_Q1		0xA000
+#define ASPEED_BMC_HOST2BMC_Q2		0xA010
+#define ASPEED_BMC_BMC2HOST_Q1		0xA020
+#define ASPEED_BMC_BMC2HOST_Q2		0xA030
+#define ASPEED_BMC_BMC2HOST_STS		0xA040
+#define	 BMC2HOST_INT_STS_DOORBELL		BIT(31)
+#define	 BMC2HOST_ENABLE_INTB			BIT(30)
+/* */
+#define	 BMC2HOST_Q1_FULL				BIT(27)
+#define	 BMC2HOST_Q1_EMPTY				BIT(26)
+#define	 BMC2HOST_Q2_FULL				BIT(25)
+#define	 BMC2HOST_Q2_EMPTY				BIT(24)
+#define	 BMC2HOST_Q1_FULL_UNMASK		BIT(23)
+#define	 BMC2HOST_Q1_EMPTY_UNMASK		BIT(22)
+#define	 BMC2HOST_Q2_FULL_UNMASK		BIT(21)
+#define	 BMC2HOST_Q2_EMPTY_UNMASK		BIT(20)
+
+#define ASPEED_BMC_HOST2BMC_STS		0xA044
+#define	 HOST2BMC_INT_STS_DOORBELL		BIT(31)
+#define	 HOST2BMC_ENABLE_INTB			BIT(30)
+#define	 HOST2BMC_Q1_FULL				BIT(27)
+#define	 HOST2BMC_Q1_EMPTY				BIT(26)
+#define	 HOST2BMC_Q2_FULL				BIT(25)
+#define	 HOST2BMC_Q2_EMPTY				BIT(24)
+#define	 HOST2BMC_Q1_FULL_UNMASK		BIT(23)
+#define	 HOST2BMC_Q1_EMPTY_UNMASK		BIT(22)
+#define	 HOST2BMC_Q2_FULL_UNMASK		BIT(21)
+#define	 HOST2BMC_Q2_EMPTY_UNMASK		BIT(20)
+
+#define ASPEED_SCU_PCIE_CONF_CTRL	0xC20
+#define  SCU_PCIE_CONF_BMC_DEV_EN			 BIT(8)
+#define  SCU_PCIE_CONF_BMC_DEV_EN_MMIO		 BIT(9)
+#define  SCU_PCIE_CONF_BMC_DEV_EN_MSI		 BIT(11)
+#define  SCU_PCIE_CONF_BMC_DEV_EN_IRQ		 BIT(13)
+#define  SCU_PCIE_CONF_BMC_DEV_EN_DMA		 BIT(14)
+#define  SCU_PCIE_CONF_BMC_DEV_EN_E2L		 BIT(15)
+#define  SCU_PCIE_CONF_BMC_DEV_EN_LPC_DECODE BIT(21)
+
+#define ASPEED_SCU_BMC_DEV_CLASS	0xC68
+
+static struct aspeed_bmc_device *file_aspeed_bmc_device(struct file *file)
+{
+	return container_of(file->private_data, struct aspeed_bmc_device,
+			miscdev);
+}
+
+static int aspeed_bmc_device_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	struct aspeed_bmc_device *bmc_device = file_aspeed_bmc_device(file);
+	unsigned long vsize = vma->vm_end - vma->vm_start;
+	pgprot_t prot = vma->vm_page_prot;
+
+	if (vma->vm_pgoff + vsize > bmc_device->bmc_mem_phy + 0x100000)
+		return -EINVAL;
+
+	prot = pgprot_noncached(prot);
+
+	if (remap_pfn_range(vma, vma->vm_start,
+		(bmc_device->bmc_mem_phy >> PAGE_SHIFT) + vma->vm_pgoff,
+		vsize, prot))
+		return -EAGAIN;
+
+	return 0;
+}
+
+static const struct file_operations aspeed_bmc_device_fops = {
+	.owner		= THIS_MODULE,
+	.mmap		= aspeed_bmc_device_mmap,
+};
+
+static ssize_t aspeed_host2bmc_queue1_rx(struct file *filp, struct kobject *kobj,
+		struct bin_attribute *attr, char *buf, loff_t off, size_t count)
+{
+	struct aspeed_bmc_device *bmc_device = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	u32 *data = (u32 *) buf;
+	u32 scu_id;
+	int ret;
+
+	ret = wait_event_interruptible(bmc_device->rx_wait0,
+		!(readl(bmc_device->reg_base + ASPEED_BMC_HOST2BMC_STS) & HOST2BMC_Q1_EMPTY));
+	if (ret)
+		return -EINTR;
+
+	data[0] = readl(bmc_device->reg_base + ASPEED_BMC_HOST2BMC_Q1);
+	regmap_read(bmc_device->scu, ASPEED_SCU04, &scu_id);
+	if (scu_id == AST2600A3_SCU04) {
+		writel(BMC2HOST_INT_STS_DOORBELL | BMC2HOST_ENABLE_INTB, bmc_device->reg_base + ASPEED_BMC_BMC2HOST_STS);
+	} else {
+		//A0 : BIT(12) A1 : BIT(15)
+		regmap_update_bits(bmc_device->scu, 0x560, BIT(15), BIT(15));
+		regmap_update_bits(bmc_device->scu, 0x560, BIT(15), 0);
+	}
+
+	return sizeof(u32);
+}
+
+static ssize_t aspeed_host2bmc_queue2_rx(struct file *filp, struct kobject *kobj,
+		struct bin_attribute *attr, char *buf, loff_t off, size_t count)
+{
+	struct aspeed_bmc_device *bmc_device = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	u32 *data = (u32 *) buf;
+	u32 scu_id;
+	int ret;
+
+	ret = wait_event_interruptible(bmc_device->rx_wait1,
+		!(readl(bmc_device->reg_base + ASPEED_BMC_HOST2BMC_STS) & HOST2BMC_Q2_EMPTY));
+	if (ret)
+		return -EINTR;
+
+	data[0] = readl(bmc_device->reg_base + ASPEED_BMC_HOST2BMC_Q2);
+	regmap_read(bmc_device->scu, ASPEED_SCU04, &scu_id);
+	if (scu_id == AST2600A3_SCU04) {
+		writel(BMC2HOST_INT_STS_DOORBELL | BMC2HOST_ENABLE_INTB, bmc_device->reg_base + ASPEED_BMC_BMC2HOST_STS);
+	} else {
+		//A0 : BIT(12) A1 : BIT(15)
+		regmap_update_bits(bmc_device->scu, 0x560, BIT(15), BIT(15));
+		regmap_update_bits(bmc_device->scu, 0x560, BIT(15), 0);
+	}
+
+	return sizeof(u32);
+}
+
+static ssize_t aspeed_bmc2host_queue1_tx(struct file *filp, struct kobject *kobj,
+		struct bin_attribute *attr, char *buf, loff_t off, size_t count)
+{
+	struct aspeed_bmc_device *bmc_device = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	u32 tx_buff;
+	u32 scu_id;
+	int ret;
+
+	if (count != sizeof(u32))
+		return -EINVAL;
+
+	ret = wait_event_interruptible(bmc_device->tx_wait0,
+		!(readl(bmc_device->reg_base + ASPEED_BMC_BMC2HOST_STS) & BMC2HOST_Q1_FULL));
+	if (ret)
+		return -EINTR;
+
+
+//	if (copy_from_user((void *)&tx_buff, buf, sizeof(u32)))
+//		return -EFAULT;
+	memcpy(&tx_buff, buf, 4);
+	writel(tx_buff, bmc_device->reg_base + ASPEED_BMC_BMC2HOST_Q1);
+	/* trigger to host
+	 * Only After AST2600A3 support DoorBell MSI
+	 */
+	regmap_read(bmc_device->scu, ASPEED_SCU04, &scu_id);
+	if (scu_id == AST2600A3_SCU04) {
+		writel(BMC2HOST_INT_STS_DOORBELL | BMC2HOST_ENABLE_INTB, bmc_device->reg_base + ASPEED_BMC_BMC2HOST_STS);
+	} else {
+		//A0 : BIT(12) A1 : BIT(15)
+		regmap_update_bits(bmc_device->scu, 0x560, BIT(15), BIT(15));
+		regmap_update_bits(bmc_device->scu, 0x560, BIT(15), 0);
+	}
+
+	return sizeof(u32);
+}
+
+static ssize_t aspeed_bmc2host_queue2_tx(struct file *filp, struct kobject *kobj,
+		struct bin_attribute *attr, char *buf, loff_t off, size_t count)
+{
+	struct aspeed_bmc_device *bmc_device = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	u32 tx_buff = 0;
+	u32 scu_id;
+	int ret;
+
+	if (count != sizeof(u32))
+		return -EINVAL;
+
+	ret = wait_event_interruptible(bmc_device->tx_wait0,
+		!(readl(bmc_device->reg_base + ASPEED_BMC_BMC2HOST_STS) & BMC2HOST_Q2_FULL));
+	if (ret)
+		return -EINTR;
+
+
+//	if (copy_from_user((void *)&tx_buff, buf, sizeof(u32)))
+//		return -EFAULT;
+	memcpy(&tx_buff, buf, 4);
+	writel(tx_buff, bmc_device->reg_base + ASPEED_BMC_BMC2HOST_Q2);
+	/* trigger to host
+	 * Only After AST2600A3 support DoorBell MSI
+	 */
+	regmap_read(bmc_device->scu, ASPEED_SCU04, &scu_id);
+	if (scu_id == AST2600A3_SCU04) {
+		writel(BMC2HOST_INT_STS_DOORBELL | BMC2HOST_ENABLE_INTB, bmc_device->reg_base + ASPEED_BMC_BMC2HOST_STS);
+	} else {
+		//A0 : BIT(12) A1 : BIT(15)
+		regmap_update_bits(bmc_device->scu, 0x560, BIT(15), BIT(15));
+		regmap_update_bits(bmc_device->scu, 0x560, BIT(15), 0);
+	}
+
+	return sizeof(u32);
+}
+
+static irqreturn_t aspeed_bmc_dev_pcie_isr(int irq, void *dev_id)
+{
+	struct aspeed_bmc_device *bmc_device = dev_id;
+
+	while (!(readl(bmc_device->reg_base + ASPEED_BMC_HOST2BMC_STS) & HOST2BMC_Q1_EMPTY))
+		readl(bmc_device->reg_base + ASPEED_BMC_HOST2BMC_Q1);
+
+	while (!(readl(bmc_device->reg_base + ASPEED_BMC_HOST2BMC_STS) & HOST2BMC_Q2_EMPTY))
+		readl(bmc_device->reg_base + ASPEED_BMC_HOST2BMC_Q2);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t aspeed_bmc_dev_isr(int irq, void *dev_id)
+{
+	struct aspeed_bmc_device *bmc_device = dev_id;
+
+	u32 host2bmc_q_sts = readl(bmc_device->reg_base + ASPEED_BMC_HOST2BMC_STS);
+
+	if (host2bmc_q_sts & HOST2BMC_INT_STS_DOORBELL)
+		writel(HOST2BMC_INT_STS_DOORBELL, bmc_device->reg_base + ASPEED_BMC_HOST2BMC_STS);
+
+	if (host2bmc_q_sts & HOST2BMC_ENABLE_INTB)
+		writel(HOST2BMC_ENABLE_INTB, bmc_device->reg_base + ASPEED_BMC_HOST2BMC_STS);
+
+	if (host2bmc_q_sts & HOST2BMC_Q1_FULL)
+		dev_info(bmc_device->dev, "Q1 Full\n");
+
+	if (host2bmc_q_sts & HOST2BMC_Q2_FULL)
+		dev_info(bmc_device->dev, "Q2 Full\n");
+
+
+	if (!(readl(bmc_device->reg_base + ASPEED_BMC_BMC2HOST_STS) & BMC2HOST_Q1_FULL))
+		wake_up_interruptible(&bmc_device->tx_wait0);
+
+	if (!(readl(bmc_device->reg_base + ASPEED_BMC_HOST2BMC_STS) & HOST2BMC_Q1_EMPTY))
+		wake_up_interruptible(&bmc_device->rx_wait0);
+
+	if (!(readl(bmc_device->reg_base + ASPEED_BMC_BMC2HOST_STS) & BMC2HOST_Q2_FULL))
+		wake_up_interruptible(&bmc_device->tx_wait1);
+
+	if (!(readl(bmc_device->reg_base + ASPEED_BMC_HOST2BMC_STS) & HOST2BMC_Q2_EMPTY))
+		wake_up_interruptible(&bmc_device->rx_wait1);
+
+	return IRQ_HANDLED;
+}
+
+static void aspeed_bmc_device_init(struct aspeed_bmc_device *bmc_device)
+{
+	u32 pcie_config_ctl = SCU_PCIE_CONF_BMC_DEV_EN_IRQ | SCU_PCIE_CONF_BMC_DEV_EN_MMIO | SCU_PCIE_CONF_BMC_DEV_EN;
+	u32 scu_id;
+
+	if (bmc_device->pcie2lpc)
+		pcie_config_ctl |= SCU_PCIE_CONF_BMC_DEV_EN_E2L | SCU_PCIE_CONF_BMC_DEV_EN_LPC_DECODE;
+
+	regmap_update_bits(bmc_device->scu, ASPEED_SCU_PCIE_CONF_CTRL, pcie_config_ctl,
+			pcie_config_ctl);
+
+	/* update class code to others as it is a MFD device */
+	regmap_write(bmc_device->scu, ASPEED_SCU_BMC_DEV_CLASS, 0xff000000);
+
+#ifdef SCU_TRIGGER_MSI
+	//SCUC24[17]: Enable PCI device 1 INTx/MSI from SCU560[15]. Will be added in next version
+	regmap_update_bits(bmc_device->scu, ASPEED_SCUC20, BIT(11) | BIT(14), BIT(11) | BIT(14));
+
+	regmap_read(bmc_device->scu, ASPEED_SCU04, &scu_id);
+	if (scu_id == AST2600A3_SCU04)
+		regmap_update_bits(bmc_device->scu, ASPEED_SCUC24,
+				PCIDEV1_INTX_MSI_HOST2BMC_EN | MSI_ROUTING_MASK,
+				PCIDEV1_INTX_MSI_HOST2BMC_EN | MSI_ROUTING_PCIe2LPC_PCIDEV1);
+	else
+		regmap_update_bits(bmc_device->scu, ASPEED_SCUC24, BIT(17) | BIT(14) | BIT(11), BIT(17) | BIT(14) | BIT(11));
+#else
+	//SCUC24[18]: Enable PCI device 1 INTx/MSI from Host-to-BMC controller. Will be added in next version
+	regmap_update_bits(bmc_device->scu, 0xc24, BIT(18) | BIT(14), BIT(18) | BIT(14));
+#endif
+
+	writel(~(BMC_MEM_BAR_SIZE - 1) | HOST2BMC_MEM_BAR_ENABLE, bmc_device->reg_base + ASPEED_BMC_MEM_BAR);
+	writel(bmc_device->bmc_mem_phy, bmc_device->reg_base + ASPEED_BMC_MEM_BAR_REMAP);
+
+	//Setting BMC to Host Q register
+	writel(BMC2HOST_Q2_FULL_UNMASK | BMC2HOST_Q1_FULL_UNMASK | BMC2HOST_ENABLE_INTB, bmc_device->reg_base + ASPEED_BMC_BMC2HOST_STS);
+	writel(HOST2BMC_Q2_FULL_UNMASK | HOST2BMC_Q1_FULL_UNMASK | HOST2BMC_ENABLE_INTB, bmc_device->reg_base + ASPEED_BMC_HOST2BMC_STS);
+}
+
+static const struct of_device_id aspeed_bmc_device_of_matches[] = {
+	{ .compatible = "aspeed,ast2600-bmc-device", },
+	{},
+};
+MODULE_DEVICE_TABLE(of, aspeed_bmc_device_of_matches);
+
+static int aspeed_bmc_device_probe(struct platform_device *pdev)
+{
+	struct aspeed_bmc_device *bmc_device;
+	struct device *dev = &pdev->dev;
+	int ret = 0;
+
+	bmc_device = devm_kzalloc(&pdev->dev, sizeof(struct aspeed_bmc_device), GFP_KERNEL);
+	if (!bmc_device)
+		return -ENOMEM;
+
+	init_waitqueue_head(&bmc_device->tx_wait0);
+	init_waitqueue_head(&bmc_device->tx_wait1);
+	init_waitqueue_head(&bmc_device->rx_wait0);
+	init_waitqueue_head(&bmc_device->rx_wait1);
+
+	bmc_device->dev = dev;
+	bmc_device->reg_base = devm_platform_ioremap_resource(pdev, 0);
+	if (IS_ERR(bmc_device->reg_base))
+		goto out_region;
+
+	bmc_device->scu = syscon_regmap_lookup_by_phandle(dev->of_node, "aspeed,scu");
+	if (IS_ERR(bmc_device->scu)) {
+		dev_err(&pdev->dev, "failed to find SCU regmap\n");
+		goto out_region;
+	}
+
+	if (of_property_read_bool(dev->of_node, "pcie2lpc"))
+		bmc_device->pcie2lpc = 1;
+
+	if (of_reserved_mem_device_init(dev))
+		dev_err(dev, "can't get reserved memory\n");
+
+	dma_set_mask_and_coherent(dev, DMA_BIT_MASK(32));
+
+	bmc_device->bmc_mem_virt = dma_alloc_coherent(&pdev->dev, BMC_MEM_BAR_SIZE, &bmc_device->bmc_mem_phy, GFP_KERNEL);
+	memset(bmc_device->bmc_mem_virt, 0, BMC_MEM_BAR_SIZE);
+
+	sysfs_bin_attr_init(&bmc_device->bin0);
+	sysfs_bin_attr_init(&bmc_device->bin1);
+
+	bmc_device->bin0.attr.name = "bmc-dev-queue1";
+	bmc_device->bin0.attr.mode = 0600;
+	bmc_device->bin0.read = aspeed_host2bmc_queue1_rx;
+	bmc_device->bin0.write = aspeed_bmc2host_queue1_tx;
+	bmc_device->bin0.size = 4;
+
+	ret = sysfs_create_bin_file(&pdev->dev.kobj, &bmc_device->bin0);
+	if (ret) {
+		dev_err(dev, "error for bin file\n");
+		goto out_dma;
+	}
+
+	bmc_device->kn0 = kernfs_find_and_get(dev->kobj.sd, bmc_device->bin0.attr.name);
+	if (!bmc_device->kn0) {
+		sysfs_remove_bin_file(&dev->kobj, &bmc_device->bin0);
+		goto out_dma;
+	}
+
+	bmc_device->bin1.attr.name = "bmc-dev-queue2";
+	bmc_device->bin1.attr.mode = 0600;
+	bmc_device->bin1.read = aspeed_host2bmc_queue2_rx;
+	bmc_device->bin1.write = aspeed_bmc2host_queue2_tx;
+	bmc_device->bin1.size = 4;
+
+	ret = sysfs_create_bin_file(&pdev->dev.kobj, &bmc_device->bin1);
+	if (ret) {
+		dev_err(dev, "error for bin file ");
+		goto out_dma;
+	}
+
+	bmc_device->kn1 = kernfs_find_and_get(dev->kobj.sd, bmc_device->bin1.attr.name);
+	if (!bmc_device->kn1) {
+		sysfs_remove_bin_file(&dev->kobj, &bmc_device->bin1);
+		goto out_dma;
+	}
+
+	dev_set_drvdata(dev, bmc_device);
+
+	aspeed_bmc_device_init(bmc_device);
+
+	bmc_device->irq =  platform_get_irq(pdev, 0);
+	if (bmc_device->irq < 0) {
+		dev_err(&pdev->dev, "platform get of irq[=%d] failed!\n", bmc_device->irq);
+		goto out_unmap;
+	}
+
+	ret = devm_request_irq(&pdev->dev, bmc_device->irq, aspeed_bmc_dev_isr,
+							0, dev_name(&pdev->dev), bmc_device);
+	if (ret) {
+		dev_err(dev, "aspeed bmc device Unable to get IRQ");
+		goto out_unmap;
+	}
+
+	bmc_device->pcie_irq =  platform_get_irq(pdev, 1);
+	if (bmc_device->pcie_irq < 0) {
+		dev_warn(&pdev->dev, "platform get of pcie irq[=%d] failed!\n", bmc_device->pcie_irq);
+	} else {
+		ret = devm_request_irq(&pdev->dev, bmc_device->pcie_irq, aspeed_bmc_dev_pcie_isr,
+								IRQF_SHARED, dev_name(&pdev->dev), bmc_device);
+		if (ret < 0) {
+			dev_warn(dev, "Failed to request PCI-E IRQ %d.\n", ret);
+			bmc_device->pcie_irq = -1;
+		}
+	}
+	bmc_device->miscdev.minor = MISC_DYNAMIC_MINOR;
+	bmc_device->miscdev.name = DEVICE_NAME;
+	bmc_device->miscdev.fops = &aspeed_bmc_device_fops;
+	bmc_device->miscdev.parent = dev;
+	ret = misc_register(&bmc_device->miscdev);
+	if (ret) {
+		dev_err(dev, "Unable to register device\n");
+		goto out_irq;
+	}
+
+	dev_info(dev, "aspeed bmc device: driver successfully loaded.\n");
+
+	return 0;
+
+out_irq:
+	devm_free_irq(&pdev->dev, bmc_device->irq, bmc_device);
+
+out_unmap:
+	iounmap(bmc_device->reg_base);
+
+out_dma:
+	dma_free_coherent(&pdev->dev, BMC_MEM_BAR_SIZE, bmc_device->bmc_mem_virt, bmc_device->bmc_mem_phy);
+
+out_region:
+	devm_kfree(&pdev->dev, bmc_device);
+	dev_warn(dev, "aspeed bmc device: driver init failed (ret=%d)!\n", ret);
+	return ret;
+}
+
+static int  aspeed_bmc_device_remove(struct platform_device *pdev)
+{
+	struct aspeed_bmc_device *bmc_device = platform_get_drvdata(pdev);
+
+	misc_deregister(&bmc_device->miscdev);
+
+	devm_free_irq(&pdev->dev, bmc_device->irq, bmc_device);
+
+	iounmap(bmc_device->reg_base);
+
+	dma_free_coherent(&pdev->dev, BMC_MEM_BAR_SIZE, bmc_device->bmc_mem_virt, bmc_device->bmc_mem_phy);
+
+	devm_kfree(&pdev->dev, bmc_device);
+
+	return 0;
+}
+
+
+static struct platform_driver aspeed_bmc_device_driver = {
+	.probe		= aspeed_bmc_device_probe,
+	.remove		= aspeed_bmc_device_remove,
+	.driver		= {
+		.name	= KBUILD_MODNAME,
+		.of_match_table = aspeed_bmc_device_of_matches,
+	},
+};
+
+module_platform_driver(aspeed_bmc_device_driver);
+
+MODULE_AUTHOR("Ryan Chen <ryan_chen@aspeedtech.com>");
+MODULE_DESCRIPTION("ASPEED BMC DEVICE Driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/soc/aspeed/aspeed-espi-comm.h b/drivers/soc/aspeed/aspeed-espi-comm.h
new file mode 100644
index 000000000000..b50393c80dc6
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-espi-comm.h
@@ -0,0 +1,196 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright 2023 Aspeed Technology Inc.
+ */
+#ifndef __ASPEED_ESPI_COMM_H__
+#define __ASPEED_ESPI_COMM_H__
+
+#include <linux/ioctl.h>
+#include <linux/types.h>
+
+/*
+ * eSPI cycle type encoding
+ *
+ * Section 5.1 Cycle Types and Packet Format,
+ * Intel eSPI Interface Base Specification, Rev 1.0, Jan. 2016.
+ */
+#define ESPI_PERIF_MEMRD32		0x00
+#define ESPI_PERIF_MEMRD64		0x02
+#define ESPI_PERIF_MEMWR32		0x01
+#define ESPI_PERIF_MEMWR64		0x03
+#define ESPI_PERIF_MSG			0x10
+#define ESPI_PERIF_MSG_D		0x11
+#define ESPI_PERIF_SUC_CMPLT		0x06
+#define ESPI_PERIF_SUC_CMPLT_D_MIDDLE	0x09
+#define ESPI_PERIF_SUC_CMPLT_D_FIRST	0x0b
+#define ESPI_PERIF_SUC_CMPLT_D_LAST	0x0d
+#define ESPI_PERIF_SUC_CMPLT_D_ONLY	0x0f
+#define ESPI_PERIF_UNSUC_CMPLT		0x0c
+#define ESPI_OOB_MSG			0x21
+#define ESPI_FLASH_READ			0x00
+#define ESPI_FLASH_WRITE		0x01
+#define ESPI_FLASH_ERASE		0x02
+#define ESPI_FLASH_SUC_CMPLT		0x06
+#define ESPI_FLASH_SUC_CMPLT_D_MIDDLE	0x09
+#define ESPI_FLASH_SUC_CMPLT_D_FIRST	0x0b
+#define ESPI_FLASH_SUC_CMPLT_D_LAST	0x0d
+#define ESPI_FLASH_SUC_CMPLT_D_ONLY	0x0f
+#define ESPI_FLASH_UNSUC_CMPLT		0x0c
+
+/*
+ * eSPI packet format structure
+ *
+ * Section 5.1 Cycle Types and Packet Format,
+ * Intel eSPI Interface Base Specification, Rev 1.0, Jan. 2016.
+ */
+struct espi_comm_hdr {
+	uint8_t cyc;
+	uint8_t len_h : 4;
+	uint8_t tag : 4;
+	uint8_t len_l;
+};
+
+struct espi_perif_mem32 {
+	uint8_t cyc;
+	uint8_t len_h : 4;
+	uint8_t tag : 4;
+	uint8_t len_l;
+	uint32_t addr_be;
+	uint8_t data[];
+} __packed;
+
+struct espi_perif_mem64 {
+	uint8_t cyc;
+	uint8_t len_h : 4;
+	uint8_t tag : 4;
+	uint8_t len_l;
+	uint32_t addr_be;
+	uint8_t data[];
+} __packed;
+
+struct espi_perif_msg {
+	uint8_t cyc;
+	uint8_t len_h : 4;
+	uint8_t tag : 4;
+	uint8_t len_l;
+	uint8_t msg_code;
+	uint8_t msg_byte[4];
+	uint8_t data[];
+} __packed;
+
+struct espi_perif_cmplt {
+	uint8_t cyc;
+	uint8_t len_h : 4;
+	uint8_t tag : 4;
+	uint8_t len_l;
+	uint8_t data[];
+} __packed;
+
+struct espi_oob_msg {
+	uint8_t cyc;
+	uint8_t len_h : 4;
+	uint8_t tag : 4;
+	uint8_t len_l;
+	uint8_t data[];
+};
+
+struct espi_flash_rwe {
+	uint8_t cyc;
+	uint8_t len_h : 4;
+	uint8_t tag : 4;
+	uint8_t len_l;
+	uint32_t addr_be;
+	uint8_t data[];
+} __packed;
+
+struct espi_flash_cmplt {
+	uint8_t cyc;
+	uint8_t len_h : 4;
+	uint8_t tag : 4;
+	uint8_t len_l;
+	uint8_t data[];
+} __packed;
+
+#define ESPI_MAX_PLD_LEN	BIT(12)
+
+/*
+ * Aspeed IOCTL for eSPI raw packet send/receive
+ *
+ * This IOCTL interface works in the eSPI packet in/out paradigm.
+ *
+ * Only the virtual wire IOCTL is a special case which does not send
+ * or receive an eSPI packet. However, to keep a more consisten use from
+ * userspace, we make all of the four channel drivers serve through the
+ * IOCTL interface.
+ *
+ * For the eSPI packet format, refer to
+ *   Section 5.1 Cycle Types and Packet Format,
+ *   Intel eSPI Interface Base Specification, Rev 1.0, Jan. 2016.
+ *
+ * For the example user apps using these IOCTL, refer to
+ *   https://github.com/AspeedTech-BMC/aspeed_app/tree/master/espi_test
+ */
+#define __ASPEED_ESPI_IOCTL_MAGIC	0xb8
+
+/*
+ * we choose the longest header and the max payload size
+ * based on the Intel specification to define the maximum
+ * eSPI packet length
+ */
+#define ESPI_MAX_PKT_LEN	(sizeof(struct espi_perif_msg) + ESPI_MAX_PLD_LEN)
+
+struct aspeed_espi_ioc {
+	uint32_t pkt_len;
+	uint8_t *pkt;
+};
+
+/*
+ * Peripheral Channel (CH0)
+ *  - ASPEED_ESPI_PERIF_PC_GET_RX
+ *      Receive an eSPI Posted/Completion packet
+ *  - ASPEED_ESPI_PERIF_PC_PUT_TX
+ *      Transmit an eSPI Posted/Completion packet
+ *  - ASPEED_ESPI_PERIF_NP_PUT_TX
+ *      Transmit an eSPI Non-Posted packet
+ */
+#define ASPEED_ESPI_PERIF_PC_GET_RX	_IOR(__ASPEED_ESPI_IOCTL_MAGIC, \
+					     0x00, struct aspeed_espi_ioc)
+#define ASPEED_ESPI_PERIF_PC_PUT_TX	_IOW(__ASPEED_ESPI_IOCTL_MAGIC, \
+					     0x01, struct aspeed_espi_ioc)
+#define ASPEED_ESPI_PERIF_NP_PUT_TX	_IOW(__ASPEED_ESPI_IOCTL_MAGIC, \
+					     0x02, struct aspeed_espi_ioc)
+/*
+ * Virtual Wire Channel (CH1)
+ *  - ASPEED_ESPI_VW_GET_GPIO_VAL
+ *      Read the input value of GPIO over the VW channel
+ *  - ASPEED_ESPI_VW_PUT_GPIO_VAL
+ *      Write the output value of GPIO over the VW channel
+ */
+#define ASPEED_ESPI_VW_GET_GPIO_VAL	_IOR(__ASPEED_ESPI_IOCTL_MAGIC, \
+					     0x10, uint8_t)
+#define ASPEED_ESPI_VW_PUT_GPIO_VAL	_IOW(__ASPEED_ESPI_IOCTL_MAGIC, \
+					     0x11, uint8_t)
+/*
+ * Out-of-band Channel (CH2)
+ *  - ASPEED_ESPI_OOB_GET_RX
+ *      Receive an eSPI OOB packet
+ *  - ASPEED_ESPI_OOB_PUT_TX
+ *      Transmit an eSPI OOB packet
+ */
+#define ASPEED_ESPI_OOB_GET_RX		_IOR(__ASPEED_ESPI_IOCTL_MAGIC, \
+					     0x20, struct aspeed_espi_ioc)
+#define ASPEED_ESPI_OOB_PUT_TX		_IOW(__ASPEED_ESPI_IOCTL_MAGIC, \
+					     0x21, struct aspeed_espi_ioc)
+/*
+ * Flash Channel (CH3)
+ *  - ASPEED_ESPI_FLASH_GET_RX
+ *      Receive an eSPI flash packet
+ *  - ASPEED_ESPI_FLASH_PUT_TX
+ *      Transmit an eSPI flash packet
+ */
+#define ASPEED_ESPI_FLASH_GET_RX	_IOR(__ASPEED_ESPI_IOCTL_MAGIC, \
+					     0x30, struct aspeed_espi_ioc)
+#define ASPEED_ESPI_FLASH_PUT_TX	_IOW(__ASPEED_ESPI_IOCTL_MAGIC, \
+					     0x31, struct aspeed_espi_ioc)
+
+#endif
diff --git a/drivers/soc/aspeed/aspeed-host-bmc-dev.c b/drivers/soc/aspeed/aspeed-host-bmc-dev.c
new file mode 100644
index 000000000000..414a26a4edd9
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-host-bmc-dev.c
@@ -0,0 +1,519 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+// Copyright (C) ASPEED Technology Inc.
+
+#include <linux/init.h>
+#include <linux/version.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+
+#include <linux/pci.h>
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/interrupt.h>
+#include <linux/wait.h>
+#include <linux/workqueue.h>
+
+
+#include <linux/miscdevice.h>
+#include <linux/module.h>
+#include <linux/serial_core.h>
+#include <linux/serial_8250.h>
+
+#define ASPEED_PCI_BMC_HOST2BMC_Q1		0x30000
+#define ASPEED_PCI_BMC_HOST2BMC_Q2		0x30010
+#define ASPEED_PCI_BMC_BMC2HOST_Q1		0x30020
+#define ASPEED_PCI_BMC_BMC2HOST_Q2		0x30030
+#define ASPEED_PCI_BMC_BMC2HOST_STS		0x30040
+#define	 BMC2HOST_INT_STS_DOORBELL		BIT(31)
+#define	 BMC2HOST_ENABLE_INTB			BIT(30)
+/* */
+#define	 BMC2HOST_Q1_FULL				BIT(27)
+#define	 BMC2HOST_Q1_EMPTY				BIT(26)
+#define	 BMC2HOST_Q2_FULL				BIT(25)
+#define	 BMC2HOST_Q2_EMPTY				BIT(24)
+#define	 BMC2HOST_Q1_FULL_UNMASK		BIT(23)
+#define	 BMC2HOST_Q1_EMPTY_UNMASK		BIT(22)
+#define	 BMC2HOST_Q2_FULL_UNMASK		BIT(21)
+#define	 BMC2HOST_Q2_EMPTY_UNMASK		BIT(20)
+
+#define ASPEED_PCI_BMC_HOST2BMC_STS		0x30044
+#define	 HOST2BMC_INT_STS_DOORBELL		BIT(31)
+#define	 HOST2BMC_ENABLE_INTB			BIT(30)
+/* */
+#define	 HOST2BMC_Q1_FULL				BIT(27)
+#define	 HOST2BMC_Q1_EMPTY				BIT(26)
+#define	 HOST2BMC_Q2_FULL				BIT(25)
+#define	 HOST2BMC_Q2_EMPTY				BIT(24)
+#define	 HOST2BMC_Q1_FULL_UNMASK		BIT(23)
+#define	 HOST2BMC_Q1_EMPTY_UNMASK		BIT(22)
+#define	 HOST2BMC_Q2_FULL_UNMASK		BIT(21)
+#define	 HOST2BMC_Q2_EMPTY_UNMASK		BIT(20)
+
+struct aspeed_pci_bmc_dev {
+	struct device *dev;
+	struct miscdevice miscdev;
+
+	unsigned long mem_bar_base;
+	unsigned long mem_bar_size;
+	void __iomem *mem_bar_reg;
+
+	unsigned long message_bar_base;
+	unsigned long message_bar_size;
+	void __iomem *msg_bar_reg;
+
+	struct bin_attribute	bin0;
+	struct bin_attribute	bin1;
+
+	struct kernfs_node	*kn0;
+	struct kernfs_node	*kn1;
+
+	/* Queue waiters for idle engine */
+	wait_queue_head_t tx_wait0;
+	wait_queue_head_t tx_wait1;
+	wait_queue_head_t rx_wait0;
+	wait_queue_head_t rx_wait1;
+
+	void __iomem *sio_mbox_reg;
+	int sio_mbox_irq;
+
+	u8 IntLine;
+	int legency_irq;
+};
+
+#define HOST_BMC_QUEUE_SIZE			(16 * 4)
+#define PCIE_DEVICE_SIO_ADDR		(0x2E * 4)
+#define BMC_MULTI_MSI	32
+
+#define DRIVER_NAME "ASPEED BMC DEVICE"
+
+#define VUART_MAX_PARMS		2
+static uint16_t vuart_ioport[VUART_MAX_PARMS];
+static uint16_t vuart_sirq[VUART_MAX_PARMS];
+
+static struct aspeed_pci_bmc_dev *file_aspeed_bmc_device(struct file *file)
+{
+	return container_of(file->private_data, struct aspeed_pci_bmc_dev,
+			miscdev);
+}
+
+static int aspeed_pci_bmc_dev_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	struct aspeed_pci_bmc_dev *pci_bmc_dev = file_aspeed_bmc_device(file);
+	unsigned long vsize = vma->vm_end - vma->vm_start;
+	pgprot_t prot = vma->vm_page_prot;
+
+	if (vma->vm_pgoff + vsize > pci_bmc_dev->mem_bar_base + 0x100000)
+		return -EINVAL;
+
+	prot = pgprot_noncached(prot);
+
+	if (remap_pfn_range(vma, vma->vm_start,
+		(pci_bmc_dev->mem_bar_base >> PAGE_SHIFT) + vma->vm_pgoff,
+		vsize, prot))
+		return -EAGAIN;
+
+	return 0;
+}
+
+static const struct file_operations aspeed_pci_bmc_dev_fops = {
+	.owner		= THIS_MODULE,
+	.mmap		= aspeed_pci_bmc_dev_mmap,
+};
+
+static ssize_t aspeed_pci_bmc_dev_queue1_rx(struct file *filp, struct kobject *kobj,
+		struct bin_attribute *attr, char *buf, loff_t off, size_t count)
+{
+	struct aspeed_pci_bmc_dev *pci_bmc_device = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	u32 *data = (u32 *) buf;
+	int ret;
+
+	ret = wait_event_interruptible(pci_bmc_device->rx_wait0,
+		!(readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_BMC2HOST_STS) & BMC2HOST_Q1_EMPTY));
+	if (ret)
+		return -EINTR;
+
+	data[0] = readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_BMC2HOST_Q1);
+	writel(HOST2BMC_INT_STS_DOORBELL | HOST2BMC_ENABLE_INTB, pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_HOST2BMC_STS);
+	return sizeof(u32);
+}
+
+static ssize_t aspeed_pci_bmc_dev_queue2_rx(struct file *filp, struct kobject *kobj,
+		struct bin_attribute *attr, char *buf, loff_t off, size_t count)
+{
+	struct aspeed_pci_bmc_dev *pci_bmc_device = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	u32 *data = (u32 *) buf;
+	int ret;
+
+	ret = wait_event_interruptible(pci_bmc_device->rx_wait1,
+		!(readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_BMC2HOST_STS) & BMC2HOST_Q2_EMPTY));
+	if (ret)
+		return -EINTR;
+
+	data[0] = readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_BMC2HOST_Q2);
+	writel(HOST2BMC_INT_STS_DOORBELL | HOST2BMC_ENABLE_INTB, pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_HOST2BMC_STS);
+	return sizeof(u32);
+}
+
+static ssize_t aspeed_pci_bmc_dev_queue1_tx(struct file *filp, struct kobject *kobj,
+		struct bin_attribute *attr, char *buf, loff_t off, size_t count)
+{
+	struct aspeed_pci_bmc_dev *pci_bmc_device = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	u32 tx_buff;
+	int ret;
+
+	if (count != sizeof(u32))
+		return -EINVAL;
+
+	ret = wait_event_interruptible(pci_bmc_device->tx_wait0,
+		!(readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_HOST2BMC_STS) & HOST2BMC_Q1_FULL));
+	if (ret)
+		return -EINTR;
+
+	memcpy(&tx_buff, buf, 4);
+	writel(tx_buff, pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_HOST2BMC_Q1);
+	//trigger to host
+	writel(HOST2BMC_INT_STS_DOORBELL | HOST2BMC_ENABLE_INTB, pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_HOST2BMC_STS);
+
+	return sizeof(u32);
+}
+
+static ssize_t aspeed_pci_bmc_dev_queue2_tx(struct file *filp, struct kobject *kobj,
+		struct bin_attribute *attr, char *buf, loff_t off, size_t count)
+{
+	struct aspeed_pci_bmc_dev *pci_bmc_device = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	u32 tx_buff = 0;
+	int ret;
+
+	if (count != sizeof(u32))
+		return -EINVAL;
+
+	ret = wait_event_interruptible(pci_bmc_device->tx_wait0,
+		!(readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_HOST2BMC_STS) & HOST2BMC_Q2_FULL));
+	if (ret)
+		return -EINTR;
+
+	memcpy(&tx_buff, buf, 4);
+	writel(tx_buff, pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_HOST2BMC_Q2);
+	//trigger to host
+	writel(HOST2BMC_INT_STS_DOORBELL | HOST2BMC_ENABLE_INTB, pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_HOST2BMC_STS);
+
+	return sizeof(u32);
+}
+
+irqreturn_t aspeed_pci_host_bmc_device_interrupt(int irq, void *dev_id)
+{
+	struct aspeed_pci_bmc_dev *pci_bmc_device = dev_id;
+	u32 bmc2host_q_sts = readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_BMC2HOST_STS);
+
+	if (bmc2host_q_sts & BMC2HOST_INT_STS_DOORBELL)
+		writel(BMC2HOST_INT_STS_DOORBELL, pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_BMC2HOST_STS);
+
+	if (bmc2host_q_sts & BMC2HOST_ENABLE_INTB)
+		writel(BMC2HOST_ENABLE_INTB, pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_BMC2HOST_STS);
+
+	if (bmc2host_q_sts & BMC2HOST_Q1_FULL)
+		dev_info(pci_bmc_device->dev, "Q1 Full\n");
+
+	if (bmc2host_q_sts & BMC2HOST_Q2_FULL)
+		dev_info(pci_bmc_device->dev, "Q2 Full\n");
+
+
+	//check q1
+	if (!(readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_HOST2BMC_STS) & HOST2BMC_Q1_FULL))
+		wake_up_interruptible(&pci_bmc_device->tx_wait0);
+
+	if (!(readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_BMC2HOST_STS) & BMC2HOST_Q1_EMPTY))
+		wake_up_interruptible(&pci_bmc_device->rx_wait0);
+	//chech q2
+	if (!(readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_HOST2BMC_STS) & HOST2BMC_Q2_FULL))
+		wake_up_interruptible(&pci_bmc_device->tx_wait1);
+
+	if (!(readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_BMC2HOST_STS) & BMC2HOST_Q2_EMPTY))
+		wake_up_interruptible(&pci_bmc_device->rx_wait1);
+
+	return IRQ_HANDLED;
+
+}
+
+irqreturn_t aspeed_pci_host_mbox_interrupt(int irq, void *dev_id)
+{
+	struct aspeed_pci_bmc_dev *pci_bmc_device = dev_id;
+	u32 isr = readl(pci_bmc_device->sio_mbox_reg + 0x94);
+
+	if (isr & BIT(7))
+		writel(BIT(7), pci_bmc_device->sio_mbox_reg + 0x94);
+
+	return IRQ_HANDLED;
+
+}
+
+#define BMC_MSI_IDX_BASE	4
+static int aspeed_pci_host_bmc_device_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+{
+	struct uart_8250_port uart[VUART_MAX_PARMS];
+	struct aspeed_pci_bmc_dev *pci_bmc_dev;
+	struct device *dev = &pdev->dev;
+	void __iomem *pcie_sio_decode_addr;
+	u16 config_cmd_val;
+	int nr_entries;
+	int rc = 0;
+	int i = 0;
+
+	pr_info("ASPEED BMC PCI ID %04x:%04x, IRQ=%u\n", pdev->vendor, pdev->device, pdev->irq);
+
+	pci_bmc_dev = kzalloc(sizeof(*pci_bmc_dev), GFP_KERNEL);
+	if (!pci_bmc_dev) {
+		rc = -ENOMEM;
+		dev_err(&pdev->dev, "kmalloc() returned NULL memory.\n");
+		goto out_err;
+	}
+
+	rc = pci_enable_device(pdev);
+	if (rc != 0) {
+		dev_err(&pdev->dev, "pci_enable_device() returned error %d\n", rc);
+		goto out_err;
+	}
+
+	/* set PCI host mastering  */
+	pci_set_master(pdev);
+
+	nr_entries = pci_alloc_irq_vectors(pdev, 1, BMC_MULTI_MSI,
+				PCI_IRQ_MSIX | PCI_IRQ_MSI);
+	if (nr_entries < 0) {
+		pci_bmc_dev->legency_irq = 1;
+		pci_read_config_word(pdev, PCI_COMMAND, &config_cmd_val);
+		config_cmd_val &= ~PCI_COMMAND_INTX_DISABLE;
+		pci_write_config_word((struct pci_dev *)pdev, PCI_COMMAND, config_cmd_val);
+
+	} else {
+		pci_bmc_dev->legency_irq = 0;
+		pci_read_config_word(pdev, PCI_COMMAND, &config_cmd_val);
+		config_cmd_val |= PCI_COMMAND_INTX_DISABLE;
+		pci_write_config_word((struct pci_dev *)pdev, PCI_COMMAND, config_cmd_val);
+		pdev->irq = pci_irq_vector(pdev, BMC_MSI_IDX_BASE);
+	}
+
+	pr_info("ASPEED BMC PCI ID %04x:%04x, IRQ=%u\n", pdev->vendor, pdev->device, pdev->irq);
+
+	init_waitqueue_head(&pci_bmc_dev->tx_wait0);
+	init_waitqueue_head(&pci_bmc_dev->tx_wait1);
+	init_waitqueue_head(&pci_bmc_dev->rx_wait0);
+	init_waitqueue_head(&pci_bmc_dev->rx_wait1);
+
+	//Get MEM bar
+	pci_bmc_dev->mem_bar_base = pci_resource_start(pdev, 0);
+	pci_bmc_dev->mem_bar_size = pci_resource_len(pdev, 0);
+
+	pr_info("BAR0 I/O Mapped Base Address is: %08lx End %08lx\n", pci_bmc_dev->mem_bar_base, pci_bmc_dev->mem_bar_size);
+
+	pci_bmc_dev->mem_bar_reg = pci_ioremap_bar(pdev, 0);
+	if (!pci_bmc_dev->mem_bar_reg) {
+		rc = -ENOMEM;
+		goto out_free0;
+	}
+
+    //Get MSG BAR info
+	pci_bmc_dev->message_bar_base = pci_resource_start(pdev, 1);
+	pci_bmc_dev->message_bar_size = pci_resource_len(pdev, 1);
+
+	pr_info("MSG BAR1 Memory Mapped Base Address is: %08lx End %08lx\n", pci_bmc_dev->message_bar_base, pci_bmc_dev->message_bar_size);
+
+	pci_bmc_dev->msg_bar_reg = pci_ioremap_bar(pdev, 1);
+	if (!pci_bmc_dev->msg_bar_reg) {
+		rc = -ENOMEM;
+		goto out_free1;
+	}
+
+	/* ERRTA40: dummy read */
+	(void)__raw_readl((void __iomem *)pci_bmc_dev->msg_bar_reg);
+
+	sysfs_bin_attr_init(&pci_bmc_dev->bin0);
+	sysfs_bin_attr_init(&pci_bmc_dev->bin1);
+
+	pci_bmc_dev->bin0.attr.name = "pci-bmc-dev-queue1";
+	pci_bmc_dev->bin0.attr.mode = 0600;
+	pci_bmc_dev->bin0.read = aspeed_pci_bmc_dev_queue1_rx;
+	pci_bmc_dev->bin0.write = aspeed_pci_bmc_dev_queue1_tx;
+	pci_bmc_dev->bin0.size = 4;
+
+	rc = sysfs_create_bin_file(&pdev->dev.kobj, &pci_bmc_dev->bin0);
+	if (rc) {
+		pr_err("error for bin file ");
+		goto out_free1;
+	}
+
+	pci_bmc_dev->kn0 = kernfs_find_and_get(dev->kobj.sd, pci_bmc_dev->bin0.attr.name);
+	if (!pci_bmc_dev->kn0) {
+		sysfs_remove_bin_file(&dev->kobj, &pci_bmc_dev->bin0);
+		goto out_free1;
+	}
+
+	pci_bmc_dev->bin1.attr.name = "pci-bmc-dev-queue2";
+	pci_bmc_dev->bin1.attr.mode = 0600;
+	pci_bmc_dev->bin1.read = aspeed_pci_bmc_dev_queue2_rx;
+	pci_bmc_dev->bin1.write = aspeed_pci_bmc_dev_queue2_tx;
+	pci_bmc_dev->bin1.size = 4;
+
+	rc = sysfs_create_bin_file(&pdev->dev.kobj, &pci_bmc_dev->bin1);
+	if (rc) {
+		sysfs_remove_bin_file(&dev->kobj, &pci_bmc_dev->bin1);
+		goto out_free1;
+	}
+
+	pci_bmc_dev->kn1 = kernfs_find_and_get(dev->kobj.sd, pci_bmc_dev->bin1.attr.name);
+	if (!pci_bmc_dev->kn1) {
+		sysfs_remove_bin_file(&dev->kobj, &pci_bmc_dev->bin1);
+		goto out_free1;
+	}
+
+	pci_bmc_dev->miscdev.minor = MISC_DYNAMIC_MINOR;
+	pci_bmc_dev->miscdev.name = DRIVER_NAME;
+	pci_bmc_dev->miscdev.fops = &aspeed_pci_bmc_dev_fops;
+	pci_bmc_dev->miscdev.parent = dev;
+
+	rc = misc_register(&pci_bmc_dev->miscdev);
+	if (rc) {
+		pr_err("host bmc register fail %d\n", rc);
+		goto out_free;
+	}
+
+	pci_set_drvdata(pdev, pci_bmc_dev);
+
+	rc = request_irq(pdev->irq, aspeed_pci_host_bmc_device_interrupt, IRQF_SHARED, "ASPEED BMC DEVICE", pci_bmc_dev);
+	if (rc) {
+		pr_err("host bmc device Unable to get IRQ %d\n", rc);
+		goto out_unreg;
+	}
+
+	/* setup mbox */
+	pcie_sio_decode_addr = pci_bmc_dev->msg_bar_reg + PCIE_DEVICE_SIO_ADDR;
+	writel(0xaa, pcie_sio_decode_addr);
+	writel(0xa5, pcie_sio_decode_addr);
+	writel(0xa5, pcie_sio_decode_addr);
+	writel(0x07, pcie_sio_decode_addr);
+	writel(0x0e, pcie_sio_decode_addr + 0x04);
+	/* disable */
+	writel(0x30, pcie_sio_decode_addr);
+	writel(0x00, pcie_sio_decode_addr + 0x04);
+	/* set decode address 0x100 */
+	writel(0x60, pcie_sio_decode_addr);
+	writel(0x01, pcie_sio_decode_addr + 0x04);
+	writel(0x61, pcie_sio_decode_addr);
+	writel(0x00, pcie_sio_decode_addr + 0x04);
+	/* enable */
+	writel(0x30, pcie_sio_decode_addr);
+	writel(0x01, pcie_sio_decode_addr + 0x04);
+	pci_bmc_dev->sio_mbox_reg = pci_bmc_dev->msg_bar_reg + 0x400;
+
+	if (pci_bmc_dev->legency_irq)
+		pci_bmc_dev->sio_mbox_irq = pdev->irq;
+	else
+		pci_bmc_dev->sio_mbox_irq = pci_irq_vector(pdev, 0x10 + 9 - BMC_MSI_IDX_BASE);
+
+	rc = request_irq(pci_bmc_dev->sio_mbox_irq, aspeed_pci_host_mbox_interrupt, IRQF_SHARED, "ASPEED SIO MBOX", pci_bmc_dev);
+	if (rc)
+		pr_err("host bmc device Unable to get IRQ %d\n", rc);
+
+	/* setup VUART */
+	memset(uart, 0, sizeof(uart));
+
+	for (i = 0; i < VUART_MAX_PARMS; i++) {
+		vuart_ioport[i] = 0x3F8 - (i * 0x100);
+		vuart_sirq[i] = 0x10 + 4 - i - BMC_MSI_IDX_BASE;
+		uart[i].port.flags = UPF_SKIP_TEST | UPF_BOOT_AUTOCONF | UPF_SHARE_IRQ;
+		uart[i].port.uartclk = 115200 * 16;
+
+		if (pci_bmc_dev->legency_irq)
+			uart[i].port.irq = pdev->irq;
+		else
+			uart[i].port.irq = pci_irq_vector(pdev, vuart_sirq[i]);
+		uart[i].port.dev = &pdev->dev;
+		uart[i].port.iotype = UPIO_MEM32;
+		uart[i].port.iobase = 0;
+		uart[i].port.mapbase = pci_bmc_dev->message_bar_base + (vuart_ioport[i] << 2);
+		uart[i].port.membase = 0;
+		uart[i].port.type = PORT_16550A;
+		uart[i].port.flags |= (UPF_IOREMAP | UPF_FIXED_PORT | UPF_FIXED_TYPE);
+		uart[i].port.regshift = 2;
+
+		rc = serial8250_register_8250_port(&uart[i]);
+		if (rc < 0) {
+			dev_err(dev, "cannot setup VUART@%xh over PCIe, rc=%d\n", vuart_ioport[i], rc);
+			goto out_unreg;
+		}
+	}
+
+	return 0;
+
+out_unreg:
+	misc_deregister(&pci_bmc_dev->miscdev);
+out_free1:
+	pci_release_region(pdev, 1);
+out_free0:
+	pci_release_region(pdev, 0);
+out_free:
+	kfree(pci_bmc_dev);
+out_err:
+	pci_disable_device(pdev);
+
+	return rc;
+
+}
+
+static void aspeed_pci_host_bmc_device_remove(struct pci_dev *pdev)
+{
+	struct aspeed_pci_bmc_dev *pci_bmc_dev = pci_get_drvdata(pdev);
+
+	free_irq(pdev->irq, pdev);
+	misc_deregister(&pci_bmc_dev->miscdev);
+	pci_release_regions(pdev);
+	kfree(pci_bmc_dev);
+	pci_disable_device(pdev);
+}
+
+/**
+ * This table holds the list of (VendorID,DeviceID) supported by this driver
+ *
+ */
+static struct pci_device_id aspeed_host_bmc_dev_pci_ids[] = {
+	{ PCI_DEVICE(0x1A03, 0x2402), },
+	{ 0, }
+};
+
+MODULE_DEVICE_TABLE(pci, aspeed_host_bmc_dev_pci_ids);
+
+static struct pci_driver aspeed_host_bmc_dev_driver = {
+	.name		= DRIVER_NAME,
+	.id_table	= aspeed_host_bmc_dev_pci_ids,
+	.probe		= aspeed_pci_host_bmc_device_probe,
+	.remove		= aspeed_pci_host_bmc_device_remove,
+};
+
+static int __init aspeed_host_bmc_device_init(void)
+{
+	int ret;
+
+	/* register pci driver */
+	ret = pci_register_driver(&aspeed_host_bmc_dev_driver);
+	if (ret < 0) {
+		pr_err("pci-driver: can't register pci driver\n");
+		return ret;
+	}
+
+	return 0;
+
+}
+
+static void aspeed_host_bmc_device_exit(void)
+{
+	/* unregister pci driver */
+	pci_unregister_driver(&aspeed_host_bmc_dev_driver);
+}
+
+late_initcall(aspeed_host_bmc_device_init);
+module_exit(aspeed_host_bmc_device_exit);
+
+MODULE_AUTHOR("Ryan Chen <ryan_chen@aspeedtech.com>");
+MODULE_DESCRIPTION("ASPEED Host BMC DEVICE Driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/soc/aspeed/aspeed-lpc-mbox.c b/drivers/soc/aspeed/aspeed-lpc-mbox.c
new file mode 100644
index 000000000000..a09ca6a175f7
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-lpc-mbox.c
@@ -0,0 +1,418 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright 2017 IBM Corporation
+ * Copyright 2021 Aspeed Technology Inc.
+ */
+#include <linux/interrupt.h>
+#include <linux/mfd/syscon.h>
+#include <linux/miscdevice.h>
+#include <linux/module.h>
+#include <linux/of_irq.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+#include <linux/poll.h>
+#include <linux/regmap.h>
+#include <linux/slab.h>
+
+#define DEVICE_NAME	"aspeed-mbox"
+
+#define ASPEED_MBOX_DR(dr, n)	(dr + (n * 4))
+#define ASPEED_MBOX_STR(str, n)	(str + (n / 8) * 4)
+#define ASPEED_MBOX_BIE(bie, n)	(bie + (n / 8) * 4)
+#define ASPEED_MBOX_HIE(hie, n) (hie + (n / 8) * 4)
+
+#define ASPEED_MBOX_BCR_RECV	BIT(7)
+#define ASPEED_MBOX_BCR_MASK	BIT(1)
+#define ASPEED_MBOX_BCR_SEND	BIT(0)
+
+/* ioctl code */
+#define ASPEED_MBOX_IOCTL		0xA3
+#define ASPEED_MBOX_IOCTL_GET_SIZE	\
+	_IOR(ASPEED_MBOX_IOCTL, 0, struct aspeed_mbox_ioctl_data)
+
+struct aspeed_mbox_ioctl_data {
+	unsigned int data;
+};
+
+struct aspeed_mbox_model {
+	unsigned int dr_num;
+
+	/* offsets to the MBOX registers */
+	unsigned int dr;
+	unsigned int str;
+	unsigned int bcr;
+	unsigned int hcr;
+	unsigned int bie;
+	unsigned int hie;
+};
+
+struct aspeed_mbox {
+	struct miscdevice miscdev;
+	struct regmap *map;
+	unsigned int base;
+	wait_queue_head_t queue;
+	struct mutex mutex;
+	const struct aspeed_mbox_model *model;
+};
+
+static atomic_t aspeed_mbox_open_count = ATOMIC_INIT(0);
+
+static u8 aspeed_mbox_inb(struct aspeed_mbox *mbox, int reg)
+{
+	/*
+	 * The mbox registers are actually only one byte but are addressed
+	 * four bytes apart. The other three bytes are marked 'reserved',
+	 * they *should* be zero but lets not rely on it.
+	 * I am going to rely on the fact we can casually read/write to them...
+	 */
+	unsigned int val = 0xff; /* If regmap throws an error return 0xff */
+	int rc = regmap_read(mbox->map, mbox->base + reg, &val);
+
+	if (rc)
+		dev_err(mbox->miscdev.parent, "regmap_read() failed with "
+			"%d (reg: 0x%08x)\n", rc, reg);
+
+	return val & 0xff;
+}
+
+static void aspeed_mbox_outb(struct aspeed_mbox *mbox, u8 data, int reg)
+{
+	int rc = regmap_write(mbox->map, mbox->base + reg, data);
+
+	if (rc)
+		dev_err(mbox->miscdev.parent, "regmap_write() failed with "
+			"%d (data: %u reg: 0x%08x)\n", rc, data, reg);
+}
+
+static struct aspeed_mbox *file_mbox(struct file *file)
+{
+	return container_of(file->private_data, struct aspeed_mbox, miscdev);
+}
+
+static int aspeed_mbox_open(struct inode *inode, struct file *file)
+{
+	struct aspeed_mbox *mbox = file_mbox(file);
+	const struct aspeed_mbox_model *model = mbox->model;
+
+	if (atomic_inc_return(&aspeed_mbox_open_count) == 1) {
+		/*
+		 * Clear the interrupt status bit if it was left on and unmask
+		 * interrupts.
+		 * ASPEED_MBOX_BCR_RECV bit is W1C, this also unmasks in 1 step
+		 */
+		aspeed_mbox_outb(mbox, ASPEED_MBOX_BCR_RECV, model->bcr);
+		return 0;
+	}
+
+	atomic_dec(&aspeed_mbox_open_count);
+	return -EBUSY;
+}
+
+static ssize_t aspeed_mbox_read(struct file *file, char __user *buf,
+				size_t count, loff_t *ppos)
+{
+	struct aspeed_mbox *mbox = file_mbox(file);
+	const struct aspeed_mbox_model *model = mbox->model;
+	char __user *p = buf;
+	ssize_t ret;
+	int i;
+
+	if (!access_ok(buf, count))
+		return -EFAULT;
+
+	if (count + *ppos > model->dr_num)
+		return -EINVAL;
+
+	if (file->f_flags & O_NONBLOCK) {
+		if (!(aspeed_mbox_inb(mbox, model->bcr) &
+				ASPEED_MBOX_BCR_RECV))
+			return -EAGAIN;
+	} else if (wait_event_interruptible(mbox->queue,
+				aspeed_mbox_inb(mbox, model->bcr) &
+				ASPEED_MBOX_BCR_RECV)) {
+		return -ERESTARTSYS;
+	}
+
+	mutex_lock(&mbox->mutex);
+
+	for (i = *ppos; count > 0 && i < model->dr_num; i++) {
+		uint8_t reg = aspeed_mbox_inb(mbox, ASPEED_MBOX_DR(model->dr, i));
+
+		ret = __put_user(reg, p);
+		if (ret)
+			goto out_unlock;
+
+		p++;
+		count--;
+	}
+
+	/* ASPEED_MBOX_BCR_RECV bit is write to clear, this also unmasks in 1 step */
+	aspeed_mbox_outb(mbox, ASPEED_MBOX_BCR_RECV, model->bcr);
+	ret = p - buf;
+
+out_unlock:
+	mutex_unlock(&mbox->mutex);
+	return ret;
+}
+
+static ssize_t aspeed_mbox_write(struct file *file, const char __user *buf,
+				size_t count, loff_t *ppos)
+{
+	struct aspeed_mbox *mbox = file_mbox(file);
+	const struct aspeed_mbox_model *model = mbox->model;
+	const char __user *p = buf;
+	ssize_t ret;
+	char c;
+	int i;
+
+	if (!access_ok(buf, count))
+		return -EFAULT;
+
+	if (count + *ppos > model->dr_num)
+		return -EINVAL;
+
+	mutex_lock(&mbox->mutex);
+
+	for (i = *ppos; count > 0 && i < model->dr_num; i++) {
+		ret = __get_user(c, p);
+		if (ret)
+			goto out_unlock;
+
+		aspeed_mbox_outb(mbox, c, ASPEED_MBOX_DR(model->dr, i));
+		p++;
+		count--;
+	}
+
+	aspeed_mbox_outb(mbox, ASPEED_MBOX_BCR_SEND, model->bcr);
+	ret = p - buf;
+
+out_unlock:
+	mutex_unlock(&mbox->mutex);
+	return ret;
+}
+
+static __poll_t aspeed_mbox_poll(struct file *file, poll_table *wait)
+{
+	struct aspeed_mbox *mbox = file_mbox(file);
+	const struct aspeed_mbox_model *model = mbox->model;
+	__poll_t mask = 0;
+
+	poll_wait(file, &mbox->queue, wait);
+
+	if (aspeed_mbox_inb(mbox, model->bcr) & ASPEED_MBOX_BCR_RECV)
+		mask |= POLLIN;
+
+	return mask;
+}
+
+static int aspeed_mbox_release(struct inode *inode, struct file *file)
+{
+	atomic_dec(&aspeed_mbox_open_count);
+	return 0;
+}
+
+static long aspeed_mbox_ioctl(struct file *file, unsigned int cmd,
+				 unsigned long param)
+{
+	long ret = 0;
+	struct aspeed_mbox *mbox = file_mbox(file);
+	const struct aspeed_mbox_model *model = mbox->model;
+	struct aspeed_mbox_ioctl_data data;
+
+	switch (cmd) {
+	case ASPEED_MBOX_IOCTL_GET_SIZE:
+		data.data = model->dr_num;
+		if (copy_to_user((void __user *)param, &data, sizeof(data)))
+			ret = -EFAULT;
+		break;
+	default:
+		ret = -ENOTTY;
+		break;
+	}
+
+	return ret;
+}
+
+static const struct file_operations aspeed_mbox_fops = {
+	.owner		= THIS_MODULE,
+	.llseek		= no_seek_end_llseek,
+	.read		= aspeed_mbox_read,
+	.write		= aspeed_mbox_write,
+	.open		= aspeed_mbox_open,
+	.release	= aspeed_mbox_release,
+	.poll		= aspeed_mbox_poll,
+	.unlocked_ioctl	= aspeed_mbox_ioctl,
+};
+
+static irqreturn_t aspeed_mbox_irq(int irq, void *arg)
+{
+	struct aspeed_mbox *mbox = arg;
+	const struct aspeed_mbox_model *model = mbox->model;
+
+	if (!(aspeed_mbox_inb(mbox, model->bcr) & ASPEED_MBOX_BCR_RECV))
+		return IRQ_NONE;
+
+	/*
+	 * Leave the status bit set so that we know the data is for us,
+	 * clear it once it has been read.
+	 */
+
+	/* Mask it off, we'll clear it when we the data gets read */
+	aspeed_mbox_outb(mbox, ASPEED_MBOX_BCR_MASK, model->bcr);
+
+	wake_up(&mbox->queue);
+	return IRQ_HANDLED;
+}
+
+static int aspeed_mbox_config_irq(struct aspeed_mbox *mbox,
+		struct platform_device *pdev)
+{
+	const struct aspeed_mbox_model *model = mbox->model;
+	struct device *dev = &pdev->dev;
+	int i, rc, irq;
+
+	irq = irq_of_parse_and_map(dev->of_node, 0);
+	if (!irq)
+		return -ENODEV;
+
+	rc = devm_request_irq(dev, irq, aspeed_mbox_irq,
+			IRQF_SHARED, DEVICE_NAME, mbox);
+	if (rc < 0) {
+		dev_err(dev, "Unable to request IRQ %d\n", irq);
+		return rc;
+	}
+
+	/*
+	 * Disable all register based interrupts.
+	 */
+	for (i = 0; i < model->dr_num / 8; ++i)
+		aspeed_mbox_outb(mbox, 0x00, ASPEED_MBOX_BIE(model->bie, i));
+
+	/* These registers are write one to clear. Clear them. */
+	for (i = 0; i < model->dr_num / 8; ++i)
+		aspeed_mbox_outb(mbox, 0xff, ASPEED_MBOX_STR(model->str, i));
+
+	aspeed_mbox_outb(mbox, ASPEED_MBOX_BCR_RECV, model->bcr);
+	return 0;
+}
+
+static int aspeed_mbox_probe(struct platform_device *pdev)
+{
+	struct aspeed_mbox *mbox;
+	struct device *dev;
+	int rc;
+
+	dev = &pdev->dev;
+
+	mbox = devm_kzalloc(dev, sizeof(*mbox), GFP_KERNEL);
+	if (!mbox)
+		return -ENOMEM;
+
+	dev_set_drvdata(&pdev->dev, mbox);
+
+	rc = of_property_read_u32(dev->of_node, "reg", &mbox->base);
+	if (rc) {
+		dev_err(dev, "Couldn't read reg device tree property\n");
+		return rc;
+	}
+
+	mbox->model = of_device_get_match_data(dev);
+	if (IS_ERR(mbox->model)) {
+		dev_err(dev, "Couldn't get model data\n");
+		return -ENODEV;
+	}
+
+	mbox->map = syscon_node_to_regmap(
+			pdev->dev.parent->of_node);
+	if (IS_ERR(mbox->map)) {
+		dev_err(dev, "Couldn't get regmap\n");
+		return -ENODEV;
+	}
+
+	mutex_init(&mbox->mutex);
+	init_waitqueue_head(&mbox->queue);
+
+	mbox->miscdev.minor = MISC_DYNAMIC_MINOR;
+	mbox->miscdev.name = DEVICE_NAME;
+	mbox->miscdev.fops = &aspeed_mbox_fops;
+	mbox->miscdev.parent = dev;
+	rc = misc_register(&mbox->miscdev);
+	if (rc) {
+		dev_err(dev, "Unable to register device\n");
+		return rc;
+	}
+
+	rc = aspeed_mbox_config_irq(mbox, pdev);
+	if (rc) {
+		dev_err(dev, "Failed to configure IRQ\n");
+		misc_deregister(&mbox->miscdev);
+		return rc;
+	}
+
+	return 0;
+}
+
+static int aspeed_mbox_remove(struct platform_device *pdev)
+{
+	struct aspeed_mbox *mbox = dev_get_drvdata(&pdev->dev);
+
+	misc_deregister(&mbox->miscdev);
+
+	return 0;
+}
+
+static const struct aspeed_mbox_model ast2400_model = {
+	.dr_num = 16,
+	.dr	= 0x0,
+	.str = 0x40,
+	.bcr = 0x48,
+	.hcr = 0x4c,
+	.bie = 0x50,
+	.hie = 0x58,
+};
+
+static const struct aspeed_mbox_model ast2500_model = {
+	.dr_num = 16,
+	.dr	= 0x0,
+	.str = 0x40,
+	.bcr = 0x48,
+	.hcr = 0x4c,
+	.bie = 0x50,
+	.hie = 0x58,
+};
+
+static const struct aspeed_mbox_model ast2600_model = {
+	.dr_num = 32,
+	.dr	= 0x0,
+	.str = 0x80,
+	.bcr = 0x90,
+	.hcr = 0x94,
+	.bie = 0xa0,
+	.hie = 0xb0,
+};
+
+static const struct of_device_id aspeed_mbox_match[] = {
+	{ .compatible = "aspeed,ast2400-mbox",
+	  .data = &ast2400_model },
+	{ .compatible = "aspeed,ast2500-mbox",
+	  .data = &ast2500_model },
+	{ .compatible = "aspeed,ast2600-mbox",
+	  .data = &ast2600_model },
+	{ },
+};
+
+static struct platform_driver aspeed_mbox_driver = {
+	.driver = {
+		.name		= DEVICE_NAME,
+		.of_match_table = aspeed_mbox_match,
+	},
+	.probe = aspeed_mbox_probe,
+	.remove = aspeed_mbox_remove,
+};
+
+module_platform_driver(aspeed_mbox_driver);
+MODULE_DEVICE_TABLE(of, aspeed_mbox_match);
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cyril Bur <cyrilbur@gmail.com>");
+MODULE_AUTHOR("Chia-Wei Wang <chiawei_wang@aspeedtech.com");
+MODULE_DESCRIPTION("Aspeed mailbox device driver");
diff --git a/drivers/soc/aspeed/aspeed-lpc-pcc.c b/drivers/soc/aspeed/aspeed-lpc-pcc.c
new file mode 100644
index 000000000000..360df00e6850
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-lpc-pcc.c
@@ -0,0 +1,406 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) ASPEED Technology Inc.
+ */
+#include <linux/bitops.h>
+#include <linux/interrupt.h>
+#include <linux/fs.h>
+#include <linux/kfifo.h>
+#include <linux/mfd/syscon.h>
+#include <linux/miscdevice.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/of_address.h>
+#include <linux/platform_device.h>
+#include <linux/poll.h>
+#include <linux/regmap.h>
+#include <linux/dma-mapping.h>
+#include <linux/sizes.h>
+
+#define DEVICE_NAME "aspeed-lpc-pcc"
+
+#define PCCR6	0x0c4
+#define   PCCR6_DMA_CUR_ADDR		GENMASK(27, 0)
+#define PCCR4	0x0d0
+#define PCCR5	0x0d4
+#define PCCR0	0x130
+#define   PCCR0_EN_DMA_INT		BIT(31)
+#define   PCCR0_EN_DMA_MODE		BIT(14)
+#define   PCCR0_ADDR_SEL_MASK		GENMASK(13, 12)
+#define   PCCR0_ADDR_SEL_SHIFT		12
+#define   PCCR0_RX_TRIG_LVL_MASK	GENMASK(10, 8)
+#define   PCCR0_RX_TRIG_LVL_SHIFT	8
+#define   PCCR0_CLR_RX_FIFO		BIT(7)
+#define   PCCR0_MODE_SEL_MASK		GENMASK(5, 4)
+#define   PCCR0_MODE_SEL_SHIFT		4
+#define   PCCR0_EN_RX_TMOUT_INT		BIT(2)
+#define   PCCR0_EN_RX_AVAIL_INT		BIT(1)
+#define   PCCR0_EN			BIT(0)
+#define PCCR1	0x134
+#define   PCCR1_BASE_ADDR_MASK		GENMASK(15, 0)
+#define   PCCR1_BASE_ADDR_SHIFT		0
+#define   PCCR1_DONT_CARE_BITS_MASK	GENMASK(21, 16)
+#define   PCCR1_DONT_CARE_BITS_SHIFT	16
+#define PCCR2	0x138
+#define   PCCR2_DMA_DONE		BIT(4)
+#define   PCCR2_DATA_RDY		PCCR2_DMA_DONE
+#define   PCCR2_RX_TMOUT_INT		BIT(2)
+#define   PCCR2_RX_AVAIL_INT		BIT(1)
+#define PCCR3	0x13c
+#define   PCCR3_FIFO_DATA_MASK		GENMASK(7, 0)
+
+#define PCC_DMA_BUFSZ	(256 * SZ_1K)
+
+enum pcc_fifo_threshold {
+	PCC_FIFO_THR_1_BYTE,
+	PCC_FIFO_THR_1_EIGHTH,
+	PCC_FIFO_THR_2_EIGHTH,
+	PCC_FIFO_THR_3_EIGHTH,
+	PCC_FIFO_THR_4_EIGHTH,
+	PCC_FIFO_THR_5_EIGHTH,
+	PCC_FIFO_THR_6_EIGHTH,
+	PCC_FIFO_THR_7_EIGHTH,
+	PCC_FIFO_THR_8_EIGHTH,
+};
+
+enum pcc_record_mode {
+	PCC_REC_1B,
+	PCC_REC_2B,
+	PCC_REC_4B,
+	PCC_REC_FULL,
+};
+
+enum pcc_port_hbits_select {
+	PCC_PORT_HBITS_SEL_NONE,
+	PCC_PORT_HBITS_SEL_45,
+	PCC_PORT_HBITS_SEL_67,
+	PCC_PORT_HBITS_SEL_89,
+};
+
+struct aspeed_pcc_dma {
+	uint32_t rptr;
+	uint8_t *virt;
+	dma_addr_t addr;
+	uint32_t size;
+};
+
+struct aspeed_pcc {
+	struct device *dev;
+	struct regmap *regmap;
+	int irq;
+	uint32_t rec_mode;
+	uint32_t port;
+	uint32_t port_xbits;
+	uint32_t port_hbits_select;
+	uint32_t dma_mode;
+	struct aspeed_pcc_dma dma;
+	struct kfifo fifo;
+	wait_queue_head_t wq;
+	struct miscdevice misc_dev;
+};
+
+static inline bool is_valid_rec_mode(uint32_t mode)
+{
+	return (mode > PCC_REC_FULL) ? false : true;
+}
+
+static inline bool is_valid_high_bits_select(uint32_t sel)
+{
+	return (sel > PCC_PORT_HBITS_SEL_89) ? false : true;
+}
+
+static ssize_t aspeed_pcc_file_read(struct file *file, char __user *buffer,
+		size_t count, loff_t *ppos)
+{
+	int rc;
+	ssize_t copied;
+
+	struct aspeed_pcc *pcc = container_of(
+			file->private_data,
+			struct aspeed_pcc,
+			misc_dev);
+
+	if (kfifo_is_empty(&pcc->fifo)) {
+		if (file->f_flags & O_NONBLOCK)
+			return -EAGAIN;
+
+		rc = wait_event_interruptible(pcc->wq,
+				!kfifo_is_empty(&pcc->fifo));
+
+		if (rc == -ERESTARTSYS)
+			return -EINTR;
+	}
+
+	rc = kfifo_to_user(&pcc->fifo, buffer, count, &copied);
+
+	return rc ? rc : copied;
+}
+
+static __poll_t aspeed_pcc_file_poll(struct file *file,
+		struct poll_table_struct *pt)
+{
+	struct aspeed_pcc *pcc = container_of(
+			file->private_data,
+			struct aspeed_pcc,
+			misc_dev);
+
+	poll_wait(file, &pcc->wq, pt);
+
+	return !kfifo_is_empty(&pcc->fifo) ? POLLIN : 0;
+}
+
+static const struct file_operations pcc_fops = {
+	.owner = THIS_MODULE,
+	.read = aspeed_pcc_file_read,
+	.poll = aspeed_pcc_file_poll,
+};
+
+static irqreturn_t aspeed_pcc_dma_isr(int irq, void *arg)
+{
+	uint32_t reg, rptr, wptr;
+	struct aspeed_pcc *pcc = (struct aspeed_pcc*)arg;
+	struct kfifo *fifo = &pcc->fifo;
+
+	regmap_read(pcc->regmap, PCCR6, &reg);
+
+	regmap_write_bits(pcc->regmap, PCCR2, PCCR2_DMA_DONE, PCCR2_DMA_DONE);
+
+	wptr = (reg & PCCR6_DMA_CUR_ADDR) - (pcc->dma.addr & PCCR6_DMA_CUR_ADDR);
+	rptr = pcc->dma.rptr;
+
+	do {
+		if (kfifo_is_full(fifo))
+			kfifo_skip(fifo);
+
+		kfifo_put(fifo, pcc->dma.virt[rptr]);
+
+		rptr = (rptr + 1) % pcc->dma.size;
+	} while (rptr != wptr);
+
+	pcc->dma.rptr = rptr;
+
+	wake_up_interruptible(&pcc->wq);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t aspeed_pcc_isr(int irq, void *arg)
+{
+	uint32_t sts, reg;
+	struct aspeed_pcc *pcc = (struct aspeed_pcc*)arg;
+	struct kfifo *fifo = &pcc->fifo;
+
+	regmap_read(pcc->regmap, PCCR2, &sts);
+
+	if (!(sts & (PCCR2_RX_TMOUT_INT | PCCR2_RX_AVAIL_INT | PCCR2_DMA_DONE)))
+		return IRQ_NONE;
+
+	if (pcc->dma_mode)
+		return aspeed_pcc_dma_isr(irq, arg);
+
+	while (sts & PCCR2_DATA_RDY) {
+		regmap_read(pcc->regmap, PCCR3, &reg);
+
+		if (kfifo_is_full(fifo))
+			kfifo_skip(fifo);
+
+		kfifo_put(fifo, reg & PCCR3_FIFO_DATA_MASK);
+
+		regmap_read(pcc->regmap, PCCR2, &sts);
+	}
+
+	wake_up_interruptible(&pcc->wq);
+
+	return IRQ_HANDLED;
+}
+
+static void aspeed_pcc_enable(struct aspeed_pcc *pcc, struct device *dev)
+{
+	/* record mode */
+	regmap_update_bits(pcc->regmap, PCCR0,
+			PCCR0_MODE_SEL_MASK,
+			pcc->rec_mode << PCCR0_MODE_SEL_SHIFT);
+
+	/* port address */
+	regmap_update_bits(pcc->regmap, PCCR1,
+			PCCR1_BASE_ADDR_MASK,
+			pcc->port << PCCR1_BASE_ADDR_SHIFT);
+
+	/* port address high bits selection or parser control */
+	regmap_update_bits(pcc->regmap, PCCR0,
+			PCCR0_ADDR_SEL_MASK,
+			pcc->port_hbits_select << PCCR0_ADDR_SEL_SHIFT);
+
+	/* port address dont care bits */
+	regmap_update_bits(pcc->regmap, PCCR1,
+			PCCR1_DONT_CARE_BITS_MASK,
+			pcc->port_xbits << PCCR1_DONT_CARE_BITS_SHIFT);
+
+	/* set DMA ring buffer size and enable interrupts */
+	if (pcc->dma_mode) {
+		regmap_write(pcc->regmap, PCCR4, pcc->dma.addr);
+		regmap_write(pcc->regmap, PCCR5, pcc->dma.size / 4);
+		regmap_update_bits(pcc->regmap, PCCR0,
+			PCCR0_EN_DMA_INT | PCCR0_EN_DMA_MODE,
+			PCCR0_EN_DMA_INT | PCCR0_EN_DMA_MODE);
+	} else {
+		regmap_update_bits(pcc->regmap, PCCR0, PCCR0_RX_TRIG_LVL_MASK,
+				   PCC_FIFO_THR_4_EIGHTH << PCCR0_RX_TRIG_LVL_SHIFT);
+		regmap_update_bits(pcc->regmap, PCCR0,
+				   PCCR0_EN_RX_TMOUT_INT | PCCR0_EN_RX_AVAIL_INT,
+				   PCCR0_EN_RX_TMOUT_INT | PCCR0_EN_RX_AVAIL_INT);
+	}
+
+	regmap_update_bits(pcc->regmap, PCCR0, PCCR0_EN, PCCR0_EN);
+
+}
+
+static int aspeed_pcc_probe(struct platform_device *pdev)
+{
+	int rc;
+	struct aspeed_pcc *pcc;
+	struct device *dev = &pdev->dev;
+	uint32_t fifo_size = PAGE_SIZE;
+
+	pcc = devm_kzalloc(&pdev->dev, sizeof(*pcc), GFP_KERNEL);
+	if (!pcc)
+		return -ENOMEM;
+
+	pcc->dev = dev;
+
+	pcc->regmap = syscon_node_to_regmap(pdev->dev.parent->of_node);
+	if (IS_ERR(pcc->regmap)) {
+		dev_err(dev, "cannot map register\n");
+		return -ENODEV;
+	}
+
+	/* disable PCC for safety */
+	regmap_update_bits(pcc->regmap, PCCR0, PCCR0_EN, 0);
+
+	rc = of_property_read_u32(dev->of_node, "port-addr", &pcc->port);
+	if (rc) {
+		dev_err(dev, "cannot get port address\n");
+		return -ENODEV;
+	}
+
+	/* optional, by default: 0 -> 1-Byte mode */
+	of_property_read_u32(dev->of_node, "rec-mode", &pcc->rec_mode);
+	if (!is_valid_rec_mode(pcc->rec_mode)) {
+		dev_err(dev, "invalid record mode: %u\n",
+				pcc->rec_mode);
+		return -EINVAL;
+	}
+
+	/* optional, by default: 0 -> no don't care bits */
+	of_property_read_u32(dev->of_node, "port-addr-xbits", &pcc->port_xbits);
+
+	/*
+	 * optional, by default: 0 -> no high address bits
+	 *
+	 * Note that when record mode is set to 1-Byte, this
+	 * property is ignored and the corresponding HW bits
+	 * behave as read/write cycle parser control with the
+	 * value set to 0b11
+	 */
+	if (pcc->rec_mode) {
+		of_property_read_u32(dev->of_node, "port-addr-hbits-select", &pcc->port_hbits_select);
+		if (!is_valid_high_bits_select(pcc->port_hbits_select)) {
+			dev_err(dev, "invalid high address bits selection: %u\n",
+				pcc->port_hbits_select);
+			return -EINVAL;
+		}
+	}
+	else
+		pcc->port_hbits_select = 0x3;
+
+	pcc->dma_mode = of_property_read_bool(dev->of_node, "dma-mode");
+	if (pcc->dma_mode) {
+		pcc->dma.size = PCC_DMA_BUFSZ;
+		pcc->dma.virt = dmam_alloc_coherent(dev,
+						    pcc->dma.size,
+						    &pcc->dma.addr,
+						    GFP_KERNEL);
+		if (!pcc->dma.virt) {
+			dev_err(dev, "cannot allocate DMA buffer\n");
+			return -ENOMEM;
+		}
+
+		fifo_size = roundup(pcc->dma.size, PAGE_SIZE);
+	}
+
+	rc = kfifo_alloc(&pcc->fifo, fifo_size, GFP_KERNEL);
+	if (rc) {
+		dev_err(dev, "cannot allocate kFIFO\n");
+		return -ENOMEM;
+	}
+
+	pcc->irq = platform_get_irq(pdev, 0);
+	if (pcc->irq < 0) {
+		dev_err(dev, "cannot get IRQ\n");
+		rc = -ENODEV;
+		goto err_free_kfifo;
+	}
+
+	rc = devm_request_irq(dev, pcc->irq, aspeed_pcc_isr, 0, DEVICE_NAME, pcc);
+	if (rc < 0) {
+		dev_err(dev, "cannot request IRQ handler\n");
+		goto err_free_kfifo;
+	}
+
+	init_waitqueue_head(&pcc->wq);
+
+	pcc->misc_dev.parent = dev;
+	pcc->misc_dev.name = devm_kasprintf(dev, GFP_KERNEL, "%s", DEVICE_NAME);
+	pcc->misc_dev.fops = &pcc_fops;
+	rc = misc_register(&pcc->misc_dev);
+	if (rc) {
+		dev_err(dev, "cannot register misc device\n");
+		goto err_free_kfifo;
+	}
+
+	aspeed_pcc_enable(pcc, dev);
+
+	dev_set_drvdata(&pdev->dev, pcc);
+
+	dev_info(dev, "module loaded\n");
+
+	return 0;
+
+err_free_kfifo:
+	kfifo_free(&pcc->fifo);
+
+	return rc;
+}
+
+static int aspeed_pcc_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct aspeed_pcc *pcc = dev_get_drvdata(dev);
+
+	kfifo_free(&pcc->fifo);
+	misc_deregister(&pcc->misc_dev);
+
+	return 0;
+}
+
+static const struct of_device_id aspeed_pcc_table[] = {
+	{ .compatible = "aspeed,ast2500-lpc-pcc" },
+	{ .compatible = "aspeed,ast2600-lpc-pcc" },
+	{ },
+};
+
+static struct platform_driver aspeed_pcc_driver = {
+	.driver = {
+		.name = "aspeed-pcc",
+		.of_match_table = aspeed_pcc_table,
+	},
+	.probe = aspeed_pcc_probe,
+	.remove = aspeed_pcc_remove,
+};
+
+module_platform_driver(aspeed_pcc_driver);
+
+MODULE_AUTHOR("Chia-Wei Wang <chiawei_wang@aspeedtech.com>");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Driver for Aspeed Post Code Capture");
diff --git a/drivers/soc/aspeed/aspeed-lpc-snoop.c b/drivers/soc/aspeed/aspeed-lpc-snoop.c
index eceeaf8dfbeb..833654e82e78 100644
--- a/drivers/soc/aspeed/aspeed-lpc-snoop.c
+++ b/drivers/soc/aspeed/aspeed-lpc-snoop.c
@@ -11,7 +11,6 @@
  */
 
 #include <linux/bitops.h>
-#include <linux/clk.h>
 #include <linux/interrupt.h>
 #include <linux/fs.h>
 #include <linux/kfifo.h>
@@ -67,7 +66,6 @@ struct aspeed_lpc_snoop_channel {
 struct aspeed_lpc_snoop {
 	struct regmap		*regmap;
 	int			irq;
-	struct clk		*clk;
 	struct aspeed_lpc_snoop_channel chan[NUM_SNOOP_CHANNELS];
 };
 
@@ -167,7 +165,7 @@ static int aspeed_lpc_snoop_config_irq(struct aspeed_lpc_snoop *lpc_snoop,
 	int rc;
 
 	lpc_snoop->irq = platform_get_irq(pdev, 0);
-	if (!lpc_snoop->irq)
+	if (lpc_snoop->irq < 0)
 		return -ENODEV;
 
 	rc = devm_request_irq(dev, lpc_snoop->irq,
@@ -293,19 +291,6 @@ static int aspeed_lpc_snoop_probe(struct platform_device *pdev)
 		return -ENODEV;
 	}
 
-	lpc_snoop->clk = devm_clk_get(dev, NULL);
-	if (IS_ERR(lpc_snoop->clk)) {
-		rc = PTR_ERR(lpc_snoop->clk);
-		if (rc != -EPROBE_DEFER)
-			dev_err(dev, "couldn't get clock\n");
-		return rc;
-	}
-	rc = clk_prepare_enable(lpc_snoop->clk);
-	if (rc) {
-		dev_err(dev, "couldn't enable clock\n");
-		return rc;
-	}
-
 	rc = aspeed_lpc_snoop_config_irq(lpc_snoop, pdev);
 	if (rc)
 		goto err;
@@ -327,8 +312,6 @@ static int aspeed_lpc_snoop_probe(struct platform_device *pdev)
 	return 0;
 
 err:
-	clk_disable_unprepare(lpc_snoop->clk);
-
 	return rc;
 }
 
@@ -340,8 +323,6 @@ static int aspeed_lpc_snoop_remove(struct platform_device *pdev)
 	aspeed_lpc_disable_snoop(lpc_snoop, 0);
 	aspeed_lpc_disable_snoop(lpc_snoop, 1);
 
-	clk_disable_unprepare(lpc_snoop->clk);
-
 	return 0;
 }
 
diff --git a/drivers/soc/aspeed/aspeed-mctp.c b/drivers/soc/aspeed/aspeed-mctp.c
new file mode 100644
index 000000000000..73759d1f27c8
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-mctp.c
@@ -0,0 +1,2349 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (c) 2020, Intel Corporation.
+
+#include <linux/aspeed-mctp.h>
+#include <linux/bitfield.h>
+#include <linux/dma-mapping.h>
+#include <linux/interrupt.h>
+#include <linux/init.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/list_sort.h>
+#include <linux/mfd/syscon.h>
+#include <linux/miscdevice.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/of_platform.h>
+#include <linux/pci.h>
+#include <linux/poll.h>
+#include <linux/ptr_ring.h>
+#include <linux/regmap.h>
+#include <linux/reset.h>
+#include <linux/slab.h>
+#include <linux/swab.h>
+#include <linux/uaccess.h>
+#include <linux/workqueue.h>
+
+#include <uapi/linux/aspeed-mctp.h>
+
+/* AST2600 MCTP Controller registers */
+#define ASPEED_MCTP_CTRL	0x000
+#define  TX_CMD_TRIGGER		BIT(0)
+#define  RX_CMD_READY		BIT(4)
+#define  MATCHING_EID		BIT(9)
+
+#define ASPEED_MCTP_TX_CMD	0x004
+#define ASPEED_MCTP_RX_CMD	0x008
+
+#define ASPEED_MCTP_INT_STS	0x00c
+#define ASPEED_MCTP_INT_EN	0x010
+#define  TX_CMD_SENT_INT	BIT(0)
+#define  TX_CMD_LAST_INT	BIT(1)
+#define  TX_CMD_WRONG_INT	BIT(2)
+#define  RX_CMD_RECEIVE_INT	BIT(8)
+#define  RX_CMD_NO_MORE_INT	BIT(9)
+
+#define ASPEED_MCTP_EID		0x014
+#define  MEMORY_SPACE_MAPPING	GENMASK(31, 28)
+#define ASPEED_MCTP_OBFF_CTRL	0x018
+
+#define ASPEED_MCTP_ENGINE_CTRL		0x01c
+#define  TX_MAX_PAYLOAD_SIZE_SHIFT	0
+#define  TX_MAX_PAYLOAD_SIZE_MASK	GENMASK(1, TX_MAX_PAYLOAD_SIZE_SHIFT)
+#define  TX_MAX_PAYLOAD_SIZE(x) \
+	(((x) << TX_MAX_PAYLOAD_SIZE_SHIFT) & TX_MAX_PAYLOAD_SIZE_MASK)
+#define  RX_MAX_PAYLOAD_SIZE_SHIFT	4
+#define  RX_MAX_PAYLOAD_SIZE_MASK	GENMASK(5, RX_MAX_PAYLOAD_SIZE_SHIFT)
+#define  RX_MAX_PAYLOAD_SIZE(x) \
+	(((x) << RX_MAX_PAYLOAD_SIZE_SHIFT) & RX_MAX_PAYLOAD_SIZE_MASK)
+#define  FIFO_LAYOUT_SHIFT		8
+#define  FIFO_LAYOUT_MASK		GENMASK(9, FIFO_LAYOUT_SHIFT)
+#define  FIFO_LAYOUT(x) \
+	(((x) << FIFO_LAYOUT_SHIFT) & FIFO_LAYOUT_MASK)
+
+#define ASPEED_MCTP_RX_BUF_ADDR		0x08
+#define ASPEED_MCTP_RX_BUF_SIZE		0x024
+#define ASPEED_MCTP_RX_BUF_RD_PTR	0x028
+#define  UPDATE_RX_RD_PTR		BIT(31)
+#define  RX_BUF_RD_PTR_MASK		GENMASK(11, 0)
+#define ASPEED_MCTP_RX_BUF_WR_PTR	0x02c
+#define  RX_BUF_WR_PTR_MASK		GENMASK(11, 0)
+
+#define ASPEED_MCTP_TX_BUF_ADDR		0x04
+#define ASPEED_MCTP_TX_BUF_SIZE		0x034
+#define ASPEED_MCTP_TX_BUF_RD_PTR	0x038
+#define  UPDATE_TX_RD_PTR		BIT(31)
+#define  TX_BUF_RD_PTR_MASK		GENMASK(11, 0)
+#define ASPEED_MCTP_TX_BUF_WR_PTR	0x03c
+#define  TX_BUF_WR_PTR_MASK		GENMASK(11, 0)
+
+#define ADDR_LEN	GENMASK(26, 0)
+#define DATA_ADDR(x)	(((x) >> 4) & ADDR_LEN)
+
+/* TX command */
+#define TX_LAST_CMD		BIT(31)
+#define TX_DATA_ADDR_SHIFT	4
+#define TX_DATA_ADDR_MASK	GENMASK(30, TX_DATA_ADDR_SHIFT)
+#define TX_DATA_ADDR(x) \
+	((DATA_ADDR(x) << TX_DATA_ADDR_SHIFT) & TX_DATA_ADDR_MASK)
+#define TX_RESERVED_1_MASK	GENMASK(1, 0) /* must be 1 */
+#define TX_RESERVED_1		1
+#define TX_STOP_AFTER_CMD	BIT(16)
+#define TX_INTERRUPT_AFTER_CMD	BIT(15)
+#define TX_PACKET_SIZE_SHIFT	2
+#define TX_PACKET_SIZE_MASK	GENMASK(12, TX_PACKET_SIZE_SHIFT)
+#define TX_PACKET_SIZE(x) \
+	(((x) << TX_PACKET_SIZE_SHIFT) & TX_PACKET_SIZE_MASK)
+#define TX_RESERVED_0_MASK	GENMASK(1, 0) /* MBZ */
+#define TX_RESERVED_0		0
+
+/* RX command */
+#define RX_INTERRUPT_AFTER_CMD	BIT(2)
+#define RX_DATA_ADDR_SHIFT	4
+#define RX_DATA_ADDR_MASK	GENMASK(30, RX_DATA_ADDR_SHIFT)
+#define RX_DATA_ADDR(x) \
+	((DATA_ADDR(x) << RX_DATA_ADDR_SHIFT) & RX_DATA_ADDR_MASK)
+
+#define ADDR_LEN_2500	GENMASK(23, 0)
+#define DATA_ADDR_2500(x)	(((x) >> 7) & ADDR_LEN_2500)
+
+/* TX command for ast2500 */
+#define TX_DATA_ADDR_MASK_2500	GENMASK(30, 8)
+#define TX_DATA_ADDR_2500(x) \
+	FIELD_PREP(TX_DATA_ADDR_MASK_2500, DATA_ADDR_2500(x))
+#define TX_PACKET_SIZE_2500(x) \
+	FIELD_PREP(GENMASK(11, 2), x)
+#define TX_PACKET_DEST_EID	GENMASK(7, 0)
+#define TX_PACKET_TARGET_ID	GENMASK(31, 16)
+#define TX_PACKET_ROUTING_TYPE	BIT(14)
+#define TX_PACKET_TAG_OWNER	BIT(13)
+#define TX_PACKET_PADDING_LEN	GENMASK(1, 0)
+
+/* Rx command for ast2500 */
+#define RX_LAST_CMD		BIT(31)
+#define RX_DATA_ADDR_MASK_2500	GENMASK(29, 7)
+#define RX_DATA_ADDR_2500(x) \
+	FIELD_PREP(RX_DATA_ADDR_MASK_2500, DATA_ADDR_2500(x))
+#define RX_PACKET_SIZE		GENMASK(30, 24)
+#define RX_PACKET_SRC_EID	GENMASK(23, 16)
+#define RX_PACKET_ROUTING_TYPE	GENMASK(15, 14)
+#define RX_PACKET_TAG_OWNER	BIT(13)
+#define RX_PACKET_SEQ_NUMBER	GENMASK(12, 11)
+#define RX_PACKET_MSG_TAG	GENMASK(10, 8)
+#define RX_PACKET_SOM		BIT(7)
+#define RX_PACKET_EOM		BIT(6)
+#define RX_PACKET_PADDING_LEN	GENMASK(5, 4)
+
+/* HW buffer sizes */
+#define TX_PACKET_COUNT		48
+#define RX_PACKET_COUNT		96
+#if (RX_PACKET_COUNT % 4 != 0)
+#error The Rx buffer size should be 4-aligned.
+#error 1.Make runaway wrap boundary can be determined in Ast2600 A1/A2.
+#error 2.Fix the runaway read pointer bug in Ast2600 A3.
+#endif
+#define TX_MAX_PACKET_COUNT	(TX_BUF_RD_PTR_MASK + 1)
+#define RX_MAX_PACKET_COUNT	(RX_BUF_RD_PTR_MASK + 1)
+
+#define TX_CMD_BUF_SIZE \
+	PAGE_ALIGN(TX_PACKET_COUNT * sizeof(struct aspeed_mctp_tx_cmd))
+
+/* Per client packet cache sizes */
+#define RX_RING_COUNT		64
+#define TX_RING_COUNT		64
+
+/* PCIe Host Controller registers */
+#define ASPEED_PCIE_LINK	0x0c0
+#define PCIE_LINK_STS		BIT(5)
+#define ASPEED_PCIE_MISC_STS_1	0x0c4
+
+/* PCI address definitions */
+#define PCI_DEV_NUM_MASK	GENMASK(4, 0)
+#define PCI_BUS_NUM_SHIFT	5
+#define PCI_BUS_NUM_MASK	GENMASK(12, PCI_BUS_NUM_SHIFT)
+#define GET_PCI_DEV_NUM(x)	((x) & PCI_DEV_NUM_MASK)
+#define GET_PCI_BUS_NUM(x)	(((x) & PCI_BUS_NUM_MASK) >> PCI_BUS_NUM_SHIFT)
+
+/* MCTP header definitions */
+#define MCTP_HDR_SRC_EID_OFFSET		14
+#define MCTP_HDR_TAG_OFFSET		15
+#define MCTP_HDR_SOM			BIT(7)
+#define MCTP_HDR_EOM			BIT(6)
+#define MCTP_HDR_SOM_EOM		(MCTP_HDR_SOM | MCTP_HDR_EOM)
+#define MCTP_HDR_TYPE_OFFSET		16
+#define MCTP_HDR_TYPE_CONTROL		0
+#define MCTP_HDR_TYPE_VDM_PCI		0x7e
+#define MCTP_HDR_TYPE_SPDM		0x5
+#define MCTP_HDR_TYPE_BASE_LAST		MCTP_HDR_TYPE_SPDM
+#define MCTP_HDR_VENDOR_OFFSET		17
+#define MCTP_HDR_VDM_TYPE_OFFSET	19
+
+/* MCTP header DW little endian mask definitions */
+/* 0th DW */
+#define MCTP_HDR_DW_LE_ROUTING_TYPE	GENMASK(26, 24)
+#define MCTP_HDR_DW_LE_PACKET_SIZE	GENMASK(9, 0)
+/* 1st DW */
+#define MCTP_HDR_DW_LE_PADDING_LEN	GENMASK(13, 12)
+/* 2nd DW */
+#define MCTP_HDR_DW_LE_TARGET_ID	GENMASK(31, 16)
+/* 3rd DW */
+#define MCTP_HDR_DW_LE_TAG_OWNER	BIT(3)
+#define MCTP_HDR_DW_LE_DEST_EID		GENMASK(23, 16)
+
+#define ASPEED_MCTP_2600		0
+#define ASPEED_MCTP_2600A3		1
+
+#define ASPEED_REVISION_ID0		0x04
+#define ASPEED_REVISION_ID1		0x14
+#define ID0_AST2600A0			0x05000303
+#define ID1_AST2600A0			0x05000303
+#define ID0_AST2600A1			0x05010303
+#define ID1_AST2600A1			0x05010303
+#define ID0_AST2600A2			0x05010303
+#define ID1_AST2600A2			0x05020303
+#define ID0_AST2600A3			0x05030303
+#define ID1_AST2600A3			0x05030303
+#define ID0_AST2620A1			0x05010203
+#define ID1_AST2620A1			0x05010203
+#define ID0_AST2620A2			0x05010203
+#define ID1_AST2620A2			0x05020203
+#define ID0_AST2620A3			0x05030203
+#define ID1_AST2620A3			0x05030203
+#define ID0_AST2605A2			0x05010103
+#define ID1_AST2605A2			0x05020103
+#define ID0_AST2605A3			0x05030103
+#define ID1_AST2605A3			0x05030103
+#define ID0_AST2625A3			0x05030403
+#define ID1_AST2625A3			0x05030403
+
+struct aspeed_mctp_match_data {
+	u32 rx_cmd_size;
+	u32 packet_unit_size;
+	bool need_address_mapping;
+	bool vdm_hdr_direct_xfer;
+	bool fifo_auto_surround;
+};
+
+struct aspeed_mctp_rx_cmd {
+	u32 rx_lo;
+	u32 rx_hi;
+};
+
+struct aspeed_mctp_tx_cmd {
+	u32 tx_lo;
+	u32 tx_hi;
+};
+
+struct mctp_buffer {
+	void *vaddr;
+	dma_addr_t dma_handle;
+};
+
+struct mctp_channel {
+	struct mctp_buffer data;
+	struct mctp_buffer cmd;
+	struct tasklet_struct tasklet;
+	u32 buffer_count;
+	u32 rd_ptr;
+	u32 wr_ptr;
+	bool stopped;
+};
+
+struct aspeed_mctp {
+	struct device *dev;
+	struct miscdevice mctp_miscdev;
+	const struct aspeed_mctp_match_data *match_data;
+	struct regmap *map;
+	struct reset_control *reset;
+	/*
+	 * The reset of the dma block in the MCTP-RC is connected to
+	 * another reset pin.
+	 */
+	struct reset_control *reset_dma;
+	struct mctp_channel tx;
+	struct mctp_channel rx;
+	struct list_head clients;
+	struct mctp_client *default_client;
+	struct list_head mctp_type_handlers;
+	/*
+	 * clients_lock protects list of clients, list of type handlers
+	 * and default client
+	 */
+	spinlock_t clients_lock;
+	struct list_head endpoints;
+	size_t endpoints_count;
+	/*
+	 * endpoints_lock protects list of endpoints
+	 */
+	struct mutex endpoints_lock;
+	struct {
+		struct regmap *map;
+		struct delayed_work rst_dwork;
+		bool need_uevent;
+	} pcie;
+	struct {
+		bool enable;
+		bool first_loop;
+		int packet_counter;
+	} rx_runaway_wa;
+	bool rx_warmup;
+	u8 eid;
+	struct platform_device *peci_mctp;
+	/* Use the flag to identify RC or EP */
+	bool rc_f;
+	/* Use the flag to identify the support of MCTP interrupt */
+	bool miss_mctp_int;
+	/* Rx hardware buffer size */
+	u32 rx_packet_count;
+	/* Rx pointer ring size */
+	u32 rx_ring_count;
+	/* Tx pointer ring size */
+	u32 tx_ring_count;
+};
+
+struct mctp_client {
+	struct kref ref;
+	struct aspeed_mctp *priv;
+	struct ptr_ring tx_queue;
+	struct ptr_ring rx_queue;
+	struct list_head link;
+	wait_queue_head_t wait_queue;
+};
+
+struct mctp_type_handler {
+	u8 mctp_type;
+	u16 pci_vendor_id;
+	u16 vdm_type;
+	u16 vdm_mask;
+	struct mctp_client *client;
+	struct list_head link;
+};
+
+union aspeed_mctp_eid_data_info {
+	struct aspeed_mctp_eid_info eid_info;
+	struct aspeed_mctp_eid_ext_info eid_ext_info;
+};
+
+enum mctp_address_type {
+	ASPEED_MCTP_GENERIC_ADDR_FORMAT = 0,
+	ASPEED_MCTP_EXTENDED_ADDR_FORMAT = 1
+};
+
+struct aspeed_mctp_endpoint {
+	union  aspeed_mctp_eid_data_info data;
+	struct list_head link;
+};
+
+struct kmem_cache *packet_cache;
+
+void data_dump(struct aspeed_mctp *priv, struct mctp_pcie_packet_data *data)
+{
+	int i;
+
+	dev_dbg(priv->dev, "Address %08x", (u32)data);
+	dev_dbg(priv->dev, "VDM header:");
+	for (i = 0; i < PCIE_VDM_HDR_SIZE_DW; i++) {
+		dev_dbg(priv->dev, "%02x %02x %02x %02x", data->hdr[i] & 0xff,
+		       (data->hdr[i] >> 8) & 0xff,
+		       (data->hdr[i] >> 16) & 0xff,
+		       (data->hdr[i] >> 24) & 0xff);
+	}
+	dev_dbg(priv->dev, "Data payload:");
+	for (i = 0; i < PCIE_VDM_DATA_SIZE_DW; i++) {
+		dev_dbg(priv->dev, "%02x %02x %02x %02x", data->payload[i] & 0xff,
+		       (data->payload[i] >> 8) & 0xff,
+		       (data->payload[i] >> 16) & 0xff,
+		       (data->payload[i] >> 24) & 0xff);
+	}
+}
+
+void *aspeed_mctp_packet_alloc(gfp_t flags)
+{
+	return kmem_cache_alloc(packet_cache, flags);
+}
+EXPORT_SYMBOL_GPL(aspeed_mctp_packet_alloc);
+
+void aspeed_mctp_packet_free(void *packet)
+{
+	kmem_cache_free(packet_cache, packet);
+}
+EXPORT_SYMBOL_GPL(aspeed_mctp_packet_free);
+
+static u16 _get_bdf(struct aspeed_mctp *priv)
+{
+	u32 reg;
+	u16 bdf;
+
+	regmap_read(priv->pcie.map, ASPEED_PCIE_LINK, &reg);
+	if (!(reg & PCIE_LINK_STS))
+		return 0;
+	regmap_read(priv->pcie.map, ASPEED_PCIE_MISC_STS_1, &reg);
+
+	reg = reg & (PCI_BUS_NUM_MASK | PCI_DEV_NUM_MASK);
+	bdf = PCI_DEVID(GET_PCI_BUS_NUM(reg), GET_PCI_DEV_NUM(reg));
+
+	return bdf;
+}
+
+static uint32_t chip_version(struct device *dev)
+{
+	struct regmap *scu;
+	u32 revid0, revid1;
+
+	scu = syscon_regmap_lookup_by_phandle(dev->of_node, "aspeed,scu");
+	if (IS_ERR(scu)) {
+		dev_err(dev, "failed to find 2600 SCU regmap\n");
+		return PTR_ERR(scu);
+	}
+	regmap_read(scu, ASPEED_REVISION_ID0, &revid0);
+	regmap_read(scu, ASPEED_REVISION_ID1, &revid1);
+	if (revid0 == ID0_AST2600A3 && revid1 == ID1_AST2600A3) {
+		/* AST2600-A3 */
+		return ASPEED_MCTP_2600A3;
+	} else if (revid0 == ID0_AST2620A3 && revid1 == ID1_AST2620A3) {
+		/* AST2620-A3 */
+		return ASPEED_MCTP_2600A3;
+	} else if (revid0 == ID0_AST2605A3 && revid1 == ID1_AST2605A3) {
+		/* AST2605-A3 */
+		return ASPEED_MCTP_2600A3;
+	} else if (revid0 == ID0_AST2625A3 && revid1 == ID1_AST2625A3) {
+		/* AST2605-A3 */
+		return ASPEED_MCTP_2600A3;
+	}
+	return ASPEED_MCTP_2600;
+}
+
+/*
+ * HW produces and expects VDM header in little endian and payload in network order.
+ * To allow userspace to use network order for the whole packet, PCIe VDM header needs
+ * to be swapped.
+ */
+static void aspeed_mctp_swap_pcie_vdm_hdr(struct mctp_pcie_packet_data *data)
+{
+	int i;
+
+	for (i = 0; i < PCIE_VDM_HDR_SIZE_DW; i++)
+		data->hdr[i] = swab32(data->hdr[i]);
+}
+
+static void aspeed_mctp_rx_trigger(struct mctp_channel *rx)
+{
+	struct aspeed_mctp *priv = container_of(rx, typeof(*priv), rx);
+	u32 reg;
+
+	/*
+	 * Even though rx_buf_addr doesn't change, if we don't do the write
+	 * here, the HW doesn't trigger RX. We're also clearing the
+	 * RX_CMD_READY bit, otherwise we're observing a rare case where
+	 * trigger isn't registered by the HW, and we're ending up with stuck
+	 * HW (not reacting to wr_ptr writes).
+	 * Also, note that we're writing 0 as wr_ptr. If we're writing other
+	 * value, the HW behaves in a bizarre way that's hard to explain...
+	 */
+	regmap_update_bits(priv->map, ASPEED_MCTP_CTRL, RX_CMD_READY, 0);
+	if (priv->match_data->fifo_auto_surround) {
+		regmap_write(priv->map, ASPEED_MCTP_RX_BUF_ADDR,
+			     rx->cmd.dma_handle);
+	} else {
+		regmap_read(priv->map, ASPEED_MCTP_RX_BUF_ADDR, &reg);
+		if (!reg) {
+			regmap_write(priv->map, ASPEED_MCTP_RX_BUF_ADDR,
+				     rx->cmd.dma_handle);
+		} else if (reg == (rx->cmd.dma_handle & GENMASK(28, 3))) {
+			dev_info(priv->dev,
+				 "Already initialized - skipping rx dma set\n");
+		} else {
+			dev_err(priv->dev,
+				"The memory of rx dma can't be changed after the controller is activated\n");
+			return;
+		}
+	}
+	regmap_write(priv->map, ASPEED_MCTP_RX_BUF_WR_PTR, 0);
+
+	/* After re-enabling RX we need to restart WA logic */
+	if (priv->rx_runaway_wa.enable)
+		priv->rx.buffer_count = priv->rx_packet_count;
+	/*
+	 * When Rx warmup MCTP controller may store first packet into the 0th to the
+	 * 3rd cmd. In ast2600 A3, If the packet isn't stored in the 0th cmd we need
+	 * to change the rx buffer size to avoid rx runaway in first loop. In ast2600
+	 * A1/A2, after first loop hardware is guaranteed to use (RX_PACKET_COUNT - 4)
+	 * buffers.
+	 */
+	priv->rx_warmup = true;
+	priv->rx_runaway_wa.first_loop = true;
+	priv->rx_runaway_wa.packet_counter = 0;
+
+	regmap_update_bits(priv->map, ASPEED_MCTP_CTRL, RX_CMD_READY,
+			   RX_CMD_READY);
+}
+
+static void aspeed_mctp_tx_trigger(struct mctp_channel *tx, bool notify)
+{
+	struct aspeed_mctp *priv = container_of(tx, typeof(*priv), tx);
+
+	if (notify) {
+		struct aspeed_mctp_tx_cmd *last_cmd;
+
+		last_cmd = (struct aspeed_mctp_tx_cmd *)tx->cmd.vaddr +
+			   (tx->wr_ptr - 1) % TX_PACKET_COUNT;
+		last_cmd->tx_lo |= TX_INTERRUPT_AFTER_CMD;
+	}
+	if (priv->match_data->fifo_auto_surround)
+		regmap_write(priv->map, ASPEED_MCTP_TX_BUF_WR_PTR, tx->wr_ptr);
+	regmap_update_bits(priv->map, ASPEED_MCTP_CTRL, TX_CMD_TRIGGER,
+			   TX_CMD_TRIGGER);
+}
+
+static void aspeed_mctp_tx_cmd_prep(u32 *tx_hdr, struct aspeed_mctp_tx_cmd *tx_cmd)
+{
+	u32 packet_size, target_id;
+	u8 dest_eid, padding_len, routing_type, tag_owner;
+
+	packet_size = FIELD_GET(MCTP_HDR_DW_LE_PACKET_SIZE, tx_hdr[0]);
+	routing_type = FIELD_GET(MCTP_HDR_DW_LE_ROUTING_TYPE, tx_hdr[0]);
+	routing_type = routing_type ? routing_type - 1 : 0;
+	padding_len = FIELD_GET(MCTP_HDR_DW_LE_PADDING_LEN, tx_hdr[1]);
+	target_id = FIELD_GET(MCTP_HDR_DW_LE_TARGET_ID, tx_hdr[2]);
+	tag_owner = FIELD_GET(MCTP_HDR_DW_LE_TAG_OWNER, tx_hdr[3]);
+	dest_eid = FIELD_GET(MCTP_HDR_DW_LE_DEST_EID, tx_hdr[3]);
+
+	tx_cmd->tx_hi = FIELD_PREP(TX_PACKET_DEST_EID, dest_eid);
+	tx_cmd->tx_lo = FIELD_PREP(TX_PACKET_TARGET_ID, target_id) |
+			TX_INTERRUPT_AFTER_CMD |
+			FIELD_PREP(TX_PACKET_ROUTING_TYPE, routing_type) |
+			FIELD_PREP(TX_PACKET_TAG_OWNER, tag_owner) |
+			TX_PACKET_SIZE_2500(packet_size) |
+			FIELD_PREP(TX_PACKET_PADDING_LEN, padding_len);
+}
+
+static void aspeed_mctp_emit_tx_cmd(struct mctp_channel *tx,
+				    struct mctp_pcie_packet *packet)
+{
+	struct aspeed_mctp *priv = container_of(tx, typeof(*priv), tx);
+	struct aspeed_mctp_tx_cmd *tx_cmd =
+		(struct aspeed_mctp_tx_cmd *)tx->cmd.vaddr + tx->wr_ptr;
+	u32 packet_sz_dw = packet->size / sizeof(u32) -
+		sizeof(packet->data.hdr) / sizeof(u32);
+	u32 offset;
+
+	data_dump(priv, &packet->data);
+	aspeed_mctp_swap_pcie_vdm_hdr(&packet->data);
+
+	if (priv->match_data->vdm_hdr_direct_xfer) {
+		offset = tx->wr_ptr * sizeof(packet->data);
+		memcpy((u8 *)tx->data.vaddr + offset, &packet->data,
+		sizeof(packet->data));
+
+		tx_cmd->tx_lo = TX_PACKET_SIZE(packet_sz_dw);
+		tx_cmd->tx_hi = TX_RESERVED_1;
+		tx_cmd->tx_hi |= TX_DATA_ADDR(tx->data.dma_handle + offset);
+	} else {
+		offset = tx->wr_ptr * sizeof(struct mctp_pcie_packet_data_2500);
+		memcpy((u8 *)tx->data.vaddr + offset, packet->data.payload,
+		       sizeof(packet->data.payload));
+		aspeed_mctp_tx_cmd_prep(packet->data.hdr, tx_cmd);
+		tx_cmd->tx_hi |= TX_DATA_ADDR_2500(tx->data.dma_handle + offset);
+		if (tx->wr_ptr == TX_PACKET_COUNT - 1)
+			tx_cmd->tx_hi |= TX_LAST_CMD;
+	}
+	dev_dbg(priv->dev, "tx->wr_prt: %d, tx_cmd: hi:%08x lo:%08x\n",
+		 tx->wr_ptr, tx_cmd->tx_hi, tx_cmd->tx_lo);
+
+	tx->wr_ptr = (tx->wr_ptr + 1) % TX_PACKET_COUNT;
+}
+
+static struct mctp_client *aspeed_mctp_client_alloc(struct aspeed_mctp *priv)
+{
+	struct mctp_client *client;
+
+	client = kzalloc(sizeof(*client), GFP_KERNEL);
+	if (!client)
+		goto out;
+
+	kref_init(&client->ref);
+	client->priv = priv;
+	ptr_ring_init(&client->tx_queue, priv->tx_ring_count, GFP_KERNEL);
+	ptr_ring_init(&client->rx_queue, priv->rx_ring_count, GFP_ATOMIC);
+
+out:
+	return client;
+}
+
+static void aspeed_mctp_client_free(struct kref *ref)
+{
+	struct mctp_client *client = container_of(ref, typeof(*client), ref);
+
+	ptr_ring_cleanup(&client->rx_queue, &aspeed_mctp_packet_free);
+	ptr_ring_cleanup(&client->tx_queue, &aspeed_mctp_packet_free);
+
+	kfree(client);
+}
+
+static void aspeed_mctp_client_get(struct mctp_client *client)
+{
+	lockdep_assert_held(&client->priv->clients_lock);
+
+	kref_get(&client->ref);
+}
+
+static void aspeed_mctp_client_put(struct mctp_client *client)
+{
+	kref_put(&client->ref, &aspeed_mctp_client_free);
+}
+
+static struct mctp_client *
+aspeed_mctp_find_handler(struct aspeed_mctp *priv,
+			 struct mctp_pcie_packet *packet)
+{
+	struct mctp_type_handler *handler;
+	u8 *hdr = (u8 *)packet->data.hdr;
+	struct mctp_client *client = NULL;
+	u8 mctp_type, som_eom;
+	u16 vendor = 0;
+	u16 vdm_type = 0;
+
+	lockdep_assert_held(&priv->clients_lock);
+
+	/*
+	 * Middle and EOM fragments cannot be matched to MCTP type.
+	 * For consistency do not match type for any fragmented messages.
+	 */
+	som_eom = hdr[MCTP_HDR_TAG_OFFSET] & MCTP_HDR_SOM_EOM;
+	if (som_eom != MCTP_HDR_SOM_EOM)
+		return NULL;
+
+	mctp_type = hdr[MCTP_HDR_TYPE_OFFSET];
+	if (mctp_type == MCTP_HDR_TYPE_VDM_PCI) {
+		vendor = *((u16 *)&hdr[MCTP_HDR_VENDOR_OFFSET]);
+		vdm_type = *((u16 *)&hdr[MCTP_HDR_VDM_TYPE_OFFSET]);
+	}
+
+	list_for_each_entry(handler, &priv->mctp_type_handlers, link) {
+		if (handler->mctp_type == mctp_type &&
+		    handler->pci_vendor_id == vendor &&
+		    handler->vdm_type == (vdm_type & handler->vdm_mask)) {
+			dev_dbg(priv->dev, "Found client for type %x vdm %x\n",
+				mctp_type, handler->vdm_type);
+			client = handler->client;
+			break;
+		}
+	}
+	return client;
+}
+
+static void aspeed_mctp_dispatch_packet(struct aspeed_mctp *priv,
+					struct mctp_pcie_packet *packet)
+{
+	struct mctp_client *client;
+	int ret;
+
+	spin_lock(&priv->clients_lock);
+
+	client = aspeed_mctp_find_handler(priv, packet);
+
+	if (!client)
+		client = priv->default_client;
+
+	if (client)
+		aspeed_mctp_client_get(client);
+
+	spin_unlock(&priv->clients_lock);
+
+	if (client) {
+		ret = ptr_ring_produce(&client->rx_queue, packet);
+		if (ret) {
+			/*
+			 * This can happen if client process does not
+			 * consume packets fast enough
+			 */
+			dev_dbg(priv->dev, "Failed to store packet in client RX queue\n");
+			aspeed_mctp_packet_free(packet);
+		} else {
+			wake_up_all(&client->wait_queue);
+		}
+		aspeed_mctp_client_put(client);
+	} else {
+		dev_dbg(priv->dev, "Failed to dispatch RX packet\n");
+		aspeed_mctp_packet_free(packet);
+	}
+}
+
+static void aspeed_mctp_tx_tasklet(unsigned long data)
+{
+	struct mctp_channel *tx = (struct mctp_channel *)data;
+	struct aspeed_mctp *priv = container_of(tx, typeof(*priv), tx);
+	struct mctp_client *client;
+	bool trigger = false;
+	bool full = false;
+	u32 rd_ptr;
+
+	if (priv->match_data->fifo_auto_surround) {
+		regmap_write(priv->map, ASPEED_MCTP_TX_BUF_RD_PTR, UPDATE_RX_RD_PTR);
+		regmap_read(priv->map, ASPEED_MCTP_TX_BUF_RD_PTR, &rd_ptr);
+		rd_ptr &= TX_BUF_RD_PTR_MASK;
+	} else {
+		rd_ptr = tx->rd_ptr;
+	}
+
+	spin_lock(&priv->clients_lock);
+
+	list_for_each_entry(client, &priv->clients, link) {
+		while (!(full = (tx->wr_ptr + 1) % TX_PACKET_COUNT == rd_ptr)) {
+			struct mctp_pcie_packet *packet;
+
+			packet = ptr_ring_consume(&client->tx_queue);
+			if (!packet)
+				break;
+
+			aspeed_mctp_emit_tx_cmd(tx, packet);
+			aspeed_mctp_packet_free(packet);
+			trigger = true;
+		}
+	}
+
+	spin_unlock(&priv->clients_lock);
+
+	if (trigger)
+		aspeed_mctp_tx_trigger(tx, full);
+}
+
+void aspeed_mctp_rx_hdr_prep(struct aspeed_mctp *priv, u8 *hdr, u32 rx_lo)
+{
+	u16 bdf;
+	u8 routing_type;
+
+	/*
+	 * MCTP controller will map the routing type to reduce one bit
+	 * 0 (Route to RC) -> 0,
+	 * 2 (Route by ID) -> 1,
+	 * 3 (Broadcast from RC) -> 2
+	 */
+	routing_type = FIELD_GET(RX_PACKET_ROUTING_TYPE, rx_lo);
+	routing_type = routing_type ? routing_type + 1 : 0;
+	bdf = _get_bdf(priv);
+	/* Length[7:0] */
+	hdr[0] = FIELD_GET(RX_PACKET_SIZE, rx_lo);
+	/* TD:EP:ATTR[1:0]:R or AT[1:0]:Length[9:8] */
+	hdr[1] = 0;
+	/* R or T9:TC[2:0]:R[3:0] */
+	hdr[2] = 0;
+	/* R or Fmt[2]:Fmt[1:0]=b'11:Type[4:3]=b'10:Type[2:0] */
+	hdr[3] = 0x70 | routing_type;
+	/* VDM message code = 0x7f */
+	hdr[4] = 0x7f;
+	/* R[1:0]:Pad len[1:0]:MCTP VDM Code[3:0] */
+	hdr[5] = FIELD_GET(RX_PACKET_PADDING_LEN, rx_lo) << 4;
+	/* TODO: PCI Requester ID: HW didn't get this information */
+	hdr[6] = 0;
+	hdr[7] = 5;
+	/* Vendor ID: 0x1AB4 */
+	hdr[8] = 0xb4;
+	hdr[9] = 0x1a;
+	/* PCI Target ID */
+	hdr[10] = bdf & 0xff;
+	hdr[11] = bdf >> 8 & 0xff;
+	/* SOM:EOM:Pkt Seq#[1:0]:TO:Msg Tag[2:0]*/
+	hdr[12] = FIELD_GET(RX_PACKET_SOM, rx_lo) << 7 |
+		  FIELD_GET(RX_PACKET_EOM, rx_lo) << 6 |
+		  FIELD_GET(RX_PACKET_SEQ_NUMBER, rx_lo) << 4 |
+		  FIELD_GET(RX_PACKET_TAG_OWNER, rx_lo) << 3 |
+		  FIELD_GET(RX_PACKET_MSG_TAG, rx_lo);
+	/* Source Endpoint ID */
+	hdr[13] = FIELD_GET(RX_PACKET_SRC_EID, rx_lo);
+	/* Destination Endpoint ID: HW didn't get this information*/
+	hdr[14] = priv->eid;
+	/* TODO: R[3:0]: header version[3:0] */
+	hdr[15] = 1;
+}
+
+static void aspeed_mctp_rx_tasklet(unsigned long data)
+{
+	struct mctp_channel *rx = (struct mctp_channel *)data;
+	struct aspeed_mctp *priv = container_of(rx, typeof(*priv), rx);
+	struct mctp_pcie_packet *rx_packet;
+	struct aspeed_mctp_rx_cmd *rx_cmd;
+	u32 hw_read_ptr;
+	u32 *hdr, *payload;
+
+	if (priv->match_data->vdm_hdr_direct_xfer && priv->match_data->fifo_auto_surround) {
+		struct mctp_pcie_packet_data *rx_buf;
+		u32 residual_cmds = 0;
+
+		/* Trigger HW read pointer update, must be done before RX loop */
+		regmap_write(priv->map, ASPEED_MCTP_RX_BUF_RD_PTR, UPDATE_RX_RD_PTR);
+
+		/*
+		 * XXX: Using rd_ptr obtained from HW is unreliable so we need to
+		 * maintain the state of buffer on our own by peeking into the buffer
+		 * and checking where the packet was written.
+		 */
+		rx_buf = (struct mctp_pcie_packet_data *)rx->data.vaddr;
+		hdr = (u32 *)&rx_buf[rx->wr_ptr];
+		if (!*hdr && priv->rx_warmup) {
+			u32 tmp_wr_ptr = rx->wr_ptr;
+
+			/*
+			 * HACK: Right after start the RX hardware can put received
+			 * packet into an unexpected offset - in order to locate
+			 * received packet driver has to scan all RX data buffers.
+			 */
+			do {
+				tmp_wr_ptr = (tmp_wr_ptr + 1) % priv->rx_packet_count;
+
+				hdr = (u32 *)&rx_buf[tmp_wr_ptr];
+			} while (!*hdr && tmp_wr_ptr != rx->wr_ptr);
+
+			if (tmp_wr_ptr != rx->wr_ptr) {
+				dev_warn(priv->dev, "Runaway RX packet found %d -> %d\n",
+					rx->wr_ptr, tmp_wr_ptr);
+				residual_cmds = abs(tmp_wr_ptr - rx->wr_ptr);
+				rx->wr_ptr = tmp_wr_ptr;
+				if (!priv->rx_runaway_wa.enable &&
+				    priv->rx_warmup)
+					regmap_write(priv->map,
+						     ASPEED_MCTP_RX_BUF_SIZE,
+						     rx->buffer_count -
+							     residual_cmds);
+				priv->rx_warmup = false;
+			}
+		} else {
+			priv->rx_warmup = false;
+		}
+
+		if (priv->rx_runaway_wa.packet_counter > priv->rx_packet_count &&
+		    priv->rx_runaway_wa.first_loop) {
+			if (priv->rx_runaway_wa.enable)
+				/*
+				 * Once we receive RX_PACKET_COUNT packets, hardware is
+				 * guaranteed to use (RX_PACKET_COUNT - 4) buffers. Decrease
+				 * buffer count by 4, then we can turn off scanning of RX
+				 * buffers. RX buffer scanning should be enabled every time
+				 * RX hardware is started.
+				 * This is just a performance optimization - we could keep
+				 * scanning RX buffers forever, but under heavy traffic it is
+				 * fairly common that rx_tasklet is executed while RX buffer
+				 * ring is empty.
+				 */
+				rx->buffer_count = priv->rx_packet_count - 4;
+			else
+				/*
+				 * Once we receive RX_PACKET_COUNT packets, we need to restore the
+				 * RX buffer size to 4 byte aligned value to avoid rx runaway.
+				 */
+				regmap_write(priv->map, ASPEED_MCTP_RX_BUF_SIZE,
+				     rx->buffer_count);
+			priv->rx_runaway_wa.first_loop = false;
+		}
+
+		while (*hdr != 0) {
+			if (FIELD_GET(MCTP_HDR_DW_LE_PACKET_SIZE, hdr[0]) * 4 >
+			    ASPEED_MCTP_MTU)
+				dev_warn(priv->dev,
+					 "Rx length %ld > MTU size %d\n",
+					 FIELD_GET(MCTP_HDR_DW_LE_PACKET_SIZE,
+						   hdr[0]) *
+						 4,
+					 ASPEED_MCTP_MTU);
+			rx_packet = aspeed_mctp_packet_alloc(GFP_ATOMIC);
+			if (rx_packet) {
+				memcpy(&rx_packet->data, hdr, sizeof(rx_packet->data));
+
+				aspeed_mctp_swap_pcie_vdm_hdr(&rx_packet->data);
+
+				aspeed_mctp_dispatch_packet(priv, rx_packet);
+			} else {
+				dev_dbg(priv->dev, "Failed to allocate RX packet\n");
+			}
+			data_dump(priv, &rx_packet->data);
+			*hdr = 0;
+			rx->wr_ptr = (rx->wr_ptr + 1) % rx->buffer_count;
+			hdr = (u32 *)&rx_buf[rx->wr_ptr];
+
+			priv->rx_runaway_wa.packet_counter++;
+		}
+
+		/*
+		 * Update HW write pointer, this can be done only after driver consumes
+		 * packets from RX ring.
+		 */
+		regmap_read(priv->map, ASPEED_MCTP_RX_BUF_RD_PTR, &hw_read_ptr);
+		hw_read_ptr &= RX_BUF_RD_PTR_MASK;
+		regmap_write(priv->map, ASPEED_MCTP_RX_BUF_WR_PTR, (hw_read_ptr));
+
+		dev_dbg(priv->dev, "RX hw ptr %02d, sw ptr %2d\n",
+			hw_read_ptr, rx->wr_ptr);
+	} else {
+		struct mctp_pcie_packet_data_2500 *rx_buf;
+
+		rx_buf = (struct mctp_pcie_packet_data_2500 *)rx->data.vaddr;
+		payload = (u32 *)&rx_buf[rx->wr_ptr];
+		rx_cmd = (struct aspeed_mctp_rx_cmd *)rx->cmd.vaddr;
+		hdr = (u32 *)&((rx_cmd + rx->wr_ptr)->rx_lo);
+
+		if (!*hdr) {
+			u32 tmp_wr_ptr = rx->wr_ptr;
+
+			/*
+			 * HACK: Right after start the RX hardware can put received
+			 * packet into an unexpected offset - in order to locate
+			 * received packet driver has to scan all RX data buffers.
+			 */
+			do {
+				tmp_wr_ptr = (tmp_wr_ptr + 1) % rx->buffer_count;
+
+				hdr = (u32 *)&((rx_cmd + tmp_wr_ptr)->rx_lo);
+			} while (!*hdr && tmp_wr_ptr != rx->wr_ptr);
+
+			if (tmp_wr_ptr != rx->wr_ptr) {
+				dev_warn(priv->dev,
+					 "Runaway RX packet found %d -> %d\n",
+					 rx->wr_ptr, tmp_wr_ptr);
+				rx->wr_ptr = tmp_wr_ptr;
+			}
+		}
+
+		while (*hdr != 0) {
+			rx_packet = aspeed_mctp_packet_alloc(GFP_ATOMIC);
+			if (rx_packet) {
+				memcpy(rx_packet->data.payload, payload,
+				       sizeof(rx_packet->data.payload));
+
+				aspeed_mctp_rx_hdr_prep(priv, (u8 *)rx_packet->data.hdr, *hdr);
+
+				aspeed_mctp_swap_pcie_vdm_hdr(&rx_packet->data);
+
+				aspeed_mctp_dispatch_packet(priv, rx_packet);
+			} else {
+				dev_dbg(priv->dev, "Failed to allocate RX packet\n");
+			}
+			dev_dbg(priv->dev,
+				"rx->wr_ptr = %d, rx_cmd->rx_lo = %08x",
+				rx->wr_ptr, *hdr);
+			data_dump(priv, &rx_packet->data);
+			*hdr = 0;
+			rx->wr_ptr = (rx->wr_ptr + 1) % rx->buffer_count;
+			payload = (u32 *)&rx_buf[rx->wr_ptr];
+			hdr = (u32 *)&((rx_cmd + rx->wr_ptr)->rx_lo);
+		}
+	}
+
+	/* Kick RX if it was stopped due to ring full condition */
+	if (rx->stopped) {
+		regmap_update_bits(priv->map, ASPEED_MCTP_CTRL, RX_CMD_READY,
+				   RX_CMD_READY);
+		rx->stopped = false;
+	}
+}
+
+static void aspeed_mctp_rx_chan_init(struct mctp_channel *rx)
+{
+	struct aspeed_mctp *priv = container_of(rx, typeof(*priv), rx);
+	u32 *rx_cmd = (u32 *)rx->cmd.vaddr;
+	struct aspeed_mctp_rx_cmd *rx_cmd_64 =
+		(struct aspeed_mctp_rx_cmd *)rx->cmd.vaddr;
+	u32 data_size = priv->match_data->packet_unit_size;
+	u32 hw_rx_count = priv->rx_packet_count;
+	struct mctp_pcie_packet_data *rx_buf = (struct mctp_pcie_packet_data *)rx->data.vaddr;
+	int i;
+
+	if (priv->match_data->vdm_hdr_direct_xfer) {
+		for (i = 0; i < priv->rx_packet_count; i++) {
+			*rx_cmd = RX_DATA_ADDR(rx->data.dma_handle + data_size * i);
+			*rx_cmd |= RX_INTERRUPT_AFTER_CMD;
+			rx_cmd++;
+		}
+	} else {
+		for (i = 0; i < priv->rx_packet_count; i++) {
+			rx_cmd_64->rx_hi = RX_DATA_ADDR_2500(
+				rx->data.dma_handle + data_size * i);
+			rx_cmd_64->rx_lo = 0;
+			if (i == priv->rx_packet_count - 1)
+				rx_cmd_64->rx_hi |= RX_LAST_CMD;
+			rx_cmd_64++;
+		}
+	}
+	/* Clear the header of rx data */
+	for (i = 0; i < priv->rx_packet_count; i++)
+		*(u32 *)&rx_buf[i] = 0;
+	rx->wr_ptr = 0;
+	rx->buffer_count = priv->rx_packet_count;
+	if (priv->match_data->fifo_auto_surround) {
+		/*
+		 * TODO: Once read pointer runaway bug is fixed in some future AST2x00
+		 * stepping then add chip revision detection and turn on this
+		 * workaround only when needed
+		 */
+		priv->rx_runaway_wa.enable =
+			(chip_version(priv->dev) == ASPEED_MCTP_2600) ? true : false;
+
+		/*
+		 * Hardware does not wrap around ASPEED_MCTP_RX_BUF_SIZE
+		 * correctly - we have to set number of buffers to n/4 -1
+		 */
+		if (priv->rx_runaway_wa.enable)
+			hw_rx_count = (priv->rx_packet_count / 4 - 1);
+
+		regmap_write(priv->map, ASPEED_MCTP_RX_BUF_SIZE, hw_rx_count);
+	}
+}
+
+static void aspeed_mctp_tx_chan_init(struct mctp_channel *tx)
+{
+	struct aspeed_mctp *priv = container_of(tx, typeof(*priv), tx);
+
+	tx->wr_ptr = 0;
+	tx->rd_ptr = 0;
+	regmap_update_bits(priv->map, ASPEED_MCTP_CTRL, TX_CMD_TRIGGER, 0);
+	regmap_write(priv->map, ASPEED_MCTP_TX_BUF_ADDR, tx->cmd.dma_handle);
+	if (priv->match_data->fifo_auto_surround) {
+		regmap_write(priv->map, ASPEED_MCTP_TX_BUF_SIZE, TX_PACKET_COUNT);
+		regmap_write(priv->map, ASPEED_MCTP_TX_BUF_WR_PTR, 0);
+	}
+}
+
+struct mctp_client *aspeed_mctp_create_client(struct aspeed_mctp *priv)
+{
+	struct mctp_client *client;
+
+	client = aspeed_mctp_client_alloc(priv);
+	if (!client)
+		return NULL;
+
+	init_waitqueue_head(&client->wait_queue);
+
+	spin_lock_bh(&priv->clients_lock);
+	list_add_tail(&client->link, &priv->clients);
+	spin_unlock_bh(&priv->clients_lock);
+
+	return client;
+}
+EXPORT_SYMBOL_GPL(aspeed_mctp_create_client);
+
+static int aspeed_mctp_open(struct inode *inode, struct file *file)
+{
+	struct miscdevice *misc = file->private_data;
+	struct platform_device *pdev = to_platform_device(misc->parent);
+	struct aspeed_mctp *priv = platform_get_drvdata(pdev);
+	struct mctp_client *client;
+
+	client = aspeed_mctp_create_client(priv);
+	if (!client)
+		return -ENOMEM;
+
+	file->private_data = client;
+
+	return 0;
+}
+
+void aspeed_mctp_delete_client(struct mctp_client *client)
+{
+	struct aspeed_mctp *priv = client->priv;
+	struct mctp_type_handler *handler, *tmp;
+
+	spin_lock_bh(&priv->clients_lock);
+
+	list_del(&client->link);
+
+	if (priv->default_client == client)
+		priv->default_client = NULL;
+
+	list_for_each_entry_safe(handler, tmp, &priv->mctp_type_handlers,
+				 link) {
+		if (handler->client == client) {
+			list_del(&handler->link);
+			kfree(handler);
+		}
+	}
+	spin_unlock_bh(&priv->clients_lock);
+
+	/* Disable the tasklet to appease lockdep */
+	local_bh_disable();
+	aspeed_mctp_client_put(client);
+	local_bh_enable();
+}
+EXPORT_SYMBOL_GPL(aspeed_mctp_delete_client);
+
+static int aspeed_mctp_release(struct inode *inode, struct file *file)
+{
+	struct mctp_client *client = file->private_data;
+
+	aspeed_mctp_delete_client(client);
+
+	return 0;
+}
+
+#define LEN_MASK_HI GENMASK(9, 8)
+#define LEN_MASK_LO GENMASK(7, 0)
+#define PCI_VDM_HDR_LEN_MASK_LO GENMASK(31, 24)
+#define PCI_VDM_HDR_LEN_MASK_HI GENMASK(17, 16)
+#define PCIE_VDM_HDR_REQUESTER_BDF_MASK GENMASK(31, 16)
+
+int aspeed_mctp_send_packet(struct mctp_client *client,
+			    struct mctp_pcie_packet *packet)
+{
+	struct aspeed_mctp *priv = client->priv;
+	u32 *hdr_dw = (u32 *)packet->data.hdr;
+	u8 *hdr = (u8 *)packet->data.hdr;
+	u16 packet_data_sz_dw;
+	u16 pci_data_len_dw;
+	int ret;
+	u16 bdf;
+
+	bdf = _get_bdf(priv);
+	if (bdf == 0)
+		return -EIO;
+
+	/*
+	 * If the data size is different from contents of PCIe VDM header,
+	 * aspeed_mctp_tx_cmd will be programmed incorrectly. This may cause
+	 * MCTP HW to stop working.
+	 */
+	pci_data_len_dw = FIELD_PREP(LEN_MASK_LO, FIELD_GET(PCI_VDM_HDR_LEN_MASK_LO, hdr_dw[0])) |
+			FIELD_PREP(LEN_MASK_HI, FIELD_GET(PCI_VDM_HDR_LEN_MASK_HI, hdr_dw[0]));
+	if (pci_data_len_dw == 0) /* According to PCIe Spec, 0 means 1024 DW */
+		pci_data_len_dw = SZ_1K;
+
+	packet_data_sz_dw = packet->size / sizeof(u32) - sizeof(packet->data.hdr) / sizeof(u32);
+	if (packet_data_sz_dw != pci_data_len_dw)
+		return -EINVAL;
+
+	be32p_replace_bits(&hdr_dw[1], bdf, PCIE_VDM_HDR_REQUESTER_BDF_MASK);
+
+	/*
+	 * XXX Don't update EID for MCTP Control messages - old EID may
+	 * interfere with MCTP discovery flow.
+	 */
+	if (priv->eid && hdr[MCTP_HDR_TYPE_OFFSET] != MCTP_HDR_TYPE_CONTROL)
+		hdr[MCTP_HDR_SRC_EID_OFFSET] = priv->eid;
+
+	ret = ptr_ring_produce_bh(&client->tx_queue, packet);
+	if (!ret)
+		tasklet_hi_schedule(&priv->tx.tasklet);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(aspeed_mctp_send_packet);
+
+struct mctp_pcie_packet *aspeed_mctp_receive_packet(struct mctp_client *client,
+						    unsigned long timeout)
+{
+	struct aspeed_mctp *priv = client->priv;
+	u16 bdf = _get_bdf(priv);
+	int ret;
+
+	if (bdf == 0)
+		return ERR_PTR(-EIO);
+
+	ret = wait_event_interruptible_timeout(client->wait_queue,
+					       __ptr_ring_peek(&client->rx_queue),
+					       timeout);
+	if (ret < 0)
+		return ERR_PTR(ret);
+	else if (ret == 0)
+		return ERR_PTR(-ETIME);
+
+	return ptr_ring_consume_bh(&client->rx_queue);
+}
+EXPORT_SYMBOL_GPL(aspeed_mctp_receive_packet);
+
+void aspeed_mctp_flush_rx_queue(struct mctp_client *client)
+{
+	struct mctp_pcie_packet *packet;
+
+	while ((packet = ptr_ring_consume_bh(&client->rx_queue)))
+		aspeed_mctp_packet_free(packet);
+}
+EXPORT_SYMBOL_GPL(aspeed_mctp_flush_rx_queue);
+
+static ssize_t aspeed_mctp_read(struct file *file, char __user *buf,
+				size_t count, loff_t *ppos)
+{
+	struct mctp_client *client = file->private_data;
+	struct aspeed_mctp *priv = client->priv;
+	struct mctp_pcie_packet *rx_packet;
+	u32 mctp_ctrl;
+	u32 mctp_int_sts;
+
+	if (count < PCIE_MCTP_MIN_PACKET_SIZE)
+		return -EINVAL;
+
+	if (count > sizeof(rx_packet->data))
+		count = sizeof(rx_packet->data);
+
+	if (priv->miss_mctp_int) {
+		regmap_read(priv->map, ASPEED_MCTP_CTRL, &mctp_ctrl);
+		if (!(mctp_ctrl & RX_CMD_READY))
+			priv->rx.stopped = true;
+		/* Polling the RX_CMD_RECEIVE_INT to ensure rx_tasklet can find the data */
+		regmap_read(priv->map, ASPEED_MCTP_INT_STS, &mctp_int_sts);
+		if (mctp_int_sts & RX_CMD_RECEIVE_INT)
+			regmap_write(priv->map, ASPEED_MCTP_INT_STS,
+				     mctp_int_sts);
+	}
+
+	tasklet_hi_schedule(&priv->rx.tasklet);
+	rx_packet = ptr_ring_consume_bh(&client->rx_queue);
+	if (!rx_packet)
+		return -EAGAIN;
+
+	if (copy_to_user(buf, &rx_packet->data, count)) {
+		dev_err(priv->dev, "copy to user failed\n");
+		count = -EFAULT;
+	}
+
+	aspeed_mctp_packet_free(rx_packet);
+
+	return count;
+}
+
+static void aspeed_mctp_flush_tx_queue(struct mctp_client *client)
+{
+	struct mctp_pcie_packet *packet;
+
+	while ((packet = ptr_ring_consume_bh(&client->tx_queue)))
+		aspeed_mctp_packet_free(packet);
+}
+
+static void aspeed_mctp_flush_all_tx_queues(struct aspeed_mctp *priv)
+{
+	struct mctp_client *client;
+
+	spin_lock_bh(&priv->clients_lock);
+	list_for_each_entry(client, &priv->clients, link)
+		aspeed_mctp_flush_tx_queue(client);
+	spin_unlock_bh(&priv->clients_lock);
+}
+
+static ssize_t aspeed_mctp_write(struct file *file, const char __user *buf,
+				 size_t count, loff_t *ppos)
+{
+	struct mctp_client *client = file->private_data;
+	struct aspeed_mctp *priv = client->priv;
+	struct mctp_pcie_packet *tx_packet;
+	int ret;
+
+	if (count < PCIE_MCTP_MIN_PACKET_SIZE)
+		return -EINVAL;
+
+	if (count > sizeof(tx_packet->data))
+		return -ENOSPC;
+
+	tx_packet = aspeed_mctp_packet_alloc(GFP_KERNEL);
+	if (!tx_packet) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	if (copy_from_user(&tx_packet->data, buf, count)) {
+		dev_err(priv->dev, "copy from user failed\n");
+		ret = -EFAULT;
+		goto out_packet;
+	}
+
+	tx_packet->size = count;
+
+	ret = aspeed_mctp_send_packet(client, tx_packet);
+	if (ret)
+		goto out_packet;
+
+	return count;
+
+out_packet:
+	aspeed_mctp_packet_free(tx_packet);
+out:
+	return ret;
+}
+
+int aspeed_mctp_add_type_handler(struct mctp_client *client, u8 mctp_type,
+				 u16 pci_vendor_id, u16 vdm_type, u16 vdm_mask)
+{
+	struct aspeed_mctp *priv = client->priv;
+	struct mctp_type_handler *handler, *new_handler;
+	int ret = 0;
+
+	if (mctp_type <= MCTP_HDR_TYPE_BASE_LAST) {
+		/* Vendor, type and type mask must be zero for types 0-5 */
+		if (pci_vendor_id != 0 || vdm_type != 0 || vdm_mask != 0)
+			return -EINVAL;
+	} else if (mctp_type == MCTP_HDR_TYPE_VDM_PCI) {
+		/* For Vendor Defined PCI type the the vendor ID must be nonzero */
+		if (pci_vendor_id == 0 || pci_vendor_id == 0xffff)
+			return -EINVAL;
+	} else {
+		return -EINVAL;
+	}
+
+	new_handler = kzalloc(sizeof(*new_handler), GFP_KERNEL);
+	if (!new_handler)
+		return -ENOMEM;
+	new_handler->mctp_type = mctp_type;
+	new_handler->pci_vendor_id = pci_vendor_id;
+	new_handler->vdm_type = vdm_type & vdm_mask;
+	new_handler->vdm_mask = vdm_mask;
+	new_handler->client = client;
+
+	spin_lock_bh(&priv->clients_lock);
+	list_for_each_entry(handler, &priv->mctp_type_handlers, link) {
+		if (handler->mctp_type == new_handler->mctp_type &&
+		    handler->pci_vendor_id == new_handler->pci_vendor_id &&
+		    handler->vdm_type == new_handler->vdm_type) {
+			if (handler->client != new_handler->client)
+				ret = -EBUSY;
+			kfree(new_handler);
+			goto out_unlock;
+		}
+	}
+	list_add_tail(&new_handler->link, &priv->mctp_type_handlers);
+out_unlock:
+	spin_unlock_bh(&priv->clients_lock);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(aspeed_mctp_add_type_handler);
+
+int aspeed_mctp_remove_type_handler(struct mctp_client *client,
+				    u8 mctp_type, u16 pci_vendor_id,
+				    u16 vdm_type, u16 vdm_mask)
+{
+	struct aspeed_mctp *priv = client->priv;
+	struct mctp_type_handler *handler, *tmp;
+	int ret = -EINVAL;
+
+	vdm_type &= vdm_mask;
+
+	spin_lock_bh(&priv->clients_lock);
+	list_for_each_entry_safe(handler, tmp, &priv->mctp_type_handlers,
+				 link) {
+		if (handler->client == client &&
+		    handler->mctp_type == mctp_type &&
+		    handler->pci_vendor_id == pci_vendor_id &&
+		    handler->vdm_type == vdm_type) {
+			list_del(&handler->link);
+			kfree(handler);
+			ret = 0;
+			break;
+		}
+	}
+	spin_unlock_bh(&priv->clients_lock);
+	return ret;
+}
+
+static int aspeed_mctp_register_default_handler(struct mctp_client *client)
+{
+	struct aspeed_mctp *priv = client->priv;
+	int ret = 0;
+
+	spin_lock_bh(&priv->clients_lock);
+
+	if (!priv->default_client)
+		priv->default_client = client;
+	else if (priv->default_client != client)
+		ret = -EBUSY;
+
+	spin_unlock_bh(&priv->clients_lock);
+
+	return ret;
+}
+
+static int
+aspeed_mctp_register_type_handler(struct mctp_client *client,
+				  void __user *userbuf)
+{
+	struct aspeed_mctp *priv = client->priv;
+	struct aspeed_mctp_type_handler_ioctl handler;
+
+	if (copy_from_user(&handler, userbuf, sizeof(handler))) {
+		dev_err(priv->dev, "copy from user failed\n");
+		return -EFAULT;
+	}
+
+	return aspeed_mctp_add_type_handler(client, handler.mctp_type,
+					    handler.pci_vendor_id,
+					    handler.vendor_type,
+					    handler.vendor_type_mask);
+}
+
+static int
+aspeed_mctp_unregister_type_handler(struct mctp_client *client,
+				    void __user *userbuf)
+{
+	struct aspeed_mctp *priv = client->priv;
+	struct aspeed_mctp_type_handler_ioctl handler;
+
+	if (copy_from_user(&handler, userbuf, sizeof(handler))) {
+		dev_err(priv->dev, "copy from user failed\n");
+		return -EFAULT;
+	}
+
+	return aspeed_mctp_remove_type_handler(client, handler.mctp_type,
+					       handler.pci_vendor_id,
+					       handler.vendor_type,
+					       handler.vendor_type_mask);
+}
+
+static int
+aspeed_mctp_filter_eid(struct aspeed_mctp *priv, void __user *userbuf)
+{
+	struct aspeed_mctp_filter_eid eid;
+
+	if (copy_from_user(&eid, userbuf, sizeof(eid))) {
+		dev_err(priv->dev, "copy from user failed\n");
+		return -EFAULT;
+	}
+
+	if (eid.enable) {
+		regmap_write(priv->map, ASPEED_MCTP_EID, eid.eid);
+		regmap_update_bits(priv->map, ASPEED_MCTP_CTRL,
+				   MATCHING_EID, MATCHING_EID);
+	} else {
+		regmap_update_bits(priv->map, ASPEED_MCTP_CTRL,
+				   MATCHING_EID, 0);
+	}
+	return 0;
+}
+
+static int aspeed_mctp_get_bdf(struct aspeed_mctp *priv, void __user *userbuf)
+{
+	struct aspeed_mctp_get_bdf bdf = { _get_bdf(priv) };
+
+	if (copy_to_user(userbuf, &bdf, sizeof(bdf))) {
+		dev_err(priv->dev, "copy to user failed\n");
+		return -EFAULT;
+	}
+	return 0;
+}
+
+static int
+aspeed_mctp_get_medium_id(struct aspeed_mctp *priv, void __user *userbuf)
+{
+	struct aspeed_mctp_get_medium_id id = { 0x09 }; /* PCIe revision 2.0 */
+
+	if (copy_to_user(userbuf, &id, sizeof(id))) {
+		dev_err(priv->dev, "copy to user failed\n");
+		return -EFAULT;
+	}
+	return 0;
+}
+
+static int
+aspeed_mctp_get_mtu(struct aspeed_mctp *priv, void __user *userbuf)
+{
+	struct aspeed_mctp_get_mtu id = { ASPEED_MCTP_MTU };
+
+	if (copy_to_user(userbuf, &id, sizeof(id))) {
+		dev_err(priv->dev, "copy to user failed\n");
+		return -EFAULT;
+	}
+	return 0;
+}
+
+int aspeed_mctp_get_eid_bdf(struct mctp_client *client, u8 eid, u16 *bdf)
+{
+	struct aspeed_mctp_endpoint *endpoint;
+	int ret = -ENOENT;
+
+	mutex_lock(&client->priv->endpoints_lock);
+	list_for_each_entry(endpoint, &client->priv->endpoints, link) {
+		if (endpoint->data.eid_info.eid == eid) {
+			*bdf = endpoint->data.eid_info.bdf;
+			ret = 0;
+			break;
+		}
+	}
+	mutex_unlock(&client->priv->endpoints_lock);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(aspeed_mctp_get_eid_bdf);
+
+int aspeed_mctp_get_eid(struct mctp_client *client, u16 bdf,
+			u8 domain_id, u8 *eid)
+{
+	struct aspeed_mctp_endpoint *endpoint;
+	int ret = -ENOENT;
+
+	mutex_lock(&client->priv->endpoints_lock);
+
+	list_for_each_entry(endpoint, &client->priv->endpoints, link) {
+		if (endpoint->data.eid_ext_info.domain_id == domain_id &&
+		    endpoint->data.eid_ext_info.bdf == bdf) {
+			*eid = endpoint->data.eid_ext_info.eid;
+			ret = 0;
+			break;
+		}
+	}
+
+	mutex_unlock(&client->priv->endpoints_lock);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(aspeed_mctp_get_eid);
+
+static int
+aspeed_mctp_get_eid_info(struct aspeed_mctp *priv, void __user *userbuf,
+			 enum mctp_address_type addr_format)
+{
+	int count = 0;
+	int ret = 0;
+	struct aspeed_mctp_get_eid_info get_eid;
+	struct aspeed_mctp_endpoint *endpoint;
+	void *user_ptr;
+	size_t count_to_copy;
+
+	if (copy_from_user(&get_eid, userbuf, sizeof(get_eid))) {
+		dev_err(priv->dev, "copy from user failed\n");
+		return -EFAULT;
+	}
+
+	mutex_lock(&priv->endpoints_lock);
+
+	if (get_eid.count == 0) {
+		count = priv->endpoints_count;
+		goto out_unlock;
+	}
+
+	user_ptr = u64_to_user_ptr(get_eid.ptr);
+	count_to_copy = get_eid.count > priv->endpoints_count ?
+					priv->endpoints_count : get_eid.count;
+	list_for_each_entry(endpoint, &priv->endpoints, link) {
+		if (endpoint->data.eid_info.eid < get_eid.start_eid)
+			continue;
+		if (count >= count_to_copy)
+			break;
+
+		if (addr_format == ASPEED_MCTP_EXTENDED_ADDR_FORMAT)
+			ret = copy_to_user(&(((struct aspeed_mctp_eid_ext_info *)
+								user_ptr)[count]),
+							&endpoint->data,
+							sizeof(struct aspeed_mctp_eid_ext_info));
+		else
+			ret = copy_to_user(&(((struct aspeed_mctp_eid_info *)
+								user_ptr)[count]),
+							&endpoint->data,
+							sizeof(struct aspeed_mctp_eid_info));
+
+		if (ret) {
+			dev_err(priv->dev, "copy to user failed\n");
+			ret = -EFAULT;
+			goto out_unlock;
+		}
+		count++;
+	}
+
+out_unlock:
+	get_eid.count = count;
+	if (copy_to_user(userbuf, &get_eid, sizeof(get_eid))) {
+		dev_err(priv->dev, "copy to user failed\n");
+		ret = -EFAULT;
+	}
+
+	mutex_unlock(&priv->endpoints_lock);
+	return ret;
+}
+
+static int
+eid_info_cmp(void *priv, const struct list_head *a, const struct list_head *b)
+{
+	struct aspeed_mctp_endpoint *endpoint_a;
+	struct aspeed_mctp_endpoint *endpoint_b;
+
+	if (a == b)
+		return 0;
+
+	endpoint_a = list_entry(a, typeof(*endpoint_a), link);
+	endpoint_b = list_entry(b, typeof(*endpoint_b), link);
+
+	if (endpoint_a->data.eid_info.eid < endpoint_b->data.eid_info.eid)
+		return -1;
+	else if (endpoint_a->data.eid_info.eid > endpoint_b->data.eid_info.eid)
+		return 1;
+
+	return 0;
+}
+
+static void aspeed_mctp_eid_info_list_remove(struct list_head *list)
+{
+	struct aspeed_mctp_endpoint *endpoint;
+	struct aspeed_mctp_endpoint *tmp;
+
+	list_for_each_entry_safe(endpoint, tmp, list, link) {
+		list_del(&endpoint->link);
+		kfree(endpoint);
+	}
+}
+
+static bool
+aspeed_mctp_eid_info_list_valid(struct list_head *list)
+{
+	struct aspeed_mctp_endpoint *endpoint;
+	struct aspeed_mctp_endpoint *next;
+
+	list_for_each_entry(endpoint, list, link) {
+		next = list_next_entry(endpoint, link);
+		if (&next->link == list)
+			break;
+
+		/* duplicted eids */
+		if (next->data.eid_info.eid == endpoint->data.eid_info.eid)
+			return false;
+	}
+
+	return true;
+}
+
+static int
+aspeed_mctp_set_eid_info(struct aspeed_mctp *priv, void __user *userbuf,
+			 enum mctp_address_type addr_format)
+{
+	struct list_head list = LIST_HEAD_INIT(list);
+	struct aspeed_mctp_set_eid_info set_eid;
+	void *user_ptr;
+	struct aspeed_mctp_endpoint *endpoint;
+	int ret = 0;
+	u8 eid = 0;
+	size_t i;
+
+	if (copy_from_user(&set_eid, userbuf, sizeof(set_eid))) {
+		dev_err(priv->dev, "copy from user failed\n");
+		return -EFAULT;
+	}
+
+	if (set_eid.count > ASPEED_MCTP_EID_INFO_MAX)
+		return -EINVAL;
+
+	user_ptr = u64_to_user_ptr(set_eid.ptr);
+	for (i = 0; i < set_eid.count; i++) {
+		endpoint = kzalloc(sizeof(*endpoint), GFP_KERNEL);
+		if (!endpoint) {
+			ret = -ENOMEM;
+			goto out;
+		}
+		memset(endpoint, 0, sizeof(*endpoint));
+
+		if (addr_format == ASPEED_MCTP_EXTENDED_ADDR_FORMAT)
+			ret = copy_from_user(&endpoint->data,
+					     &(((struct aspeed_mctp_eid_ext_info *)
+								user_ptr)[i]),
+					     sizeof(struct aspeed_mctp_eid_ext_info));
+		else
+			ret = copy_from_user(&endpoint->data,
+					     &(((struct aspeed_mctp_eid_info *)
+								user_ptr)[i]),
+						 sizeof(struct aspeed_mctp_eid_info));
+
+		if (ret) {
+			dev_err(priv->dev, "copy from user failed\n");
+			kfree(endpoint);
+			ret = -EFAULT;
+			goto out;
+		}
+
+		/* Detect self EID */
+		if (_get_bdf(priv) == endpoint->data.eid_info.bdf) {
+			/*
+			 * XXX Use smallest EID with matching BDF.
+			 * On some platforms there could be multiple endpoints
+			 * with same BDF in routing table.
+			 */
+			if (eid == 0 || endpoint->data.eid_info.eid < eid)
+				eid = endpoint->data.eid_info.eid;
+		}
+
+	list_add_tail(&endpoint->link, &list);
+	}
+
+	list_sort(NULL, &list, eid_info_cmp);
+	if (!aspeed_mctp_eid_info_list_valid(&list)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	mutex_lock(&priv->endpoints_lock);
+	if (list_empty(&priv->endpoints))
+		list_splice_init(&list, &priv->endpoints);
+	else
+		list_swap(&list, &priv->endpoints);
+	priv->endpoints_count = set_eid.count;
+	priv->eid = eid;
+	mutex_unlock(&priv->endpoints_lock);
+out:
+	aspeed_mctp_eid_info_list_remove(&list);
+	return ret;
+}
+
+static int aspeed_mctp_set_own_eid(struct aspeed_mctp *priv, void __user *userbuf)
+{
+	struct aspeed_mctp_set_own_eid data;
+
+	if (copy_from_user(&data, userbuf, sizeof(data))) {
+		dev_err(priv->dev, "copy from user failed\n");
+		return -EFAULT;
+	}
+
+	priv->eid = data.eid;
+
+	return 0;
+}
+
+static long
+aspeed_mctp_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	struct mctp_client *client = file->private_data;
+	struct aspeed_mctp *priv = client->priv;
+	void __user *userbuf = (void __user *)arg;
+	int ret;
+
+	switch (cmd) {
+	case ASPEED_MCTP_IOCTL_FILTER_EID:
+		ret = aspeed_mctp_filter_eid(priv, userbuf);
+	break;
+
+	case ASPEED_MCTP_IOCTL_GET_BDF:
+		ret = aspeed_mctp_get_bdf(priv, userbuf);
+	break;
+
+	case ASPEED_MCTP_IOCTL_GET_MEDIUM_ID:
+		ret = aspeed_mctp_get_medium_id(priv, userbuf);
+	break;
+
+	case ASPEED_MCTP_IOCTL_GET_MTU:
+		ret = aspeed_mctp_get_mtu(priv, userbuf);
+	break;
+
+	case ASPEED_MCTP_IOCTL_REGISTER_DEFAULT_HANDLER:
+		ret = aspeed_mctp_register_default_handler(client);
+	break;
+
+	case ASPEED_MCTP_IOCTL_REGISTER_TYPE_HANDLER:
+		ret = aspeed_mctp_register_type_handler(client, userbuf);
+	break;
+
+	case ASPEED_MCTP_IOCTL_UNREGISTER_TYPE_HANDLER:
+		ret = aspeed_mctp_unregister_type_handler(client, userbuf);
+	break;
+
+	case ASPEED_MCTP_IOCTL_GET_EID_INFO:
+		ret = aspeed_mctp_get_eid_info(priv, userbuf, ASPEED_MCTP_GENERIC_ADDR_FORMAT);
+	break;
+
+	case ASPEED_MCTP_IOCTL_GET_EID_EXT_INFO:
+		ret = aspeed_mctp_get_eid_info(priv, userbuf, ASPEED_MCTP_EXTENDED_ADDR_FORMAT);
+	break;
+
+	case ASPEED_MCTP_IOCTL_SET_EID_INFO:
+		ret = aspeed_mctp_set_eid_info(priv, userbuf, ASPEED_MCTP_GENERIC_ADDR_FORMAT);
+	break;
+
+	case ASPEED_MCTP_IOCTL_SET_EID_EXT_INFO:
+		ret = aspeed_mctp_set_eid_info(priv, userbuf, ASPEED_MCTP_EXTENDED_ADDR_FORMAT);
+	break;
+
+	case ASPEED_MCTP_IOCTL_SET_OWN_EID:
+		ret = aspeed_mctp_set_own_eid(priv, userbuf);
+	break;
+
+	default:
+		dev_err(priv->dev, "Command not found\n");
+		ret = -ENOTTY;
+	}
+
+	return ret;
+}
+
+static __poll_t aspeed_mctp_poll(struct file *file,
+				 struct poll_table_struct *pt)
+{
+	struct mctp_client *client = file->private_data;
+	__poll_t ret = 0;
+	struct aspeed_mctp *priv = client->priv;
+	struct mctp_channel *rx = &priv->rx;
+	u32 mctp_ctrl;
+	u32 mctp_int_sts;
+
+	if (priv->miss_mctp_int) {
+		regmap_read(priv->map, ASPEED_MCTP_CTRL, &mctp_ctrl);
+		if (!(mctp_ctrl & RX_CMD_READY))
+			rx->stopped = true;
+		/* Polling the RX_CMD_RECEIVE_INT to ensure rx_tasklet can find the data */
+		regmap_read(priv->map, ASPEED_MCTP_INT_STS, &mctp_int_sts);
+		if (mctp_int_sts & RX_CMD_RECEIVE_INT)
+			regmap_write(priv->map, ASPEED_MCTP_INT_STS,
+				     mctp_int_sts);
+	}
+
+	tasklet_hi_schedule(&priv->rx.tasklet);
+	poll_wait(file, &client->wait_queue, pt);
+
+	if (!ptr_ring_full_bh(&client->tx_queue))
+		ret |= EPOLLOUT;
+
+	if (__ptr_ring_peek(&client->rx_queue))
+		ret |= EPOLLIN;
+
+	return ret;
+}
+
+static const struct file_operations aspeed_mctp_fops = {
+	.owner = THIS_MODULE,
+	.open = aspeed_mctp_open,
+	.release = aspeed_mctp_release,
+	.read = aspeed_mctp_read,
+	.write = aspeed_mctp_write,
+	.unlocked_ioctl = aspeed_mctp_ioctl,
+	.poll = aspeed_mctp_poll,
+};
+
+static const struct regmap_config aspeed_mctp_regmap_cfg = {
+	.reg_bits	= 32,
+	.reg_stride	= 4,
+	.val_bits	= 32,
+	.max_register	= ASPEED_MCTP_TX_BUF_WR_PTR,
+};
+
+struct device_type aspeed_mctp_type = {
+	.name		= "aspeed-mctp",
+};
+
+static void aspeed_mctp_send_pcie_uevent(struct kobject *kobj, bool ready)
+{
+	char *pcie_not_ready_event[] = { ASPEED_MCTP_READY "=0", NULL };
+	char *pcie_ready_event[] = { ASPEED_MCTP_READY "=1", NULL };
+
+	kobject_uevent_env(kobj, KOBJ_CHANGE,
+			   ready ? pcie_ready_event : pcie_not_ready_event);
+}
+
+static u16 aspeed_mctp_pcie_setup(struct aspeed_mctp *priv)
+{
+	u32 reg;
+	u16 bdf;
+
+	regmap_read(priv->pcie.map, ASPEED_PCIE_MISC_STS_1, &reg);
+
+	reg = reg & (PCI_BUS_NUM_MASK | PCI_DEV_NUM_MASK);
+	bdf = PCI_DEVID(GET_PCI_BUS_NUM(reg), GET_PCI_DEV_NUM(reg));
+	if (reg != 0)
+		cancel_delayed_work(&priv->pcie.rst_dwork);
+	else {
+		schedule_delayed_work(&priv->pcie.rst_dwork,
+				      msecs_to_jiffies(1000));
+		bdf = 0;
+	}
+	return bdf;
+}
+
+static void aspeed_mctp_irq_enable(struct aspeed_mctp *priv)
+{
+	u32 enable = TX_CMD_SENT_INT | TX_CMD_WRONG_INT |
+		     RX_CMD_RECEIVE_INT | RX_CMD_NO_MORE_INT;
+
+	regmap_write(priv->map, ASPEED_MCTP_INT_EN, enable);
+}
+
+static void aspeed_mctp_irq_disable(struct aspeed_mctp *priv)
+{
+	regmap_write(priv->map, ASPEED_MCTP_INT_EN, 0);
+}
+
+static void aspeed_mctp_reset_work(struct work_struct *work)
+{
+	struct aspeed_mctp *priv = container_of(work, typeof(*priv),
+						pcie.rst_dwork.work);
+	struct kobject *kobj = &priv->mctp_miscdev.this_device->kobj;
+	u16 bdf;
+
+	if (priv->pcie.need_uevent) {
+		aspeed_mctp_send_pcie_uevent(kobj, false);
+		priv->pcie.need_uevent = false;
+	}
+
+	bdf = aspeed_mctp_pcie_setup(priv);
+	if (bdf) {
+		if (priv->match_data->need_address_mapping)
+			regmap_update_bits(priv->map, ASPEED_MCTP_EID,
+					   MEMORY_SPACE_MAPPING, BIT(31));
+		/*
+		 * In some condition, tx som and eom will not match expected result.
+		 * e.g. When Maximum Transmit Unit (MTU) set to 64 byte, and then transfer
+		 * size set between 61 ~ 124 (MTU-3 ~ 2*MTU-4), the engine will set all
+		 * packet vdm header eom to 1, no matter what it setted. To fix that
+		 * issue, the driver set MTU to next level(e.g. 64 to 128).
+		 */
+		regmap_update_bits(priv->map, ASPEED_MCTP_ENGINE_CTRL,
+				   TX_MAX_PAYLOAD_SIZE_MASK,
+				   FIELD_GET(TX_MAX_PAYLOAD_SIZE_MASK, fls(ASPEED_MCTP_MTU >> 6)));
+		aspeed_mctp_flush_all_tx_queues(priv);
+		if (!priv->miss_mctp_int)
+			aspeed_mctp_irq_enable(priv);
+		aspeed_mctp_rx_trigger(&priv->rx);
+		aspeed_mctp_send_pcie_uevent(kobj, true);
+	}
+}
+
+static void aspeed_mctp_channels_init(struct aspeed_mctp *priv)
+{
+	aspeed_mctp_rx_chan_init(&priv->rx);
+	aspeed_mctp_tx_chan_init(&priv->tx);
+}
+
+static irqreturn_t aspeed_mctp_irq_handler(int irq, void *arg)
+{
+	struct aspeed_mctp *priv = arg;
+	u32 handled = 0;
+	u32 status;
+
+	regmap_read(priv->map, ASPEED_MCTP_INT_STS, &status);
+	regmap_write(priv->map, ASPEED_MCTP_INT_STS, status);
+
+	if (status & TX_CMD_SENT_INT) {
+		tasklet_hi_schedule(&priv->tx.tasklet);
+		if (!priv->match_data->fifo_auto_surround)
+			priv->tx.rd_ptr = priv->tx.rd_ptr + 1 % TX_PACKET_COUNT;
+		handled |= TX_CMD_SENT_INT;
+	}
+
+	if (status & TX_CMD_WRONG_INT) {
+		/* TODO: print the actual command */
+		dev_warn(priv->dev, "TX wrong");
+
+		handled |= TX_CMD_WRONG_INT;
+	}
+
+	if (status & RX_CMD_RECEIVE_INT) {
+		tasklet_hi_schedule(&priv->rx.tasklet);
+
+		handled |= RX_CMD_RECEIVE_INT;
+	}
+
+	if (status & RX_CMD_NO_MORE_INT) {
+		dev_dbg(priv->dev, "RX full");
+		priv->rx.stopped = true;
+		tasklet_hi_schedule(&priv->rx.tasklet);
+
+		handled |= RX_CMD_NO_MORE_INT;
+	}
+
+	if (!handled)
+		return IRQ_NONE;
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t aspeed_mctp_pcie_rst_irq_handler(int irq, void *arg)
+{
+	struct aspeed_mctp *priv = arg;
+
+	aspeed_mctp_channels_init(priv);
+
+	priv->pcie.need_uevent = true;
+	priv->eid = 0;
+
+	schedule_delayed_work(&priv->pcie.rst_dwork, 0);
+
+	return IRQ_HANDLED;
+}
+
+static void aspeed_mctp_drv_init(struct aspeed_mctp *priv)
+{
+	INIT_LIST_HEAD(&priv->clients);
+	INIT_LIST_HEAD(&priv->mctp_type_handlers);
+	INIT_LIST_HEAD(&priv->endpoints);
+
+	spin_lock_init(&priv->clients_lock);
+	mutex_init(&priv->endpoints_lock);
+
+	INIT_DELAYED_WORK(&priv->pcie.rst_dwork, aspeed_mctp_reset_work);
+
+	tasklet_init(&priv->tx.tasklet, aspeed_mctp_tx_tasklet,
+		     (unsigned long)&priv->tx);
+	tasklet_init(&priv->rx.tasklet, aspeed_mctp_rx_tasklet,
+		     (unsigned long)&priv->rx);
+}
+
+static void aspeed_mctp_drv_fini(struct aspeed_mctp *priv)
+{
+	aspeed_mctp_eid_info_list_remove(&priv->endpoints);
+	tasklet_disable(&priv->tx.tasklet);
+	tasklet_kill(&priv->tx.tasklet);
+	tasklet_disable(&priv->rx.tasklet);
+	tasklet_kill(&priv->rx.tasklet);
+
+	cancel_delayed_work_sync(&priv->pcie.rst_dwork);
+}
+
+static int aspeed_mctp_resources_init(struct aspeed_mctp *priv)
+{
+	struct platform_device *pdev = to_platform_device(priv->dev);
+	void __iomem *regs;
+
+	regs = devm_platform_ioremap_resource(pdev, 0);
+	if (IS_ERR(regs)) {
+		dev_err(priv->dev, "Failed to get regmap!\n");
+		return PTR_ERR(regs);
+	}
+
+	priv->map = devm_regmap_init_mmio(priv->dev, regs,
+					  &aspeed_mctp_regmap_cfg);
+	if (IS_ERR(priv->map))
+		return PTR_ERR(priv->map);
+
+	priv->reset =
+		priv->rc_f ?
+			      devm_reset_control_get_by_index(priv->dev, 0) :
+			      devm_reset_control_get_shared_by_index(priv->dev, 0);
+	if (IS_ERR(priv->reset)) {
+		dev_err(priv->dev, "Failed to get reset!\n");
+		return PTR_ERR(priv->reset);
+	}
+
+	if (priv->rc_f) {
+		priv->reset_dma = devm_reset_control_get_shared_by_index(priv->dev, 1);
+		if (IS_ERR(priv->reset_dma)) {
+			dev_err(priv->dev, "Failed to get ep reset!\n");
+			return PTR_ERR(priv->reset_dma);
+		}
+	}
+	priv->pcie.map =
+		syscon_regmap_lookup_by_phandle(priv->dev->of_node,
+						"aspeed,pcieh");
+	if (IS_ERR(priv->pcie.map)) {
+		dev_err(priv->dev, "Failed to find PCIe Host regmap!\n");
+		return PTR_ERR(priv->pcie.map);
+	}
+
+	platform_set_drvdata(pdev, priv);
+
+	return 0;
+}
+
+static int aspeed_mctp_dma_init(struct aspeed_mctp *priv)
+{
+	struct mctp_channel *tx = &priv->tx;
+	struct mctp_channel *rx = &priv->rx;
+	int ret = -ENOMEM;
+
+	BUILD_BUG_ON(TX_PACKET_COUNT >= TX_MAX_PACKET_COUNT);
+	BUILD_BUG_ON(RX_PACKET_COUNT >= RX_MAX_PACKET_COUNT);
+
+	tx->cmd.vaddr = dma_alloc_coherent(priv->dev, TX_CMD_BUF_SIZE,
+					   &tx->cmd.dma_handle, GFP_KERNEL);
+
+	if (!tx->cmd.vaddr)
+		return ret;
+
+	tx->data.vaddr = dma_alloc_coherent(
+		priv->dev,
+		PAGE_ALIGN(TX_PACKET_COUNT *
+			   priv->match_data->packet_unit_size),
+		&tx->data.dma_handle, GFP_KERNEL);
+
+	if (!tx->data.vaddr)
+		goto out_tx_data;
+
+	rx->cmd.vaddr = dma_alloc_coherent(
+		priv->dev,
+		PAGE_ALIGN(priv->rx_packet_count * priv->match_data->rx_cmd_size),
+		&rx->cmd.dma_handle, GFP_KERNEL);
+
+	if (!rx->cmd.vaddr)
+		goto out_tx_cmd;
+
+	rx->data.vaddr = dma_alloc_coherent(
+		priv->dev,
+		PAGE_ALIGN(priv->rx_packet_count * priv->match_data->packet_unit_size),
+		&rx->data.dma_handle, GFP_KERNEL);
+
+	if (!rx->data.vaddr)
+		goto out_rx_data;
+
+	return 0;
+out_rx_data:
+	dma_free_coherent(
+		priv->dev,
+		PAGE_ALIGN(priv->rx_packet_count * priv->match_data->rx_cmd_size),
+		rx->cmd.vaddr, rx->cmd.dma_handle);
+
+out_tx_cmd:
+	dma_free_coherent(priv->dev,
+			  PAGE_ALIGN(TX_PACKET_COUNT *
+				     priv->match_data->packet_unit_size),
+			  tx->data.vaddr, tx->data.dma_handle);
+
+out_tx_data:
+	dma_free_coherent(priv->dev, TX_CMD_BUF_SIZE, tx->cmd.vaddr,
+			  tx->cmd.dma_handle);
+	return ret;
+}
+
+static void aspeed_mctp_dma_fini(struct aspeed_mctp *priv)
+{
+	struct mctp_channel *tx = &priv->tx;
+	struct mctp_channel *rx = &priv->rx;
+
+	dma_free_coherent(priv->dev, TX_CMD_BUF_SIZE, tx->cmd.vaddr,
+			  tx->cmd.dma_handle);
+
+	dma_free_coherent(
+		priv->dev,
+		PAGE_ALIGN(priv->rx_packet_count * priv->match_data->rx_cmd_size),
+		rx->cmd.vaddr, rx->cmd.dma_handle);
+
+	dma_free_coherent(priv->dev,
+			  PAGE_ALIGN(TX_PACKET_COUNT *
+				     priv->match_data->packet_unit_size),
+			  tx->data.vaddr, tx->data.dma_handle);
+
+	dma_free_coherent(priv->dev,
+			  PAGE_ALIGN(priv->rx_packet_count *
+				     priv->match_data->packet_unit_size),
+			  rx->data.vaddr, rx->data.dma_handle);
+}
+
+static int aspeed_mctp_irq_init(struct aspeed_mctp *priv)
+{
+	struct platform_device *pdev = to_platform_device(priv->dev);
+	int irq, ret;
+
+	irq = platform_get_irq_byname_optional(pdev, "mctp");
+	if (irq < 0) {
+		/* mctp irq is option */
+		priv->miss_mctp_int = 1;
+	} else {
+		ret = devm_request_irq(priv->dev, irq, aspeed_mctp_irq_handler,
+				       IRQF_SHARED, dev_name(&pdev->dev), priv);
+		if (ret)
+			return ret;
+		aspeed_mctp_irq_enable(priv);
+	}
+	irq = platform_get_irq_byname(pdev, "pcie");
+	if (!irq)
+		return -ENODEV;
+
+	ret = devm_request_irq(priv->dev, irq, aspeed_mctp_pcie_rst_irq_handler,
+			       IRQF_SHARED, dev_name(&pdev->dev), priv);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static void aspeed_mctp_hw_reset(struct aspeed_mctp *priv)
+{
+	u32 reg;
+
+	/*
+	 * XXX: We need to skip the reset when we probe multiple times.
+	 * Currently calling reset more than once seems to make the HW upset,
+	 * however, we do need to reset once after the first boot before we're
+	 * able to use the HW.
+	 */
+	if (!priv->rc_f) {
+		regmap_read(priv->map, ASPEED_MCTP_TX_BUF_ADDR, &reg);
+
+		if (reg) {
+			dev_info(priv->dev,
+				"Already initialized - skipping hardware reset\n");
+			return;
+		}
+	}
+
+	if (reset_control_deassert(priv->reset) != 0)
+		dev_warn(priv->dev, "Failed to deassert reset\n");
+
+	if (priv->rc_f) {
+		if (reset_control_deassert(priv->reset_dma) != 0)
+			dev_warn(priv->dev, "Failed to deassert ep reset\n");
+	}
+}
+
+static int aspeed_mctp_probe(struct platform_device *pdev)
+{
+	struct aspeed_mctp *priv;
+	int ret;
+	u16 bdf;
+
+	priv = devm_kzalloc(&pdev->dev, sizeof(*priv), GFP_KERNEL);
+	if (!priv) {
+		ret = -ENOMEM;
+		goto out;
+	}
+	priv->dev = &pdev->dev;
+	priv->rc_f =
+		of_find_property(priv->dev->of_node, "pcie_rc", NULL) ? 1 : 0;
+	priv->match_data = of_device_get_match_data(priv->dev);
+
+	ret = device_property_read_u32(priv->dev, "aspeed,rx-packet-count",
+				       &priv->rx_packet_count);
+	if (ret) {
+		priv->rx_packet_count = RX_PACKET_COUNT;
+	} else if (priv->rx_packet_count % 4 ||
+		   priv->rx_packet_count >= RX_MAX_PACKET_COUNT) {
+		dev_err(priv->dev,
+			"The aspeed,rx-packet-count:%d should be 4-aligned and less than %ld",
+			priv->rx_packet_count, RX_MAX_PACKET_COUNT);
+		ret = -EINVAL;
+		goto out;
+	}
+
+	ret = device_property_read_u32(priv->dev, "aspeed,rx-ring-count",
+				       &priv->rx_ring_count);
+	if (ret)
+		priv->rx_ring_count = RX_RING_COUNT;
+
+	ret = device_property_read_u32(priv->dev, "aspeed,tx-ring-count",
+				       &priv->tx_ring_count);
+	if (ret)
+		priv->tx_ring_count = TX_RING_COUNT;
+
+	aspeed_mctp_drv_init(priv);
+
+	ret = aspeed_mctp_resources_init(priv);
+	if (ret) {
+		dev_err(priv->dev, "Failed to init resources\n");
+		goto out_drv;
+	}
+
+	ret = aspeed_mctp_dma_init(priv);
+	if (ret) {
+		dev_err(priv->dev, "Failed to init DMA\n");
+		goto out_drv;
+	}
+
+	aspeed_mctp_hw_reset(priv);
+
+	aspeed_mctp_channels_init(priv);
+
+	priv->mctp_miscdev.parent = priv->dev;
+	priv->mctp_miscdev.minor = MISC_DYNAMIC_MINOR;
+	priv->mctp_miscdev.name =
+		priv->rc_f ? "aspeed-mctp1" : "aspeed-mctp";
+	priv->mctp_miscdev.fops = &aspeed_mctp_fops;
+	ret = misc_register(&priv->mctp_miscdev);
+	if (ret) {
+		dev_err(priv->dev, "Failed to register miscdev\n");
+		goto out_dma;
+	}
+	priv->mctp_miscdev.this_device->type = &aspeed_mctp_type;
+
+	ret = aspeed_mctp_irq_init(priv);
+	if (ret) {
+		dev_err(priv->dev, "Failed to init IRQ!\n");
+		goto out_dma;
+	}
+	bdf = aspeed_mctp_pcie_setup(priv);
+	if (bdf != 0) {
+		if (priv->match_data->need_address_mapping)
+			regmap_update_bits(priv->map, ASPEED_MCTP_EID,
+					   MEMORY_SPACE_MAPPING, BIT(31));
+		/*
+		 * In some condition, tx som and eom will not match expected result.
+		 * e.g. When Maximum Transmit Unit (MTU) set to 64 byte, and then transfer
+		 * size set between 61 ~ 124 (MTU-3 ~ 2*MTU-4), the engine will set all
+		 * packet vdm header eom to 1, no matter what it setted. To fix that
+		 * issue, the driver set MTU to next level(e.g. 64 to 128).
+		 */
+		regmap_update_bits(priv->map, ASPEED_MCTP_ENGINE_CTRL,
+				   TX_MAX_PAYLOAD_SIZE_MASK,
+				   FIELD_GET(TX_MAX_PAYLOAD_SIZE_MASK, fls(ASPEED_MCTP_MTU >> 6)));
+		aspeed_mctp_rx_trigger(&priv->rx);
+	}
+
+	priv->peci_mctp = platform_device_register_data(
+		priv->dev, priv->rc_f ? "peci1-mctp" : "peci0-mctp",
+		PLATFORM_DEVID_NONE, NULL, 0);
+	if (IS_ERR(priv->peci_mctp))
+		dev_err(priv->dev, "Failed to register peci-mctp device\n");
+
+	return 0;
+
+out_dma:
+	aspeed_mctp_dma_fini(priv);
+out_drv:
+	aspeed_mctp_drv_fini(priv);
+out:
+	dev_err(&pdev->dev, "Failed to probe Aspeed MCTP: %d\n", ret);
+	return ret;
+}
+
+static int aspeed_mctp_remove(struct platform_device *pdev)
+{
+	struct aspeed_mctp *priv = platform_get_drvdata(pdev);
+
+	platform_device_unregister(priv->peci_mctp);
+
+	misc_deregister(&priv->mctp_miscdev);
+
+	aspeed_mctp_irq_disable(priv);
+
+	aspeed_mctp_dma_fini(priv);
+
+	aspeed_mctp_drv_fini(priv);
+
+	if (priv->rc_f)
+		reset_control_assert(priv->reset_dma);
+
+	reset_control_assert(priv->reset);
+
+	return 0;
+}
+
+static const struct aspeed_mctp_match_data ast2500_mctp_match_data = {
+	.rx_cmd_size = sizeof(struct aspeed_mctp_rx_cmd),
+	.packet_unit_size = 128,
+	.need_address_mapping = true,
+	.vdm_hdr_direct_xfer = false,
+	.fifo_auto_surround = false,
+};
+
+static const struct aspeed_mctp_match_data ast2600_mctp_match_data = {
+	.rx_cmd_size = sizeof(u32),
+	.packet_unit_size = sizeof(struct mctp_pcie_packet_data),
+	.need_address_mapping = false,
+	.vdm_hdr_direct_xfer = true,
+	.fifo_auto_surround = true,
+};
+
+static const struct of_device_id aspeed_mctp_match_table[] = {
+	{ .compatible = "aspeed,ast2500-mctp", .data = &ast2500_mctp_match_data},
+	{ .compatible = "aspeed,ast2600-mctp", .data = &ast2600_mctp_match_data},
+	{ }
+};
+
+static struct platform_driver aspeed_mctp_driver = {
+	.driver	= {
+		.name		= "aspeed-mctp",
+		.of_match_table	= of_match_ptr(aspeed_mctp_match_table),
+	},
+	.probe	= aspeed_mctp_probe,
+	.remove	= aspeed_mctp_remove,
+};
+
+static int __init aspeed_mctp_init(void)
+{
+	packet_cache =
+		kmem_cache_create_usercopy("mctp-packet",
+					   sizeof(struct mctp_pcie_packet),
+					   0, 0, 0,
+					   sizeof(struct mctp_pcie_packet),
+					   NULL);
+	if (!packet_cache)
+		return -ENOMEM;
+
+	return platform_driver_register(&aspeed_mctp_driver);
+}
+
+static void __exit aspeed_mctp_exit(void)
+{
+	platform_driver_unregister(&aspeed_mctp_driver);
+	kmem_cache_destroy(packet_cache);
+}
+
+module_init(aspeed_mctp_init)
+module_exit(aspeed_mctp_exit)
+
+MODULE_DEVICE_TABLE(of, aspeed_mctp_match_table);
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Iwona Winiarska <iwona.winiarska@intel.com>");
+MODULE_DESCRIPTION("Aspeed MCTP driver");
diff --git a/drivers/soc/aspeed/aspeed-otp.c b/drivers/soc/aspeed/aspeed-otp.c
new file mode 100644
index 000000000000..b85c91848e98
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-otp.c
@@ -0,0 +1,639 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) ASPEED Technology Inc.
+ */
+
+#include <linux/errno.h>
+#include <linux/fs.h>
+#include <linux/miscdevice.h>
+#include <linux/slab.h>
+#include <linux/platform_device.h>
+#include <linux/regmap.h>
+#include <linux/spinlock.h>
+#include <linux/uaccess.h>
+#include <linux/mfd/syscon.h>
+#include <linux/of.h>
+#include <linux/module.h>
+#include <asm/io.h>
+#include <uapi/linux/aspeed-otp.h>
+
+#define ASPEED_REVISION_ID0	0x04
+#define ASPEED_REVISION_ID1	0x14
+#define ID0_AST2600A0	0x05000303
+#define ID1_AST2600A0	0x05000303
+#define ID0_AST2600A1	0x05010303
+#define ID1_AST2600A1	0x05010303
+#define ID0_AST2600A2	0x05010303
+#define ID1_AST2600A2	0x05020303
+#define ID0_AST2600A3	0x05030303
+#define ID1_AST2600A3	0x05030303
+#define ID0_AST2620A1	0x05010203
+#define ID1_AST2620A1	0x05010203
+#define ID0_AST2620A2	0x05010203
+#define ID1_AST2620A2	0x05020203
+#define ID0_AST2620A3	0x05030203
+#define ID1_AST2620A3	0x05030203
+#define ID0_AST2605A2	0x05010103
+#define ID1_AST2605A2	0x05020103
+#define ID0_AST2605A3	0x05030103
+#define ID1_AST2605A3	0x05030103
+#define ID0_AST2625A3	0x05030403
+#define ID1_AST2625A3	0x05030403
+
+#define OTP_PROTECT_KEY	0x0
+#define  OTP_PASSWD	0x349fe38a
+#define OTP_COMMAND	0x4
+#define OTP_TIMING	0x8
+#define OTP_ADDR	0x10
+#define OTP_STATUS	0x14
+#define OTP_COMPARE_1	0x20
+#define OTP_COMPARE_2	0x24
+#define OTP_COMPARE_3	0x28
+#define OTP_COMPARE_4	0x2c
+#define SW_REV_ID0	0x68
+#define SW_REV_ID1	0x6c
+#define SEC_KEY_NUM	0x78
+#define RETRY		20
+
+struct aspeed_otp {
+	struct miscdevice miscdev;
+	void __iomem *reg_base;
+	bool is_open;
+	u32 otp_ver;
+	u32 *data;
+};
+
+static DEFINE_SPINLOCK(otp_state_lock);
+
+static inline u32 aspeed_otp_read(struct aspeed_otp *ctx, u32 reg)
+{
+	int val;
+
+	val = readl(ctx->reg_base + reg);
+	// printk("read:reg = 0x%08x, val = 0x%08x\n", reg, val);
+	return val;
+}
+
+static inline void aspeed_otp_write(struct aspeed_otp *ctx, u32 val, u32 reg)
+{
+	// printk("write:reg = 0x%08x, val = 0x%08x\n", reg, val);
+	writel(val, ctx->reg_base + reg);
+}
+
+static uint32_t chip_version(u32 revid0, u32 revid1)
+{
+	if (revid0 == ID0_AST2600A0 && revid1 == ID1_AST2600A0) {
+		/* AST2600-A0 */
+		return OTP_A0;
+	} else if (revid0 == ID0_AST2600A1 && revid1 == ID1_AST2600A1) {
+		/* AST2600-A1 */
+		return OTP_A1;
+	} else if (revid0 == ID0_AST2600A2 && revid1 == ID1_AST2600A2) {
+		/* AST2600-A2 */
+		return OTP_A2;
+	} else if (revid0 == ID0_AST2600A3 && revid1 == ID1_AST2600A3) {
+		/* AST2600-A3 */
+		return OTP_A3;
+	} else if (revid0 == ID0_AST2620A1 && revid1 == ID1_AST2620A1) {
+		/* AST2620-A1 */
+		return OTP_A1;
+	} else if (revid0 == ID0_AST2620A2 && revid1 == ID1_AST2620A2) {
+		/* AST2620-A2 */
+		return OTP_A2;
+	} else if (revid0 == ID0_AST2620A3 && revid1 == ID1_AST2620A3) {
+		/* AST2620-A3 */
+		return OTP_A3;
+	} else if (revid0 == ID0_AST2605A2 && revid1 == ID1_AST2605A2) {
+		/* AST2605-A2 */
+		return OTP_A2;
+	} else if (revid0 == ID0_AST2605A3 && revid1 == ID1_AST2605A3) {
+		/* AST2605-A3 */
+		return OTP_A3;
+	} else if (revid0 == ID0_AST2625A3 && revid1 == ID1_AST2625A3) {
+		/* AST2605-A3 */
+		return OTP_A3;
+	}
+	return -1;
+}
+
+static void wait_complete(struct aspeed_otp *ctx)
+{
+	int reg;
+	int i = 0;
+
+	do {
+		reg = aspeed_otp_read(ctx, OTP_STATUS);
+		if ((reg & 0x6) == 0x6)
+			i++;
+	} while (i != 2);
+}
+
+static void otp_write(struct aspeed_otp *ctx, u32 otp_addr, u32 val)
+{
+	aspeed_otp_write(ctx, otp_addr, OTP_ADDR); //write address
+	aspeed_otp_write(ctx, val, OTP_COMPARE_1); //write val
+	aspeed_otp_write(ctx, 0x23b1e362, OTP_COMMAND); //write command
+	wait_complete(ctx);
+}
+
+static void otp_soak(struct aspeed_otp *ctx, int soak)
+{
+	if (ctx->otp_ver == OTP_A2 || ctx->otp_ver == OTP_A3) {
+		switch (soak) {
+		case 0: //default
+			otp_write(ctx, 0x3000, 0x0); // Write MRA
+			otp_write(ctx, 0x5000, 0x0); // Write MRB
+			otp_write(ctx, 0x1000, 0x0); // Write MR
+			break;
+		case 1: //normal program
+			otp_write(ctx, 0x3000, 0x1320); // Write MRA
+			otp_write(ctx, 0x5000, 0x1008); // Write MRB
+			otp_write(ctx, 0x1000, 0x0024); // Write MR
+			aspeed_otp_write(ctx, 0x04191388, OTP_TIMING); // 200us
+			break;
+		case 2: //soak program
+			otp_write(ctx, 0x3000, 0x1320); // Write MRA
+			otp_write(ctx, 0x5000, 0x0007); // Write MRB
+			otp_write(ctx, 0x1000, 0x0100); // Write MR
+			aspeed_otp_write(ctx, 0x04193a98, OTP_TIMING); // 600us
+			break;
+		}
+	} else {
+		switch (soak) {
+		case 0: //default
+			otp_write(ctx, 0x3000, 0x0); // Write MRA
+			otp_write(ctx, 0x5000, 0x0); // Write MRB
+			otp_write(ctx, 0x1000, 0x0); // Write MR
+			break;
+		case 1: //normal program
+			otp_write(ctx, 0x3000, 0x4021); // Write MRA
+			otp_write(ctx, 0x5000, 0x302f); // Write MRB
+			otp_write(ctx, 0x1000, 0x4020); // Write MR
+			aspeed_otp_write(ctx, 0x04190760, OTP_TIMING); // 75us
+			break;
+		case 2: //soak program
+			otp_write(ctx, 0x3000, 0x4021); // Write MRA
+			otp_write(ctx, 0x5000, 0x1027); // Write MRB
+			otp_write(ctx, 0x1000, 0x4820); // Write MR
+			aspeed_otp_write(ctx, 0x041930d4, OTP_TIMING); // 500us
+			break;
+		}
+	}
+
+	wait_complete(ctx);
+}
+
+static int verify_bit(struct aspeed_otp *ctx, u32 otp_addr, int bit_offset, int value)
+{
+	u32 ret[2];
+
+	if (otp_addr % 2 == 0)
+		aspeed_otp_write(ctx, otp_addr, OTP_ADDR); //Read address
+	else
+		aspeed_otp_write(ctx, otp_addr - 1, OTP_ADDR); //Read address
+
+	aspeed_otp_write(ctx, 0x23b1e361, OTP_COMMAND); //trigger read
+	wait_complete(ctx);
+	ret[0] = aspeed_otp_read(ctx, OTP_COMPARE_1);
+	ret[1] = aspeed_otp_read(ctx, OTP_COMPARE_2);
+
+	if (otp_addr % 2 == 0) {
+		if (((ret[0] >> bit_offset) & 1) == value)
+			return 0;
+		else
+			return -1;
+	} else {
+		if (((ret[1] >> bit_offset) & 1) == value)
+			return 0;
+		else
+			return -1;
+	}
+}
+
+static void otp_prog(struct aspeed_otp *ctx, u32 otp_addr, u32 prog_bit)
+{
+	otp_write(ctx, 0x0, prog_bit);
+	aspeed_otp_write(ctx, otp_addr, OTP_ADDR); //write address
+	aspeed_otp_write(ctx, prog_bit, OTP_COMPARE_1); //write data
+	aspeed_otp_write(ctx, 0x23b1e364, OTP_COMMAND); //write command
+	wait_complete(ctx);
+}
+
+static void _otp_prog_bit(struct aspeed_otp *ctx, u32 value, u32 prog_address, u32 bit_offset)
+{
+	int prog_bit;
+
+	if (prog_address % 2 == 0) {
+		if (value)
+			prog_bit = ~(0x1 << bit_offset);
+		else
+			return;
+	} else {
+		if (ctx->otp_ver != OTP_A3)
+			prog_address |= 1 << 15;
+		if (!value)
+			prog_bit = 0x1 << bit_offset;
+		else
+			return;
+	}
+	otp_prog(ctx, prog_address, prog_bit);
+}
+
+static int otp_prog_bit(struct aspeed_otp *ctx, u32 value, u32 prog_address, u32 bit_offset)
+{
+	int pass;
+	int i;
+
+	otp_soak(ctx, 1);
+	_otp_prog_bit(ctx, value, prog_address, bit_offset);
+	pass = 0;
+
+	for (i = 0; i < RETRY; i++) {
+		if (verify_bit(ctx, prog_address, bit_offset, value) != 0) {
+			otp_soak(ctx, 2);
+			_otp_prog_bit(ctx, value, prog_address, bit_offset);
+			if (verify_bit(ctx, prog_address, bit_offset, value) != 0) {
+				otp_soak(ctx, 1);
+			} else {
+				pass = 1;
+				break;
+			}
+		} else {
+			pass = 1;
+			break;
+		}
+	}
+	otp_soak(ctx, 0);
+	return pass;
+}
+
+static void otp_read_conf_dw(struct aspeed_otp *ctx, u32 offset, u32 *buf)
+{
+	u32 config_offset;
+
+	config_offset = 0x800;
+	config_offset |= (offset / 8) * 0x200;
+	config_offset |= (offset % 8) * 0x2;
+
+	aspeed_otp_write(ctx, config_offset, OTP_ADDR); //Read address
+	aspeed_otp_write(ctx, 0x23b1e361, OTP_COMMAND); //trigger read
+	wait_complete(ctx);
+	buf[0] = aspeed_otp_read(ctx, OTP_COMPARE_1);
+}
+
+static void otp_read_conf(struct aspeed_otp *ctx, u32 offset, u32 len)
+{
+	int i, j;
+
+	otp_soak(ctx, 0);
+	for (i = offset, j = 0; j < len; i++, j++)
+		otp_read_conf_dw(ctx, i, &ctx->data[j]);
+}
+
+static void otp_read_data_2dw(struct aspeed_otp *ctx, u32 offset, u32 *buf)
+{
+	aspeed_otp_write(ctx, offset, OTP_ADDR); //Read address
+	aspeed_otp_write(ctx, 0x23b1e361, OTP_COMMAND); //trigger read
+	wait_complete(ctx);
+	buf[0] = aspeed_otp_read(ctx, OTP_COMPARE_1);
+	buf[1] = aspeed_otp_read(ctx, OTP_COMPARE_2);
+}
+
+static void otp_read_data(struct aspeed_otp *ctx, u32 offset, u32 len)
+{
+	int i, j;
+	u32 ret[2];
+
+	otp_soak(ctx, 0);
+
+	i = offset;
+	j = 0;
+	if (offset % 2) {
+		otp_read_data_2dw(ctx, i - 1, ret);
+		ctx->data[0] = ret[1];
+		i++;
+		j++;
+	}
+	for (; j < len; i += 2, j += 2)
+		otp_read_data_2dw(ctx, i, &ctx->data[j]);
+}
+
+static int otp_prog_data(struct aspeed_otp *ctx, u32 value, u32 dw_offset, u32 bit_offset)
+{
+	u32 read[2];
+	int otp_bit;
+
+	if (dw_offset % 2 == 0) {
+		otp_read_data_2dw(ctx, dw_offset, read);
+		otp_bit = (read[0] >> bit_offset) & 0x1;
+
+		if (otp_bit == 1 && value == 0) {
+			pr_err("OTPDATA%X[%X] = 1\n", dw_offset, bit_offset);
+			pr_err("OTP is programed, which can't be cleaned\n");
+			return -EINVAL;
+		}
+	} else {
+		otp_read_data_2dw(ctx, dw_offset - 1, read);
+		otp_bit = (read[1] >> bit_offset) & 0x1;
+
+		if (otp_bit == 0 && value == 1) {
+			pr_err("OTPDATA%X[%X] = 1\n", dw_offset, bit_offset);
+			pr_err("OTP is programed, which can't be writen\n");
+			return -EINVAL;
+		}
+	}
+	if (otp_bit == value) {
+		pr_err("OTPDATA%X[%X] = %d\n", dw_offset, bit_offset, value);
+		pr_err("No need to program\n");
+		return 0;
+	}
+
+	return otp_prog_bit(ctx, value, dw_offset, bit_offset);
+}
+
+static int otp_prog_conf(struct aspeed_otp *ctx, u32 value, u32 dw_offset, u32 bit_offset)
+{
+	u32 read;
+	u32 prog_address = 0;
+	int otp_bit;
+
+	otp_read_conf_dw(ctx, dw_offset, &read);
+
+	prog_address = 0x800;
+	prog_address |= (dw_offset / 8) * 0x200;
+	prog_address |= (dw_offset % 8) * 0x2;
+	otp_bit = (read >> bit_offset) & 0x1;
+	if (otp_bit == value) {
+		pr_err("OTPCFG%X[%X] = %d\n", dw_offset, bit_offset, value);
+		pr_err("No need to program\n");
+		return 0;
+	}
+	if (otp_bit == 1 && value == 0) {
+		pr_err("OTPCFG%X[%X] = 1\n", dw_offset, bit_offset);
+		pr_err("OTP is programed, which can't be clean\n");
+		return -EINVAL;
+	}
+
+	return otp_prog_bit(ctx, value, prog_address, bit_offset);
+}
+
+struct aspeed_otp *glob_ctx;
+
+void otp_read_data_buf(u32 offset, u32 *buf, u32 len)
+{
+	int i, j;
+	u32 ret[2];
+
+	aspeed_otp_write(glob_ctx, OTP_PASSWD, OTP_PROTECT_KEY);
+
+	otp_soak(glob_ctx, 0);
+
+	i = offset;
+	j = 0;
+	if (offset % 2) {
+		otp_read_data_2dw(glob_ctx, i - 1, ret);
+		buf[0] = ret[1];
+		i++;
+		j++;
+	}
+	for (; j < len; i += 2, j += 2)
+		otp_read_data_2dw(glob_ctx, i, &buf[j]);
+	aspeed_otp_write(glob_ctx, 0, OTP_PROTECT_KEY);
+}
+EXPORT_SYMBOL(otp_read_data_buf);
+
+void otp_read_conf_buf(u32 offset, u32 *buf, u32 len)
+{
+	int i, j;
+
+	aspeed_otp_write(glob_ctx, OTP_PASSWD, OTP_PROTECT_KEY);
+	otp_soak(glob_ctx, 0);
+	for (i = offset, j = 0; j < len; i++, j++)
+		otp_read_conf_dw(glob_ctx, i, &buf[j]);
+	aspeed_otp_write(glob_ctx, 0, OTP_PROTECT_KEY);
+}
+EXPORT_SYMBOL(otp_read_conf_buf);
+
+static long otp_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	struct miscdevice *c = file->private_data;
+	struct aspeed_otp *ctx = container_of(c, struct aspeed_otp, miscdev);
+	void __user *argp = (void __user *)arg;
+	struct otp_read xfer;
+	struct otp_prog prog;
+	u32 reg_read[2];
+	int ret = 0;
+
+	switch (cmd) {
+	case ASPEED_OTP_READ_DATA:
+		if (copy_from_user(&xfer, argp, sizeof(struct otp_read)))
+			return -EFAULT;
+		if ((xfer.offset + xfer.len) > 0x800) {
+			pr_err("out of range");
+			return -EINVAL;
+		}
+
+		aspeed_otp_write(ctx, OTP_PASSWD, OTP_PROTECT_KEY);
+		otp_read_data(ctx, xfer.offset, xfer.len);
+		aspeed_otp_write(ctx, 0, OTP_PROTECT_KEY);
+
+		if (copy_to_user(xfer.data, ctx->data, xfer.len * 4))
+			return -EFAULT;
+		if (copy_to_user(argp, &xfer, sizeof(struct otp_read)))
+			return -EFAULT;
+		break;
+	case ASPEED_OTP_READ_CONF:
+		if (copy_from_user(&xfer, argp, sizeof(struct otp_read)))
+			return -EFAULT;
+		if ((xfer.offset + xfer.len) > 0x800) {
+			pr_err("out of range");
+			return -EINVAL;
+		}
+
+		aspeed_otp_write(ctx, OTP_PASSWD, OTP_PROTECT_KEY);
+		otp_read_conf(ctx, xfer.offset, xfer.len);
+		aspeed_otp_write(ctx, 0, OTP_PROTECT_KEY);
+
+		if (copy_to_user(xfer.data, ctx->data, xfer.len * 4))
+			return -EFAULT;
+		if (copy_to_user(argp, &xfer, sizeof(struct otp_read)))
+			return -EFAULT;
+		break;
+	case ASPEED_OTP_PROG_DATA:
+		if (copy_from_user(&prog, argp, sizeof(struct otp_prog)))
+			return -EFAULT;
+		if (prog.bit_offset >= 32 || (prog.value != 0 && prog.value != 1)) {
+			pr_err("out of range");
+			return -EINVAL;
+		}
+		if (prog.dw_offset >= 0x800) {
+			pr_err("out of range");
+			return -EINVAL;
+		}
+		aspeed_otp_write(ctx, OTP_PASSWD, OTP_PROTECT_KEY);
+		ret = otp_prog_data(ctx, prog.value, prog.dw_offset, prog.bit_offset);
+		break;
+	case ASPEED_OTP_PROG_CONF:
+		if (copy_from_user(&prog, argp, sizeof(struct otp_prog)))
+			return -EFAULT;
+		if (prog.bit_offset >= 32 || (prog.value != 0 && prog.value != 1)) {
+			pr_err("out of range");
+			return -EINVAL;
+		}
+		if (prog.dw_offset >= 0x20) {
+			pr_err("out of range");
+			return -EINVAL;
+		}
+		aspeed_otp_write(ctx, OTP_PASSWD, OTP_PROTECT_KEY);
+		ret = otp_prog_conf(ctx, prog.value, prog.dw_offset, prog.bit_offset);
+		break;
+	case ASPEED_OTP_VER:
+		if (copy_to_user(argp, &ctx->otp_ver, sizeof(u32)))
+			return -EFAULT;
+		break;
+	case ASPEED_OTP_SW_RID:
+		reg_read[0] = aspeed_otp_read(ctx, SW_REV_ID0);
+		reg_read[1] = aspeed_otp_read(ctx, SW_REV_ID1);
+		if (copy_to_user(argp, reg_read, sizeof(u32) * 2))
+			return -EFAULT;
+		break;
+	case ASPEED_SEC_KEY_NUM:
+		reg_read[0] = aspeed_otp_read(ctx, SEC_KEY_NUM) & 7;
+		if (copy_to_user(argp, reg_read, sizeof(u32)))
+			return -EFAULT;
+		break;
+	}
+	return ret;
+}
+
+static int otp_open(struct inode *inode, struct file *file)
+{
+	struct miscdevice *c = file->private_data;
+	struct aspeed_otp *ctx = container_of(c, struct aspeed_otp, miscdev);
+
+	spin_lock(&otp_state_lock);
+
+	if (ctx->is_open) {
+		spin_unlock(&otp_state_lock);
+		return -EBUSY;
+	}
+
+	ctx->is_open = true;
+
+	spin_unlock(&otp_state_lock);
+
+	return 0;
+}
+
+static int otp_release(struct inode *inode, struct file *file)
+{
+	struct miscdevice *c = file->private_data;
+	struct aspeed_otp *ctx = container_of(c, struct aspeed_otp, miscdev);
+
+	spin_lock(&otp_state_lock);
+
+	ctx->is_open = false;
+
+	spin_unlock(&otp_state_lock);
+
+	return 0;
+}
+
+static const struct file_operations otp_fops = {
+	.owner =		THIS_MODULE,
+	.unlocked_ioctl =	otp_ioctl,
+	.open =			otp_open,
+	.release =		otp_release,
+};
+
+static const struct of_device_id aspeed_otp_of_matches[] = {
+	{ .compatible = "aspeed,ast2600-otp" },
+	{ }
+};
+MODULE_DEVICE_TABLE(of, aspeed_otp_of_matches);
+
+static int aspeed_otp_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct regmap *scu;
+	struct aspeed_otp *priv;
+	struct resource *res;
+	u32 revid0, revid1;
+	int rc;
+
+	priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL);
+	glob_ctx = priv;
+	if (!priv)
+		return -ENOMEM;
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		dev_err(&pdev->dev, "cannot get IORESOURCE_MEM\n");
+		return -ENOENT;
+	}
+
+	priv->reg_base = devm_ioremap_resource(&pdev->dev, res);
+	if (!priv->reg_base)
+		return -EIO;
+
+	scu = syscon_regmap_lookup_by_phandle(dev->of_node, "aspeed,scu");
+	if (IS_ERR(scu)) {
+		dev_err(dev, "failed to find 2600 SCU regmap\n");
+		return PTR_ERR(scu);
+	}
+
+	regmap_read(scu, ASPEED_REVISION_ID0, &revid0);
+	regmap_read(scu, ASPEED_REVISION_ID1, &revid1);
+
+	priv->otp_ver = chip_version(revid0, revid1);
+
+	if (priv->otp_ver == -1) {
+		dev_err(dev, "invalid SCU\n");
+		return -EINVAL;
+	}
+
+	priv->data = kmalloc(8192, GFP_KERNEL);
+	if (!priv->data)
+		return -ENOMEM;
+
+	dev_set_drvdata(dev, priv);
+
+	/* Set up the miscdevice */
+	priv->miscdev.minor = MISC_DYNAMIC_MINOR;
+	priv->miscdev.name = "aspeed-otp";
+	priv->miscdev.fops = &otp_fops;
+
+	/* Register the device */
+	rc = misc_register(&priv->miscdev);
+	if (rc) {
+		dev_err(dev, "Unable to register device\n");
+		return rc;
+	}
+
+	return 0;
+}
+
+static int aspeed_otp_remove(struct platform_device *pdev)
+{
+	struct aspeed_otp *ctx = dev_get_drvdata(&pdev->dev);
+
+	kfree(ctx->data);
+	misc_deregister(&ctx->miscdev);
+
+	return 0;
+}
+
+static struct platform_driver aspeed_otp_driver = {
+	.probe = aspeed_otp_probe,
+	.remove = aspeed_otp_remove,
+	.driver = {
+		.name = KBUILD_MODNAME,
+		.of_match_table = aspeed_otp_of_matches,
+	},
+};
+
+module_platform_driver(aspeed_otp_driver);
+
+MODULE_AUTHOR("Johnny Huang <johnny_huang@aspeedtech.com>");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("ASPEED OTP Driver");
diff --git a/drivers/soc/aspeed/aspeed-ssp.c b/drivers/soc/aspeed/aspeed-ssp.c
new file mode 100644
index 000000000000..1e84c2907267
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-ssp.c
@@ -0,0 +1,208 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+// Copyright (C) ASPEED Technology Inc.
+
+#include <linux/io.h>
+#include <linux/fs.h>
+#include <linux/mod_devicetable.h>
+#include <linux/module.h>
+#include <linux/miscdevice.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/firmware.h>
+#include <linux/platform_device.h>
+#include <linux/interrupt.h>
+#include <linux/of.h>
+#include <linux/of_reserved_mem.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/mfd/syscon.h>
+#include <linux/regmap.h>
+#include <linux/dma-mapping.h>
+
+#define SSP_FILE_NAME			"ast2600_ssp.bin"
+#define AST2600_CVIC_TRIGGER		0x28
+#define AST2600_CVIC_PENDING_STATUS	0x18
+#define AST2600_CVIC_PENDING_CLEAR	0x1C
+
+#define SSP_CTRL_REG			0xa00
+#define SSP_CTRL_RESET_ASSERT		BIT(1)
+#define SSP_CTRL_EN			BIT(0)
+
+#define SSP_MEM_BASE_REG		0xa04
+#define SSP_IMEM_LIMIT_REG		0xa08
+#define SSP_DMEM_LIMIT_REG		0xa0c
+#define SSP_CACHE_RANGE_REG		0xa40
+#define SSP_CACHE_INVALID_REG		0xa44
+#define SSP_CACHE_CTRL_REG		0xa48
+#define SSP_CACHE_CLEAR_ICACHE		BIT(2)
+#define SSP_CACHE_CLEAR_DCACHE		BIT(1)
+#define SSP_CACHE_EN			BIT(0)
+
+#define SSP_TOTAL_MEM_SZ		(32 * 1024 * 1024)
+#define SSP_CACHED_MEM_SZ		(16 * 1024 * 1024)
+#define SSP_UNCACHED_MEM_SZ		(SSP_TOTAL_MEM_SZ - SSP_CACHED_MEM_SZ)
+#define SSP_CACHE_1ST_16MB_ENABLE	BIT(0)
+
+struct ast2600_ssp {
+	struct device	*dev;
+	struct regmap	*scu;
+	dma_addr_t		ssp_mem_phy_addr;
+	void __iomem	*ssp_mem_vir_addr;
+	void __iomem	*cvic;
+	int			irq[16];
+	int			n_irq;
+};
+
+static int ast_ssp_open(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+static int ast_ssp_release(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+static const struct file_operations ast_ssp_fops = {
+	.owner		= THIS_MODULE,
+	.open		= ast_ssp_open,
+	.release	= ast_ssp_release,
+	.llseek		= no_llseek,
+};
+
+struct miscdevice ast_ssp_misc = {
+	.minor = MISC_DYNAMIC_MINOR,
+	.name = "ast-ssp",
+	.fops = &ast_ssp_fops,
+};
+
+static irqreturn_t ast2600_ssp_interrupt(int irq, void *dev_id)
+{
+	struct ast2600_ssp *priv = dev_id;
+	u32 isr = readl(priv->cvic + AST2600_CVIC_PENDING_STATUS);
+
+	dev_info(priv->dev, "isr %x\n", isr);
+	writel(isr, priv->cvic + AST2600_CVIC_PENDING_CLEAR);
+
+	return IRQ_HANDLED;
+}
+static int ast_ssp_probe(struct platform_device *pdev)
+{
+	struct device_node *np, *mnode = dev_of_node(&pdev->dev);
+	const struct firmware *firmware;
+	struct ast2600_ssp *priv;
+	int i, ret;
+
+	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	priv->dev = &pdev->dev;
+	priv->scu = syscon_regmap_lookup_by_phandle(priv->dev->of_node, "aspeed,scu");
+	if (IS_ERR(priv->scu)) {
+		dev_err(priv->dev, "failed to find SCU regmap\n");
+		return -EINVAL;
+	}
+	platform_set_drvdata(pdev, priv);
+
+	ret = misc_register(&ast_ssp_misc);
+	if (ret) {
+		pr_err("can't misc_register :(\n");
+		return -EIO;
+	}
+	dev_set_drvdata(ast_ssp_misc.this_device, pdev);
+
+	ret = of_reserved_mem_device_init(&pdev->dev);
+	if (ret) {
+		dev_err(priv->dev,
+			"failed to initialize reserved mem: %d\n", ret);
+	}
+	ret = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));
+	priv->ssp_mem_vir_addr = dma_alloc_coherent(priv->dev, SSP_TOTAL_MEM_SZ,
+					   &priv->ssp_mem_phy_addr, GFP_KERNEL);
+
+	dev_info(priv->dev, "Virtual addr = 0x%08x, PHY addr = 0x%08x\n",
+	       (uint32_t)priv->ssp_mem_vir_addr, priv->ssp_mem_phy_addr);
+	if (request_firmware(&firmware, SSP_FILE_NAME, &pdev->dev) < 0) {
+		dev_err(&pdev->dev, "don't have %s\n", SSP_FILE_NAME);
+		release_firmware(firmware);
+		return 0;
+	}
+
+	memcpy(priv->ssp_mem_vir_addr, (void *)firmware->data, firmware->size);
+	release_firmware(firmware);
+
+	np = of_parse_phandle(mnode, "aspeed,cvic", 0);
+	if (!np) {
+		dev_err(&pdev->dev, "can't find CVIC\n");
+		return -EINVAL;
+	}
+
+	priv->cvic = devm_of_iomap(&pdev->dev, np, 0, NULL);
+	if (IS_ERR(priv->cvic)) {
+		dev_err(&pdev->dev, "can't map CVIC\n");
+		return -EINVAL;
+	}
+
+	i = 0;
+	while (0 != (priv->irq[i] = irq_of_parse_and_map(mnode, i))) {
+		ret = request_irq(priv->irq[i], ast2600_ssp_interrupt, 0,
+				  "ssp-sw-irq", priv);
+		i++;
+	}
+	priv->n_irq = i;
+	dev_info(priv->dev, "%d ISRs registered\n", priv->n_irq);
+
+	regmap_write(priv->scu, SSP_CTRL_REG, 0);
+	mdelay(1);
+	regmap_write(priv->scu, SSP_MEM_BASE_REG, priv->ssp_mem_phy_addr);
+	regmap_write(priv->scu, SSP_IMEM_LIMIT_REG, priv->ssp_mem_phy_addr + SSP_CACHED_MEM_SZ);
+	regmap_write(priv->scu, SSP_DMEM_LIMIT_REG, priv->ssp_mem_phy_addr + SSP_TOTAL_MEM_SZ);
+
+	regmap_write(priv->scu, SSP_CACHE_RANGE_REG, SSP_CACHE_1ST_16MB_ENABLE);
+
+	regmap_write(priv->scu, SSP_CTRL_REG, SSP_CTRL_RESET_ASSERT);
+	mdelay(1);
+	regmap_write(priv->scu, SSP_CTRL_REG, 0);
+	mdelay(1);
+	regmap_write(priv->scu, SSP_CTRL_REG, SSP_CTRL_EN);
+	dev_info(priv->dev, "Init successful\n");
+	return 0;
+}
+
+static int ast_ssp_remove(struct platform_device *pdev)
+{
+	struct ast2600_ssp *priv = platform_get_drvdata(pdev);
+	int i;
+
+	dev_info(priv->dev, "SSP module removed\n");
+	regmap_write(priv->scu, SSP_CTRL_REG, 0);
+	for (i = 0; i < priv->n_irq; i++)
+		free_irq(priv->irq[i], priv);
+
+	dma_free_coherent(priv->dev, SSP_TOTAL_MEM_SZ, priv->ssp_mem_vir_addr, priv->ssp_mem_phy_addr);
+	kfree(priv);
+
+	misc_deregister((struct miscdevice *)&ast_ssp_misc);
+
+	return 0;
+}
+
+static const struct of_device_id of_ast_ssp_match_table[] = {
+	{ .compatible = "aspeed,ast2600-ssp", },
+	{},
+};
+MODULE_DEVICE_TABLE(of, of_ast_ssp_match_table);
+
+static struct platform_driver ast_ssp_driver = {
+	.probe		= ast_ssp_probe,
+	.remove		= ast_ssp_remove,
+	.driver		= {
+		.name	= KBUILD_MODNAME,
+		.of_match_table = of_ast_ssp_match_table,
+	},
+};
+
+module_platform_driver(ast_ssp_driver);
+
+MODULE_LICENSE("Dual BSD/GPL");
diff --git a/drivers/soc/aspeed/aspeed-udma.c b/drivers/soc/aspeed/aspeed-udma.c
new file mode 100644
index 000000000000..819115e738fb
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-udma.c
@@ -0,0 +1,464 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright 2020 Aspeed Technology Inc.
+ */
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+#include <linux/dma-mapping.h>
+#include <linux/spinlock.h>
+#include <linux/soc/aspeed/aspeed-udma.h>
+#include <linux/delay.h>
+
+#define DEVICE_NAME "aspeed-udma"
+
+/* UART DMA registers offset */
+#define UDMA_TX_DMA_EN		0x000
+#define UDMA_RX_DMA_EN		0x004
+#define UDMA_MISC		0x008
+#define   UDMA_MISC_TX_BUFSZ_MASK	GENMASK(1, 0)
+#define   UDMA_MISC_TX_BUFSZ_SHIFT	0
+#define   UDMA_MISC_RX_BUFSZ_MASK	GENMASK(3, 2)
+#define   UDMA_MISC_RX_BUFSZ_SHIFT	2
+#define UDMA_TIMEOUT_TIMER	0x00c
+#define UDMA_TX_DMA_RST		0x020
+#define UDMA_RX_DMA_RST		0x024
+#define UDMA_TX_DMA_INT_EN	0x030
+#define UDMA_TX_DMA_INT_STAT	0x034
+#define UDMA_RX_DMA_INT_EN	0x038
+#define UDMA_RX_DMA_INT_STAT	0x03c
+
+#define UDMA_CHX_OFF(x)		((x) * 0x20)
+#define UDMA_CHX_TX_RD_PTR(x)	(0x040 + UDMA_CHX_OFF(x))
+#define UDMA_CHX_TX_WR_PTR(x)	(0x044 + UDMA_CHX_OFF(x))
+#define UDMA_CHX_TX_BUF_BASE(x)	(0x048 + UDMA_CHX_OFF(x))
+#define UDMA_CHX_TX_CTRL(x)	(0x04c + UDMA_CHX_OFF(x))
+#define   UDMA_TX_CTRL_TMOUT_DISABLE	BIT(4)
+#define   UDMA_TX_CTRL_BUFSZ_MASK	GENMASK(3, 0)
+#define   UDMA_TX_CTRL_BUFSZ_SHIFT	0
+#define UDMA_CHX_RX_RD_PTR(x)	(0x050 + UDMA_CHX_OFF(x))
+#define UDMA_CHX_RX_WR_PTR(x)	(0x054 + UDMA_CHX_OFF(x))
+#define UDMA_CHX_RX_BUF_BASE(x)	(0x058 + UDMA_CHX_OFF(x))
+#define UDMA_CHX_RX_CTRL(x)	(0x05c + UDMA_CHX_OFF(x))
+#define   UDMA_RX_CTRL_TMOUT_DISABLE	BIT(4)
+#define   UDMA_RX_CTRL_BUFSZ_MASK	GENMASK(3, 0)
+#define   UDMA_RX_CTRL_BUFSZ_SHIFT	0
+
+#define UDMA_MAX_CHANNEL	14
+#define UDMA_TIMEOUT		0x200
+
+enum aspeed_udma_bufsz_code {
+	UDMA_BUFSZ_CODE_1KB,
+	UDMA_BUFSZ_CODE_4KB,
+	UDMA_BUFSZ_CODE_16KB,
+	UDMA_BUFSZ_CODE_64KB,
+
+	/*
+	 * 128KB and above are supported ONLY for
+	 * virtual UARTs. For physical UARTs, the
+	 * size code is wrapped around at the 64K
+	 * boundary.
+	 */
+	UDMA_BUFSZ_CODE_128KB,
+	UDMA_BUFSZ_CODE_256KB,
+	UDMA_BUFSZ_CODE_512KB,
+	UDMA_BUFSZ_CODE_1024KB,
+	UDMA_BUFSZ_CODE_2048KB,
+	UDMA_BUFSZ_CODE_4096KB,
+	UDMA_BUFSZ_CODE_8192KB,
+	UDMA_BUFSZ_CODE_16384KB,
+};
+
+struct aspeed_udma_chan {
+	dma_addr_t dma_addr;
+
+	struct circ_buf *rb;
+	u32 rb_sz;
+
+	aspeed_udma_cb_t cb;
+	void *cb_arg;
+
+	bool dis_tmout;
+};
+
+struct aspeed_udma {
+	struct device *dev;
+	u8 __iomem *regs;
+	int irq;
+	struct aspeed_udma_chan tx_chs[UDMA_MAX_CHANNEL];
+	struct aspeed_udma_chan rx_chs[UDMA_MAX_CHANNEL];
+	spinlock_t lock;
+};
+
+struct aspeed_udma udma[1];
+
+static int aspeed_udma_get_bufsz_code(u32 buf_sz)
+{
+	switch (buf_sz) {
+	case 0x400:
+		return UDMA_BUFSZ_CODE_1KB;
+	case 0x1000:
+		return UDMA_BUFSZ_CODE_4KB;
+	case 0x4000:
+		return UDMA_BUFSZ_CODE_16KB;
+	case 0x10000:
+		return UDMA_BUFSZ_CODE_64KB;
+	case 0x20000:
+		return UDMA_BUFSZ_CODE_128KB;
+	case 0x40000:
+		return UDMA_BUFSZ_CODE_256KB;
+	case 0x80000:
+		return UDMA_BUFSZ_CODE_512KB;
+	case 0x100000:
+		return UDMA_BUFSZ_CODE_1024KB;
+	case 0x200000:
+		return UDMA_BUFSZ_CODE_2048KB;
+	case 0x400000:
+		return UDMA_BUFSZ_CODE_4096KB;
+	case 0x800000:
+		return UDMA_BUFSZ_CODE_8192KB;
+	case 0x1000000:
+		return UDMA_BUFSZ_CODE_16384KB;
+	default:
+		return -1;
+	}
+
+	return -1;
+}
+
+static u32 aspeed_udma_get_tx_rptr(u32 ch_no)
+{
+	return readl(udma->regs + UDMA_CHX_TX_RD_PTR(ch_no));
+}
+
+static u32 aspeed_udma_get_rx_wptr(u32 ch_no)
+{
+	return readl(udma->regs + UDMA_CHX_RX_WR_PTR(ch_no));
+}
+
+static void aspeed_udma_set_ptr(u32 ch_no, u32 ptr, bool is_tx)
+{
+	writel(ptr, udma->regs +
+	       ((is_tx) ? UDMA_CHX_TX_WR_PTR(ch_no) : UDMA_CHX_RX_RD_PTR(ch_no)));
+}
+
+void aspeed_udma_set_tx_wptr(u32 ch_no, u32 wptr)
+{
+	aspeed_udma_set_ptr(ch_no, wptr, true);
+}
+EXPORT_SYMBOL(aspeed_udma_set_tx_wptr);
+
+void aspeed_udma_set_rx_rptr(u32 ch_no, u32 rptr)
+{
+	aspeed_udma_set_ptr(ch_no, rptr, false);
+}
+EXPORT_SYMBOL(aspeed_udma_set_rx_rptr);
+
+static int aspeed_udma_free_chan(u32 ch_no, bool is_tx)
+{
+	u32 reg;
+	unsigned long flags;
+
+	if (ch_no > UDMA_MAX_CHANNEL)
+		return -EINVAL;
+
+	spin_lock_irqsave(&udma->lock, flags);
+
+	reg = readl(udma->regs +
+			((is_tx) ? UDMA_TX_DMA_INT_EN : UDMA_RX_DMA_INT_EN));
+	reg &= ~(0x1 << ch_no);
+
+	writel(reg, udma->regs +
+			((is_tx) ? UDMA_TX_DMA_INT_EN : UDMA_RX_DMA_INT_EN));
+
+	spin_unlock_irqrestore(&udma->lock, flags);
+
+	return 0;
+}
+
+int aspeed_udma_free_tx_chan(u32 ch_no)
+{
+	return aspeed_udma_free_chan(ch_no, true);
+}
+EXPORT_SYMBOL(aspeed_udma_free_tx_chan);
+
+int aspeed_udma_free_rx_chan(u32 ch_no)
+{
+	return aspeed_udma_free_chan(ch_no, false);
+}
+EXPORT_SYMBOL(aspeed_udma_free_rx_chan);
+
+static int aspeed_udma_request_chan(u32 ch_no, dma_addr_t addr,
+		struct circ_buf *rb, u32 rb_sz,
+		aspeed_udma_cb_t cb, void *id, bool dis_tmout, bool is_tx)
+{
+	int retval = 0;
+	int rbsz_code;
+
+	u32 reg;
+	unsigned long flags;
+	struct aspeed_udma_chan *ch;
+
+	if (ch_no > UDMA_MAX_CHANNEL) {
+		retval = -EINVAL;
+		goto out;
+	}
+
+	if (IS_ERR_OR_NULL(rb) || IS_ERR_OR_NULL(rb->buf)) {
+		retval = -EINVAL;
+		goto out;
+	}
+
+	rbsz_code = aspeed_udma_get_bufsz_code(rb_sz);
+	if (rbsz_code < 0) {
+		retval = -EINVAL;
+		goto out;
+	}
+
+	spin_lock_irqsave(&udma->lock, flags);
+
+	if (is_tx) {
+		reg = readl(udma->regs + UDMA_TX_DMA_INT_EN);
+		if (reg & (0x1 << ch_no)) {
+			retval = -EBUSY;
+			goto unlock_n_out;
+		}
+
+		reg |= (0x1 << ch_no);
+		writel(reg, udma->regs + UDMA_TX_DMA_INT_EN);
+
+		reg = readl(udma->regs + UDMA_CHX_TX_CTRL(ch_no));
+		reg |= (dis_tmout) ? UDMA_TX_CTRL_TMOUT_DISABLE : 0;
+		reg |= (rbsz_code << UDMA_TX_CTRL_BUFSZ_SHIFT) & UDMA_TX_CTRL_BUFSZ_MASK;
+		writel(reg, udma->regs + UDMA_CHX_TX_CTRL(ch_no));
+
+		writel(addr, udma->regs + UDMA_CHX_TX_BUF_BASE(ch_no));
+	} else {
+		reg = readl(udma->regs + UDMA_RX_DMA_INT_EN);
+		if (reg & (0x1 << ch_no)) {
+			retval = -EBUSY;
+			goto unlock_n_out;
+		}
+
+		reg |= (0x1 << ch_no);
+		writel(reg, udma->regs + UDMA_RX_DMA_INT_EN);
+
+		reg = readl(udma->regs + UDMA_CHX_RX_CTRL(ch_no));
+		reg |= (dis_tmout) ? UDMA_RX_CTRL_TMOUT_DISABLE : 0;
+		reg |= (rbsz_code << UDMA_RX_CTRL_BUFSZ_SHIFT) & UDMA_RX_CTRL_BUFSZ_MASK;
+		writel(reg, udma->regs + UDMA_CHX_RX_CTRL(ch_no));
+
+		writel(addr, udma->regs + UDMA_CHX_RX_BUF_BASE(ch_no));
+	}
+
+	ch = (is_tx) ? &udma->tx_chs[ch_no] : &udma->rx_chs[ch_no];
+	ch->rb = rb;
+	ch->rb_sz = rb_sz;
+	ch->cb = cb;
+	ch->cb_arg = id;
+	ch->dma_addr = addr;
+	ch->dis_tmout = dis_tmout;
+
+unlock_n_out:
+	spin_unlock_irqrestore(&udma->lock, flags);
+out:
+	return 0;
+}
+
+int aspeed_udma_request_tx_chan(u32 ch_no, dma_addr_t addr,
+		struct circ_buf *rb, u32 rb_sz,
+		aspeed_udma_cb_t cb, void *id, bool dis_tmout)
+{
+	return aspeed_udma_request_chan(ch_no, addr, rb, rb_sz, cb, id,
+									dis_tmout, true);
+}
+EXPORT_SYMBOL(aspeed_udma_request_tx_chan);
+
+int aspeed_udma_request_rx_chan(u32 ch_no, dma_addr_t addr,
+		struct circ_buf *rb, u32 rb_sz,
+		aspeed_udma_cb_t cb, void *id, bool dis_tmout)
+{
+	return aspeed_udma_request_chan(ch_no, addr, rb, rb_sz, cb, id,
+									dis_tmout, false);
+}
+EXPORT_SYMBOL(aspeed_udma_request_rx_chan);
+
+static void aspeed_udma_chan_ctrl(u32 ch_no, u32 op, bool is_tx)
+{
+	unsigned long flags;
+	u32 reg_en, reg_rst;
+	u32 reg_en_off = (is_tx) ? UDMA_TX_DMA_EN : UDMA_RX_DMA_EN;
+	u32 reg_rst_off = (is_tx) ? UDMA_TX_DMA_RST : UDMA_TX_DMA_RST;
+
+	if (ch_no > UDMA_MAX_CHANNEL)
+		return;
+
+	spin_lock_irqsave(&udma->lock, flags);
+
+	reg_en = readl(udma->regs + reg_en_off);
+	reg_rst = readl(udma->regs + reg_rst_off);
+
+	switch (op) {
+	case ASPEED_UDMA_OP_ENABLE:
+		reg_en |= (0x1 << ch_no);
+		writel(reg_en, udma->regs + reg_en_off);
+		break;
+	case ASPEED_UDMA_OP_DISABLE:
+		reg_en &= ~(0x1 << ch_no);
+		writel(reg_en, udma->regs + reg_en_off);
+		break;
+	case ASPEED_UDMA_OP_RESET:
+		reg_en &= ~(0x1 << ch_no);
+		writel(reg_en, udma->regs + reg_en_off);
+
+		reg_rst |= (0x1 << ch_no);
+		writel(reg_rst, udma->regs + reg_rst_off);
+
+		udelay(100);
+
+		reg_rst &= ~(0x1 << ch_no);
+		writel(reg_rst, udma->regs + reg_rst_off);
+		break;
+	default:
+		break;
+	}
+
+	spin_unlock_irqrestore(&udma->lock, flags);
+}
+
+void aspeed_udma_tx_chan_ctrl(u32 ch_no, enum aspeed_udma_ops op)
+{
+	aspeed_udma_chan_ctrl(ch_no, op, true);
+}
+EXPORT_SYMBOL(aspeed_udma_tx_chan_ctrl);
+
+void aspeed_udma_rx_chan_ctrl(u32 ch_no, enum aspeed_udma_ops op)
+{
+	aspeed_udma_chan_ctrl(ch_no, op, false);
+}
+EXPORT_SYMBOL(aspeed_udma_rx_chan_ctrl);
+
+static irqreturn_t aspeed_udma_isr(int irq, void *arg)
+{
+	u32 bit;
+	unsigned long tx_stat = readl(udma->regs + UDMA_TX_DMA_INT_STAT);
+	unsigned long rx_stat = readl(udma->regs + UDMA_RX_DMA_INT_STAT);
+
+	if (udma != (struct aspeed_udma *)arg)
+		return IRQ_NONE;
+
+	if (tx_stat == 0 && rx_stat == 0)
+		return IRQ_NONE;
+
+	for_each_set_bit(bit, &tx_stat, UDMA_MAX_CHANNEL) {
+		writel((0x1 << bit), udma->regs + UDMA_TX_DMA_INT_STAT);
+		if (udma->tx_chs[bit].cb)
+			udma->tx_chs[bit].cb(aspeed_udma_get_tx_rptr(bit),
+					udma->tx_chs[bit].cb_arg);
+	}
+
+	for_each_set_bit(bit, &rx_stat, UDMA_MAX_CHANNEL) {
+		writel((0x1 << bit), udma->regs + UDMA_RX_DMA_INT_STAT);
+		if (udma->rx_chs[bit].cb)
+			udma->rx_chs[bit].cb(aspeed_udma_get_rx_wptr(bit),
+					udma->rx_chs[bit].cb_arg);
+	}
+
+	return IRQ_HANDLED;
+}
+
+static int aspeed_udma_probe(struct platform_device *pdev)
+{
+	int i, rc;
+	uint32_t reg;
+	struct resource *res;
+	struct device *dev = &pdev->dev;
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (IS_ERR_OR_NULL(res)) {
+		dev_err(dev, "failed to get register base\n");
+		return -ENODEV;
+	}
+
+	udma->regs = devm_ioremap_resource(dev, res);
+	if (IS_ERR_OR_NULL(udma->regs)) {
+		dev_err(dev, "failed to map registers\n");
+		return PTR_ERR(udma->regs);
+	}
+
+	/* disable for safety */
+	writel(0x0, udma->regs + UDMA_TX_DMA_EN);
+	writel(0x0, udma->regs + UDMA_RX_DMA_EN);
+
+	udma->irq = platform_get_irq(pdev, 0);
+	if (udma->irq < 0) {
+		dev_err(dev, "failed to get IRQ number\n");
+		return -ENODEV;
+	}
+
+	rc = devm_request_irq(dev, udma->irq, aspeed_udma_isr,
+			IRQF_SHARED, DEVICE_NAME, udma);
+	if (rc) {
+		dev_err(dev, "failed to request IRQ handler\n");
+		return rc;
+	}
+
+	/*
+	 * For AST2600 A1 legacy design.
+	 *  - TX ringbuffer size: 4KB
+	 *  - RX ringbuffer size: 64KB
+	 *  - Timeout timer disabled
+	 */
+	reg = ((UDMA_BUFSZ_CODE_4KB << UDMA_MISC_TX_BUFSZ_SHIFT) & UDMA_MISC_TX_BUFSZ_MASK) |
+	      ((UDMA_BUFSZ_CODE_64KB << UDMA_MISC_RX_BUFSZ_SHIFT) & UDMA_MISC_RX_BUFSZ_MASK);
+	writel(reg, udma->regs + UDMA_MISC);
+
+	for (i = 0; i < UDMA_MAX_CHANNEL; ++i) {
+		writel(0, udma->regs + UDMA_CHX_TX_WR_PTR(i));
+		writel(0, udma->regs + UDMA_CHX_RX_RD_PTR(i));
+	}
+
+	writel(0xffffffff, udma->regs + UDMA_TX_DMA_RST);
+	writel(0x0, udma->regs + UDMA_TX_DMA_RST);
+
+	writel(0xffffffff, udma->regs + UDMA_RX_DMA_RST);
+	writel(0x0, udma->regs + UDMA_RX_DMA_RST);
+
+	writel(0x0, udma->regs + UDMA_TX_DMA_INT_EN);
+	writel(0xffffffff, udma->regs + UDMA_TX_DMA_INT_STAT);
+	writel(0x0, udma->regs + UDMA_RX_DMA_INT_EN);
+	writel(0xffffffff, udma->regs + UDMA_RX_DMA_INT_STAT);
+
+	writel(UDMA_TIMEOUT, udma->regs + UDMA_TIMEOUT_TIMER);
+
+	spin_lock_init(&udma->lock);
+
+	dev_set_drvdata(dev, udma);
+
+	return 0;
+}
+
+static const struct of_device_id aspeed_udma_match[] = {
+	{ .compatible = "aspeed,ast2500-udma" },
+	{ .compatible = "aspeed,ast2600-udma" },
+	{ },
+};
+
+static struct platform_driver aspeed_udma_driver = {
+	.driver = {
+		.name = DEVICE_NAME,
+		.of_match_table = aspeed_udma_match,
+
+	},
+	.probe = aspeed_udma_probe,
+};
+
+module_platform_driver(aspeed_udma_driver);
+
+MODULE_AUTHOR("Chia-Wei Wang <chiawei_wang@aspeedtech.com>");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Aspeed UDMA Engine Driver");
diff --git a/drivers/soc/aspeed/aspeed-usb-ahp.c b/drivers/soc/aspeed/aspeed-usb-ahp.c
new file mode 100644
index 000000000000..c07703229989
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-usb-ahp.c
@@ -0,0 +1,47 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright 2021 Aspeed Technology Inc.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/regmap.h>
+#include <asm/io.h>
+
+static const struct of_device_id aspeed_usb_ahp_dt_ids[] = {
+	{
+		.compatible = "aspeed,ast2600-usb2ahp",
+	},
+	{ }
+};
+MODULE_DEVICE_TABLE(of, aspeed_usb_ahp_dt_ids);
+
+static int aspeed_usb_ahp_probe(struct platform_device *pdev)
+{
+	dev_info(&pdev->dev, "Initialized USB2AHP\n");
+
+	return 0;
+}
+
+static int aspeed_usb_ahp_remove(struct platform_device *pdev)
+{
+	dev_info(&pdev->dev, "Remove USB2AHP\n");
+
+	return 0;
+}
+
+static struct platform_driver aspeed_usb_ahp_driver = {
+	.probe		= aspeed_usb_ahp_probe,
+	.remove		= aspeed_usb_ahp_remove,
+	.driver		= {
+		.name	= KBUILD_MODNAME,
+		.of_match_table	= aspeed_usb_ahp_dt_ids,
+	},
+};
+module_platform_driver(aspeed_usb_ahp_driver);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Neal Liu <neal_liu@aspeedtech.com>");
diff --git a/drivers/soc/aspeed/aspeed-usb-phy.c b/drivers/soc/aspeed/aspeed-usb-phy.c
new file mode 100644
index 000000000000..f92e3683ffdd
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-usb-phy.c
@@ -0,0 +1,70 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright 2021 Aspeed Technology Inc.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/regmap.h>
+#include <asm/io.h>
+
+struct usb_phy_ctrl {
+	u32 offset;
+	u32 set_bit;
+};
+
+static const struct of_device_id aspeed_usb_phy_dt_ids[] = {
+	{
+		.compatible = "aspeed,ast2600-uphyb",
+	},
+	{ }
+};
+MODULE_DEVICE_TABLE(of, aspeed_usb_phy_dt_ids);
+
+static int aspeed_usb_phy_probe(struct platform_device *pdev)
+{
+	struct device_node *node = pdev->dev.of_node;
+	struct usb_phy_ctrl *ctrl_data;
+	void __iomem *base;
+	int ret;
+
+	ctrl_data = devm_kzalloc(&pdev->dev, sizeof(struct usb_phy_ctrl), GFP_KERNEL);
+	if (!ctrl_data)
+		return -ENOMEM;
+
+	base = of_iomap(node, 0);
+
+	ret = of_property_read_u32_array(node, "ctrl", (u32 *)ctrl_data, 2);
+	if (ret < 0) {
+		dev_err(&pdev->dev, "Could not read ctrl property\n");
+		return -EINVAL;
+	}
+
+	writel(readl(base + ctrl_data->offset) | BIT(ctrl_data->set_bit),
+		base + ctrl_data->offset);
+
+	dev_info(&pdev->dev, "Initialized USB PHY\n");
+
+	return 0;
+}
+
+static int aspeed_usb_phy_remove(struct platform_device *pdev)
+{
+	return 0;
+}
+
+static struct platform_driver aspeed_usb_phy_driver = {
+	.probe		= aspeed_usb_phy_probe,
+	.remove		= aspeed_usb_phy_remove,
+	.driver		= {
+		.name	= KBUILD_MODNAME,
+		.of_match_table	= aspeed_usb_phy_dt_ids,
+	},
+};
+module_platform_driver(aspeed_usb_phy_driver);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Neal Liu <neal_liu@aspeedtech.com>");
diff --git a/drivers/soc/aspeed/ast2500-espi.c b/drivers/soc/aspeed/ast2500-espi.c
new file mode 100644
index 000000000000..7d6bea62581b
--- /dev/null
+++ b/drivers/soc/aspeed/ast2500-espi.c
@@ -0,0 +1,1511 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright 2023 Aspeed Technology Inc.
+ */
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/clk.h>
+#include <linux/sizes.h>
+#include <linux/module.h>
+#include <linux/bitfield.h>
+#include <linux/of_device.h>
+#include <linux/interrupt.h>
+#include <linux/platform_device.h>
+#include <linux/miscdevice.h>
+#include <linux/dma-mapping.h>
+#include <linux/uaccess.h>
+#include <linux/vmalloc.h>
+#include <linux/poll.h>
+
+#include "ast2500-espi.h"
+
+#define DEVICE_NAME		"aspeed-espi"
+
+#define PERIF_MCYC_UNLOCK	0xfedc756e
+#define PERIF_MCYC_ALIGN	SZ_64K
+#define PERIF_MMBI_ALIGN	SZ_64K
+#define PERIF_MMBI_INST_NUM	8
+
+#define OOB_DMA_DESC_NUM	8
+#define OOB_DMA_DESC_CUSTOM	0x4
+
+#define FLASH_SAFS_ALIGN	SZ_16M
+
+struct ast2500_espi_perif {
+	struct {
+		bool enable;
+		void *virt;
+		dma_addr_t taddr;
+		uint32_t saddr;
+		uint32_t size;
+	} mcyc;
+
+	struct {
+		bool enable;
+		void *np_tx_virt;
+		dma_addr_t np_tx_addr;
+		void *pc_tx_virt;
+		dma_addr_t pc_tx_addr;
+		void *pc_rx_virt;
+		dma_addr_t pc_rx_addr;
+	} dma;
+
+	bool rx_ready;
+	wait_queue_head_t wq;
+
+	spinlock_t lock;
+	struct mutex np_tx_mtx;
+	struct mutex pc_tx_mtx;
+	struct mutex pc_rx_mtx;
+
+	struct miscdevice mdev;
+};
+
+struct ast2500_espi_vw {
+	struct {
+		bool hw_mode;
+		uint32_t grp;
+		uint32_t dir;
+		uint32_t val;
+	} gpio;
+
+	struct miscdevice mdev;
+};
+
+struct ast2500_espi_oob {
+	struct {
+		bool enable;
+		void *tx_virt;
+		dma_addr_t tx_addr;
+		void *rx_virt;
+		dma_addr_t rx_addr;
+	} dma;
+
+	bool rx_ready;
+	wait_queue_head_t wq;
+
+	spinlock_t lock;
+	struct mutex tx_mtx;
+	struct mutex rx_mtx;
+
+	struct miscdevice mdev;
+};
+
+struct ast2500_espi_flash {
+	struct {
+		uint32_t mode;
+		phys_addr_t taddr;
+		uint32_t size;
+	} safs;
+
+	struct {
+		bool enable;
+		void *tx_virt;
+		dma_addr_t tx_addr;
+		void *rx_virt;
+		dma_addr_t rx_addr;
+	} dma;
+
+	bool rx_ready;
+	wait_queue_head_t wq;
+
+	spinlock_t lock;
+	struct mutex rx_mtx;
+	struct mutex tx_mtx;
+
+	struct miscdevice mdev;
+};
+
+struct ast2500_espi {
+	struct device *dev;
+	void __iomem *regs;
+	struct clk *clk;
+	int irq;
+
+	struct ast2500_espi_perif perif;
+	struct ast2500_espi_vw vw;
+	struct ast2500_espi_oob oob;
+	struct ast2500_espi_flash flash;
+};
+
+/* peripheral channel (CH0) */
+static long ast2500_espi_perif_pc_get_rx(struct file *fp,
+					 struct ast2500_espi_perif *perif,
+					 struct aspeed_espi_ioc *ioc)
+{
+	uint32_t reg, cyc, tag, len;
+	struct ast2500_espi *espi;
+	struct espi_comm_hdr *hdr;
+	unsigned long flags;
+	uint32_t pkt_len;
+	uint8_t *pkt;
+	int i, rc;
+
+	espi = container_of(perif, struct ast2500_espi, perif);
+
+	if (fp->f_flags & O_NONBLOCK) {
+		if (!mutex_trylock(&perif->pc_rx_mtx))
+			return -EAGAIN;
+
+		if (!perif->rx_ready) {
+			rc = -ENODATA;
+			goto unlock_mtx_n_out;
+		}
+	} else {
+		mutex_lock(&perif->pc_rx_mtx);
+
+		if (!perif->rx_ready) {
+			rc = wait_event_interruptible(perif->wq, perif->rx_ready);
+			if (rc == -ERESTARTSYS) {
+				rc = -EINTR;
+				goto unlock_mtx_n_out;
+			}
+		}
+	}
+
+	/*
+	 * common header (i.e. cycle type, tag, and length)
+	 * part is written to HW registers
+	 */
+	reg = readl(espi->regs + ESPI_PERIF_PC_RX_CTRL);
+	cyc = FIELD_GET(ESPI_PERIF_PC_RX_CTRL_CYC, reg);
+	tag = FIELD_GET(ESPI_PERIF_PC_RX_CTRL_TAG, reg);
+	len = FIELD_GET(ESPI_PERIF_PC_RX_CTRL_LEN, reg);
+
+	/*
+	 * calculate the length of the rest part of the
+	 * eSPI packet to be read from HW and copied to
+	 * user space.
+	 */
+	switch (cyc) {
+	case ESPI_PERIF_MSG:
+		pkt_len = sizeof(struct espi_perif_msg);
+		break;
+	case ESPI_PERIF_MSG_D:
+		pkt_len = ((len) ? len : ESPI_MAX_PLD_LEN) +
+			  sizeof(struct espi_perif_msg);
+		break;
+	case ESPI_PERIF_SUC_CMPLT_D_MIDDLE:
+	case ESPI_PERIF_SUC_CMPLT_D_FIRST:
+	case ESPI_PERIF_SUC_CMPLT_D_LAST:
+	case ESPI_PERIF_SUC_CMPLT_D_ONLY:
+		pkt_len = ((len) ? len : ESPI_MAX_PLD_LEN) +
+			  sizeof(struct espi_perif_cmplt);
+		break;
+	case ESPI_PERIF_SUC_CMPLT:
+	case ESPI_PERIF_UNSUC_CMPLT:
+		pkt_len = sizeof(struct espi_perif_cmplt);
+		break;
+	default:
+		rc = -EFAULT;
+		goto unlock_mtx_n_out;
+	}
+
+	if (ioc->pkt_len < pkt_len) {
+		rc = -EINVAL;
+		goto unlock_mtx_n_out;
+	}
+
+	pkt = vmalloc(pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_mtx_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+	hdr->cyc = cyc;
+	hdr->tag = tag;
+	hdr->len_h = len >> 8;
+	hdr->len_l = len & 0xff;
+
+	if (perif->dma.enable) {
+		memcpy(hdr + 1, perif->dma.pc_rx_virt, pkt_len - sizeof(*hdr));
+	} else {
+		for (i = sizeof(*hdr); i < pkt_len; ++i)
+			reg = readl(espi->regs + ESPI_PERIF_PC_RX_DATA) & 0xff;
+	}
+
+	if (copy_to_user((void __user *)ioc->pkt, pkt, pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	spin_lock_irqsave(&perif->lock, flags);
+
+	writel(ESPI_PERIF_PC_RX_CTRL_SERV_PEND, espi->regs + ESPI_PERIF_PC_RX_CTRL);
+	perif->rx_ready = 0;
+
+	spin_unlock_irqrestore(&perif->lock, flags);
+
+	rc = 0;
+
+free_n_out:
+	vfree(pkt);
+
+unlock_mtx_n_out:
+	mutex_unlock(&perif->pc_rx_mtx);
+
+	return rc;
+}
+
+static long ast2500_espi_perif_pc_put_tx(struct file *fp,
+					 struct ast2500_espi_perif *perif,
+					 struct aspeed_espi_ioc *ioc)
+{
+	uint32_t reg, cyc, tag, len;
+	struct ast2500_espi *espi;
+	struct espi_comm_hdr *hdr;
+	uint8_t *pkt;
+	int i, rc;
+
+	espi = container_of(perif, struct ast2500_espi, perif);
+
+	if (!mutex_trylock(&perif->pc_tx_mtx))
+		return -EAGAIN;
+
+	reg = readl(espi->regs + ESPI_PERIF_PC_TX_CTRL);
+	if (reg & ESPI_PERIF_PC_TX_CTRL_TRIG_PEND) {
+		rc = -EBUSY;
+		goto unlock_n_out;
+	}
+
+	pkt = vmalloc(ioc->pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+
+	if (copy_from_user(pkt, (void __user *)ioc->pkt, ioc->pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	/*
+	 * common header (i.e. cycle type, tag, and length)
+	 * part is written to HW registers
+	 */
+	if (perif->dma.enable) {
+		memcpy(perif->dma.pc_tx_virt, hdr + 1, ioc->pkt_len - sizeof(*hdr));
+		dma_wmb();
+	} else {
+		for (i = sizeof(*hdr); i < ioc->pkt_len; ++i)
+			writel(pkt[i], espi->regs + ESPI_PERIF_PC_TX_DATA);
+	}
+
+	cyc = hdr->cyc;
+	tag = hdr->tag;
+	len = (hdr->len_h << 8) | (hdr->len_l & 0xff);
+
+	reg = FIELD_PREP(ESPI_PERIF_PC_TX_CTRL_CYC, cyc)
+	      | FIELD_PREP(ESPI_PERIF_PC_TX_CTRL_TAG, tag)
+	      | FIELD_PREP(ESPI_PERIF_PC_TX_CTRL_LEN, len)
+	      | ESPI_PERIF_PC_TX_CTRL_TRIG_PEND;
+	writel(reg, espi->regs + ESPI_PERIF_PC_TX_CTRL);
+
+	rc = 0;
+
+free_n_out:
+	vfree(pkt);
+
+unlock_n_out:
+	mutex_unlock(&perif->pc_tx_mtx);
+
+	return rc;
+}
+
+static long ast2500_espi_perif_np_put_tx(struct file *fp,
+					 struct ast2500_espi_perif *perif,
+					 struct aspeed_espi_ioc *ioc)
+{
+	uint32_t reg, cyc, tag, len;
+	struct ast2500_espi *espi;
+	struct espi_comm_hdr *hdr;
+	uint8_t *pkt;
+	int i, rc;
+
+	espi = container_of(perif, struct ast2500_espi, perif);
+
+	if (!mutex_trylock(&perif->np_tx_mtx))
+		return -EAGAIN;
+
+	reg = readl(espi->regs + ESPI_PERIF_NP_TX_CTRL);
+	if (reg & ESPI_PERIF_NP_TX_CTRL_TRIG_PEND) {
+		rc = -EBUSY;
+		goto unlock_n_out;
+	}
+
+	pkt = vmalloc(ioc->pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+
+	if (copy_from_user(pkt, (void __user *)ioc->pkt, ioc->pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	/*
+	 * common header (i.e. cycle type, tag, and length)
+	 * part is written to HW registers
+	 */
+	if (perif->dma.enable) {
+		memcpy(perif->dma.np_tx_virt, hdr + 1, ioc->pkt_len - sizeof(*hdr));
+		dma_wmb();
+	} else {
+		for (i = sizeof(*hdr); i < ioc->pkt_len; ++i)
+			writel(pkt[i], espi->regs + ESPI_PERIF_NP_TX_DATA);
+	}
+
+	cyc = hdr->cyc;
+	tag = hdr->tag;
+	len = (hdr->len_h << 8) | (hdr->len_l & 0xff);
+
+	reg = FIELD_PREP(ESPI_PERIF_NP_TX_CTRL_CYC, cyc)
+	      | FIELD_PREP(ESPI_PERIF_NP_TX_CTRL_TAG, tag)
+	      | FIELD_PREP(ESPI_PERIF_NP_TX_CTRL_LEN, len)
+	      | ESPI_PERIF_NP_TX_CTRL_TRIG_PEND;
+	writel(reg, espi->regs + ESPI_PERIF_NP_TX_CTRL);
+
+	rc = 0;
+
+free_n_out:
+	vfree(pkt);
+
+unlock_n_out:
+	mutex_unlock(&perif->np_tx_mtx);
+
+	return rc;
+}
+
+static long ast2500_espi_perif_ioctl(struct file *fp, unsigned int cmd, unsigned long arg)
+{
+	struct ast2500_espi_perif *perif;
+	struct aspeed_espi_ioc ioc;
+
+	perif = container_of(fp->private_data, struct ast2500_espi_perif, mdev);
+
+	if (copy_from_user(&ioc, (void __user *)arg, sizeof(ioc)))
+		return -EFAULT;
+
+	if (ioc.pkt_len > ESPI_MAX_PKT_LEN)
+		return -EINVAL;
+
+	switch (cmd) {
+	case ASPEED_ESPI_PERIF_PC_GET_RX:
+		return ast2500_espi_perif_pc_get_rx(fp, perif, &ioc);
+	case ASPEED_ESPI_PERIF_PC_PUT_TX:
+		return ast2500_espi_perif_pc_put_tx(fp, perif, &ioc);
+	case ASPEED_ESPI_PERIF_NP_PUT_TX:
+		return ast2500_espi_perif_np_put_tx(fp, perif, &ioc);
+	default:
+		break;
+	};
+
+	return -EINVAL;
+}
+
+static int ast2500_espi_perif_mmap(struct file *fp, struct vm_area_struct *vma)
+{
+	struct ast2500_espi_perif *perif;
+	unsigned long vm_size;
+	pgprot_t vm_prot;
+
+	perif = container_of(fp->private_data, struct ast2500_espi_perif, mdev);
+	if (!perif->mcyc.enable)
+		return -EPERM;
+
+	vm_size = vma->vm_end - vma->vm_start;
+	vm_prot = vma->vm_page_prot;
+
+	if (((vma->vm_pgoff << PAGE_SHIFT) + vm_size) > perif->mcyc.size)
+		return -EINVAL;
+
+	vm_prot = pgprot_noncached(vm_prot);
+
+	if (remap_pfn_range(vma, vma->vm_start,
+			    (perif->mcyc.taddr >> PAGE_SHIFT) + vma->vm_pgoff,
+			    vm_size, vm_prot))
+		return -EAGAIN;
+
+	return 0;
+}
+
+static const struct file_operations ast2500_espi_perif_fops = {
+	.owner = THIS_MODULE,
+	.mmap = ast2500_espi_perif_mmap,
+	.unlocked_ioctl = ast2500_espi_perif_ioctl,
+};
+
+static void ast2500_espi_perif_isr(struct ast2500_espi *espi)
+{
+	struct ast2500_espi_perif *perif;
+	unsigned long flags;
+	uint32_t sts;
+
+	perif = &espi->perif;
+
+	sts = readl(espi->regs + ESPI_INT_STS);
+
+	if (sts & ESPI_INT_STS_PERIF_PC_RX_CMPLT) {
+		writel(ESPI_INT_STS_PERIF_PC_RX_CMPLT, espi->regs + ESPI_INT_STS);
+
+		spin_lock_irqsave(&perif->lock, flags);
+		perif->rx_ready = true;
+		spin_unlock_irqrestore(&perif->lock, flags);
+
+		wake_up_interruptible(&perif->wq);
+	}
+}
+
+static void ast2500_espi_perif_reset(struct ast2500_espi *espi)
+{
+	struct ast2500_espi_perif *perif;
+	struct device *dev;
+	uint32_t reg, mask;
+
+	dev = espi->dev;
+
+	perif = &espi->perif;
+
+	if (perif->mcyc.enable) {
+		mask = ~(perif->mcyc.size - 1);
+		writel(PERIF_MCYC_UNLOCK, espi->regs + ESPI_PERIF_MCYC_MASK);
+		writel(mask, espi->regs + ESPI_PERIF_MCYC_MASK);
+
+		writel(perif->mcyc.saddr, espi->regs + ESPI_PERIF_MCYC_SADDR);
+		writel(perif->mcyc.taddr, espi->regs + ESPI_PERIF_MCYC_TADDR);
+	}
+
+	if (perif->dma.enable) {
+		writel(perif->dma.np_tx_addr, espi->regs + ESPI_PERIF_NP_TX_DMA);
+		writel(perif->dma.pc_tx_addr, espi->regs + ESPI_PERIF_PC_TX_DMA);
+		writel(perif->dma.pc_rx_addr, espi->regs + ESPI_PERIF_PC_RX_DMA);
+
+		reg = readl(espi->regs + ESPI_CTRL)
+		      | ESPI_CTRL_PERIF_NP_TX_DMA_EN
+		      | ESPI_CTRL_PERIF_PC_TX_DMA_EN
+		      | ESPI_CTRL_PERIF_PC_RX_DMA_EN;
+		writel(reg, espi->regs + ESPI_CTRL);
+	}
+
+	reg = readl(espi->regs + ESPI_INT_EN) | ESPI_INT_EN_PERIF_PC_RX_CMPLT;
+	writel(reg, espi->regs + ESPI_INT_EN);
+
+	reg = readl(espi->regs + ESPI_CTRL) | ESPI_CTRL_PERIF_SW_RDY;
+	writel(reg, espi->regs + ESPI_CTRL);
+}
+
+static int ast2500_espi_perif_probe(struct ast2500_espi *espi)
+{
+	struct ast2500_espi_perif *perif;
+	struct device *dev;
+	int rc;
+
+	dev = espi->dev;
+
+	perif = &espi->perif;
+
+	init_waitqueue_head(&perif->wq);
+
+	spin_lock_init(&perif->lock);
+
+	mutex_init(&perif->np_tx_mtx);
+	mutex_init(&perif->pc_tx_mtx);
+	mutex_init(&perif->pc_rx_mtx);
+
+	perif->mcyc.enable = of_property_read_bool(dev->of_node, "perif-mcyc-enable");
+	if (perif->mcyc.enable) {
+		rc = of_property_read_u32(dev->of_node, "perif-mcyc-src-addr", &perif->mcyc.saddr);
+		if (rc || !IS_ALIGNED(perif->mcyc.saddr, PERIF_MCYC_ALIGN)) {
+			dev_err(dev, "cannot get 64KB-aligned memory cycle host address\n");
+			return -ENODEV;
+		}
+
+		rc = of_property_read_u32(dev->of_node, "perif-mcyc-size", &perif->mcyc.size);
+		if (rc || !IS_ALIGNED(perif->mcyc.size, PERIF_MCYC_ALIGN)) {
+			dev_err(dev, "cannot get 64KB-aligned memory cycle size\n");
+			return -EINVAL;
+		}
+
+		perif->mcyc.virt = dmam_alloc_coherent(dev, perif->mcyc.size,
+						       &perif->mcyc.taddr, GFP_KERNEL);
+		if (!perif->mcyc.virt) {
+			dev_err(dev, "cannot allocate memory cycle\n");
+			return -ENOMEM;
+		}
+	}
+
+	perif->dma.enable = of_property_read_bool(dev->of_node, "perif-dma-mode");
+	if (perif->dma.enable) {
+		perif->dma.pc_tx_virt = dmam_alloc_coherent(dev, PAGE_SIZE,
+							    &perif->dma.pc_tx_addr, GFP_KERNEL);
+		if (!perif->dma.pc_tx_virt) {
+			dev_err(dev, "cannot allocate posted TX DMA buffer\n");
+			return -ENOMEM;
+		}
+
+		perif->dma.pc_rx_virt = dmam_alloc_coherent(dev, PAGE_SIZE,
+							    &perif->dma.pc_rx_addr, GFP_KERNEL);
+		if (!perif->dma.pc_rx_virt) {
+			dev_err(dev, "cannot allocate posted RX DMA buffer\n");
+			return -ENOMEM;
+		}
+
+		perif->dma.np_tx_virt = dmam_alloc_coherent(dev, PAGE_SIZE,
+							    &perif->dma.np_tx_addr, GFP_KERNEL);
+		if (!perif->dma.np_tx_virt) {
+			dev_err(dev, "cannot allocate non-posted TX DMA buffer\n");
+			return -ENOMEM;
+		}
+	}
+
+	perif->mdev.parent = dev;
+	perif->mdev.minor = MISC_DYNAMIC_MINOR;
+	perif->mdev.name = devm_kasprintf(dev, GFP_KERNEL, "%s-peripheral", DEVICE_NAME);
+	perif->mdev.fops = &ast2500_espi_perif_fops;
+	rc = misc_register(&perif->mdev);
+	if (rc) {
+		dev_err(dev, "cannot register device %s\n", perif->mdev.name);
+		return rc;
+	}
+
+	ast2500_espi_perif_reset(espi);
+
+	return 0;
+}
+
+/* virtual wire channel (CH1) */
+static long ast2500_espi_vw_ioctl(struct file *fp, unsigned int cmd, unsigned long arg)
+{
+	struct ast2500_espi_vw *vw;
+	struct ast2500_espi *espi;
+	uint32_t gpio;
+
+	vw = container_of(fp->private_data, struct ast2500_espi_vw, mdev);
+	espi = container_of(vw, struct ast2500_espi, vw);
+	gpio = vw->gpio.val;
+
+	switch (cmd) {
+	case ASPEED_ESPI_VW_GET_GPIO_VAL:
+		if (put_user(gpio, (uint32_t __user *)arg))
+			return -EFAULT;
+		break;
+	case ASPEED_ESPI_VW_PUT_GPIO_VAL:
+		if (get_user(gpio, (uint32_t __user *)arg))
+			return -EFAULT;
+
+		writel(gpio, espi->regs + ESPI_VW_GPIO_VAL);
+		break;
+	default:
+		return -EINVAL;
+	};
+
+	return 0;
+}
+
+static const struct file_operations ast2500_espi_vw_fops = {
+	.owner = THIS_MODULE,
+	.unlocked_ioctl = ast2500_espi_vw_ioctl,
+};
+
+static void ast2500_espi_vw_isr(struct ast2500_espi *espi)
+{
+	struct ast2500_espi_vw *vw;
+	uint32_t reg, sts, sts_sysevt;
+
+	vw = &espi->vw;
+
+	sts = readl(espi->regs + ESPI_INT_STS);
+
+	if (sts & ESPI_INT_STS_VW_SYSEVT) {
+		sts_sysevt = readl(espi->regs + ESPI_VW_SYSEVT_INT_STS);
+
+		if (sts_sysevt & ESPI_VW_SYSEVT_INT_STS_HOST_RST_WARN) {
+			reg = readl(espi->regs + ESPI_VW_SYSEVT) | ESPI_VW_SYSEVT_HOST_RST_ACK;
+			writel(reg, espi->regs + ESPI_VW_SYSEVT);
+			writel(ESPI_VW_SYSEVT_INT_STS_HOST_RST_WARN, espi->regs + ESPI_VW_SYSEVT_INT_STS);
+		}
+
+		if (sts_sysevt & ESPI_VW_SYSEVT_INT_STS_OOB_RST_WARN) {
+			reg = readl(espi->regs + ESPI_VW_SYSEVT) | ESPI_VW_SYSEVT_OOB_RST_ACK;
+			writel(reg, espi->regs + ESPI_VW_SYSEVT);
+			writel(ESPI_VW_SYSEVT_INT_STS_OOB_RST_WARN, espi->regs + ESPI_VW_SYSEVT_INT_STS);
+		}
+
+		writel(ESPI_INT_STS_VW_SYSEVT, espi->regs + ESPI_INT_STS);
+	}
+
+	if (sts & ESPI_INT_STS_VW_SYSEVT1) {
+		sts_sysevt = readl(espi->regs + ESPI_VW_SYSEVT1_INT_STS);
+
+		if (sts_sysevt & ESPI_VW_SYSEVT1_INT_STS_SUSPEND_WARN) {
+			reg = readl(espi->regs + ESPI_VW_SYSEVT1) | ESPI_VW_SYSEVT1_SUSPEND_ACK;
+			writel(reg, espi->regs);
+			writel(ESPI_VW_SYSEVT1_INT_STS_SUSPEND_WARN, espi->regs + ESPI_VW_SYSEVT1_INT_STS);
+		}
+
+		writel(ESPI_INT_STS_VW_SYSEVT, espi->regs + ESPI_INT_STS);
+	}
+
+	if (sts & ESPI_INT_STS_VW_GPIO) {
+		vw->gpio.val = readl(espi->regs + ESPI_VW_GPIO_VAL);
+		writel(ESPI_INT_STS_VW_GPIO, espi->regs + ESPI_INT_STS);
+	}
+}
+
+static void ast2500_espi_vw_reset(struct ast2500_espi *espi)
+{
+	uint32_t reg;
+	struct ast2500_espi_vw *vw = &espi->vw;
+
+	writel(vw->gpio.grp, espi->regs + ESPI_VW_GPIO_GRP);
+	writel(vw->gpio.dir, espi->regs + ESPI_VW_GPIO_DIR);
+
+	vw->gpio.val = readl(espi->regs + ESPI_VW_GPIO_VAL);
+
+	/* Host Reset Warn and OOB Reset Warn system events */
+	reg = readl(espi->regs + ESPI_VW_SYSEVT_INT_T2)
+	      | ESPI_VW_SYSEVT_INT_T2_HOST_RST_WARN
+	      | ESPI_VW_SYSEVT_INT_T2_OOB_RST_WARN;
+	writel(reg, espi->regs + ESPI_VW_SYSEVT_INT_T2);
+
+	reg = readl(espi->regs + ESPI_VW_SYSEVT_INT_EN)
+	      | ESPI_VW_SYSEVT_INT_EN_HOST_RST_WARN
+	      | ESPI_VW_SYSEVT_INT_EN_OOB_RST_WARN;
+	writel(reg, espi->regs + ESPI_VW_SYSEVT_INT_EN);
+
+	/* Suspend Warn system event */
+	reg = readl(espi->regs + ESPI_VW_SYSEVT1_INT_T0) | ESPI_VW_SYSEVT1_INT_T0_SUSPEND_WARN;
+	writel(reg, espi->regs + ESPI_VW_SYSEVT1_INT_T0);
+
+	reg = readl(espi->regs + ESPI_VW_SYSEVT1_INT_EN) | ESPI_VW_SYSEVT1_INT_EN_SUSPEND_WARN;
+	writel(reg, espi->regs + ESPI_VW_SYSEVT1_INT_EN);
+
+	reg = readl(espi->regs + ESPI_INT_EN)
+	      | ESPI_INT_EN_VW_GPIO
+	      | ESPI_INT_EN_VW_SYSEVT
+	      | ESPI_INT_EN_VW_SYSEVT1;
+	writel(reg, espi->regs + ESPI_INT_EN);
+
+	reg = readl(espi->regs + ESPI_VW_SYSEVT)
+	      | ESPI_VW_SYSEVT_SLV_BOOT_STS
+	      | ESPI_VW_SYSEVT_SLV_BOOT_DONE;
+	writel(reg, espi->regs + ESPI_VW_SYSEVT);
+
+	reg = readl(espi->regs + ESPI_CTRL)
+	      | ((vw->gpio.hw_mode) ? 0 : ESPI_CTRL_VW_GPIO_SW)
+	      | ESPI_CTRL_VW_SW_RDY;
+	writel(reg, espi->regs + ESPI_CTRL);
+}
+
+static int ast2500_espi_vw_probe(struct ast2500_espi *espi)
+{
+	int rc;
+	struct device *dev = espi->dev;
+	struct ast2500_espi_vw *vw = &espi->vw;
+
+	writel(0x0, espi->regs + ESPI_VW_SYSEVT_INT_EN);
+	writel(0xffffffff, espi->regs + ESPI_VW_SYSEVT_INT_STS);
+
+	writel(0x0, espi->regs + ESPI_VW_SYSEVT1_INT_EN);
+	writel(0xffffffff, espi->regs + ESPI_VW_SYSEVT1_INT_STS);
+
+	vw->gpio.hw_mode = of_property_read_bool(dev->of_node, "vw-gpio-hw-mode");
+	of_property_read_u32(dev->of_node, "vw-gpio-group", &vw->gpio.grp);
+	of_property_read_u32(dev->of_node, "vw-gpio-direction", &vw->gpio.dir);
+
+	vw->mdev.parent = dev;
+	vw->mdev.minor = MISC_DYNAMIC_MINOR;
+	vw->mdev.name = devm_kasprintf(dev, GFP_KERNEL, "%s-vw", DEVICE_NAME);
+	vw->mdev.fops = &ast2500_espi_vw_fops;
+	rc = misc_register(&vw->mdev);
+	if (rc) {
+		dev_err(dev, "cannot register device %s\n", vw->mdev.name);
+		return rc;
+	}
+
+	ast2500_espi_vw_reset(espi);
+
+	return 0;
+}
+
+/* out-of-band channel (CH2) */
+static long ast2500_espi_oob_get_rx(struct file *fp,
+				    struct ast2500_espi_oob *oob,
+				    struct aspeed_espi_ioc *ioc)
+{
+	uint32_t reg, cyc, tag, len;
+	struct ast2500_espi *espi;
+	struct espi_comm_hdr *hdr;
+	unsigned long flags;
+	uint32_t pkt_len;
+	uint8_t *pkt;
+	int i, rc;
+
+	espi = container_of(oob, struct ast2500_espi, oob);
+
+	if (fp->f_flags & O_NONBLOCK) {
+		if (!mutex_trylock(&oob->rx_mtx))
+			return -EAGAIN;
+
+		if (!oob->rx_ready) {
+			rc = -ENODATA;
+			goto unlock_mtx_n_out;
+		}
+	} else {
+		mutex_lock(&oob->rx_mtx);
+
+		if (!oob->rx_ready) {
+			rc = wait_event_interruptible(oob->wq, oob->rx_ready);
+			if (rc == -ERESTARTSYS) {
+				rc = -EINTR;
+				goto unlock_mtx_n_out;
+			}
+		}
+	}
+
+	/*
+	 * common header (i.e. cycle type, tag, and length)
+	 * part is written to HW registers
+	 */
+	reg = readl(espi->regs + ESPI_OOB_RX_CTRL);
+	cyc = FIELD_GET(ESPI_OOB_RX_CTRL_CYC, reg);
+	tag = FIELD_GET(ESPI_OOB_RX_CTRL_TAG, reg);
+	len = FIELD_GET(ESPI_OOB_RX_CTRL_LEN, reg);
+
+	/*
+	 * calculate the length of the rest part of the
+	 * eSPI packet to be read from HW and copied to
+	 * user space.
+	 */
+	pkt_len = ((len) ? len : ESPI_MAX_PLD_LEN) + sizeof(struct espi_comm_hdr);
+
+	if (ioc->pkt_len < pkt_len) {
+		rc = -EINVAL;
+		goto unlock_mtx_n_out;
+	}
+
+	pkt = vmalloc(pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_mtx_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+	hdr->cyc = cyc;
+	hdr->tag = tag;
+	hdr->len_h = len >> 8;
+	hdr->len_l = len & 0xff;
+
+	if (oob->dma.enable) {
+		memcpy(hdr + 1, oob->dma.rx_virt, pkt_len - sizeof(*hdr));
+	} else {
+		for (i = sizeof(*hdr); i < pkt_len; ++i)
+			pkt[i] = readl(espi->regs + ESPI_OOB_RX_DATA) & 0xff;
+	}
+
+	if (copy_to_user((void __user *)ioc->pkt, pkt, pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	spin_lock_irqsave(&oob->lock, flags);
+
+	writel(ESPI_OOB_RX_CTRL_SERV_PEND, espi->regs + ESPI_OOB_RX_CTRL);
+	oob->rx_ready = 0;
+
+	spin_unlock_irqrestore(&oob->lock, flags);
+
+	rc = 0;
+
+free_n_out:
+	vfree(pkt);
+
+unlock_mtx_n_out:
+	mutex_unlock(&oob->rx_mtx);
+
+	return rc;
+}
+
+static long ast2500_espi_oob_put_tx(struct file *fp,
+				    struct ast2500_espi_oob *oob,
+				    struct aspeed_espi_ioc *ioc)
+{
+	uint32_t reg, cyc, tag, len;
+	struct ast2500_espi *espi;
+	struct espi_comm_hdr *hdr;
+	uint8_t *pkt;
+	int i, rc;
+
+	espi = container_of(oob, struct ast2500_espi, oob);
+
+	if (!mutex_trylock(&oob->tx_mtx))
+		return -EAGAIN;
+
+	reg = readl(espi->regs + ESPI_OOB_TX_CTRL);
+	if (reg & ESPI_OOB_TX_CTRL_TRIG_PEND) {
+		rc = -EBUSY;
+		goto unlock_mtx_n_out;
+	}
+
+	if (ioc->pkt_len > ESPI_MAX_PKT_LEN) {
+		rc = -EINVAL;
+		goto unlock_mtx_n_out;
+	}
+
+	pkt = vmalloc(ioc->pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_mtx_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+
+	if (copy_from_user(pkt, (void __user *)ioc->pkt, ioc->pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	/*
+	 * common header (i.e. cycle type, tag, and length)
+	 * part is written to HW registers
+	 */
+	if (oob->dma.enable) {
+		memcpy(oob->dma.tx_virt, hdr + 1, ioc->pkt_len - sizeof(*hdr));
+		dma_wmb();
+	} else {
+		for (i = sizeof(*hdr); i < ioc->pkt_len; ++i)
+			writel(pkt[i], espi->regs + ESPI_OOB_TX_DATA);
+	}
+
+	cyc = hdr->cyc;
+	tag = hdr->tag;
+	len = (hdr->len_h << 8) | (hdr->len_l & 0xff);
+
+	reg = FIELD_PREP(ESPI_OOB_TX_CTRL_CYC, cyc)
+	      | FIELD_PREP(ESPI_OOB_TX_CTRL_TAG, tag)
+	      | FIELD_PREP(ESPI_OOB_TX_CTRL_LEN, len)
+	      | ESPI_OOB_TX_CTRL_TRIG_PEND;
+	writel(reg, espi->regs + ESPI_OOB_TX_CTRL);
+
+	rc = 0;
+
+free_n_out:
+	vfree(pkt);
+
+unlock_mtx_n_out:
+	mutex_unlock(&oob->tx_mtx);
+
+	return rc;
+}
+
+static long ast2500_espi_oob_ioctl(struct file *fp, unsigned int cmd, unsigned long arg)
+{
+	struct ast2500_espi_oob *oob;
+	struct aspeed_espi_ioc ioc;
+
+	oob = container_of(fp->private_data, struct ast2500_espi_oob, mdev);
+
+	if (copy_from_user(&ioc, (void __user *)arg, sizeof(ioc)))
+		return -EFAULT;
+
+	if (ioc.pkt_len > ESPI_MAX_PKT_LEN)
+		return -EINVAL;
+
+	switch (cmd) {
+	case ASPEED_ESPI_OOB_GET_RX:
+		return ast2500_espi_oob_get_rx(fp, oob, &ioc);
+	case ASPEED_ESPI_OOB_PUT_TX:
+		return ast2500_espi_oob_put_tx(fp, oob, &ioc);
+	default:
+		break;
+	};
+
+	return -EINVAL;
+}
+
+static const struct file_operations ast2500_espi_oob_fops = {
+	.owner = THIS_MODULE,
+	.unlocked_ioctl = ast2500_espi_oob_ioctl,
+};
+
+static void ast2500_espi_oob_isr(struct ast2500_espi *espi)
+{
+	struct ast2500_espi_oob *oob;
+	unsigned long flags;
+	uint32_t sts;
+
+	oob = &espi->oob;
+
+	sts = readl(espi->regs + ESPI_INT_STS);
+
+	if (sts & ESPI_INT_STS_OOB_RX_CMPLT) {
+		writel(ESPI_INT_STS_OOB_RX_CMPLT, espi->regs + ESPI_INT_STS);
+
+		spin_lock_irqsave(&oob->lock, flags);
+		oob->rx_ready = true;
+		spin_unlock_irqrestore(&oob->lock, flags);
+
+		wake_up_interruptible(&oob->wq);
+	}
+}
+
+static void ast2500_espi_oob_reset(struct ast2500_espi *espi)
+{
+	struct ast2500_espi_oob *oob;
+	uint32_t reg;
+
+	oob = &espi->oob;
+
+	if (oob->dma.enable) {
+		writel(oob->dma.tx_addr, espi->regs + ESPI_OOB_TX_DMA);
+		writel(oob->dma.rx_addr, espi->regs + ESPI_OOB_RX_DMA);
+
+		reg = readl(espi->regs + ESPI_CTRL)
+		      | ESPI_CTRL_OOB_TX_DMA_EN
+		      | ESPI_CTRL_OOB_RX_DMA_EN;
+		writel(reg, espi->regs + ESPI_CTRL);
+	}
+
+	writel(ESPI_INT_EN_OOB_RX_CMPLT, espi->regs + ESPI_INT_EN);
+
+	reg = readl(espi->regs + ESPI_CTRL) | ESPI_CTRL_OOB_SW_RDY;
+	writel(reg, espi->regs + ESPI_CTRL);
+}
+
+static int ast2500_espi_oob_probe(struct ast2500_espi *espi)
+{
+	struct ast2500_espi_oob *oob;
+	struct device *dev;
+	int rc;
+
+	dev = espi->dev;
+
+	oob = &espi->oob;
+
+	init_waitqueue_head(&oob->wq);
+
+	spin_lock_init(&oob->lock);
+
+	mutex_init(&oob->tx_mtx);
+	mutex_init(&oob->rx_mtx);
+
+	oob->dma.enable = of_property_read_bool(dev->of_node, "oob-dma-mode");
+	if (oob->dma.enable) {
+		oob->dma.tx_virt = dmam_alloc_coherent(dev, PAGE_SIZE, &oob->dma.tx_addr, GFP_KERNEL);
+		if (!oob->dma.tx_virt) {
+			dev_err(dev, "cannot allocate DMA TX buffer\n");
+			return -ENOMEM;
+		}
+
+		oob->dma.rx_virt = dmam_alloc_coherent(dev, PAGE_SIZE, &oob->dma.rx_addr, GFP_KERNEL);
+		if (!oob->dma.rx_virt) {
+			dev_err(dev, "cannot allocate DMA TX buffer\n");
+			return -ENOMEM;
+		}
+	}
+
+	oob->mdev.parent = dev;
+	oob->mdev.minor = MISC_DYNAMIC_MINOR;
+	oob->mdev.name = devm_kasprintf(dev, GFP_KERNEL, "%s-oob", DEVICE_NAME);
+	oob->mdev.fops = &ast2500_espi_oob_fops;
+	rc = misc_register(&oob->mdev);
+	if (rc) {
+		dev_err(dev, "cannot register device %s\n", oob->mdev.name);
+		return rc;
+	}
+
+	ast2500_espi_oob_reset(espi);
+
+	return 0;
+}
+
+/* flash channel (CH3) */
+static long ast2500_espi_flash_get_rx(struct file *fp,
+				      struct ast2500_espi_flash *flash,
+				      struct aspeed_espi_ioc *ioc)
+{
+	uint32_t reg, cyc, tag, len;
+	struct ast2500_espi *espi;
+	struct espi_comm_hdr *hdr;
+	unsigned long flags;
+	uint32_t pkt_len;
+	uint8_t *pkt;
+	int i, rc;
+
+	rc = 0;
+
+	espi = container_of(flash, struct ast2500_espi, flash);
+
+	if (fp->f_flags & O_NONBLOCK) {
+		if (!mutex_trylock(&flash->rx_mtx))
+			return -EAGAIN;
+
+		if (!flash->rx_ready) {
+			rc = -ENODATA;
+			goto unlock_mtx_n_out;
+		}
+	} else {
+		mutex_lock(&flash->rx_mtx);
+
+		if (!flash->rx_ready) {
+			rc = wait_event_interruptible(flash->wq, flash->rx_ready);
+			if (rc == -ERESTARTSYS) {
+				rc = -EINTR;
+				goto unlock_mtx_n_out;
+			}
+		}
+	}
+
+	/*
+	 * common header (i.e. cycle type, tag, and length)
+	 * part is written to HW registers
+	 */
+	reg = readl(espi->regs + ESPI_FLASH_RX_CTRL);
+	cyc = FIELD_GET(ESPI_FLASH_RX_CTRL_CYC, reg);
+	tag = FIELD_GET(ESPI_FLASH_RX_CTRL_TAG, reg);
+	len = FIELD_GET(ESPI_FLASH_RX_CTRL_LEN, reg);
+
+	/*
+	 * calculate the length of the rest part of the
+	 * eSPI packet to be read from HW and copied to
+	 * user space.
+	 */
+	switch (cyc) {
+	case ESPI_FLASH_WRITE:
+		pkt_len = ((len) ? len : ESPI_MAX_PLD_LEN) +
+			  sizeof(struct espi_flash_rwe);
+		break;
+	case ESPI_FLASH_READ:
+	case ESPI_FLASH_ERASE:
+		pkt_len = sizeof(struct espi_flash_rwe);
+		break;
+	case ESPI_FLASH_SUC_CMPLT_D_MIDDLE:
+	case ESPI_FLASH_SUC_CMPLT_D_FIRST:
+	case ESPI_FLASH_SUC_CMPLT_D_LAST:
+	case ESPI_FLASH_SUC_CMPLT_D_ONLY:
+		pkt_len = ((len) ? len : ESPI_MAX_PLD_LEN) +
+			  sizeof(struct espi_flash_cmplt);
+		break;
+	case ESPI_FLASH_SUC_CMPLT:
+	case ESPI_FLASH_UNSUC_CMPLT:
+		pkt_len = sizeof(struct espi_flash_cmplt);
+		break;
+	default:
+		rc = -EFAULT;
+		goto unlock_mtx_n_out;
+	}
+
+	if (ioc->pkt_len < pkt_len) {
+		rc = -EINVAL;
+		goto unlock_mtx_n_out;
+	}
+
+	pkt = vmalloc(pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_mtx_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+	hdr->cyc = cyc;
+	hdr->tag = tag;
+	hdr->len_h = len >> 8;
+	hdr->len_l = len & 0xff;
+
+	if (flash->dma.enable) {
+		memcpy(hdr + 1, flash->dma.rx_virt, pkt_len - sizeof(*hdr));
+	} else {
+		for (i = sizeof(*hdr); i < pkt_len; ++i)
+			pkt[i] = readl(espi->regs + ESPI_FLASH_RX_DATA) & 0xff;
+	}
+
+	if (copy_to_user((void __user *)ioc->pkt, pkt, pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	spin_lock_irqsave(&flash->lock, flags);
+
+	writel(ESPI_FLASH_RX_CTRL_SERV_PEND, espi->regs + ESPI_FLASH_RX_CTRL);
+	flash->rx_ready = 0;
+
+	spin_unlock_irqrestore(&flash->lock, flags);
+
+	rc = 0;
+
+free_n_out:
+	vfree(pkt);
+
+unlock_mtx_n_out:
+	mutex_unlock(&flash->rx_mtx);
+
+	return rc;
+}
+
+static long ast2500_espi_flash_put_tx(struct file *fp,
+				      struct ast2500_espi_flash *flash,
+				      struct aspeed_espi_ioc *ioc)
+{
+	uint32_t reg, cyc, tag, len;
+	struct ast2500_espi *espi;
+	struct espi_comm_hdr *hdr;
+	uint8_t *pkt;
+	int i, rc;
+
+	espi = container_of(flash, struct ast2500_espi, flash);
+
+	if (!mutex_trylock(&flash->tx_mtx))
+		return -EAGAIN;
+
+	reg = readl(espi->regs + ESPI_FLASH_TX_CTRL);
+	if (reg & ESPI_FLASH_TX_CTRL_TRIG_PEND) {
+		rc = -EBUSY;
+		goto unlock_mtx_n_out;
+	}
+
+	pkt = vmalloc(ioc->pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_mtx_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+
+	if (copy_from_user(pkt, (void __user *)ioc->pkt, ioc->pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	/*
+	 * common header (i.e. cycle type, tag, and length)
+	 * part is written to HW registers
+	 */
+	if (flash->dma.enable) {
+		memcpy(flash->dma.tx_virt, hdr + 1, ioc->pkt_len - sizeof(*hdr));
+		dma_wmb();
+	} else {
+		for (i = sizeof(*hdr); i < ioc->pkt_len; ++i)
+			writel(pkt[i], espi->regs + ESPI_FLASH_TX_DATA);
+	}
+
+	cyc = hdr->cyc;
+	tag = hdr->tag;
+	len = (hdr->len_h << 8) | (hdr->len_l & 0xff);
+
+	reg = FIELD_PREP(ESPI_FLASH_TX_CTRL_CYC, cyc)
+	      | FIELD_PREP(ESPI_FLASH_TX_CTRL_TAG, tag)
+	      | FIELD_PREP(ESPI_FLASH_TX_CTRL_LEN, len)
+	      | ESPI_FLASH_TX_CTRL_TRIG_PEND;
+	writel(reg, espi->regs + ESPI_FLASH_TX_CTRL);
+
+	rc = 0;
+
+free_n_out:
+	vfree(pkt);
+
+unlock_mtx_n_out:
+	mutex_unlock(&flash->tx_mtx);
+
+	return rc;
+}
+
+static long ast2500_espi_flash_ioctl(struct file *fp, unsigned int cmd, unsigned long arg)
+{
+	struct ast2500_espi_flash *flash;
+	struct aspeed_espi_ioc ioc;
+
+	flash = container_of(fp->private_data, struct ast2500_espi_flash, mdev);
+
+	if (copy_from_user(&ioc, (void __user *)arg, sizeof(ioc)))
+		return -EFAULT;
+
+	if (ioc.pkt_len > ESPI_MAX_PKT_LEN)
+		return -EINVAL;
+
+	switch (cmd) {
+	case ASPEED_ESPI_FLASH_GET_RX:
+		return ast2500_espi_flash_get_rx(fp, flash, &ioc);
+	case ASPEED_ESPI_FLASH_PUT_TX:
+		return ast2500_espi_flash_put_tx(fp, flash, &ioc);
+	default:
+		break;
+	};
+
+	return -EINVAL;
+}
+
+static const struct file_operations ast2500_espi_flash_fops = {
+	.owner = THIS_MODULE,
+	.unlocked_ioctl = ast2500_espi_flash_ioctl,
+};
+
+static void ast2500_espi_flash_isr(struct ast2500_espi *espi)
+{
+	struct ast2500_espi_flash *flash;
+	unsigned long flags;
+	uint32_t sts;
+
+	flash = &espi->flash;
+
+	sts = readl(espi->regs + ESPI_INT_STS_FLASH);
+
+	if (sts & ESPI_INT_STS_FLASH_RX_CMPLT) {
+		spin_lock_irqsave(&flash->lock, flags);
+		flash->rx_ready = true;
+		spin_unlock_irqrestore(&flash->lock, flags);
+
+		wake_up_interruptible(&flash->wq);
+
+		writel(ESPI_INT_STS_FLASH_RX_CMPLT, espi->regs + ESPI_INT_STS_FLASH);
+	}
+}
+
+static void ast2500_espi_flash_reset(struct ast2500_espi *espi)
+{
+	uint32_t reg;
+	struct ast2500_espi_flash *flash = &espi->flash;
+
+	if (flash->safs.mode == SAFS_MODE_MIX) {
+		reg = FIELD_PREP(ESPI_FLASH_SAFS_TADDR_BASE, flash->safs.taddr >> 24)
+			| FIELD_PREP(ESPI_FLASH_SAFS_TADDR_MASK, (~(flash->safs.size - 1)) >> 24);
+		writel(reg, espi->regs + ESPI_FLASH_SAFS_TADDR);
+	} else {
+		reg = readl(espi->regs + ESPI_CTRL) | ESPI_CTRL_FLASH_SAFS_SW_MODE;
+		writel(reg, espi->regs + ESPI_CTRL);
+	}
+
+	if (flash->dma.enable) {
+		writel(flash->dma.tx_addr, espi->regs + ESPI_FLASH_TX_DMA);
+		writel(flash->dma.rx_addr, espi->regs + ESPI_FLASH_RX_DMA);
+
+		reg = readl(espi->regs + ESPI_CTRL)
+		      | ESPI_CTRL_FLASH_TX_DMA_EN
+			  | ESPI_CTRL_FLASH_RX_DMA_EN;
+		writel(reg, espi->regs + ESPI_CTRL);
+	}
+
+	reg = readl(espi->regs + ESPI_INT_EN) | ESPI_INT_EN_FLASH_RX_CMPLT;
+	writel(reg, espi->regs + ESPI_INT_EN);
+
+	reg = readl(espi->regs + ESPI_CTRL) | ESPI_CTRL_FLASH_SW_RDY;
+	writel(reg, espi->regs + ESPI_CTRL);
+}
+
+static int ast2500_espi_flash_probe(struct ast2500_espi *espi)
+{
+	struct ast2500_espi_flash *flash;
+	struct device *dev;
+	int rc;
+
+	dev = espi->dev;
+
+	flash = &espi->flash;
+
+	init_waitqueue_head(&flash->wq);
+
+	spin_lock_init(&flash->lock);
+
+	mutex_init(&flash->tx_mtx);
+	mutex_init(&flash->rx_mtx);
+
+	flash->safs.mode = SAFS_MODE_MIX;
+
+	of_property_read_u32(dev->of_node, "flash-safs-mode", &flash->safs.mode);
+	if (flash->safs.mode == SAFS_MODE_MIX) {
+		rc = of_property_read_u32(dev->of_node, "flash-safs-taddr", &flash->safs.taddr);
+		if (rc || !IS_ALIGNED(flash->safs.taddr, FLASH_SAFS_ALIGN)) {
+			dev_err(dev, "cannot get 16MB-aligned SAFS target address\n");
+			return -ENODEV;
+		}
+
+		rc = of_property_read_u32(dev->of_node, "flash-safs-size", &flash->safs.size);
+		if (rc || !IS_ALIGNED(flash->safs.size, FLASH_SAFS_ALIGN)) {
+			dev_err(dev, "cannot get 16MB-aligned SAFS size\n");
+			return -ENODEV;
+		}
+	}
+
+	flash->dma.enable = of_property_read_bool(dev->of_node, "flash-dma-mode");
+	if (flash->dma.enable) {
+		flash->dma.tx_virt = dmam_alloc_coherent(dev, PAGE_SIZE, &flash->dma.tx_addr, GFP_KERNEL);
+		if (!flash->dma.tx_virt) {
+			dev_err(dev, "cannot allocate DMA TX buffer\n");
+			return -ENOMEM;
+		}
+
+		flash->dma.rx_virt = dmam_alloc_coherent(dev, PAGE_SIZE, &flash->dma.rx_addr, GFP_KERNEL);
+		if (!flash->dma.rx_virt) {
+			dev_err(dev, "cannot allocate DMA RX buffer\n");
+			return -ENOMEM;
+		}
+	}
+
+	flash->mdev.parent = dev;
+	flash->mdev.minor = MISC_DYNAMIC_MINOR;
+	flash->mdev.name = devm_kasprintf(dev, GFP_KERNEL, "%s-flash", DEVICE_NAME);
+	flash->mdev.fops = &ast2500_espi_flash_fops;
+	rc = misc_register(&flash->mdev);
+	if (rc) {
+		dev_err(dev, "cannot register device %s\n", flash->mdev.name);
+		return rc;
+	}
+
+	ast2500_espi_flash_reset(espi);
+
+	return 0;
+}
+
+/* global control */
+static irqreturn_t ast2500_espi_isr(int irq, void *arg)
+{
+	struct ast2500_espi *espi;
+	uint32_t sts;
+
+	espi = (struct ast2500_espi *)arg;
+
+	sts = readl(espi->regs + ESPI_INT_STS);
+	if (!sts)
+		return IRQ_NONE;
+
+	if (sts & ESPI_INT_STS_PERIF)
+		ast2500_espi_perif_isr(espi);
+
+	if (sts & ESPI_INT_STS_VW)
+		ast2500_espi_vw_isr(espi);
+
+	if (sts & ESPI_INT_STS_OOB)
+		ast2500_espi_oob_isr(espi);
+
+	if (sts & ESPI_INT_STS_FLASH)
+		ast2500_espi_flash_isr(espi);
+
+	if (sts & ESPI_INT_STS_RST_DEASSERT) {
+		ast2500_espi_perif_reset(espi);
+		ast2500_espi_vw_reset(espi);
+		ast2500_espi_oob_reset(espi);
+		ast2500_espi_flash_reset(espi);
+		writel(ESPI_INT_STS_RST_DEASSERT, espi->regs + ESPI_INT_STS);
+	}
+
+	return IRQ_HANDLED;
+}
+
+static int ast2500_espi_probe(struct platform_device *pdev)
+{
+	struct ast2500_espi *espi;
+	struct resource *res;
+	struct device *dev;
+	uint32_t reg;
+	int rc;
+
+	dev = &pdev->dev;
+
+	espi = devm_kzalloc(dev, sizeof(*espi), GFP_KERNEL);
+	if (!espi)
+		return -ENOMEM;
+
+	espi->dev = dev;
+
+	rc = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(64));
+	if (rc) {
+		dev_err(dev, "cannot set 64-bits DMA mask\n");
+		return rc;
+	}
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		dev_err(dev, "cannot get resource\n");
+		return -ENODEV;
+	}
+
+	espi->regs = devm_ioremap_resource(dev, res);
+	if (IS_ERR(espi->regs)) {
+		dev_err(dev, "cannot map registers\n");
+		return PTR_ERR(espi->regs);
+	}
+
+	espi->irq = platform_get_irq(pdev, 0);
+	if (espi->irq < 0) {
+		dev_err(dev, "cannot get IRQ number\n");
+		return -ENODEV;
+	}
+
+	espi->clk = devm_clk_get(dev, NULL);
+	if (IS_ERR(espi->clk)) {
+		dev_err(dev, "cannot get clock control\n");
+		return PTR_ERR(espi->clk);
+	}
+
+	rc = clk_prepare_enable(espi->clk);
+	if (rc) {
+		dev_err(dev, "cannot enable clocks\n");
+		return rc;
+	}
+
+	writel(0x0, espi->regs + ESPI_INT_EN);
+	writel(0xffffffff, espi->regs + ESPI_INT_STS);
+
+	rc = ast2500_espi_perif_probe(espi);
+	if (rc) {
+		dev_err(dev, "cannot init peripheral channel, rc=%d\n", rc);
+		return rc;
+	}
+
+	rc = ast2500_espi_vw_probe(espi);
+	if (rc) {
+		dev_err(dev, "cannot init vw channel, rc=%d\n", rc);
+		return rc;
+	}
+
+	rc = ast2500_espi_oob_probe(espi);
+	if (rc) {
+		dev_err(dev, "cannot init oob channel, rc=%d\n", rc);
+		return rc;
+	}
+
+	rc = ast2500_espi_flash_probe(espi);
+	if (rc) {
+		dev_err(dev, "cannot init flash channel, rc=%d\n", rc);
+		return rc;
+	}
+
+	rc = devm_request_irq(dev, espi->irq, ast2500_espi_isr, 0, dev_name(dev), espi);
+	if (rc) {
+		dev_err(dev, "cannot request IRQ\n");
+		return rc;
+	}
+
+	reg = readl(espi->regs + ESPI_INT_EN) | ESPI_INT_EN_RST_DEASSERT;
+	writel(reg, espi->regs + ESPI_INT_EN);
+
+	dev_set_drvdata(dev, espi);
+
+	dev_info(dev, "module loaded\n");
+
+	return 0;
+}
+
+static const struct of_device_id ast2500_espi_of_matches[] = {
+	{ .compatible = "aspeed,ast2500-espi" },
+	{ },
+};
+
+static struct platform_driver ast2500_espi_driver = {
+	.driver = {
+		.name = "ast2500-espi",
+		.of_match_table = ast2500_espi_of_matches,
+	},
+	.probe = ast2500_espi_probe,
+};
+
+module_platform_driver(ast2500_espi_driver);
+
+MODULE_AUTHOR("Chia-Wei Wang <chiawei_wang@aspeedtech.com>");
+MODULE_DESCRIPTION("Control of AST2500 eSPI Device");
+MODULE_LICENSE("GPL");
diff --git a/drivers/soc/aspeed/ast2500-espi.h b/drivers/soc/aspeed/ast2500-espi.h
new file mode 100644
index 000000000000..ee45631ea96c
--- /dev/null
+++ b/drivers/soc/aspeed/ast2500-espi.h
@@ -0,0 +1,211 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright 2023 Aspeed Technology Inc.
+ */
+#ifndef _AST2500_ESPI_H_
+#define _AST2500_ESPI_H_
+
+#include <linux/bits.h>
+#include "aspeed-espi-comm.h"
+
+/* registers */
+#define ESPI_CTRL				0x000
+#define   ESPI_CTRL_OOB_RX_SW_RST		BIT(28)
+#define   ESPI_CTRL_FLASH_TX_DMA_EN		BIT(23)
+#define   ESPI_CTRL_FLASH_RX_DMA_EN		BIT(22)
+#define   ESPI_CTRL_OOB_TX_DMA_EN		BIT(21)
+#define   ESPI_CTRL_OOB_RX_DMA_EN		BIT(20)
+#define   ESPI_CTRL_PERIF_NP_TX_DMA_EN		BIT(19)
+#define   ESPI_CTRL_PERIF_PC_TX_DMA_EN		BIT(17)
+#define   ESPI_CTRL_PERIF_PC_RX_DMA_EN		BIT(16)
+#define   ESPI_CTRL_FLASH_SAFS_SW_MODE		BIT(10)
+#define   ESPI_CTRL_VW_GPIO_SW			BIT(9)
+#define   ESPI_CTRL_FLASH_SW_RDY		BIT(7)
+#define   ESPI_CTRL_OOB_SW_RDY			BIT(4)
+#define   ESPI_CTRL_VW_SW_RDY			BIT(3)
+#define   ESPI_CTRL_PERIF_SW_RDY		BIT(1)
+#define ESPI_STS				0x004
+#define ESPI_INT_STS				0x008
+#define   ESPI_INT_STS_RST_DEASSERT		BIT(31)
+#define   ESPI_INT_STS_OOB_RX_TMOUT		BIT(23)
+#define   ESPI_INT_STS_VW_SYSEVT1		BIT(22)
+#define   ESPI_INT_STS_FLASH_TX_ERR		BIT(21)
+#define   ESPI_INT_STS_OOB_TX_ERR		BIT(20)
+#define   ESPI_INT_STS_FLASH_TX_ABT		BIT(19)
+#define   ESPI_INT_STS_OOB_TX_ABT		BIT(18)
+#define   ESPI_INT_STS_PERIF_NP_TX_ABT		BIT(17)
+#define   ESPI_INT_STS_PERIF_PC_TX_ABT		BIT(16)
+#define   ESPI_INT_STS_FLASH_RX_ABT		BIT(15)
+#define   ESPI_INT_STS_OOB_RX_ABT		BIT(14)
+#define   ESPI_INT_STS_PERIF_NP_RX_ABT		BIT(13)
+#define   ESPI_INT_STS_PERIF_PC_RX_ABT		BIT(12)
+#define   ESPI_INT_STS_PERIF_NP_TX_ERR		BIT(11)
+#define   ESPI_INT_STS_PERIF_PC_TX_ERR		BIT(10)
+#define   ESPI_INT_STS_VW_GPIO			BIT(9)
+#define   ESPI_INT_STS_VW_SYSEVT		BIT(8)
+#define   ESPI_INT_STS_FLASH_TX_CMPLT		BIT(7)
+#define   ESPI_INT_STS_FLASH_RX_CMPLT		BIT(6)
+#define   ESPI_INT_STS_OOB_TX_CMPLT		BIT(5)
+#define   ESPI_INT_STS_OOB_RX_CMPLT		BIT(4)
+#define   ESPI_INT_STS_PERIF_NP_TX_CMPLT	BIT(3)
+#define   ESPI_INT_STS_PERIF_PC_TX_CMPLT	BIT(1)
+#define   ESPI_INT_STS_PERIF_PC_RX_CMPLT	BIT(0)
+#define ESPI_INT_EN				0x00c
+#define   ESPI_INT_EN_RST_DEASSERT		BIT(31)
+#define   ESPI_INT_EN_OOB_RX_TMOUT		BIT(23)
+#define   ESPI_INT_EN_VW_SYSEVT1		BIT(22)
+#define   ESPI_INT_EN_FLASH_TX_ERR		BIT(21)
+#define   ESPI_INT_EN_OOB_TX_ERR		BIT(20)
+#define   ESPI_INT_EN_FLASH_TX_ABT		BIT(19)
+#define   ESPI_INT_EN_OOB_TX_ABT		BIT(18)
+#define   ESPI_INT_EN_PERIF_NP_TX_ABT		BIT(17)
+#define   ESPI_INT_EN_PERIF_PC_TX_ABT		BIT(16)
+#define   ESPI_INT_EN_FLASH_RX_ABT		BIT(15)
+#define   ESPI_INT_EN_OOB_RX_ABT		BIT(14)
+#define   ESPI_INT_EN_PERIF_NP_RX_ABT		BIT(13)
+#define   ESPI_INT_EN_PERIF_PC_RX_ABT		BIT(12)
+#define   ESPI_INT_EN_PERIF_NP_TX_ERR		BIT(11)
+#define   ESPI_INT_EN_PERIF_PC_TX_ERR		BIT(10)
+#define   ESPI_INT_EN_VW_GPIO			BIT(9)
+#define   ESPI_INT_EN_VW_SYSEVT			BIT(8)
+#define   ESPI_INT_EN_FLASH_TX_CMPLT		BIT(7)
+#define   ESPI_INT_EN_FLASH_RX_CMPLT		BIT(6)
+#define   ESPI_INT_EN_OOB_TX_CMPLT		BIT(5)
+#define   ESPI_INT_EN_OOB_RX_CMPLT		BIT(4)
+#define   ESPI_INT_EN_PERIF_NP_TX_CMPLT		BIT(3)
+#define   ESPI_INT_EN_PERIF_PC_TX_CMPLT		BIT(1)
+#define   ESPI_INT_EN_PERIF_PC_RX_CMPLT		BIT(0)
+#define ESPI_PERIF_PC_RX_DMA			0x010
+#define ESPI_PERIF_PC_RX_CTRL			0x014
+#define   ESPI_PERIF_PC_RX_CTRL_SERV_PEND	BIT(31)
+#define   ESPI_PERIF_PC_RX_CTRL_LEN		GENMASK(23, 12)
+#define   ESPI_PERIF_PC_RX_CTRL_TAG		GENMASK(11, 8)
+#define   ESPI_PERIF_PC_RX_CTRL_CYC		GENMASK(7, 0)
+#define ESPI_PERIF_PC_RX_DATA			0x018
+#define ESPI_PERIF_PC_TX_DMA			0x020
+#define ESPI_PERIF_PC_TX_CTRL			0x024
+#define	  ESPI_PERIF_PC_TX_CTRL_TRIG_PEND	BIT(31)
+#define	  ESPI_PERIF_PC_TX_CTRL_LEN		GENMASK(23, 12)
+#define	  ESPI_PERIF_PC_TX_CTRL_TAG		GENMASK(11, 8)
+#define	  ESPI_PERIF_PC_TX_CTRL_CYC		GENMASK(7, 0)
+#define ESPI_PERIF_PC_TX_DATA			0x028
+#define ESPI_PERIF_NP_TX_DMA			0x030
+#define ESPI_PERIF_NP_TX_CTRL			0x034
+#define   ESPI_PERIF_NP_TX_CTRL_TRIG_PEND	BIT(31)
+#define	  ESPI_PERIF_NP_TX_CTRL_LEN		GENMASK(23, 12)
+#define	  ESPI_PERIF_NP_TX_CTRL_TAG		GENMASK(11, 8)
+#define	  ESPI_PERIF_NP_TX_CTRL_CYC		GENMASK(7, 0)
+#define ESPI_PERIF_NP_TX_DATA			0x038
+#define ESPI_OOB_RX_DMA				0x040
+#define ESPI_OOB_RX_CTRL			0x044
+#define	  ESPI_OOB_RX_CTRL_SERV_PEND		BIT(31)
+#define	  ESPI_OOB_RX_CTRL_LEN			GENMASK(23, 12)
+#define	  ESPI_OOB_RX_CTRL_TAG			GENMASK(11, 8)
+#define	  ESPI_OOB_RX_CTRL_CYC			GENMASK(7, 0)
+#define ESPI_OOB_RX_DATA			0x048
+#define ESPI_OOB_TX_DMA				0x050
+#define ESPI_OOB_TX_CTRL			0x054
+#define	  ESPI_OOB_TX_CTRL_TRIG_PEND		BIT(31)
+#define	  ESPI_OOB_TX_CTRL_LEN			GENMASK(23, 12)
+#define	  ESPI_OOB_TX_CTRL_TAG			GENMASK(11, 8)
+#define	  ESPI_OOB_TX_CTRL_CYC			GENMASK(7, 0)
+#define ESPI_OOB_TX_DATA			0x058
+#define ESPI_FLASH_RX_DMA			0x060
+#define ESPI_FLASH_RX_CTRL			0x064
+#define	  ESPI_FLASH_RX_CTRL_SERV_PEND		BIT(31)
+#define	  ESPI_FLASH_RX_CTRL_LEN		GENMASK(23, 12)
+#define	  ESPI_FLASH_RX_CTRL_TAG		GENMASK(11, 8)
+#define	  ESPI_FLASH_RX_CTRL_CYC		GENMASK(7, 0)
+#define ESPI_FLASH_RX_DATA			0x068
+#define ESPI_FLASH_TX_DMA			0x070
+#define ESPI_FLASH_TX_CTRL			0x074
+#define	  ESPI_FLASH_TX_CTRL_TRIG_PEND		BIT(31)
+#define	  ESPI_FLASH_TX_CTRL_LEN		GENMASK(23, 12)
+#define	  ESPI_FLASH_TX_CTRL_TAG		GENMASK(11, 8)
+#define	  ESPI_FLASH_TX_CTRL_CYC		GENMASK(7, 0)
+#define ESPI_FLASH_TX_DATA			0x078
+#define ESPI_PERIF_MCYC_SADDR			0x084
+#define ESPI_PERIF_MCYC_TADDR			0x088
+#define ESPI_PERIF_MCYC_MASK			0x08c
+#define ESPI_FLASH_SAFS_TADDR			0x090
+#define   ESPI_FLASH_SAFS_TADDR_BASE		GENMASK(31, 24)
+#define   ESPI_FLASH_SAFS_TADDR_MASK		GENMASK(15, 8)
+#define ESPI_VW_SYSEVT_INT_EN			0x094
+#define   ESPI_VW_SYSEVT_INT_EN_HOST_RST_WARN	BIT(8)
+#define   ESPI_VW_SYSEVT_INT_EN_OOB_RST_WARN	BIT(6)
+#define ESPI_VW_SYSEVT				0x098
+#define   ESPI_VW_SYSEVT_HOST_RST_ACK		BIT(27)
+#define   ESPI_VW_SYSEVT_SLV_BOOT_STS		BIT(23)
+#define   ESPI_VW_SYSEVT_SLV_BOOT_DONE		BIT(20)
+#define   ESPI_VW_SYSEVT_OOB_RST_ACK		BIT(16)
+#define   ESPI_VW_SYSEVT_HOST_RST_WARN		BIT(8)
+#define   ESPI_VW_SYSEVT_OOB_RST_WARN		BIT(6)
+#define ESPI_VW_GPIO_VAL			0x09c
+#define ESPI_GEN_CAP_N_CONF			0x0a0
+#define ESPI_CH0_CAP_N_CONF			0x0a4
+#define ESPI_CH1_CAP_N_CONF			0x0a8
+#define ESPI_CH2_CAP_N_CONF			0x0ac
+#define ESPI_CH3_CAP_N_CONF			0x0b0
+#define ESPI_CH3_CAP_N_CONF2			0x0b4
+#define ESPI_VW_GPIO_DIR			0x0c0
+#define ESPI_VW_GPIO_GRP			0x0c4
+#define ESPI_VW_SYSEVT1_INT_EN			0x100
+#define   ESPI_VW_SYSEVT1_INT_EN_SUSPEND_WARN	BIT(0)
+#define ESPI_VW_SYSEVT1				0x104
+#define   ESPI_VW_SYSEVT1_SUSPEND_ACK		BIT(20)
+#define   ESPI_VW_SYSEVT1_SUSPEND_WARN		BIT(0)
+#define ESPI_VW_SYSEVT_INT_T0			0x110
+#define ESPI_VW_SYSEVT_INT_T1			0x114
+#define ESPI_VW_SYSEVT_INT_T2			0x118
+#define   ESPI_VW_SYSEVT_INT_T2_HOST_RST_WARN	BIT(8)
+#define   ESPI_VW_SYSEVT_INT_T2_OOB_RST_WARN	BIT(6)
+#define ESPI_VW_SYSEVT_INT_STS			0x11c
+#define   ESPI_VW_SYSEVT_INT_STS_HOST_RST_WARN	BIT(8)
+#define   ESPI_VW_SYSEVT_INT_STS_OOB_RST_WARN	BIT(6)
+#define ESPI_VW_SYSEVT1_INT_T0			0x120
+#define   ESPI_VW_SYSEVT1_INT_T0_SUSPEND_WARN	BIT(0)
+#define ESPI_VW_SYSEVT1_INT_T1			0x124
+#define ESPI_VW_SYSEVT1_INT_T2			0x128
+#define ESPI_VW_SYSEVT1_INT_STS			0x12c
+#define   ESPI_VW_SYSEVT1_INT_STS_SUSPEND_WARN	BIT(0)
+
+/* collect ESPI_INT_STS bits for convenience */
+#define ESPI_INT_STS_PERIF			\
+	(ESPI_INT_STS_PERIF_NP_TX_ABT |		\
+	 ESPI_INT_STS_PERIF_PC_TX_ABT |		\
+	 ESPI_INT_STS_PERIF_NP_RX_ABT |		\
+	 ESPI_INT_STS_PERIF_PC_RX_ABT |		\
+	 ESPI_INT_STS_PERIF_NP_TX_ERR |		\
+	 ESPI_INT_STS_PERIF_PC_TX_ERR |		\
+	 ESPI_INT_STS_PERIF_NP_TX_CMPLT |	\
+	 ESPI_INT_STS_PERIF_PC_TX_CMPLT |	\
+	 ESPI_INT_STS_PERIF_PC_RX_CMPLT)
+
+#define ESPI_INT_STS_VW			\
+	(ESPI_INT_STS_VW_SYSEVT1 |	\
+	 ESPI_INT_STS_VW_GPIO    |	\
+	 ESPI_INT_STS_VW_SYSEVT)
+
+#define ESPI_INT_STS_OOB		\
+	(ESPI_INT_STS_OOB_RX_TMOUT |	\
+	 ESPI_INT_STS_OOB_TX_ERR |	\
+	 ESPI_INT_STS_OOB_TX_ABT |	\
+	 ESPI_INT_STS_OOB_RX_ABT |	\
+	 ESPI_INT_STS_OOB_TX_CMPLT |	\
+	 ESPI_INT_STS_OOB_RX_CMPLT)
+
+#define ESPI_INT_STS_FLASH		\
+	(ESPI_INT_STS_FLASH_TX_ERR |	\
+	 ESPI_INT_STS_FLASH_TX_ABT |	\
+	 ESPI_INT_STS_FLASH_RX_ABT |	\
+	 ESPI_INT_STS_FLASH_TX_CMPLT |	\
+	 ESPI_INT_STS_FLASH_RX_CMPLT)
+
+/* consistent with DTS property "flash-safs-mode" */
+enum ast2500_safs_mode {
+	SAFS_MODE_MIX = 0x0,
+	SAFS_MODE_SW,
+	SAFS_MODES,
+};
+
+#endif
diff --git a/drivers/soc/aspeed/ast2600-espi.c b/drivers/soc/aspeed/ast2600-espi.c
new file mode 100644
index 000000000000..62c1e8cab4d2
--- /dev/null
+++ b/drivers/soc/aspeed/ast2600-espi.c
@@ -0,0 +1,1883 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright 2023 Aspeed Technology Inc.
+ */
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/clk.h>
+#include <linux/sizes.h>
+#include <linux/module.h>
+#include <linux/bitfield.h>
+#include <linux/of_device.h>
+#include <linux/interrupt.h>
+#include <linux/platform_device.h>
+#include <linux/miscdevice.h>
+#include <linux/dma-mapping.h>
+#include <linux/uaccess.h>
+#include <linux/vmalloc.h>
+#include <linux/poll.h>
+#include <linux/delay.h>
+
+#include "ast2600-espi.h"
+
+#define DEVICE_NAME		"aspeed-espi"
+
+#define PERIF_MCYC_ALIGN	SZ_64K
+#define PERIF_MMBI_ALIGN	SZ_64K
+#define PERIF_MMBI_INST_NUM	8
+
+#define OOB_DMA_DESC_NUM	8
+#define OOB_DMA_DESC_CUSTOM	0x4
+
+#define FLASH_SAFS_ALIGN	SZ_16M
+
+struct ast2600_espi_perif_mmbi {
+	void *b2h_virt;
+	void *h2b_virt;
+	dma_addr_t b2h_addr;
+	dma_addr_t h2b_addr;
+	struct miscdevice b2h_mdev;
+	struct miscdevice h2b_mdev;
+	bool host_rwp_update;
+	wait_queue_head_t wq;
+	struct ast2600_espi_perif *perif;
+};
+
+struct ast2600_espi_perif {
+	struct {
+		bool enable;
+		int irq;
+		void *virt;
+		dma_addr_t taddr;
+		uint32_t saddr;
+		uint32_t size;
+		uint32_t inst_size;
+		struct ast2600_espi_perif_mmbi inst[PERIF_MMBI_INST_NUM];
+	} mmbi;
+
+	struct {
+		bool enable;
+		void *virt;
+		dma_addr_t taddr;
+		uint32_t saddr;
+		uint32_t size;
+	} mcyc;
+
+	struct {
+		bool enable;
+		void *np_tx_virt;
+		dma_addr_t np_tx_addr;
+		void *pc_tx_virt;
+		dma_addr_t pc_tx_addr;
+		void *pc_rx_virt;
+		dma_addr_t pc_rx_addr;
+	} dma;
+
+	bool rx_ready;
+	wait_queue_head_t wq;
+
+	spinlock_t lock;
+	struct mutex np_tx_mtx;
+	struct mutex pc_tx_mtx;
+	struct mutex pc_rx_mtx;
+
+	struct miscdevice mdev;
+};
+
+struct ast2600_espi_vw {
+	struct {
+		bool hw_mode;
+		uint32_t grp;
+		uint32_t dir;
+		uint32_t val;
+	} gpio;
+
+	struct miscdevice mdev;
+};
+
+struct ast2600_espi_oob_dma_tx_desc {
+	uint32_t data_addr;
+	uint8_t cyc;
+	uint16_t tag : 4;
+	uint16_t len : 12;
+	uint8_t msg_type : 3;
+	uint8_t raz0 : 1;
+	uint8_t pec : 1;
+	uint8_t int_en : 1;
+	uint8_t pause : 1;
+	uint8_t raz1 : 1;
+	uint32_t raz2;
+	uint32_t raz3;
+} __packed;
+
+struct ast2600_espi_oob_dma_rx_desc {
+	uint32_t data_addr;
+	uint8_t cyc;
+	uint16_t tag : 4;
+	uint16_t len : 12;
+	uint8_t raz : 7;
+	uint8_t dirty : 1;
+} __packed;
+
+struct ast2600_espi_oob {
+	struct {
+		bool enable;
+		struct ast2600_espi_oob_dma_tx_desc *txd_virt;
+		dma_addr_t txd_addr;
+		struct ast2600_espi_oob_dma_rx_desc *rxd_virt;
+		dma_addr_t rxd_addr;
+		void *tx_virt;
+		dma_addr_t tx_addr;
+		void *rx_virt;
+		dma_addr_t rx_addr;
+	} dma;
+
+	bool rx_ready;
+	wait_queue_head_t wq;
+
+	spinlock_t lock;
+	struct mutex tx_mtx;
+	struct mutex rx_mtx;
+
+	struct miscdevice mdev;
+};
+
+struct ast2600_espi_flash {
+	struct {
+		uint32_t mode;
+		phys_addr_t taddr;
+		uint32_t size;
+	} safs;
+
+	struct {
+		bool enable;
+		void *tx_virt;
+		dma_addr_t tx_addr;
+		void *rx_virt;
+		dma_addr_t rx_addr;
+	} dma;
+
+	bool rx_ready;
+	wait_queue_head_t wq;
+
+	spinlock_t lock;
+	struct mutex rx_mtx;
+	struct mutex tx_mtx;
+
+	struct miscdevice mdev;
+};
+
+struct ast2600_espi {
+	struct device *dev;
+	void __iomem *regs;
+	struct clk *clk;
+	int irq;
+
+	struct ast2600_espi_perif perif;
+	struct ast2600_espi_vw vw;
+	struct ast2600_espi_oob oob;
+	struct ast2600_espi_flash flash;
+};
+
+/* peripheral channel (CH0) */
+static int ast2600_espi_mmbi_b2h_mmap(struct file *fp, struct vm_area_struct *vma)
+{
+	struct ast2600_espi_perif_mmbi *mmbi;
+	struct ast2600_espi_perif *perif;
+	struct ast2600_espi *espi;
+	unsigned long vm_size;
+	pgprot_t prot;
+
+	mmbi = container_of(fp->private_data, struct ast2600_espi_perif_mmbi, b2h_mdev);
+
+	perif = mmbi->perif;
+
+	espi = container_of(perif, struct ast2600_espi, perif);
+
+	vm_size = vma->vm_end - vma->vm_start;
+	prot = vma->vm_page_prot;
+
+	if (((vma->vm_pgoff << PAGE_SHIFT) + vm_size) > (SZ_4K << perif->mmbi.inst_size))
+		return -EINVAL;
+
+	prot = pgprot_noncached(prot);
+
+	if (remap_pfn_range(vma, vma->vm_start,
+			    (mmbi->b2h_addr >> PAGE_SHIFT) + vma->vm_pgoff,
+			    vm_size, prot))
+		return -EAGAIN;
+
+	return 0;
+}
+
+static int ast2600_espi_mmbi_h2b_mmap(struct file *fp, struct vm_area_struct *vma)
+{
+	struct ast2600_espi_perif_mmbi *mmbi;
+	struct ast2600_espi_perif *perif;
+	struct ast2600_espi *espi;
+	unsigned long vm_size;
+	pgprot_t prot;
+
+	mmbi = container_of(fp->private_data, struct ast2600_espi_perif_mmbi, h2b_mdev);
+
+	perif = mmbi->perif;
+
+	espi = container_of(perif, struct ast2600_espi, perif);
+
+	vm_size = vma->vm_end - vma->vm_start;
+	prot = vma->vm_page_prot;
+
+	if (((vma->vm_pgoff << PAGE_SHIFT) + vm_size) > (SZ_4K << perif->mmbi.inst_size))
+		return -EINVAL;
+
+	prot = pgprot_noncached(prot);
+
+	if (remap_pfn_range(vma, vma->vm_start,
+			    (mmbi->h2b_addr >> PAGE_SHIFT) + vma->vm_pgoff,
+			    vm_size, prot))
+		return -EAGAIN;
+
+	return 0;
+}
+
+static __poll_t ast2600_espi_mmbi_h2b_poll(struct file *fp, struct poll_table_struct *pt)
+{
+	struct ast2600_espi_perif_mmbi *mmbi;
+
+	mmbi = container_of(fp->private_data, struct ast2600_espi_perif_mmbi, h2b_mdev);
+
+	poll_wait(fp, &mmbi->wq, pt);
+
+	if (!mmbi->host_rwp_update)
+		return 0;
+
+	mmbi->host_rwp_update = false;
+
+	return EPOLLIN;
+}
+
+static long ast2600_espi_perif_pc_get_rx(struct file *fp,
+					 struct ast2600_espi_perif *perif,
+					 struct aspeed_espi_ioc *ioc)
+{
+	uint32_t reg, cyc, tag, len;
+	struct ast2600_espi *espi;
+	struct espi_comm_hdr *hdr;
+	unsigned long flags;
+	uint32_t pkt_len;
+	uint8_t *pkt;
+	int i, rc;
+
+	espi = container_of(perif, struct ast2600_espi, perif);
+
+	if (fp->f_flags & O_NONBLOCK) {
+		if (!mutex_trylock(&perif->pc_rx_mtx))
+			return -EAGAIN;
+
+		if (!perif->rx_ready) {
+			rc = -ENODATA;
+			goto unlock_mtx_n_out;
+		}
+	} else {
+		mutex_lock(&perif->pc_rx_mtx);
+
+		if (!perif->rx_ready) {
+			rc = wait_event_interruptible(perif->wq, perif->rx_ready);
+			if (rc == -ERESTARTSYS) {
+				rc = -EINTR;
+				goto unlock_mtx_n_out;
+			}
+		}
+	}
+
+	/*
+	 * common header (i.e. cycle type, tag, and length)
+	 * part is written to HW registers
+	 */
+	reg = readl(espi->regs + ESPI_PERIF_PC_RX_CTRL);
+	cyc = FIELD_GET(ESPI_PERIF_PC_RX_CTRL_CYC, reg);
+	tag = FIELD_GET(ESPI_PERIF_PC_RX_CTRL_TAG, reg);
+	len = FIELD_GET(ESPI_PERIF_PC_RX_CTRL_LEN, reg);
+
+	/*
+	 * calculate the length of the rest part of the
+	 * eSPI packet to be read from HW and copied to
+	 * user space.
+	 */
+	switch (cyc) {
+	case ESPI_PERIF_MSG:
+		pkt_len = sizeof(struct espi_perif_msg);
+		break;
+	case ESPI_PERIF_MSG_D:
+		pkt_len = ((len) ? len : ESPI_MAX_PLD_LEN) +
+			  sizeof(struct espi_perif_msg);
+		break;
+	case ESPI_PERIF_SUC_CMPLT_D_MIDDLE:
+	case ESPI_PERIF_SUC_CMPLT_D_FIRST:
+	case ESPI_PERIF_SUC_CMPLT_D_LAST:
+	case ESPI_PERIF_SUC_CMPLT_D_ONLY:
+		pkt_len = ((len) ? len : ESPI_MAX_PLD_LEN) +
+			  sizeof(struct espi_perif_cmplt);
+		break;
+	case ESPI_PERIF_SUC_CMPLT:
+	case ESPI_PERIF_UNSUC_CMPLT:
+		pkt_len = sizeof(struct espi_perif_cmplt);
+		break;
+	default:
+		rc = -EFAULT;
+		goto unlock_mtx_n_out;
+	}
+
+	if (ioc->pkt_len < pkt_len) {
+		rc = -EINVAL;
+		goto unlock_mtx_n_out;
+	}
+
+	pkt = vmalloc(pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_mtx_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+	hdr->cyc = cyc;
+	hdr->tag = tag;
+	hdr->len_h = len >> 8;
+	hdr->len_l = len & 0xff;
+
+	if (perif->dma.enable) {
+		memcpy(hdr + 1, perif->dma.pc_rx_virt, pkt_len - sizeof(*hdr));
+	} else {
+		for (i = sizeof(*hdr); i < pkt_len; ++i)
+			reg = readl(espi->regs + ESPI_PERIF_PC_RX_DATA) & 0xff;
+	}
+
+	if (copy_to_user((void __user *)ioc->pkt, pkt, pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	spin_lock_irqsave(&perif->lock, flags);
+
+	writel(ESPI_PERIF_PC_RX_CTRL_SERV_PEND, espi->regs + ESPI_PERIF_PC_RX_CTRL);
+	perif->rx_ready = 0;
+
+	spin_unlock_irqrestore(&perif->lock, flags);
+
+	rc = 0;
+
+free_n_out:
+	vfree(pkt);
+
+unlock_mtx_n_out:
+	mutex_unlock(&perif->pc_rx_mtx);
+
+	return rc;
+}
+
+static long ast2600_espi_perif_pc_put_tx(struct file *fp,
+					 struct ast2600_espi_perif *perif,
+					 struct aspeed_espi_ioc *ioc)
+{
+	uint32_t reg, cyc, tag, len;
+	struct ast2600_espi *espi;
+	struct espi_comm_hdr *hdr;
+	uint8_t *pkt;
+	int i, rc;
+
+	espi = container_of(perif, struct ast2600_espi, perif);
+
+	if (!mutex_trylock(&perif->pc_tx_mtx))
+		return -EAGAIN;
+
+	reg = readl(espi->regs + ESPI_PERIF_PC_TX_CTRL);
+	if (reg & ESPI_PERIF_PC_TX_CTRL_TRIG_PEND) {
+		rc = -EBUSY;
+		goto unlock_n_out;
+	}
+
+	pkt = vmalloc(ioc->pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+
+	if (copy_from_user(pkt, (void __user *)ioc->pkt, ioc->pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	/*
+	 * common header (i.e. cycle type, tag, and length)
+	 * part is written to HW registers
+	 */
+	if (perif->dma.enable) {
+		memcpy(perif->dma.pc_tx_virt, hdr + 1, ioc->pkt_len - sizeof(*hdr));
+		dma_wmb();
+	} else {
+		for (i = sizeof(*hdr); i < ioc->pkt_len; ++i)
+			writel(pkt[i], espi->regs + ESPI_PERIF_PC_TX_DATA);
+	}
+
+	cyc = hdr->cyc;
+	tag = hdr->tag;
+	len = (hdr->len_h << 8) | (hdr->len_l & 0xff);
+
+	reg = FIELD_PREP(ESPI_PERIF_PC_TX_CTRL_CYC, cyc)
+	      | FIELD_PREP(ESPI_PERIF_PC_TX_CTRL_TAG, tag)
+	      | FIELD_PREP(ESPI_PERIF_PC_TX_CTRL_LEN, len)
+	      | ESPI_PERIF_PC_TX_CTRL_TRIG_PEND;
+	writel(reg, espi->regs + ESPI_PERIF_PC_TX_CTRL);
+
+	rc = 0;
+
+free_n_out:
+	vfree(pkt);
+
+unlock_n_out:
+	mutex_unlock(&perif->pc_tx_mtx);
+
+	return rc;
+}
+
+static long ast2600_espi_perif_np_put_tx(struct file *fp,
+					 struct ast2600_espi_perif *perif,
+					 struct aspeed_espi_ioc *ioc)
+{
+	uint32_t reg, cyc, tag, len;
+	struct ast2600_espi *espi;
+	struct espi_comm_hdr *hdr;
+	uint8_t *pkt;
+	int i, rc;
+
+	espi = container_of(perif, struct ast2600_espi, perif);
+
+	if (!mutex_trylock(&perif->np_tx_mtx))
+		return -EAGAIN;
+
+	reg = readl(espi->regs + ESPI_PERIF_NP_TX_CTRL);
+	if (reg & ESPI_PERIF_NP_TX_CTRL_TRIG_PEND) {
+		rc = -EBUSY;
+		goto unlock_n_out;
+	}
+
+	pkt = vmalloc(ioc->pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+
+	if (copy_from_user(pkt, (void __user *)ioc->pkt, ioc->pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	/*
+	 * common header (i.e. cycle type, tag, and length)
+	 * part is written to HW registers
+	 */
+	if (perif->dma.enable) {
+		memcpy(perif->dma.np_tx_virt, hdr + 1, ioc->pkt_len - sizeof(*hdr));
+		dma_wmb();
+	} else {
+		for (i = sizeof(*hdr); i < ioc->pkt_len; ++i)
+			writel(pkt[i], espi->regs + ESPI_PERIF_NP_TX_DATA);
+	}
+
+	cyc = hdr->cyc;
+	tag = hdr->tag;
+	len = (hdr->len_h << 8) | (hdr->len_l & 0xff);
+
+	reg = FIELD_PREP(ESPI_PERIF_NP_TX_CTRL_CYC, cyc)
+	      | FIELD_PREP(ESPI_PERIF_NP_TX_CTRL_TAG, tag)
+	      | FIELD_PREP(ESPI_PERIF_NP_TX_CTRL_LEN, len)
+	      | ESPI_PERIF_NP_TX_CTRL_TRIG_PEND;
+	writel(reg, espi->regs + ESPI_PERIF_NP_TX_CTRL);
+
+	rc = 0;
+
+free_n_out:
+	vfree(pkt);
+
+unlock_n_out:
+	mutex_unlock(&perif->np_tx_mtx);
+
+	return rc;
+}
+
+static long ast2600_espi_perif_ioctl(struct file *fp, unsigned int cmd, unsigned long arg)
+{
+	struct ast2600_espi_perif *perif;
+	struct aspeed_espi_ioc ioc;
+
+	perif = container_of(fp->private_data, struct ast2600_espi_perif, mdev);
+
+	if (copy_from_user(&ioc, (void __user *)arg, sizeof(ioc)))
+		return -EFAULT;
+
+	if (ioc.pkt_len > ESPI_MAX_PKT_LEN)
+		return -EINVAL;
+
+	switch (cmd) {
+	case ASPEED_ESPI_PERIF_PC_GET_RX:
+		return ast2600_espi_perif_pc_get_rx(fp, perif, &ioc);
+	case ASPEED_ESPI_PERIF_PC_PUT_TX:
+		return ast2600_espi_perif_pc_put_tx(fp, perif, &ioc);
+	case ASPEED_ESPI_PERIF_NP_PUT_TX:
+		return ast2600_espi_perif_np_put_tx(fp, perif, &ioc);
+	default:
+		break;
+	};
+
+	return -EINVAL;
+}
+
+static int ast2600_espi_perif_mmap(struct file *fp, struct vm_area_struct *vma)
+{
+	struct ast2600_espi_perif *perif;
+	unsigned long vm_size;
+	pgprot_t vm_prot;
+
+	perif = container_of(fp->private_data, struct ast2600_espi_perif, mdev);
+	if (!perif->mcyc.enable)
+		return -EPERM;
+
+	vm_size = vma->vm_end - vma->vm_start;
+	vm_prot = vma->vm_page_prot;
+
+	if (((vma->vm_pgoff << PAGE_SHIFT) + vm_size) > perif->mcyc.size)
+		return -EINVAL;
+
+	vm_prot = pgprot_noncached(vm_prot);
+
+	if (remap_pfn_range(vma, vma->vm_start,
+			    (perif->mcyc.taddr >> PAGE_SHIFT) + vma->vm_pgoff,
+			    vm_size, vm_prot))
+		return -EAGAIN;
+
+	return 0;
+}
+
+static const struct file_operations ast2600_espi_mmbi_b2h_fops = {
+	.owner = THIS_MODULE,
+	.mmap = ast2600_espi_mmbi_b2h_mmap,
+};
+
+static const struct file_operations ast2600_espi_mmbi_h2b_fops = {
+	.owner = THIS_MODULE,
+	.mmap = ast2600_espi_mmbi_h2b_mmap,
+	.poll = ast2600_espi_mmbi_h2b_poll,
+};
+
+static const struct file_operations ast2600_espi_perif_fops = {
+	.owner = THIS_MODULE,
+	.mmap = ast2600_espi_perif_mmap,
+	.unlocked_ioctl = ast2600_espi_perif_ioctl,
+};
+
+static irqreturn_t ast2600_espi_perif_mmbi_isr(int irq, void *arg)
+{
+	struct ast2600_espi_perif_mmbi *mmbi;
+	struct ast2600_espi_perif *perif;
+	struct ast2600_espi *espi;
+	uint32_t sts, tmp;
+	uint32_t *p;
+	int i;
+
+	espi = (struct ast2600_espi *)arg;
+
+	perif = &espi->perif;
+
+	sts = readl(espi->regs + ESPI_MMBI_INT_STS);
+	if (!sts)
+		return IRQ_NONE;
+
+	for (i = 0, tmp = sts; i < PERIF_MMBI_INST_NUM; ++i, tmp >>= 2) {
+		if (!(tmp & 0x3))
+			continue;
+
+		mmbi = &perif->mmbi.inst[i];
+
+		p = (uint32_t *)mmbi->h2b_virt;
+		p[0] = readl(espi->regs + ESPI_MMBI_HOST_RWP(i));
+		p[1] = readl(espi->regs + ESPI_MMBI_HOST_RWP(i) + 4);
+
+		mmbi->host_rwp_update = true;
+
+		wake_up_interruptible(&mmbi->wq);
+	}
+
+	writel(sts, espi->regs + ESPI_MMBI_INT_STS);
+
+	return IRQ_HANDLED;
+}
+
+static void ast2600_espi_perif_isr(struct ast2600_espi *espi)
+{
+	struct ast2600_espi_perif *perif;
+	unsigned long flags;
+	uint32_t sts;
+
+	perif = &espi->perif;
+
+	sts = readl(espi->regs + ESPI_INT_STS);
+
+	if (sts & ESPI_INT_STS_PERIF_PC_RX_CMPLT) {
+		writel(ESPI_INT_STS_PERIF_PC_RX_CMPLT, espi->regs + ESPI_INT_STS);
+
+		spin_lock_irqsave(&perif->lock, flags);
+		perif->rx_ready = true;
+		spin_unlock_irqrestore(&perif->lock, flags);
+
+		wake_up_interruptible(&perif->wq);
+	}
+}
+
+static void ast2600_espi_perif_reset(struct ast2600_espi *espi)
+{
+	struct ast2600_espi_perif *perif;
+	struct device *dev;
+	uint32_t reg, mask;
+
+	dev = espi->dev;
+
+	perif = &espi->perif;
+
+	if (perif->mmbi.enable) {
+		mask = ~(perif->mmbi.size - 1);
+		writel(mask, espi->regs + ESPI_PERIF_MMBI_MASK);
+		writel(perif->mmbi.saddr, espi->regs + ESPI_PERIF_MMBI_SADDR);
+		writel(perif->mmbi.taddr, espi->regs + ESPI_PERIF_MMBI_TADDR);
+
+		reg = readl(espi->regs + ESPI_CTRL2) & ~(ESPI_CTRL2_MMBI_RD_DIS | ESPI_CTRL2_MMBI_WR_DIS);
+		writel(reg, espi->regs + ESPI_CTRL2);
+
+		writel(0xffffffff, espi->regs + ESPI_MMBI_INT_EN);
+
+		reg = FIELD_PREP(ESPI_MMBI_CTRL_INST_SZ, perif->mmbi.inst_size)
+			| FIELD_PREP(ESPI_MMBI_CTRL_TOTAL_SZ, perif->mmbi.inst_size)
+			| ESPI_MMBI_CTRL_EN;
+		writel(reg, espi->regs + ESPI_MMBI_CTRL);
+	}
+
+	if (perif->mcyc.enable) {
+		mask = ~(perif->mcyc.size - 1);
+		writel(mask, espi->regs + ESPI_PERIF_MCYC_MASK);
+		writel(perif->mcyc.saddr, espi->regs + ESPI_PERIF_MCYC_SADDR);
+		writel(perif->mcyc.taddr, espi->regs + ESPI_PERIF_MCYC_TADDR);
+
+		reg = readl(espi->regs + ESPI_CTRL2) & ~(ESPI_CTRL2_MCYC_RD_DIS | ESPI_CTRL2_MCYC_WR_DIS);
+		writel(reg, espi->regs + ESPI_CTRL2);
+	}
+
+	if (perif->dma.enable) {
+		writel(perif->dma.np_tx_addr, espi->regs + ESPI_PERIF_NP_TX_DMA);
+		writel(perif->dma.pc_tx_addr, espi->regs + ESPI_PERIF_PC_TX_DMA);
+		writel(perif->dma.pc_rx_addr, espi->regs + ESPI_PERIF_PC_RX_DMA);
+
+		reg = readl(espi->regs + ESPI_CTRL)
+		      | ESPI_CTRL_PERIF_NP_TX_DMA_EN
+		      | ESPI_CTRL_PERIF_PC_TX_DMA_EN
+		      | ESPI_CTRL_PERIF_PC_RX_DMA_EN;
+		writel(reg, espi->regs + ESPI_CTRL);
+	}
+
+	reg = readl(espi->regs + ESPI_INT_EN) | ESPI_INT_EN_PERIF_PC_RX_CMPLT;
+	writel(reg, espi->regs + ESPI_INT_EN);
+
+	reg = readl(espi->regs + ESPI_CTRL) | ESPI_CTRL_PERIF_SW_RDY;
+	writel(reg, espi->regs + ESPI_CTRL);
+}
+
+static int ast2600_espi_perif_probe(struct ast2600_espi *espi)
+{
+	struct ast2600_espi_perif_mmbi *mmbi;
+	struct ast2600_espi_perif *perif;
+	struct platform_device *pdev;
+	struct device *dev;
+	int i, rc;
+
+	dev = espi->dev;
+
+	perif = &espi->perif;
+
+	init_waitqueue_head(&perif->wq);
+
+	spin_lock_init(&perif->lock);
+
+	mutex_init(&perif->np_tx_mtx);
+	mutex_init(&perif->pc_tx_mtx);
+	mutex_init(&perif->pc_rx_mtx);
+
+	perif->mmbi.enable = of_property_read_bool(dev->of_node, "perif-mmbi-enable");
+	if (perif->mmbi.enable) {
+		pdev = container_of(dev, struct platform_device, dev);
+
+		perif->mmbi.irq = platform_get_irq(pdev, 1);
+		if (perif->mmbi.irq < 0) {
+			dev_err(dev, "cannot get MMBI IRQ number\n");
+			return -ENODEV;
+		}
+
+		rc = of_property_read_u32(dev->of_node, "perif-mmbi-src-addr", &perif->mmbi.saddr);
+		if (rc || !IS_ALIGNED(perif->mmbi.saddr, PERIF_MMBI_ALIGN)) {
+			dev_err(dev, "cannot get 64KB-aligned MMBI host address\n");
+			return -ENODEV;
+		}
+
+		rc = of_property_read_u32(dev->of_node, "perif-mmbi-instance-size", &perif->mmbi.inst_size);
+		if (rc || perif->mmbi.inst_size >= MMBI_INST_SIZE_TYPES) {
+			dev_err(dev, "cannot get valid MMBI instance size\n");
+			return -EINVAL;
+		}
+
+		perif->mmbi.size = (SZ_8K << perif->mmbi.inst_size) * PERIF_MMBI_INST_NUM;
+		perif->mmbi.virt = dmam_alloc_coherent(dev, perif->mmbi.size,
+						       &perif->mmbi.taddr, GFP_KERNEL);
+		if (!perif->mmbi.virt) {
+			dev_err(dev, "cannot allocate MMBI\n");
+			return -ENOMEM;
+		}
+
+		for (i = 0; i < PERIF_MMBI_INST_NUM; ++i) {
+			mmbi = &perif->mmbi.inst[i];
+
+			init_waitqueue_head(&mmbi->wq);
+
+			mmbi->perif = perif;
+			mmbi->host_rwp_update = false;
+
+			mmbi->b2h_virt = perif->mmbi.virt + ((SZ_4K << perif->mmbi.inst_size) * i);
+			mmbi->b2h_addr = perif->mmbi.taddr + ((SZ_4K << perif->mmbi.inst_size) * i);
+			mmbi->b2h_mdev.parent = dev;
+			mmbi->b2h_mdev.minor = MISC_DYNAMIC_MINOR;
+			mmbi->b2h_mdev.name = devm_kasprintf(dev, GFP_KERNEL, "%s-mmbi-b2h%d", DEVICE_NAME, i);
+			mmbi->b2h_mdev.fops = &ast2600_espi_mmbi_b2h_fops;
+			rc = misc_register(&mmbi->b2h_mdev);
+			if (rc) {
+				dev_err(dev, "cannot register device %s\n", mmbi->b2h_mdev.name);
+				return rc;
+			}
+
+			mmbi->h2b_virt = perif->mmbi.virt + ((SZ_4K << perif->mmbi.inst_size) * (i + PERIF_MMBI_INST_NUM));
+			mmbi->h2b_addr = perif->mmbi.taddr + ((SZ_4K << perif->mmbi.inst_size) * (i + PERIF_MMBI_INST_NUM));
+			mmbi->h2b_mdev.parent = dev;
+			mmbi->h2b_mdev.minor = MISC_DYNAMIC_MINOR;
+			mmbi->h2b_mdev.name = devm_kasprintf(dev, GFP_KERNEL, "%s-mmbi-h2b%d", DEVICE_NAME, i);
+			mmbi->h2b_mdev.fops = &ast2600_espi_mmbi_h2b_fops;
+			rc = misc_register(&mmbi->h2b_mdev);
+			if (rc) {
+				dev_err(dev, "cannot register device %s\n", mmbi->h2b_mdev.name);
+				return rc;
+			}
+		}
+	}
+
+	perif->mcyc.enable = of_property_read_bool(dev->of_node, "perif-mcyc-enable");
+	if (perif->mcyc.enable) {
+		if (perif->mmbi.enable) {
+			dev_err(dev, "cannot enable memory cycle, occupied by MMBI\n");
+			return -EPERM;
+		}
+
+		rc = of_property_read_u32(dev->of_node, "perif-mcyc-src-addr", &perif->mcyc.saddr);
+		if (rc || !IS_ALIGNED(perif->mcyc.saddr, PERIF_MCYC_ALIGN)) {
+			dev_err(dev, "cannot get 64KB-aligned memory cycle host address\n");
+			return -ENODEV;
+		}
+
+		rc = of_property_read_u32(dev->of_node, "perif-mcyc-size", &perif->mcyc.size);
+		if (rc || !IS_ALIGNED(perif->mcyc.size, PERIF_MCYC_ALIGN)) {
+			dev_err(dev, "cannot get 64KB-aligned memory cycle size\n");
+			return -EINVAL;
+		}
+
+		perif->mcyc.virt = dmam_alloc_coherent(dev, perif->mcyc.size,
+						       &perif->mcyc.taddr, GFP_KERNEL);
+		if (!perif->mcyc.virt) {
+			dev_err(dev, "cannot allocate memory cycle\n");
+			return -ENOMEM;
+		}
+	}
+
+	perif->dma.enable = of_property_read_bool(dev->of_node, "perif-dma-mode");
+	if (perif->dma.enable) {
+		perif->dma.pc_tx_virt = dmam_alloc_coherent(dev, PAGE_SIZE,
+							    &perif->dma.pc_tx_addr, GFP_KERNEL);
+		if (!perif->dma.pc_tx_virt) {
+			dev_err(dev, "cannot allocate posted TX DMA buffer\n");
+			return -ENOMEM;
+		}
+
+		perif->dma.pc_rx_virt = dmam_alloc_coherent(dev, PAGE_SIZE,
+							    &perif->dma.pc_rx_addr, GFP_KERNEL);
+		if (!perif->dma.pc_rx_virt) {
+			dev_err(dev, "cannot allocate posted RX DMA buffer\n");
+			return -ENOMEM;
+		}
+
+		perif->dma.np_tx_virt = dmam_alloc_coherent(dev, PAGE_SIZE,
+							    &perif->dma.np_tx_addr, GFP_KERNEL);
+		if (!perif->dma.np_tx_virt) {
+			dev_err(dev, "cannot allocate non-posted TX DMA buffer\n");
+			return -ENOMEM;
+		}
+	}
+
+	perif->mdev.parent = dev;
+	perif->mdev.minor = MISC_DYNAMIC_MINOR;
+	perif->mdev.name = devm_kasprintf(dev, GFP_KERNEL, "%s-peripheral", DEVICE_NAME);
+	perif->mdev.fops = &ast2600_espi_perif_fops;
+	rc = misc_register(&perif->mdev);
+	if (rc) {
+		dev_err(dev, "cannot register device %s\n", perif->mdev.name);
+		return rc;
+	}
+
+	ast2600_espi_perif_reset(espi);
+
+	if (perif->mmbi.enable) {
+		rc = devm_request_irq(dev, espi->perif.mmbi.irq,
+				      ast2600_espi_perif_mmbi_isr, 0, dev_name(dev), espi);
+		if (rc) {
+			dev_err(dev, "cannot request MMBI IRQ\n");
+			return rc;
+		}
+	}
+
+	return 0;
+}
+
+/* virtual wire channel (CH1) */
+static long ast2600_espi_vw_ioctl(struct file *fp, unsigned int cmd, unsigned long arg)
+{
+	struct ast2600_espi_vw *vw;
+	struct ast2600_espi *espi;
+	uint32_t gpio;
+
+	vw = container_of(fp->private_data, struct ast2600_espi_vw, mdev);
+	espi = container_of(vw, struct ast2600_espi, vw);
+	gpio = vw->gpio.val;
+
+	switch (cmd) {
+	case ASPEED_ESPI_VW_GET_GPIO_VAL:
+		if (put_user(gpio, (uint32_t __user *)arg))
+			return -EFAULT;
+		break;
+	case ASPEED_ESPI_VW_PUT_GPIO_VAL:
+		if (get_user(gpio, (uint32_t __user *)arg))
+			return -EFAULT;
+
+		writel(gpio, espi->regs + ESPI_VW_GPIO_VAL);
+		break;
+	default:
+		return -EINVAL;
+	};
+
+	return 0;
+}
+
+static const struct file_operations ast2600_espi_vw_fops = {
+	.owner = THIS_MODULE,
+	.unlocked_ioctl = ast2600_espi_vw_ioctl,
+};
+
+static void ast2600_espi_vw_isr(struct ast2600_espi *espi)
+{
+	struct ast2600_espi_vw *vw;
+	uint32_t sts;
+
+	vw = &espi->vw;
+
+	sts = readl(espi->regs + ESPI_INT_STS);
+
+	if (sts & ESPI_INT_STS_VW_GPIO) {
+		vw->gpio.val = readl(espi->regs + ESPI_VW_GPIO_VAL);
+		writel(ESPI_INT_STS_VW_GPIO, espi->regs + ESPI_INT_STS);
+	}
+}
+
+static void ast2600_espi_vw_reset(struct ast2600_espi *espi)
+{
+	uint32_t reg;
+	struct ast2600_espi_vw *vw = &espi->vw;
+
+	writel(vw->gpio.grp, espi->regs + ESPI_VW_GPIO_GRP);
+	writel(vw->gpio.dir, espi->regs + ESPI_VW_GPIO_DIR);
+
+	vw->gpio.val = readl(espi->regs + ESPI_VW_GPIO_VAL);
+
+	writel(ESPI_INT_EN_VW_GPIO, espi->regs + ESPI_INT_EN);
+
+	reg = readl(espi->regs + ESPI_CTRL2) & ~(ESPI_CTRL2_VW_TX_SORT);
+	writel(reg, espi->regs + ESPI_CTRL2);
+
+	reg = readl(espi->regs + ESPI_CTRL)
+	      | ((vw->gpio.hw_mode) ? 0 : ESPI_CTRL_VW_GPIO_SW)
+	      | ESPI_CTRL_VW_SW_RDY;
+	writel(reg, espi->regs + ESPI_CTRL);
+}
+
+static int ast2600_espi_vw_probe(struct ast2600_espi *espi)
+{
+	int rc;
+	struct device *dev = espi->dev;
+	struct ast2600_espi_vw *vw = &espi->vw;
+
+	vw->gpio.hw_mode = of_property_read_bool(dev->of_node, "vw-gpio-hw-mode");
+	of_property_read_u32(dev->of_node, "vw-gpio-group", &vw->gpio.grp);
+	of_property_read_u32(dev->of_node, "vw-gpio-direction", &vw->gpio.dir);
+
+	vw->mdev.parent = dev;
+	vw->mdev.minor = MISC_DYNAMIC_MINOR;
+	vw->mdev.name = devm_kasprintf(dev, GFP_KERNEL, "%s-vw", DEVICE_NAME);
+	vw->mdev.fops = &ast2600_espi_vw_fops;
+	rc = misc_register(&vw->mdev);
+	if (rc) {
+		dev_err(dev, "cannot register device %s\n", vw->mdev.name);
+		return rc;
+	}
+
+	ast2600_espi_vw_reset(espi);
+
+	return 0;
+}
+
+/* out-of-band channel (CH2) */
+static long ast2600_espi_oob_dma_get_rx(struct file *fp,
+					struct ast2600_espi_oob *oob,
+					struct aspeed_espi_ioc *ioc)
+{
+	struct ast2600_espi_oob_dma_rx_desc *d;
+	struct ast2600_espi *espi;
+	struct espi_comm_hdr *hdr;
+	uint32_t wptr, pkt_len;
+	unsigned long flags;
+	uint8_t *pkt;
+	int rc;
+
+	espi = container_of(oob, struct ast2600_espi, oob);
+
+	wptr = readl(espi->regs + ESPI_OOB_RX_DESC_WPTR);
+
+	d = &oob->dma.rxd_virt[wptr];
+
+	if (!d->dirty)
+		return -EFAULT;
+
+	pkt_len = ((d->len) ? d->len : ESPI_MAX_PLD_LEN) + sizeof(struct espi_comm_hdr);
+
+	if (ioc->pkt_len < pkt_len)
+		return -EINVAL;
+
+	pkt = vmalloc(pkt_len);
+	if (!pkt)
+		return -ENOMEM;
+
+	hdr = (struct espi_comm_hdr *)pkt;
+	hdr->cyc = d->cyc;
+	hdr->tag = d->tag;
+	hdr->len_h = d->len >> 8;
+	hdr->len_l = d->len & 0xff;
+	memcpy(hdr + 1, oob->dma.rx_virt + (PAGE_SIZE * wptr), pkt_len - sizeof(*hdr));
+
+	if (copy_to_user((void __user *)ioc->pkt, pkt, pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	spin_lock_irqsave(&oob->lock, flags);
+
+	/* make current descriptor available again */
+	d->dirty = 0;
+
+	wptr = (wptr + 1) % OOB_DMA_DESC_NUM;
+	writel(wptr | ESPI_OOB_RX_DESC_WPTR_RECV_EN, espi->regs + ESPI_OOB_RX_DESC_WPTR);
+
+	/* set ready flag base on the next RX descriptor */
+	oob->rx_ready = oob->dma.rxd_virt[wptr].dirty;
+
+	spin_unlock_irqrestore(&oob->lock, flags);
+
+	rc = 0;
+
+free_n_out:
+	vfree(pkt);
+
+	return rc;
+}
+
+static long ast2600_espi_oob_get_rx(struct file *fp,
+				    struct ast2600_espi_oob *oob,
+				    struct aspeed_espi_ioc *ioc)
+{
+	uint32_t reg, cyc, tag, len;
+	struct ast2600_espi *espi;
+	struct espi_comm_hdr *hdr;
+	unsigned long flags;
+	uint32_t pkt_len;
+	uint8_t *pkt;
+	int i, rc;
+
+	espi = container_of(oob, struct ast2600_espi, oob);
+
+	if (fp->f_flags & O_NONBLOCK) {
+		if (!mutex_trylock(&oob->rx_mtx))
+			return -EAGAIN;
+
+		if (!oob->rx_ready) {
+			rc = -ENODATA;
+			goto unlock_mtx_n_out;
+		}
+	} else {
+		mutex_lock(&oob->rx_mtx);
+
+		if (!oob->rx_ready) {
+			rc = wait_event_interruptible(oob->wq, oob->rx_ready);
+			if (rc == -ERESTARTSYS) {
+				rc = -EINTR;
+				goto unlock_mtx_n_out;
+			}
+		}
+	}
+
+	if (oob->dma.enable) {
+		rc = ast2600_espi_oob_dma_get_rx(fp, oob, ioc);
+		goto unlock_mtx_n_out;
+	}
+
+	/*
+	 * common header (i.e. cycle type, tag, and length)
+	 * part is written to HW registers
+	 */
+	reg = readl(espi->regs + ESPI_OOB_RX_CTRL);
+	cyc = FIELD_GET(ESPI_OOB_RX_CTRL_CYC, reg);
+	tag = FIELD_GET(ESPI_OOB_RX_CTRL_TAG, reg);
+	len = FIELD_GET(ESPI_OOB_RX_CTRL_LEN, reg);
+
+	/*
+	 * calculate the length of the rest part of the
+	 * eSPI packet to be read from HW and copied to
+	 * user space.
+	 */
+	pkt_len = ((len) ? len : ESPI_MAX_PLD_LEN) + sizeof(struct espi_comm_hdr);
+
+	if (ioc->pkt_len < pkt_len) {
+		rc = -EINVAL;
+		goto unlock_mtx_n_out;
+	}
+
+	pkt = vmalloc(pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_mtx_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+	hdr->cyc = cyc;
+	hdr->tag = tag;
+	hdr->len_h = len >> 8;
+	hdr->len_l = len & 0xff;
+
+	for (i = sizeof(*hdr); i < pkt_len; ++i)
+		pkt[i] = readl(espi->regs + ESPI_OOB_RX_DATA) & 0xff;
+
+	if (copy_to_user((void __user *)ioc->pkt, pkt, pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	spin_lock_irqsave(&oob->lock, flags);
+
+	writel(ESPI_OOB_RX_CTRL_SERV_PEND, espi->regs + ESPI_OOB_RX_CTRL);
+	oob->rx_ready = 0;
+
+	spin_unlock_irqrestore(&oob->lock, flags);
+
+	rc = 0;
+
+free_n_out:
+	vfree(pkt);
+
+unlock_mtx_n_out:
+	mutex_unlock(&oob->rx_mtx);
+
+	return rc;
+}
+
+static long ast2600_espi_oob_dma_put_tx(struct file *fp,
+					struct ast2600_espi_oob *oob,
+					struct aspeed_espi_ioc *ioc)
+{
+	struct ast2600_espi_oob_dma_tx_desc *d;
+	struct ast2600_espi *espi;
+	struct espi_comm_hdr *hdr;
+	uint32_t rptr, wptr;
+	uint8_t *pkt;
+	int rc;
+
+	espi = container_of(oob, struct ast2600_espi, oob);
+
+	pkt = vzalloc(ioc->pkt_len);
+	if (!pkt)
+		return -ENOMEM;
+
+	hdr = (struct espi_comm_hdr *)pkt;
+
+	if (copy_from_user(pkt, (void __user *)ioc->pkt, ioc->pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	/* kick HW to update descriptor read/write pointer */
+	writel(ESPI_OOB_TX_DESC_RPTR_UPDATE, espi->regs + ESPI_OOB_TX_DESC_RPTR);
+
+	rptr = readl(espi->regs + ESPI_OOB_TX_DESC_RPTR);
+	wptr = readl(espi->regs + ESPI_OOB_TX_DESC_WPTR);
+
+	if (((wptr + 1) % OOB_DMA_DESC_NUM) == rptr) {
+		rc = -EBUSY;
+		goto free_n_out;
+	}
+
+	d = &oob->dma.txd_virt[wptr];
+	d->cyc = hdr->cyc;
+	d->tag = hdr->tag;
+	d->len = (hdr->len_h << 8) | (hdr->len_l & 0xff);
+	d->msg_type = OOB_DMA_DESC_CUSTOM;
+
+	memcpy(oob->dma.tx_virt + (PAGE_SIZE * wptr), hdr + 1,  ioc->pkt_len - sizeof(*hdr));
+
+	dma_wmb();
+
+	wptr = (wptr + 1) % OOB_DMA_DESC_NUM;
+	writel(wptr | ESPI_OOB_TX_DESC_WPTR_SEND_EN, espi->regs + ESPI_OOB_TX_DESC_WPTR);
+
+	rc = 0;
+
+free_n_out:
+	vfree(pkt);
+
+	return rc;
+}
+
+static long ast2600_espi_oob_put_tx(struct file *fp,
+				    struct ast2600_espi_oob *oob,
+				    struct aspeed_espi_ioc *ioc)
+{
+	uint32_t reg, cyc, tag, len;
+	struct ast2600_espi *espi;
+	struct espi_comm_hdr *hdr;
+	uint8_t *pkt;
+	int i, rc;
+
+	espi = container_of(oob, struct ast2600_espi, oob);
+
+	if (!mutex_trylock(&oob->tx_mtx))
+		return -EAGAIN;
+
+	if (oob->dma.enable) {
+		rc = ast2600_espi_oob_dma_put_tx(fp, oob, ioc);
+		goto unlock_mtx_n_out;
+	}
+
+	reg = readl(espi->regs + ESPI_OOB_TX_CTRL);
+	if (reg & ESPI_OOB_TX_CTRL_TRIG_PEND) {
+		rc = -EBUSY;
+		goto unlock_mtx_n_out;
+	}
+
+	if (ioc->pkt_len > ESPI_MAX_PKT_LEN) {
+		rc = -EINVAL;
+		goto unlock_mtx_n_out;
+	}
+
+	pkt = vmalloc(ioc->pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_mtx_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+
+	if (copy_from_user(pkt, (void __user *)ioc->pkt, ioc->pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	/*
+	 * common header (i.e. cycle type, tag, and length)
+	 * part is written to HW registers
+	 */
+	for (i = sizeof(*hdr); i < ioc->pkt_len; ++i)
+		writel(pkt[i], espi->regs + ESPI_OOB_TX_DATA);
+
+	cyc = hdr->cyc;
+	tag = hdr->tag;
+	len = (hdr->len_h << 8) | (hdr->len_l & 0xff);
+
+	reg = FIELD_PREP(ESPI_OOB_TX_CTRL_CYC, cyc)
+	      | FIELD_PREP(ESPI_OOB_TX_CTRL_TAG, tag)
+	      | FIELD_PREP(ESPI_OOB_TX_CTRL_LEN, len)
+	      | ESPI_OOB_TX_CTRL_TRIG_PEND;
+	writel(reg, espi->regs + ESPI_OOB_TX_CTRL);
+
+	rc = 0;
+
+free_n_out:
+	vfree(pkt);
+
+unlock_mtx_n_out:
+	mutex_unlock(&oob->tx_mtx);
+
+	return rc;
+}
+
+static long ast2600_espi_oob_ioctl(struct file *fp, unsigned int cmd, unsigned long arg)
+{
+	struct ast2600_espi_oob *oob;
+	struct aspeed_espi_ioc ioc;
+
+	oob = container_of(fp->private_data, struct ast2600_espi_oob, mdev);
+
+	if (copy_from_user(&ioc, (void __user *)arg, sizeof(ioc)))
+		return -EFAULT;
+
+	if (ioc.pkt_len > ESPI_MAX_PKT_LEN)
+		return -EINVAL;
+
+	switch (cmd) {
+	case ASPEED_ESPI_OOB_GET_RX:
+		return ast2600_espi_oob_get_rx(fp, oob, &ioc);
+	case ASPEED_ESPI_OOB_PUT_TX:
+		return ast2600_espi_oob_put_tx(fp, oob, &ioc);
+	};
+
+	return -EINVAL;
+}
+
+static const struct file_operations ast2600_espi_oob_fops = {
+	.owner = THIS_MODULE,
+	.unlocked_ioctl = ast2600_espi_oob_ioctl,
+};
+
+static void ast2600_espi_oob_isr(struct ast2600_espi *espi)
+{
+	struct ast2600_espi_oob *oob;
+	unsigned long flags;
+	uint32_t sts;
+
+	oob = &espi->oob;
+
+	sts = readl(espi->regs + ESPI_INT_STS);
+
+	if (sts & ESPI_INT_STS_OOB_RX_CMPLT) {
+		writel(ESPI_INT_STS_OOB_RX_CMPLT, espi->regs + ESPI_INT_STS);
+
+		spin_lock_irqsave(&oob->lock, flags);
+		oob->rx_ready = true;
+		spin_unlock_irqrestore(&oob->lock, flags);
+
+		wake_up_interruptible(&oob->wq);
+	}
+}
+
+static void ast2600_espi_oob_reset(struct ast2600_espi *espi)
+{
+	struct ast2600_espi_oob *oob;
+	dma_addr_t tx_addr, rx_addr;
+	uint32_t reg;
+	int i;
+
+	oob = &espi->oob;
+	if (oob->dma.enable) {
+		tx_addr = oob->dma.tx_addr;
+		rx_addr = oob->dma.rx_addr;
+
+		for (i = 0; i < OOB_DMA_DESC_NUM; ++i) {
+			oob->dma.txd_virt[i].data_addr = tx_addr;
+			tx_addr += PAGE_SIZE;
+
+			oob->dma.rxd_virt[i].data_addr = rx_addr;
+			oob->dma.rxd_virt[i].dirty = 0;
+			rx_addr += PAGE_SIZE;
+		}
+
+		reg = readl(espi->regs + ESPI_CTRL);
+		writel(reg & ~ESPI_CTRL_OOB_RX_SW_RST, espi->regs + ESPI_CTRL);
+		udelay(1);
+		writel(reg, espi->regs + ESPI_CTRL);
+
+		writel(oob->dma.txd_addr, espi->regs + ESPI_OOB_TX_DMA);
+		writel(0x0, espi->regs + ESPI_OOB_TX_DESC_WPTR);
+		writel(OOB_DMA_DESC_NUM, espi->regs + ESPI_OOB_TX_DESC_NUM);
+
+		writel(oob->dma.rxd_addr, espi->regs + ESPI_OOB_RX_DMA);
+		writel(0x0, espi->regs + ESPI_OOB_RX_DESC_WPTR);
+		writel(OOB_DMA_DESC_NUM, espi->regs + ESPI_OOB_RX_DESC_NUM);
+
+		reg = readl(espi->regs + ESPI_CTRL)
+		      | ESPI_CTRL_OOB_TX_DMA_EN
+		      | ESPI_CTRL_OOB_RX_DMA_EN;
+		writel(reg, espi->regs + ESPI_CTRL);
+
+		/* activate RX DMA to make OOB_FREE */
+		writel(ESPI_OOB_RX_DESC_WPTR_RECV_EN, espi->regs + ESPI_OOB_RX_DESC_WPTR);
+	}
+
+	writel(ESPI_INT_EN_OOB_RX_CMPLT, espi->regs + ESPI_INT_EN);
+
+	reg = readl(espi->regs + ESPI_CTRL) | ESPI_CTRL_OOB_SW_RDY;
+	writel(reg, espi->regs + ESPI_CTRL);
+}
+
+static int ast2600_espi_oob_probe(struct ast2600_espi *espi)
+{
+	struct ast2600_espi_oob *oob;
+	struct device *dev;
+	int rc;
+
+	dev = espi->dev;
+
+	oob = &espi->oob;
+
+	init_waitqueue_head(&oob->wq);
+
+	spin_lock_init(&oob->lock);
+
+	mutex_init(&oob->tx_mtx);
+	mutex_init(&oob->rx_mtx);
+
+	oob->dma.enable = of_property_read_bool(dev->of_node, "oob-dma-mode");
+	if (oob->dma.enable) {
+		oob->dma.txd_virt = dmam_alloc_coherent(dev, sizeof(*oob->dma.txd_virt) * OOB_DMA_DESC_NUM, &oob->dma.txd_addr, GFP_KERNEL);
+		if (!oob->dma.txd_virt) {
+			dev_err(dev, "cannot allocate DMA TX descriptor\n");
+			return -ENOMEM;
+		}
+		oob->dma.tx_virt = dmam_alloc_coherent(dev, PAGE_SIZE * OOB_DMA_DESC_NUM, &oob->dma.tx_addr, GFP_KERNEL);
+		if (!oob->dma.tx_virt) {
+			dev_err(dev, "cannot allocate DMA TX buffer\n");
+			return -ENOMEM;
+		}
+
+		oob->dma.rxd_virt = dmam_alloc_coherent(dev, sizeof(*oob->dma.rxd_virt) * OOB_DMA_DESC_NUM, &oob->dma.rxd_addr, GFP_KERNEL);
+		if (!oob->dma.rxd_virt) {
+			dev_err(dev, "cannot allocate DMA RX descriptor\n");
+			return -ENOMEM;
+		}
+
+		oob->dma.rx_virt = dmam_alloc_coherent(dev, PAGE_SIZE * OOB_DMA_DESC_NUM, &oob->dma.rx_addr, GFP_KERNEL);
+		if (!oob->dma.rx_virt) {
+			dev_err(dev, "cannot allocate DMA TX buffer\n");
+			return -ENOMEM;
+		}
+	}
+
+	oob->mdev.parent = dev;
+	oob->mdev.minor = MISC_DYNAMIC_MINOR;
+	oob->mdev.name = devm_kasprintf(dev, GFP_KERNEL, "%s-oob", DEVICE_NAME);
+	oob->mdev.fops = &ast2600_espi_oob_fops;
+	rc = misc_register(&oob->mdev);
+	if (rc) {
+		dev_err(dev, "cannot register device %s\n", oob->mdev.name);
+		return rc;
+	}
+
+	ast2600_espi_oob_reset(espi);
+
+	return 0;
+}
+
+/* flash channel (CH3) */
+static long ast2600_espi_flash_get_rx(struct file *fp,
+				      struct ast2600_espi_flash *flash,
+				      struct aspeed_espi_ioc *ioc)
+{
+	uint32_t reg, cyc, tag, len;
+	struct ast2600_espi *espi;
+	struct espi_comm_hdr *hdr;
+	unsigned long flags;
+	uint32_t pkt_len;
+	uint8_t *pkt;
+	int i, rc;
+
+	rc = 0;
+
+	espi = container_of(flash, struct ast2600_espi, flash);
+
+	if (fp->f_flags & O_NONBLOCK) {
+		if (!mutex_trylock(&flash->rx_mtx))
+			return -EAGAIN;
+
+		if (!flash->rx_ready) {
+			rc = -ENODATA;
+			goto unlock_mtx_n_out;
+		}
+	} else {
+		mutex_lock(&flash->rx_mtx);
+
+		if (!flash->rx_ready) {
+			rc = wait_event_interruptible(flash->wq, flash->rx_ready);
+			if (rc == -ERESTARTSYS) {
+				rc = -EINTR;
+				goto unlock_mtx_n_out;
+			}
+		}
+	}
+
+	/*
+	 * common header (i.e. cycle type, tag, and length)
+	 * part is written to HW registers
+	 */
+	reg = readl(espi->regs + ESPI_FLASH_RX_CTRL);
+	cyc = FIELD_GET(ESPI_FLASH_RX_CTRL_CYC, reg);
+	tag = FIELD_GET(ESPI_FLASH_RX_CTRL_TAG, reg);
+	len = FIELD_GET(ESPI_FLASH_RX_CTRL_LEN, reg);
+
+	/*
+	 * calculate the length of the rest part of the
+	 * eSPI packet to be read from HW and copied to
+	 * user space.
+	 */
+	switch (cyc) {
+	case ESPI_FLASH_WRITE:
+		pkt_len = ((len) ? len : ESPI_MAX_PLD_LEN) +
+			  sizeof(struct espi_flash_rwe);
+		break;
+	case ESPI_FLASH_READ:
+	case ESPI_FLASH_ERASE:
+		pkt_len = sizeof(struct espi_flash_rwe);
+		break;
+	case ESPI_FLASH_SUC_CMPLT_D_MIDDLE:
+	case ESPI_FLASH_SUC_CMPLT_D_FIRST:
+	case ESPI_FLASH_SUC_CMPLT_D_LAST:
+	case ESPI_FLASH_SUC_CMPLT_D_ONLY:
+		pkt_len = ((len) ? len : ESPI_MAX_PLD_LEN) +
+			  sizeof(struct espi_flash_cmplt);
+		break;
+	case ESPI_FLASH_SUC_CMPLT:
+	case ESPI_FLASH_UNSUC_CMPLT:
+		pkt_len = sizeof(struct espi_flash_cmplt);
+		break;
+	default:
+		rc = -EFAULT;
+		goto unlock_mtx_n_out;
+	}
+
+	if (ioc->pkt_len < pkt_len) {
+		rc = -EINVAL;
+		goto unlock_mtx_n_out;
+	}
+
+	pkt = vmalloc(pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_mtx_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+	hdr->cyc = cyc;
+	hdr->tag = tag;
+	hdr->len_h = len >> 8;
+	hdr->len_l = len & 0xff;
+
+	if (flash->dma.enable) {
+		memcpy(hdr + 1, flash->dma.rx_virt, pkt_len - sizeof(*hdr));
+	} else {
+		for (i = sizeof(*hdr); i < pkt_len; ++i)
+			pkt[i] = readl(espi->regs + ESPI_FLASH_RX_DATA) & 0xff;
+	}
+
+	if (copy_to_user((void __user *)ioc->pkt, pkt, pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	spin_lock_irqsave(&flash->lock, flags);
+
+	writel(ESPI_FLASH_RX_CTRL_SERV_PEND, espi->regs + ESPI_FLASH_RX_CTRL);
+	flash->rx_ready = 0;
+
+	spin_unlock_irqrestore(&flash->lock, flags);
+
+	rc = 0;
+
+free_n_out:
+	vfree(pkt);
+
+unlock_mtx_n_out:
+	mutex_unlock(&flash->rx_mtx);
+
+	return rc;
+}
+
+static long ast2600_espi_flash_put_tx(struct file *fp,
+				      struct ast2600_espi_flash *flash,
+				      struct aspeed_espi_ioc *ioc)
+{
+	uint32_t reg, cyc, tag, len;
+	struct ast2600_espi *espi;
+	struct espi_comm_hdr *hdr;
+	uint8_t *pkt;
+	int i, rc;
+
+	espi = container_of(flash, struct ast2600_espi, flash);
+
+	if (!mutex_trylock(&flash->tx_mtx))
+		return -EAGAIN;
+
+	reg = readl(espi->regs + ESPI_FLASH_TX_CTRL);
+	if (reg & ESPI_FLASH_TX_CTRL_TRIG_PEND) {
+		rc = -EBUSY;
+		goto unlock_mtx_n_out;
+	}
+
+	pkt = vmalloc(ioc->pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_mtx_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+
+	if (copy_from_user(pkt, (void __user *)ioc->pkt, ioc->pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	/*
+	 * common header (i.e. cycle type, tag, and length)
+	 * part is written to HW registers
+	 */
+	if (flash->dma.enable) {
+		memcpy(flash->dma.tx_virt, hdr + 1, ioc->pkt_len - sizeof(*hdr));
+		dma_wmb();
+	} else {
+		for (i = sizeof(*hdr); i < ioc->pkt_len; ++i)
+			writel(pkt[i], espi->regs + ESPI_FLASH_TX_DATA);
+	}
+
+	cyc = hdr->cyc;
+	tag = hdr->tag;
+	len = (hdr->len_h << 8) | (hdr->len_l & 0xff);
+
+	reg = FIELD_PREP(ESPI_FLASH_TX_CTRL_CYC, cyc)
+	      | FIELD_PREP(ESPI_FLASH_TX_CTRL_TAG, tag)
+	      | FIELD_PREP(ESPI_FLASH_TX_CTRL_LEN, len)
+	      | ESPI_FLASH_TX_CTRL_TRIG_PEND;
+	writel(reg, espi->regs + ESPI_FLASH_TX_CTRL);
+
+	rc = 0;
+
+free_n_out:
+	vfree(pkt);
+
+unlock_mtx_n_out:
+	mutex_unlock(&flash->tx_mtx);
+
+	return rc;
+}
+
+static long ast2600_espi_flash_ioctl(struct file *fp, unsigned int cmd, unsigned long arg)
+{
+	struct ast2600_espi_flash *flash;
+	struct aspeed_espi_ioc ioc;
+
+	flash = container_of(fp->private_data, struct ast2600_espi_flash, mdev);
+
+	if (copy_from_user(&ioc, (void __user *)arg, sizeof(ioc)))
+		return -EFAULT;
+
+	if (ioc.pkt_len > ESPI_MAX_PKT_LEN)
+		return -EINVAL;
+
+	switch (cmd) {
+	case ASPEED_ESPI_FLASH_GET_RX:
+		return ast2600_espi_flash_get_rx(fp, flash, &ioc);
+	case ASPEED_ESPI_FLASH_PUT_TX:
+		return ast2600_espi_flash_put_tx(fp, flash, &ioc);
+	default:
+		break;
+	};
+
+	return -EINVAL;
+}
+
+static const struct file_operations ast2600_espi_flash_fops = {
+	.owner = THIS_MODULE,
+	.unlocked_ioctl = ast2600_espi_flash_ioctl,
+};
+
+static void ast2600_espi_flash_isr(struct ast2600_espi *espi)
+{
+	struct ast2600_espi_flash *flash;
+	unsigned long flags;
+	uint32_t sts;
+
+	flash = &espi->flash;
+
+	sts = readl(espi->regs + ESPI_INT_STS_FLASH);
+
+	if (sts & ESPI_INT_STS_FLASH_RX_CMPLT) {
+		writel(ESPI_INT_STS_FLASH_RX_CMPLT, espi->regs + ESPI_INT_STS_FLASH);
+
+		spin_lock_irqsave(&flash->lock, flags);
+		flash->rx_ready = true;
+		spin_unlock_irqrestore(&flash->lock, flags);
+
+		wake_up_interruptible(&flash->wq);
+	}
+}
+
+static void ast2600_espi_flash_reset(struct ast2600_espi *espi)
+{
+	uint32_t reg;
+	struct ast2600_espi_flash *flash = &espi->flash;
+
+	reg = readl(espi->regs + ESPI_CTRL)
+	      | FIELD_PREP(ESPI_CTRL_FLASH_SAFS_MODE, flash->safs.mode);
+	writel(reg, espi->regs + ESPI_CTRL);
+
+	if (flash->safs.mode == SAFS_MODE_MIX) {
+		reg = FIELD_PREP(ESPI_FLASH_SAFS_TADDR_BASE, flash->safs.taddr >> 24)
+			| FIELD_PREP(ESPI_FLASH_SAFS_TADDR_MASK, (~(flash->safs.size - 1)) >> 24);
+		writel(reg, espi->regs + ESPI_FLASH_SAFS_TADDR);
+	}
+
+	if (flash->dma.enable) {
+		writel(flash->dma.tx_addr, espi->regs + ESPI_FLASH_TX_DMA);
+		writel(flash->dma.rx_addr, espi->regs + ESPI_FLASH_RX_DMA);
+
+		reg = readl(espi->regs + ESPI_CTRL)
+		      | ESPI_CTRL_FLASH_TX_DMA_EN
+			  | ESPI_CTRL_FLASH_RX_DMA_EN;
+		writel(reg, espi->regs + ESPI_CTRL);
+	}
+
+	reg = readl(espi->regs + ESPI_INT_EN) | ESPI_INT_EN_FLASH_RX_CMPLT;
+	writel(reg, espi->regs + ESPI_INT_EN);
+
+	reg = readl(espi->regs + ESPI_CTRL) | ESPI_CTRL_FLASH_SW_RDY;
+	writel(reg, espi->regs + ESPI_CTRL);
+}
+
+static int ast2600_espi_flash_probe(struct ast2600_espi *espi)
+{
+	struct ast2600_espi_flash *flash;
+	struct device *dev;
+	int rc;
+
+	dev = espi->dev;
+
+	flash = &espi->flash;
+
+	init_waitqueue_head(&flash->wq);
+
+	spin_lock_init(&flash->lock);
+
+	mutex_init(&flash->tx_mtx);
+	mutex_init(&flash->rx_mtx);
+
+	flash->safs.mode = SAFS_MODE_HW;
+
+	of_property_read_u32(dev->of_node, "flash-safs-mode", &flash->safs.mode);
+	if (flash->safs.mode == SAFS_MODE_MIX) {
+		rc = of_property_read_u32(dev->of_node, "flash-safs-taddr", &flash->safs.taddr);
+		if (rc || !IS_ALIGNED(flash->safs.taddr, FLASH_SAFS_ALIGN)) {
+			dev_err(dev, "cannot get 16MB-aligned SAFS target address\n");
+			return -ENODEV;
+		}
+
+		rc = of_property_read_u32(dev->of_node, "flash-safs-size", &flash->safs.size);
+		if (rc || !IS_ALIGNED(flash->safs.size, FLASH_SAFS_ALIGN)) {
+			dev_err(dev, "cannot get 16MB-aligned SAFS size\n");
+			return -ENODEV;
+		}
+	}
+
+	flash->dma.enable = of_property_read_bool(dev->of_node, "flash-dma-mode");
+	if (flash->dma.enable) {
+		flash->dma.tx_virt = dmam_alloc_coherent(dev, PAGE_SIZE, &flash->dma.tx_addr, GFP_KERNEL);
+		if (!flash->dma.tx_virt) {
+			dev_err(dev, "cannot allocate DMA TX buffer\n");
+			return -ENOMEM;
+		}
+
+		flash->dma.rx_virt = dmam_alloc_coherent(dev, PAGE_SIZE, &flash->dma.rx_addr, GFP_KERNEL);
+		if (!flash->dma.rx_virt) {
+			dev_err(dev, "cannot allocate DMA RX buffer\n");
+			return -ENOMEM;
+		}
+	}
+
+	flash->mdev.parent = dev;
+	flash->mdev.minor = MISC_DYNAMIC_MINOR;
+	flash->mdev.name = devm_kasprintf(dev, GFP_KERNEL, "%s-flash", DEVICE_NAME);
+	flash->mdev.fops = &ast2600_espi_flash_fops;
+	rc = misc_register(&flash->mdev);
+	if (rc) {
+		dev_err(dev, "cannot register device %s\n", flash->mdev.name);
+		return rc;
+	}
+
+	ast2600_espi_flash_reset(espi);
+
+	return 0;
+}
+
+/* global control */
+static irqreturn_t ast2600_espi_isr(int irq, void *arg)
+{
+	struct ast2600_espi *espi;
+	uint32_t sts;
+
+	espi = (struct ast2600_espi *)arg;
+
+	sts = readl(espi->regs + ESPI_INT_STS);
+	if (!sts)
+		return IRQ_NONE;
+
+	if (sts & ESPI_INT_STS_PERIF)
+		ast2600_espi_perif_isr(espi);
+
+	if (sts & ESPI_INT_STS_VW)
+		ast2600_espi_vw_isr(espi);
+
+	if (sts & ESPI_INT_STS_OOB)
+		ast2600_espi_oob_isr(espi);
+
+	if (sts & ESPI_INT_STS_FLASH)
+		ast2600_espi_flash_isr(espi);
+
+	if (sts & ESPI_INT_STS_RST_DEASSERT) {
+		ast2600_espi_perif_reset(espi);
+		ast2600_espi_vw_reset(espi);
+		ast2600_espi_oob_reset(espi);
+		ast2600_espi_flash_reset(espi);
+		writel(ESPI_INT_STS_RST_DEASSERT, espi->regs + ESPI_INT_STS);
+	}
+
+	return IRQ_HANDLED;
+}
+
+static int ast2600_espi_probe(struct platform_device *pdev)
+{
+	struct ast2600_espi *espi;
+	struct resource *res;
+	struct device *dev;
+	uint32_t reg;
+	int rc;
+
+	dev = &pdev->dev;
+
+	espi = devm_kzalloc(dev, sizeof(*espi), GFP_KERNEL);
+	if (!espi)
+		return -ENOMEM;
+
+	espi->dev = dev;
+
+	rc = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(64));
+	if (rc) {
+		dev_err(dev, "cannot set 64-bits DMA mask\n");
+		return rc;
+	}
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		dev_err(dev, "cannot get resource\n");
+		return -ENODEV;
+	}
+
+	espi->regs = devm_ioremap_resource(dev, res);
+	if (IS_ERR(espi->regs)) {
+		dev_err(dev, "cannot map registers\n");
+		return PTR_ERR(espi->regs);
+	}
+
+	espi->irq = platform_get_irq(pdev, 0);
+	if (espi->irq < 0) {
+		dev_err(dev, "cannot get IRQ number\n");
+		return -ENODEV;
+	}
+
+	espi->clk = devm_clk_get(dev, NULL);
+	if (IS_ERR(espi->clk)) {
+		dev_err(dev, "cannot get clock control\n");
+		return PTR_ERR(espi->clk);
+	}
+
+	rc = clk_prepare_enable(espi->clk);
+	if (rc) {
+		dev_err(dev, "cannot enable clocks\n");
+		return rc;
+	}
+
+	writel(0x0, espi->regs + ESPI_INT_EN);
+	writel(0xffffffff, espi->regs + ESPI_INT_STS);
+
+	writel(0x0, espi->regs + ESPI_MMBI_INT_EN);
+	writel(0xffffffff, espi->regs + ESPI_MMBI_INT_STS);
+
+	rc = ast2600_espi_perif_probe(espi);
+	if (rc) {
+		dev_err(dev, "cannot init peripheral channel, rc=%d\n", rc);
+		return rc;
+	}
+
+	rc = ast2600_espi_vw_probe(espi);
+	if (rc) {
+		dev_err(dev, "cannot init vw channel, rc=%d\n", rc);
+		return rc;
+	}
+
+	rc = ast2600_espi_oob_probe(espi);
+	if (rc) {
+		dev_err(dev, "cannot init oob channel, rc=%d\n", rc);
+		return rc;
+	}
+
+	rc = ast2600_espi_flash_probe(espi);
+	if (rc) {
+		dev_err(dev, "cannot init flash channel, rc=%d\n", rc);
+		return rc;
+	}
+
+	rc = devm_request_irq(dev, espi->irq, ast2600_espi_isr, 0, dev_name(dev), espi);
+	if (rc) {
+		dev_err(dev, "cannot request IRQ\n");
+		return rc;
+	}
+
+	reg = readl(espi->regs + ESPI_INT_EN) | ESPI_INT_EN_RST_DEASSERT;
+	writel(reg, espi->regs + ESPI_INT_EN);
+
+	dev_set_drvdata(dev, espi);
+
+	dev_info(dev, "module loaded\n");
+
+	return 0;
+}
+
+static const struct of_device_id ast2600_espi_of_matches[] = {
+	{ .compatible = "aspeed,ast2600-espi" },
+	{ },
+};
+
+static struct platform_driver ast2600_espi_driver = {
+	.driver = {
+		.name = "ast2600-espi",
+		.of_match_table = ast2600_espi_of_matches,
+	},
+	.probe = ast2600_espi_probe,
+};
+
+module_platform_driver(ast2600_espi_driver);
+
+MODULE_AUTHOR("Chia-Wei Wang <chiawei_wang@aspeedtech.com>");
+MODULE_DESCRIPTION("Control of AST2600 eSPI Device");
+MODULE_LICENSE("GPL");
diff --git a/drivers/soc/aspeed/ast2600-espi.h b/drivers/soc/aspeed/ast2600-espi.h
new file mode 100644
index 000000000000..46405b2d4af3
--- /dev/null
+++ b/drivers/soc/aspeed/ast2600-espi.h
@@ -0,0 +1,252 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright 2023 Aspeed Technology Inc.
+ */
+#ifndef _AST2600_ESPI_H_
+#define _AST2600_ESPI_H_
+
+#include <linux/bits.h>
+#include "aspeed-espi-comm.h"
+
+/* registers */
+#define ESPI_CTRL				0x000
+#define   ESPI_CTRL_OOB_RX_SW_RST		BIT(28)
+#define   ESPI_CTRL_FLASH_TX_DMA_EN		BIT(23)
+#define   ESPI_CTRL_FLASH_RX_DMA_EN		BIT(22)
+#define   ESPI_CTRL_OOB_TX_DMA_EN		BIT(21)
+#define   ESPI_CTRL_OOB_RX_DMA_EN		BIT(20)
+#define   ESPI_CTRL_PERIF_NP_TX_DMA_EN		BIT(19)
+#define   ESPI_CTRL_PERIF_PC_TX_DMA_EN		BIT(17)
+#define   ESPI_CTRL_PERIF_PC_RX_DMA_EN		BIT(16)
+#define   ESPI_CTRL_FLASH_SAFS_MODE		GENMASK(11, 10)
+#define   ESPI_CTRL_VW_GPIO_SW			BIT(9)
+#define   ESPI_CTRL_FLASH_SW_RDY		BIT(7)
+#define   ESPI_CTRL_OOB_SW_RDY			BIT(4)
+#define   ESPI_CTRL_VW_SW_RDY			BIT(3)
+#define   ESPI_CTRL_PERIF_SW_RDY		BIT(1)
+#define ESPI_STS				0x004
+#define ESPI_INT_STS				0x008
+#define   ESPI_INT_STS_RST_DEASSERT		BIT(31)
+#define   ESPI_INT_STS_OOB_RX_TMOUT		BIT(23)
+#define   ESPI_INT_STS_VW_SYSEVT1		BIT(22)
+#define   ESPI_INT_STS_FLASH_TX_ERR		BIT(21)
+#define   ESPI_INT_STS_OOB_TX_ERR		BIT(20)
+#define   ESPI_INT_STS_FLASH_TX_ABT		BIT(19)
+#define   ESPI_INT_STS_OOB_TX_ABT		BIT(18)
+#define   ESPI_INT_STS_PERIF_NP_TX_ABT		BIT(17)
+#define   ESPI_INT_STS_PERIF_PC_TX_ABT		BIT(16)
+#define   ESPI_INT_STS_FLASH_RX_ABT		BIT(15)
+#define   ESPI_INT_STS_OOB_RX_ABT		BIT(14)
+#define   ESPI_INT_STS_PERIF_NP_RX_ABT		BIT(13)
+#define   ESPI_INT_STS_PERIF_PC_RX_ABT		BIT(12)
+#define   ESPI_INT_STS_PERIF_NP_TX_ERR		BIT(11)
+#define   ESPI_INT_STS_PERIF_PC_TX_ERR		BIT(10)
+#define   ESPI_INT_STS_VW_GPIO			BIT(9)
+#define   ESPI_INT_STS_VW_SYSEVT		BIT(8)
+#define   ESPI_INT_STS_FLASH_TX_CMPLT		BIT(7)
+#define   ESPI_INT_STS_FLASH_RX_CMPLT		BIT(6)
+#define   ESPI_INT_STS_OOB_TX_CMPLT		BIT(5)
+#define   ESPI_INT_STS_OOB_RX_CMPLT		BIT(4)
+#define   ESPI_INT_STS_PERIF_NP_TX_CMPLT	BIT(3)
+#define   ESPI_INT_STS_PERIF_PC_TX_CMPLT	BIT(1)
+#define   ESPI_INT_STS_PERIF_PC_RX_CMPLT	BIT(0)
+#define ESPI_INT_EN				0x00c
+#define   ESPI_INT_EN_RST_DEASSERT		BIT(31)
+#define   ESPI_INT_EN_OOB_RX_TMOUT		BIT(23)
+#define   ESPI_INT_EN_VW_SYSEVT1		BIT(22)
+#define   ESPI_INT_EN_FLASH_TX_ERR		BIT(21)
+#define   ESPI_INT_EN_OOB_TX_ERR		BIT(20)
+#define   ESPI_INT_EN_FLASH_TX_ABT		BIT(19)
+#define   ESPI_INT_EN_OOB_TX_ABT		BIT(18)
+#define   ESPI_INT_EN_PERIF_NP_TX_ABT		BIT(17)
+#define   ESPI_INT_EN_PERIF_PC_TX_ABT		BIT(16)
+#define   ESPI_INT_EN_FLASH_RX_ABT		BIT(15)
+#define   ESPI_INT_EN_OOB_RX_ABT		BIT(14)
+#define   ESPI_INT_EN_PERIF_NP_RX_ABT		BIT(13)
+#define   ESPI_INT_EN_PERIF_PC_RX_ABT		BIT(12)
+#define   ESPI_INT_EN_PERIF_NP_TX_ERR		BIT(11)
+#define   ESPI_INT_EN_PERIF_PC_TX_ERR		BIT(10)
+#define   ESPI_INT_EN_VW_GPIO			BIT(9)
+#define   ESPI_INT_EN_VW_SYSEVT			BIT(8)
+#define   ESPI_INT_EN_FLASH_TX_CMPLT		BIT(7)
+#define   ESPI_INT_EN_FLASH_RX_CMPLT		BIT(6)
+#define   ESPI_INT_EN_OOB_TX_CMPLT		BIT(5)
+#define   ESPI_INT_EN_OOB_RX_CMPLT		BIT(4)
+#define   ESPI_INT_EN_PERIF_NP_TX_CMPLT		BIT(3)
+#define   ESPI_INT_EN_PERIF_PC_TX_CMPLT		BIT(1)
+#define   ESPI_INT_EN_PERIF_PC_RX_CMPLT		BIT(0)
+#define ESPI_PERIF_PC_RX_DMA			0x010
+#define ESPI_PERIF_PC_RX_CTRL			0x014
+#define   ESPI_PERIF_PC_RX_CTRL_SERV_PEND	BIT(31)
+#define   ESPI_PERIF_PC_RX_CTRL_LEN		GENMASK(23, 12)
+#define   ESPI_PERIF_PC_RX_CTRL_TAG		GENMASK(11, 8)
+#define   ESPI_PERIF_PC_RX_CTRL_CYC		GENMASK(7, 0)
+#define ESPI_PERIF_PC_RX_DATA			0x018
+#define ESPI_PERIF_PC_TX_DMA			0x020
+#define ESPI_PERIF_PC_TX_CTRL			0x024
+#define	  ESPI_PERIF_PC_TX_CTRL_TRIG_PEND	BIT(31)
+#define	  ESPI_PERIF_PC_TX_CTRL_LEN		GENMASK(23, 12)
+#define	  ESPI_PERIF_PC_TX_CTRL_TAG		GENMASK(11, 8)
+#define	  ESPI_PERIF_PC_TX_CTRL_CYC		GENMASK(7, 0)
+#define ESPI_PERIF_PC_TX_DATA			0x028
+#define ESPI_PERIF_NP_TX_DMA			0x030
+#define ESPI_PERIF_NP_TX_CTRL			0x034
+#define   ESPI_PERIF_NP_TX_CTRL_TRIG_PEND	BIT(31)
+#define	  ESPI_PERIF_NP_TX_CTRL_LEN		GENMASK(23, 12)
+#define	  ESPI_PERIF_NP_TX_CTRL_TAG		GENMASK(11, 8)
+#define	  ESPI_PERIF_NP_TX_CTRL_CYC		GENMASK(7, 0)
+#define ESPI_PERIF_NP_TX_DATA			0x038
+#define ESPI_OOB_RX_DMA				0x040
+#define ESPI_OOB_RX_CTRL			0x044
+#define	  ESPI_OOB_RX_CTRL_SERV_PEND		BIT(31)
+#define	  ESPI_OOB_RX_CTRL_LEN			GENMASK(23, 12)
+#define	  ESPI_OOB_RX_CTRL_TAG			GENMASK(11, 8)
+#define	  ESPI_OOB_RX_CTRL_CYC			GENMASK(7, 0)
+#define ESPI_OOB_RX_DATA			0x048
+#define ESPI_OOB_TX_DMA				0x050
+#define ESPI_OOB_TX_CTRL			0x054
+#define	  ESPI_OOB_TX_CTRL_TRIG_PEND		BIT(31)
+#define	  ESPI_OOB_TX_CTRL_LEN			GENMASK(23, 12)
+#define	  ESPI_OOB_TX_CTRL_TAG			GENMASK(11, 8)
+#define	  ESPI_OOB_TX_CTRL_CYC			GENMASK(7, 0)
+#define ESPI_OOB_TX_DATA			0x058
+#define ESPI_FLASH_RX_DMA			0x060
+#define ESPI_FLASH_RX_CTRL			0x064
+#define	  ESPI_FLASH_RX_CTRL_SERV_PEND		BIT(31)
+#define	  ESPI_FLASH_RX_CTRL_LEN		GENMASK(23, 12)
+#define	  ESPI_FLASH_RX_CTRL_TAG		GENMASK(11, 8)
+#define	  ESPI_FLASH_RX_CTRL_CYC		GENMASK(7, 0)
+#define ESPI_FLASH_RX_DATA			0x068
+#define ESPI_FLASH_TX_DMA			0x070
+#define ESPI_FLASH_TX_CTRL			0x074
+#define	  ESPI_FLASH_TX_CTRL_TRIG_PEND		BIT(31)
+#define	  ESPI_FLASH_TX_CTRL_LEN		GENMASK(23, 12)
+#define	  ESPI_FLASH_TX_CTRL_TAG		GENMASK(11, 8)
+#define	  ESPI_FLASH_TX_CTRL_CYC		GENMASK(7, 0)
+#define ESPI_FLASH_TX_DATA			0x078
+#define ESPI_CTRL2				0x080
+#define   ESPI_CTRL2_VW_TX_SORT			BIT(30)
+#define   ESPI_CTRL2_MCYC_RD_DIS		BIT(6)
+#define   ESPI_CTRL2_MMBI_RD_DIS		ESPI_CTRL2_MCYC_RD_DIS
+#define   ESPI_CTRL2_MCYC_WR_DIS		BIT(4)
+#define   ESPI_CTRL2_MMBI_WR_DIS		ESPI_CTRL2_MCYC_WR_DIS
+#define ESPI_PERIF_MCYC_SADDR			0x084
+#define ESPI_PERIF_MMBI_SADDR			ESPI_PERIF_MCYC_SADDR
+#define ESPI_PERIF_MCYC_TADDR			0x088
+#define ESPI_PERIF_MMBI_TADDR			ESPI_PERIF_MCYC_TADDR
+#define ESPI_PERIF_MCYC_MASK			0x08c
+#define ESPI_PERIF_MMBI_MASK			ESPI_PERIF_MCYC_MASK
+#define ESPI_FLASH_SAFS_TADDR			0x090
+#define   ESPI_FLASH_SAFS_TADDR_BASE		GENMASK(31, 24)
+#define   ESPI_FLASH_SAFS_TADDR_MASK		GENMASK(15, 8)
+#define ESPI_VW_SYSEVT_INT_EN			0x094
+#define ESPI_VW_SYSEVT				0x098
+#define   ESPI_VW_SYSEVT_HOST_RST_ACK		BIT(27)
+#define   ESPI_VW_SYSEVT_RST_CPU_INIT		BIT(26)
+#define   ESPI_VW_SYSEVT_SLV_BOOT_STS		BIT(23)
+#define   ESPI_VW_SYSEVT_NON_FATAL_ERR		BIT(22)
+#define   ESPI_VW_SYSEVT_FATAL_ERR			BIT(21)
+#define   ESPI_VW_SYSEVT_SLV_BOOT_DONE		BIT(20)
+#define   ESPI_VW_SYSEVT_OOB_RST_ACK		BIT(16)
+#define   ESPI_VW_SYSEVT_NMI_OUT			BIT(10)
+#define   ESPI_VW_SYSEVT_SMI_OUT			BIT(9)
+#define   ESPI_VW_SYSEVT_HOST_RST_WARN		BIT(8)
+#define   ESPI_VW_SYSEVT_OOB_RST_WARN		BIT(6)
+#define   ESPI_VW_SYSEVT_PLTRSTN			BIT(5)
+#define   ESPI_VW_SYSEVT_SUSPEND			BIT(4)
+#define   ESPI_VW_SYSEVT_S5_SLEEP			BIT(2)
+#define   ESPI_VW_SYSEVT_S4_SLEEP			BIT(1)
+#define   ESPI_VW_SYSEVT_S3_SLEEP			BIT(0)
+#define ESPI_VW_GPIO_VAL			0x09c
+#define ESPI_GEN_CAP_N_CONF			0x0a0
+#define ESPI_CH0_CAP_N_CONF			0x0a4
+#define ESPI_CH1_CAP_N_CONF			0x0a8
+#define ESPI_CH2_CAP_N_CONF			0x0ac
+#define ESPI_CH3_CAP_N_CONF			0x0b0
+#define ESPI_CH3_CAP_N_CONF2			0x0b4
+#define ESPI_VW_GPIO_DIR			0x0c0
+#define ESPI_VW_GPIO_GRP			0x0c4
+#define ESPI_VW_SYSEVT1_INT_EN			0x100
+#define ESPI_VW_SYSEVT1				0x104
+#define   ESPI_VW_SYSEVT1_SUSPEND_ACK		BIT(20)
+#define   ESPI_VW_SYSEVT1_SUSPEND_WARN		BIT(0)
+#define ESPI_VW_SYSEVT_INT_T0			0x110
+#define ESPI_VW_SYSEVT_INT_T1			0x114
+#define ESPI_VW_SYSEVT_INT_T2			0x118
+#define ESPI_VW_SYSEVT_INT_STS			0x11c
+#define ESPI_VW_SYSEVT1_INT_T0			0x120
+#define ESPI_VW_SYSEVT1_INT_T1			0x124
+#define ESPI_VW_SYSEVT1_INT_T2			0x128
+#define ESPI_VW_SYSEVT1_INT_STS			0x12c
+#define ESPI_OOB_RX_DESC_NUM			0x130
+#define ESPI_OOB_RX_DESC_RPTR			0x134
+#define	  ESPI_OOB_RX_DESC_RPTR_UPDATE		BIT(31)
+#define ESPI_OOB_RX_DESC_WPTR			0x138
+#define   ESPI_OOB_RX_DESC_WPTR_RECV_EN		BIT(31)
+#define ESPI_OOB_TX_DESC_NUM			0x140
+#define ESPI_OOB_TX_DESC_RPTR			0x144
+#define	  ESPI_OOB_TX_DESC_RPTR_UPDATE		BIT(31)
+#define ESPI_OOB_TX_DESC_WPTR			0x148
+#define	  ESPI_OOB_TX_DESC_WPTR_SEND_EN		BIT(31)
+#define ESPI_MMBI_CTRL				0x800
+#define   ESPI_MMBI_CTRL_INST_SZ		GENMASK(10, 8)
+#define   ESPI_MMBI_CTRL_TOTAL_SZ		GENMASK(6, 4)
+#define   ESPI_MMBI_CTRL_EN			BIT(0)
+#define ESPI_MMBI_INT_STS			0x808
+#define ESPI_MMBI_INT_EN			0x80c
+#define ESPI_MMBI_HOST_RWP(x)			(0x810 + ((x) << 3))
+
+/* collect ESPI_INT_STS bits for convenience */
+#define ESPI_INT_STS_PERIF			\
+	(ESPI_INT_STS_PERIF_NP_TX_ABT |		\
+	 ESPI_INT_STS_PERIF_PC_TX_ABT |		\
+	 ESPI_INT_STS_PERIF_NP_RX_ABT |		\
+	 ESPI_INT_STS_PERIF_PC_RX_ABT |		\
+	 ESPI_INT_STS_PERIF_NP_TX_ERR |		\
+	 ESPI_INT_STS_PERIF_PC_TX_ERR |		\
+	 ESPI_INT_STS_PERIF_NP_TX_CMPLT |	\
+	 ESPI_INT_STS_PERIF_PC_TX_CMPLT |	\
+	 ESPI_INT_STS_PERIF_PC_RX_CMPLT)
+
+#define ESPI_INT_STS_VW			\
+	(ESPI_INT_STS_VW_SYSEVT1 |	\
+	 ESPI_INT_STS_VW_GPIO    |	\
+	 ESPI_INT_STS_VW_SYSEVT)
+
+#define ESPI_INT_STS_OOB		\
+	(ESPI_INT_STS_OOB_RX_TMOUT |	\
+	 ESPI_INT_STS_OOB_TX_ERR |	\
+	 ESPI_INT_STS_OOB_TX_ABT |	\
+	 ESPI_INT_STS_OOB_RX_ABT |	\
+	 ESPI_INT_STS_OOB_TX_CMPLT |	\
+	 ESPI_INT_STS_OOB_RX_CMPLT)
+
+#define ESPI_INT_STS_FLASH		\
+	(ESPI_INT_STS_FLASH_TX_ERR |	\
+	 ESPI_INT_STS_FLASH_TX_ABT |	\
+	 ESPI_INT_STS_FLASH_RX_ABT |	\
+	 ESPI_INT_STS_FLASH_TX_CMPLT |	\
+	 ESPI_INT_STS_FLASH_RX_CMPLT)
+
+/* consistent with DTS property "flash-safs-mode" */
+enum ast2600_safs_mode {
+	SAFS_MODE_MIX = 0x0,
+	SAFS_MODE_SW,
+	SAFS_MODE_HW,
+	SAFS_MODES,
+};
+
+/* consistent with DTS property "perif-mmbi-instance-size" */
+enum ast2600_mmbi_instance_size {
+	MMBI_INST_SIZE_8KB = 0x0,
+	MMBI_INST_SIZE_16KB,
+	MMBI_INST_SIZE_32KB,
+	MMBI_INST_SIZE_64KB,
+	MMBI_INST_SIZE_128KB,
+	MMBI_INST_SIZE_256KB,
+	MMBI_INST_SIZE_512KB,
+	MMBI_INST_SIZE_1024KB,
+	MMBI_INST_SIZE_TYPES,
+};
+
+#endif
diff --git a/drivers/soc/aspeed/rvas/Kconfig b/drivers/soc/aspeed/rvas/Kconfig
new file mode 100644
index 000000000000..ef02e1b769d7
--- /dev/null
+++ b/drivers/soc/aspeed/rvas/Kconfig
@@ -0,0 +1,9 @@
+menu "ASPEED RVAS drivers"
+
+config ASPEED_RVAS
+	tristate "ASPEED RVAS driver"
+	default n
+	help
+	  Driver for ASPEED RVAS Engine
+
+endmenu
diff --git a/drivers/soc/aspeed/rvas/Makefile b/drivers/soc/aspeed/rvas/Makefile
new file mode 100644
index 000000000000..1cccd7e37f68
--- /dev/null
+++ b/drivers/soc/aspeed/rvas/Makefile
@@ -0,0 +1,3 @@
+obj-$(CONFIG_ASPEED_RVAS) += rvas.o
+rvas-y := video_main.o hardware_engines.o video_engine.o
+
diff --git a/drivers/soc/aspeed/rvas/hardware_engines.c b/drivers/soc/aspeed/rvas/hardware_engines.c
new file mode 100644
index 000000000000..da8d5b7fc21c
--- /dev/null
+++ b/drivers/soc/aspeed/rvas/hardware_engines.c
@@ -0,0 +1,2225 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * File Name     : hardware_engines.c
+ * Description   : AST2600 frame grabber hardware engines
+ *
+ * Copyright (C) 2019-2021 ASPEED Technology Inc. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/dma-mapping.h>
+#include <linux/mm.h>
+#include <asm/cacheflush.h>
+#include <stdbool.h>
+#include <linux/time.h>
+
+#include "hardware_engines.h"
+#include "video.h"
+#include "video_debug.h"
+
+static u32 dwBucketSizeRegOffset[BSE_MAX_BUCKET_SIZE_REGS] = { 0x20, 0x24, 0x28,
+	0x2c, 0x30, 0x34, 0x38, 0x3c, 0x40, 0x44, 0x48, 0x4c, 0x50, 0x54, 0x58,
+	0x5c };
+static u32 arrBuckSizeRegIndex[16] = { 3, 5, 8, 6, 1, 7, 11, 10, 14, 13, 2, 4,
+	9, 12, 0, 15 };
+
+static struct Resolution resTable1[0x3B - 0x30 + 1] = { { 800, 600 }, { 1024, 768 }, {
+	1280, 1024 }, { 1600, 1200 }, { 1920, 1200 }, { 1280, 800 },
+	{ 1440, 900 }, { 1680, 1050 }, { 1920, 1080 }, { 1366, 768 }, { 1600,
+		900 }, { 1152, 864 }, };
+
+static struct Resolution resTable2[0x52 - 0x50 + 1] = { { 320, 240 }, { 400, 300 }, {
+	512, 384 }, };
+
+static void prepare_bse_descriptor_2(struct Descriptor *pDAddress, u32 dwSourceAddress,
+	u32 dwDestAddress, bool bNotLastEntry, u16 wStride, u8 bytesPerPixel,
+	u32 dwFetchWidthPixels, u32 dwFetchHeight,
+	bool bInterrupt, u8 byBuckSizeRegIndex);
+
+static struct BSEAggregateRegister set_up_bse_bucket_2(struct AstRVAS *pAstRVAS,
+	u8 *abyBitIndexes, u8 byTotalBucketCount, u8 byBSBytesPerPixel,
+	u32 dwFetchWidthPixels, u32 dwFetchHeight, u32 dwBucketSizeIndex);
+
+
+static inline u32 ast_video_read(u32 video_reg_base, u32 reg)
+{
+	u32 val = readl((void *)(video_reg_base + reg));
+
+	return val;
+}
+
+// Get color depth
+static void ast_video_get_color_mode(u8 byNewColorMode, struct VideoGeometry *pvg)
+{
+	switch (byNewColorMode) {
+	case MODE_EGA:
+		pvg->gmt = VGAGraphicsMode; //4pp mode12/mode6A
+		pvg->byBitsPerPixel = 4;
+		break;
+
+	case MODE_VGA:
+		pvg->gmt = VGAGraphicsMode; //mode 13
+		pvg->byBitsPerPixel = 8;
+		break;
+
+	case MODE_BPP16:
+		pvg->gmt = AGAGraphicsMode;
+		pvg->byBitsPerPixel = 16;
+		break;
+
+	case MODE_BPP32:
+		pvg->gmt = AGAGraphicsMode;
+		pvg->byBitsPerPixel = 32;
+		break;
+
+	case MODE_TEXT:
+		pvg->gmt = TextMode;
+		pvg->byBitsPerPixel = 0;
+		break;
+
+	case MODE_CGA:
+		break;
+
+	default:
+		pvg->byBitsPerPixel = 8;
+		break;
+	}
+}
+
+//Mode ID mapping - use ID as index to the resolution table
+static void ast_video_get_indexed_mode(struct ModeInfo *pModeInfo, struct VideoGeometry *pvg)
+{
+	u8 byModeIndex = (pModeInfo->byModeID & 0xf0);
+
+	VIDEO_DBG("Mode ID %#x\n", pModeInfo->byModeID);
+	pvg->byModeID = pModeInfo->byModeID;
+
+	if (pModeInfo->byModeID == 0x12) {
+		pvg->wScreenWidth = 640;
+		pvg->wScreenHeight = 480;
+	} else if (byModeIndex == 0x20) {
+		pvg->wScreenWidth = 640;
+		pvg->wScreenHeight = 480;
+	} else if (byModeIndex == 0x30) {
+		pvg->wScreenWidth =
+			resTable1[pModeInfo->byModeID & 0x0f].wWidth;
+		pvg->wScreenHeight =
+			resTable1[pModeInfo->byModeID & 0x0f].wHeight;
+	} else if (byModeIndex == 0x50) {
+		pvg->wScreenWidth =
+			resTable2[pModeInfo->byModeID & 0x03].wWidth;
+		pvg->wScreenHeight =
+			resTable2[pModeInfo->byModeID & 0x03].wHeight;
+	} else if (byModeIndex == 0x60) {
+		pvg->wScreenWidth = 800;
+		pvg->wScreenHeight = 600;
+	} else {
+		VIDEO_DBG("Mode ID %#x\n", pModeInfo->byModeID);
+		pvg->wScreenWidth = 0;
+		pvg->wScreenHeight = 0;
+	}
+}
+
+//check special modes
+static void ast_video_set_special_modes(struct ModeInfo *pModeInfo, struct AstRVAS *pAstRVAS)
+{
+	u8 byVGACR1 = readb((void *)(pAstRVAS->grce_reg_base + GRCE_CRTC_OFFSET + 0x1)); //number of chars per line
+	u8 byVGACR7 = readb((void *)(pAstRVAS->grce_reg_base + GRCE_CRTC_OFFSET + 0x7));
+	u8 byVGACR12 = readb((void *)(pAstRVAS->grce_reg_base + GRCE_CRTC_OFFSET + 0x12));
+	u8 byVGASR1 = readb((void *)(pAstRVAS->grce_reg_base + GRCE_SEQ_OFFSET + 0x1));
+	struct VideoGeometry *pvg = &pAstRVAS->current_vg;
+	u32 dwHorizontalDisplayEnd = 0;
+	u32 dwVerticalDisplayEnd = 0;
+
+	dwHorizontalDisplayEnd = (byVGACR1 + 1) << 3;
+	dwVerticalDisplayEnd = (((byVGACR7 & 0x40) << 3)
+		| ((byVGACR7 & 0x2) << 7) | byVGACR12) + 1;
+
+	VIDEO_DBG("byVGACR1=0x%x,byVGACR7=0x%x,byVGACR12=0x%x\n", byVGACR1,
+		byVGACR7, byVGACR12);
+	VIDEO_DBG(
+		"Mode ID %#x, dwHorizontalDisplayEnd 0x%x, dwVerticalDisplayEnd 0x%x\n",
+		pModeInfo->byModeID, dwHorizontalDisplayEnd,
+		dwVerticalDisplayEnd);
+
+	// set up special mode
+	if (VGAGraphicsMode == pvg->gmt && (pvg->byBitsPerPixel == 8)) { // mode 13
+		pvg->wScreenHeight = 200;
+		pvg->wScreenWidth = 320;
+		pvg->wStride = 320;
+	} else if (TextMode == pvg->gmt) { // text mode
+		pvg->wScreenHeight = dwVerticalDisplayEnd;
+		pvg->wScreenWidth = dwHorizontalDisplayEnd;
+
+		if (!(byVGASR1 & 0x1))
+			pvg->wScreenWidth += (byVGACR1 + 1);
+
+		pvg->wStride = pvg->wScreenWidth;
+	} else if (pvg->byBitsPerPixel == 4)
+		pvg->wStride = pvg->wScreenWidth;
+}
+
+static u32 ast_video_get_pitch(struct AstRVAS *pAstRVAS)
+{
+	u32 dwPitch = 0;
+	u8 byVGACR13 = 0;
+	u8 byVGACR14 = 0;
+	u8 byVGACR17 = 0;
+	u16 wOffsetUpper = 0;
+	u16 wOffset = 0;
+	struct VideoGeometry *pvg = &pAstRVAS->current_vg;
+
+	//read actual register
+	byVGACR13 = readb((void *)(pAstRVAS->grce_reg_base + GRCE_CRTC_OFFSET + 0x13));
+	byVGACR14 = readb((void *)(pAstRVAS->grce_reg_base + GRCE_CRTC_OFFSET + 0x14));
+	byVGACR17 = readb((void *)(pAstRVAS->grce_reg_base + GRCE_CRTC_OFFSET + 0x17));
+	wOffsetUpper = readb((void *)(pAstRVAS->grce_reg_base + 0xb0));
+
+	wOffset = (wOffsetUpper << 8) | byVGACR13;
+	VIDEO_DBG(
+		"wOffsetUpper= %#x, byVGACR13= %#x, byVGACR14= %#x, byVGACR17= %#x, wOffset= %#x\n",
+		wOffsetUpper, byVGACR13, byVGACR14, byVGACR17, wOffset);
+
+	if (byVGACR14 & 0x40)
+		dwPitch = wOffset << 3; //DW mode
+	else if (byVGACR17 & 0x40)
+		dwPitch = wOffset << 1; //byte mode
+	else
+		dwPitch = wOffset << 2; //word mode
+
+	if (pvg->gmt != TextMode) {
+		u8 byBppPowerOfTwo = 0;
+
+		if (pvg->byBitsPerPixel == 32)
+			byBppPowerOfTwo = 2;
+		else if (pvg->byBitsPerPixel == 16)
+			byBppPowerOfTwo = 1;
+		else if (pvg->byBitsPerPixel == 8)
+			byBppPowerOfTwo = 0;
+		else
+			byBppPowerOfTwo = 3;	// 4bpp
+
+		//convert it to logic width in pixel
+		if (pvg->byBitsPerPixel > 4)
+			dwPitch >>= byBppPowerOfTwo;
+		else
+			dwPitch <<= byBppPowerOfTwo;
+	}
+
+	return dwPitch;
+}
+
+void update_video_geometry(struct AstRVAS *ast_rvas)
+{
+	struct ModeInfo *pModeInfo;
+	struct NewModeInfoHeader *pNMIH;
+	struct DisplayEnd *pDE;
+	u8 byNewColorMode = 0;
+	u32 VGA_Scratch_Register_350 = 0; //VIDEO_NEW_MODE_INFO_HEADER
+	u32 VGA_Scratch_Register_354 = 0; //VIDEO_HDE
+	u32 VGA_Scratch_Register_34C = 0; //VIDEO_HDE
+	struct VideoGeometry *cur_vg = &ast_rvas->current_vg;
+
+
+	VGA_Scratch_Register_350 = ast_video_read(ast_rvas->grce_reg_base,
+			AST_VIDEO_SCRATCH_350);
+	VGA_Scratch_Register_34C = ast_video_read(ast_rvas->grce_reg_base,
+			AST_VIDEO_SCRATCH_34C);
+	VGA_Scratch_Register_354 = ast_video_read(ast_rvas->grce_reg_base,
+			AST_VIDEO_SCRATCH_354);
+
+	pModeInfo = (struct ModeInfo *) (&VGA_Scratch_Register_34C);
+	pNMIH = (struct NewModeInfoHeader *) (&VGA_Scratch_Register_350);
+	pDE = (struct DisplayEnd *) (&VGA_Scratch_Register_354);
+	VIDEO_DBG(
+			"pModeInfo: byColorMode: %#x byModeID: %#x byRefreshRateIndex: %#x byScanLines: %#x\n",
+			pModeInfo->byColorMode, pModeInfo->byModeID,
+			pModeInfo->byRefreshRateIndex, pModeInfo->byScanLines);
+	VIDEO_DBG(
+			"pNMIH: byColorDepth: %#x byDisplayInfo: %#x byMhzPixelClock: %#x byReserved: %#x\n",
+			pNMIH->byColorDepth, pNMIH->byDisplayInfo,
+			pNMIH->byMhzPixelClock, pNMIH->byReserved);
+	VIDEO_DBG("pDE: HDE: %#x VDE: %#x\n", pDE->HDE, pDE->VDE);
+
+	byNewColorMode = ((pModeInfo->byColorMode) & 0xf0) >> 4;
+	VIDEO_DBG("byNewColorMode= %#x,byModeID=0x%x\n", byNewColorMode,
+			pModeInfo->byModeID);
+	ast_video_get_color_mode(byNewColorMode, cur_vg);
+
+	if (pNMIH->byDisplayInfo == MODE_GET_INFO_DE) {
+		cur_vg->wScreenWidth = pDE->HDE;
+		cur_vg->wScreenHeight = pDE->VDE;
+		cur_vg->byBitsPerPixel = pNMIH->byColorDepth;
+		cur_vg->byModeID = pModeInfo->byModeID;
+	} else
+		ast_video_get_indexed_mode(pModeInfo, cur_vg);
+
+	cur_vg->wStride = (u16) ast_video_get_pitch(ast_rvas);
+	VIDEO_DBG("Calculated pitch in pixels= %u\n", cur_vg->wStride);
+
+	if (cur_vg->wStride < cur_vg->wScreenWidth)
+		cur_vg->wStride = cur_vg->wScreenWidth;
+
+	VIDEO_DBG(
+			"Before current display width %u, height %u, pitch %u, color depth %u, mode %d\n",
+			cur_vg->wScreenWidth, cur_vg->wScreenHeight,
+			cur_vg->wStride, cur_vg->byBitsPerPixel, cur_vg->gmt);
+
+	if ((cur_vg->gmt == TextMode)
+			|| ((cur_vg->gmt == VGAGraphicsMode)
+					&& (pModeInfo->byModeID == 0x13))) {
+		ast_video_set_special_modes(pModeInfo, ast_rvas);
+	}
+
+	//mode transition
+	if (cur_vg->wScreenHeight < 200 || cur_vg->wScreenWidth < 320)
+		cur_vg->gmt = InvalidMode;
+
+	if (cur_vg->gmt == TextMode) {
+		u8 byVGACR9 = readb((void *)(ast_rvas->grce_reg_base + GRCE_CRTC_OFFSET + 0x9));
+		u32 dwCharacterHeight = ((byVGACR9) & 0x1f) + 1;
+
+		VIDEO_DBG("byModeID=0x%x,dwCharacterHeight=%d\n",
+				cur_vg->byModeID, dwCharacterHeight);
+
+		if ((dwCharacterHeight != 8) && (dwCharacterHeight != 14)
+				&& (dwCharacterHeight != 16))
+			cur_vg->gmt = InvalidMode;
+
+		if ((cur_vg->wScreenWidth > 720)
+				|| cur_vg->wScreenHeight > 400)
+			cur_vg->gmt = InvalidMode;
+	}
+
+	VIDEO_DBG(
+			"current display width %u, height %u, pitch %u, color depth %u, mode %d\n",
+			cur_vg->wScreenWidth, cur_vg->wScreenHeight,
+			cur_vg->wStride, cur_vg->byBitsPerPixel, cur_vg->gmt);
+
+}
+//
+//check and update current video geometry
+//
+bool video_geometry_change(struct AstRVAS *ast_rvas, u32 dwGRCEStatus)
+{
+	bool b_geometry_changed = false;
+	struct VideoGeometry *cur_vg = &ast_rvas->current_vg;
+	struct VideoGeometry pre_vg;
+
+	memcpy(&pre_vg, cur_vg, sizeof(pre_vg));
+	update_video_geometry(ast_rvas);
+	b_geometry_changed = memcmp(&pre_vg, cur_vg, sizeof(struct VideoGeometry))
+			!= 0;
+	VIDEO_DBG("b_geometry_changed: %d\n", b_geometry_changed);
+	return b_geometry_changed;
+}
+
+void ioctl_get_video_geometry(struct RvasIoctl *ri, struct AstRVAS *ast_rvas)
+{
+	memcpy(&ri->vg, &ast_rvas->current_vg, sizeof(struct VideoGeometry));
+//	VIDEO_DBG("b_geometry_changed: %d\n", b_geometry_changed);
+}
+
+void print_frame_buffer(u32 dwSizeByBytes, struct VGAMemInfo FBInfo)
+{
+	u32 iter = 0;
+	u32 *pdwFrameBufferAddrBase = NULL;
+	u32 dwNumMappedPages = 0;
+
+	dwNumMappedPages = ((dwSizeByBytes + 4095) >> 12);
+	pdwFrameBufferAddrBase = (u32 *) ioremap(FBInfo.dwFBPhysStart, dwNumMappedPages << 12);
+
+	if (pdwFrameBufferAddrBase) {
+		VIDEO_DBG("==============%s===========\n", __func__);
+
+		for (iter = 0; iter < (dwSizeByBytes >> 2); iter++) {
+			VIDEO_DBG("0x%x, ", pdwFrameBufferAddrBase[iter]);
+
+			if ((iter % 16) == 0)
+				VIDEO_DBG("\n");
+		}
+
+		VIDEO_DBG("===========END=============\n");
+		iounmap((void *) pdwFrameBufferAddrBase);
+	}
+}
+
+void ioctl_get_grc_register(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	u32 virt_add = 0;
+	u32 size = 0;
+
+	VIDEO_DBG("Start\n");
+	virt_add = (u32)get_virt_add_rsvd_mem((u32) ri->rmh, pAstRVAS);
+	size = ri->rmh1_mem_size;
+
+	if (virt_is_valid_rsvd_mem((u32) ri->rmh, size, pAstRVAS)) {
+		memcpy((void *) virt_add,
+			(const void *) (pAstRVAS->grce_reg_base), 0x40);
+		memset((void *) (((u8 *) virt_add) + 0x40), 0x0, 0x20);
+		memcpy((void *) (((u8 *) virt_add) + 0x60),
+			(const void *) (pAstRVAS->grce_reg_base + 0x60),
+			GRCE_SIZE - 0x60);
+		ri->rs = SuccessStatus;
+	} else
+		ri->rs = InvalidMemoryHandle;
+}
+
+void ioctl_read_snoop_map(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+
+	struct ContextTable *pct = get_context_entry(ri->rc, pAstRVAS);
+	u32 virt_add = 0;
+	u32 size = 0;
+
+	virt_add = (u32)get_virt_add_rsvd_mem((u32) ri->rmh, pAstRVAS);
+	size = ri->rmh_mem_size;
+
+	disable_grce_tse_interrupt(pAstRVAS);
+	VIDEO_DBG("Start\n");
+
+	if (pct) {
+		if (virt_is_valid_rsvd_mem((u32) ri->rmh, size, pAstRVAS)) {
+			update_all_snoop_context(pAstRVAS);
+			memcpy((void *) virt_add, pct->aqwSnoopMap,
+				sizeof(pct->aqwSnoopMap));
+
+			if (ri->flag) {
+				///get the context snoop address
+				memset(pct->aqwSnoopMap, 0x00,
+					sizeof(pct->aqwSnoopMap));
+				memset(&(pct->sa), 0x00, sizeof(pct->sa));
+			}
+			ri->rs = SuccessStatus;
+		} else
+			ri->rs = InvalidMemoryHandle;
+	} else
+		ri->rs = InvalidContextHandle;
+
+	enable_grce_tse_interrupt(pAstRVAS);
+}
+
+void ioctl_read_snoop_aggregate(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	struct ContextTable *pct = get_context_entry(ri->rc, pAstRVAS);
+
+	disable_grce_tse_interrupt(pAstRVAS);
+
+	if (pct) {
+		update_all_snoop_context(pAstRVAS);
+		memcpy(&(ri->sa), &(pct->sa), sizeof(pct->sa));
+		VIDEO_DBG("ri->sa.qwCol: %#llx qwRow: %#llx flag: %u\n",
+			ri->sa.qwCol, ri->sa.qwRow, ri->flag);
+
+		if (ri->flag)
+			memset(&(pct->sa), 0x00, sizeof(pct->sa));
+
+		ri->rs = SuccessStatus;
+	} else {
+		ri->rs = InvalidContextHandle;
+		VIDEO_DBG("Invalid Context\n");
+	}
+
+	enable_grce_tse_interrupt(pAstRVAS);
+}
+
+void ioctl_set_tse_tsicr(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	u32 addrTSICR;
+
+	pAstRVAS->tse_tsicr = ri->tse_counter;
+	addrTSICR = pAstRVAS->fg_reg_base + TSE_TileSnoop_Interrupt_Count;
+	writel(pAstRVAS->tse_tsicr, (void *)addrTSICR);// max wait time before interrupt
+	ri->rs = SuccessStatus;
+}
+
+
+void ioctl_get_tse_tsicr(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	ri->tse_counter = pAstRVAS->tse_tsicr;
+	ri->rs = SuccessStatus;
+}
+
+// Get the screen offset from the GRC registers
+u32 get_screen_offset(struct AstRVAS *pAstRVAS)
+{
+	u32 dwScreenOffset = 0;
+	u32 addrVGACRC = pAstRVAS->grce_reg_base + GRCE_CRTC + 0xC; // Ch
+	u32 addrVGACRD = pAstRVAS->grce_reg_base + GRCE_CRTC + 0xD; // Dh
+	u32 addrVGACRAF = pAstRVAS->grce_reg_base + GRCE_CRTCEXT + 0x2F;
+
+	if (pAstRVAS->current_vg.gmt == AGAGraphicsMode) {
+		dwScreenOffset = ((readb((void *)addrVGACRAF)) << 16) | ((readb((void *)addrVGACRC)) << 8) |
+				(readb((void *)addrVGACRD));
+		dwScreenOffset *= pAstRVAS->current_vg.byBitsPerPixel >> 3;
+	}
+
+	VIDEO_DBG("ScreenOffset: %#8.8x\n", dwScreenOffset);
+
+	return dwScreenOffset;
+}
+
+void reset_snoop_engine(struct AstRVAS *pAstRVAS)
+{
+	u32 addr_snoop = pAstRVAS->fg_reg_base + TSE_SnoopMap_Offset;
+	u32 reg_value = 0;
+	u32 iter;
+
+	writel(0x0, (void *)(pAstRVAS->fg_reg_base + TSE_SnoopCommand_Register_Offset));
+	writel(0x3, (void *)(pAstRVAS->fg_reg_base + TSE_Status_Register_Offset));
+	reg_value = readl((void *)(pAstRVAS->fg_reg_base + TSE_Status_Register_Offset));
+	reg_value = readl((void *)(pAstRVAS->fg_reg_base + TSE_CS0Reg));
+	reg_value = readl((void *)(pAstRVAS->fg_reg_base + TSE_CS1Reg));
+	reg_value = readl((void *)(pAstRVAS->fg_reg_base + TSE_RS0Reg));
+	reg_value = readl((void *)(pAstRVAS->fg_reg_base + TSE_RS1Reg));
+
+	//Clear TSRR00 to TSRR126 (TSRR01 to TSRR127), Snoop Map
+	for (iter = 0; iter < 0x80; ++iter) {
+		reg_value = readl((void *)addr_snoop)+1;
+		writel(reg_value, (void *)addr_snoop);
+	}
+
+	reg_value = readl((void *)(pAstRVAS->fg_reg_base + TSE_TileCount_Register_Offset));
+}
+
+void set_snoop_engine(bool b_geom_chg, struct AstRVAS *pAstRVAS)
+{
+	u32 tscmd_reg = pAstRVAS->fg_reg_base + TSE_SnoopCommand_Register_Offset;
+	u32 tsfbsa_reg = pAstRVAS->fg_reg_base + TSE_FrameBuffer_Offset;
+	u32 tsulr_reg = pAstRVAS->fg_reg_base + TSE_UpperLimit_Offset;
+	u32 new_tsfbsa = 0;
+	u32 tscmd = 0;
+	u8 byBytesPerPixel = 0x0;
+	u8 byTSCMDBytesPerPixel = 0x0;
+	int cContext;
+	u32 dwStride;
+	struct ContextTable **ppctContextTable = pAstRVAS->ppctContextTable;
+
+	// Calculate Start Address into the Frame Buffer
+	new_tsfbsa = get_screen_offset(pAstRVAS);
+	tscmd = readl((void *)(pAstRVAS->fg_reg_base + TSE_SnoopCommand_Register_Offset));
+
+	tscmd &= (1<<TSCMD_INT_ENBL_BIT);
+
+	VIDEO_DBG("Latest TSFBSA: %#8.8x\n", new_tsfbsa);
+//	VIDEO_DBG(
+//	        "ri->vg: bpp: %u Mode: %#x gmt: %d Width: %u Height: %u Stride: %u geom_chg: %d\n",
+//	        ri->vg.byBitsPerPixel, ri->vg.byModeID, ri->vg.gmt,
+//	        ri->vg.wScreenWidth, ri->vg.wScreenHeight, ri->vg.wStride,
+//	        b_geom_chg);
+	VIDEO_DBG(
+		"pAstRVAS->current_vg: bpp %u Mode:%#x gmt:%d Width:%u Height:%u Stride:%u\n",
+		pAstRVAS->current_vg.byBitsPerPixel,
+		pAstRVAS->current_vg.byModeID, pAstRVAS->current_vg.gmt,
+		pAstRVAS->current_vg.wScreenWidth,
+		pAstRVAS->current_vg.wScreenHeight,
+		pAstRVAS->current_vg.wStride);
+
+	if (b_geom_chg || (readl((void *)tsfbsa_reg) != new_tsfbsa)) {
+		byBytesPerPixel = pAstRVAS->current_vg.byBitsPerPixel >> 3;
+
+		if ((pAstRVAS->current_vg.gmt == VGAGraphicsMode)
+			|| (pAstRVAS->current_vg.byBitsPerPixel == 4))
+			byTSCMDBytesPerPixel = 0;
+		else {
+			switch (byBytesPerPixel) {
+			case 1:
+				byTSCMDBytesPerPixel = 0;
+				break;
+
+			case 2:
+				byTSCMDBytesPerPixel = 1;
+				break;
+
+			case 3:
+			case 4:
+				byTSCMDBytesPerPixel = 2;
+				break;
+			}
+		}
+		dwStride = pAstRVAS->current_vg.wStride;
+
+		if (byBytesPerPixel == 3)
+			dwStride = (dwStride + dwStride + dwStride) >> 2;
+		else if (pAstRVAS->current_vg.byBitsPerPixel == 4)
+			dwStride >>= 1;
+
+		// set TSE SCR
+		// start the tile snoop engine
+		// flip the 15 bit
+		if (!(readl((void *)tscmd_reg) & TSCMD_SCREEN_OWNER))
+			tscmd |= TSCMD_SCREEN_OWNER;
+
+		tscmd |= (dwStride << TSCMD_PITCH_BIT) | (1 << TSCMD_CPT_BIT)
+			| (1 << TSCMD_RPT_BIT)
+			| (byTSCMDBytesPerPixel << TSCMD_BPP_BIT)
+			| (1 << TSCMD_VGA_MODE_BIT) | (1 << TSCMD_TSE_ENBL_BIT);
+		VIDEO_DBG("tscmd: %#8.8x\n", tscmd);
+		// set the TSFBSA & TSULR
+		writel(new_tsfbsa, (void *)tsfbsa_reg);
+		writel(BSE_UPPER_LIMIT, (void *)tsulr_reg);
+		writel(tscmd, (void *)tscmd_reg);
+		//reset snoop information
+		get_snoop_map_data(pAstRVAS);
+		memset((void *) pAstRVAS->accrued_sm, 0,
+			sizeof(pAstRVAS->accrued_sm));
+		memset((void *) &pAstRVAS->accrued_sa, 0,
+			sizeof(pAstRVAS->accrued_sa));
+
+		for (cContext = 0; cContext < MAX_NUM_CONTEXT; cContext++) {
+			if (ppctContextTable[cContext]) {
+				memset(ppctContextTable[cContext]->aqwSnoopMap,
+					0,
+					sizeof(ppctContextTable[cContext]->aqwSnoopMap));
+				memset(&(ppctContextTable[cContext]->sa), 0,
+					sizeof(ppctContextTable[cContext]->sa));
+			}
+		}      // for each context
+	} // if
+}
+
+//
+// ReadSnoopMap to Clear
+//
+void get_snoop_map_data(struct AstRVAS *pAstRVAS)
+{
+	u32 dwSMDword;
+	u64 aqwSnoopMap[SNOOP_MAP_QWORD_COUNT];
+	//u32 dw_iter;
+
+	get_snoop_aggregate(pAstRVAS);
+	memcpy((void *) aqwSnoopMap,
+		(const void *) (pAstRVAS->fg_reg_base + TSE_SnoopMap_Offset),
+		sizeof(aqwSnoopMap));
+
+	//VIDEO_DBG("Snoop Map:\n");
+	//VIDEO_DBG("==========\n");
+
+	//for (dw_iter = 0; dw_iter < SNOOP_MAP_QWORD_COUNT; ++dw_iter)
+		//VIDEO_DBG("[%2u]: 0x%16.16llx\n", dw_iter, aqwSnoopMap[dw_iter]);
+
+
+	//VIDEO_DBG("==========\n\n");
+
+	// copy 512 snoop map
+	for (dwSMDword = 0; dwSMDword < SNOOP_MAP_QWORD_COUNT; ++dwSMDword)
+		pAstRVAS->accrued_sm[dwSMDword] |= aqwSnoopMap[dwSMDword];
+}
+
+void get_snoop_aggregate(struct AstRVAS *pAstRVAS)
+{
+	u64 qwRow = 0;
+	u64 qwCol = 0;
+
+	// copy the snoop aggregate,row 64 bits
+	qwRow = readl((void *)(pAstRVAS->fg_reg_base + TSE_RS1Reg));
+	qwRow = qwRow << 32;
+	qwRow |= readl((void *)(pAstRVAS->fg_reg_base + TSE_RS0Reg));
+
+	// column
+	qwCol = readl((void *)(pAstRVAS->fg_reg_base + TSE_CS1Reg));
+	qwCol = qwCol << 32;
+	qwCol |= readl((void *)(pAstRVAS->fg_reg_base + TSE_CS0Reg));
+
+	VIDEO_DBG("Snoop Aggregate Row: 0x%16.16llx\n", qwRow);
+	VIDEO_DBG("Snoop Aggregate Col: 0x%16.16llx\n", qwCol);
+	VIDEO_DBG("DRIVER:: %s\n", __func__);
+	VIDEO_DBG("DRIVER:: row [%#llx]\n", qwRow);
+	VIDEO_DBG("DRIVER:: col [%#llx]\n", qwCol);
+
+	pAstRVAS->accrued_sa.qwCol |= qwCol;
+	pAstRVAS->accrued_sa.qwRow |= qwRow;
+}
+
+//
+//
+//
+u64 reinterpret_32bpp_snoop_row_as_24bpp(u64 theSnoopRow)
+{
+	u64 qwResult = 0;
+	u64 qwSourceBit = 1;
+	u32 cSourceBit;
+	u64 qwBitResult = 0;
+
+	for (cSourceBit = 0; cSourceBit < 64; ++cSourceBit) {
+		if (theSnoopRow & qwSourceBit) {
+			qwBitResult = ((cSourceBit * 128) / 96);
+			qwResult |= (((u64) 3) << qwBitResult);
+		}
+
+		qwSourceBit <<= 1;
+	}
+
+	return qwResult;
+}
+
+//
+//one tile: 32x32,
+//
+void convert_snoop_map(struct AstRVAS *pAstRVAS)
+{
+	u32 dwAllRows = (pAstRVAS->current_vg.wScreenHeight + 31) >> 5;
+	u32 cRow;
+
+	for (cRow = 0; cRow < dwAllRows; ++cRow)
+		pAstRVAS->accrued_sm[cRow] =
+			reinterpret_32bpp_snoop_row_as_24bpp(
+				pAstRVAS->accrued_sm[cRow]);
+
+	pAstRVAS->accrued_sa.qwCol = reinterpret_32bpp_snoop_row_as_24bpp(
+		pAstRVAS->accrued_sa.qwCol);
+}
+
+//
+//
+//
+void update_all_snoop_context(struct AstRVAS *pAstRVAS)
+{
+	u32 cContext;
+	u32 iSMDword;
+	struct ContextTable **ppctContextTable = pAstRVAS->ppctContextTable;
+
+	if (pAstRVAS->current_vg.byBitsPerPixel == 24)
+		convert_snoop_map(pAstRVAS);
+
+	for (cContext = 0; cContext < MAX_NUM_CONTEXT; cContext++)
+		if (ppctContextTable[cContext]) {
+			for (iSMDword = 0; iSMDword < SNOOP_MAP_QWORD_COUNT;
+				iSMDword++)
+				ppctContextTable[cContext]->aqwSnoopMap[iSMDword] |=
+					pAstRVAS->accrued_sm[iSMDword];
+
+			ppctContextTable[cContext]->sa.qwRow |=
+				pAstRVAS->accrued_sa.qwRow;
+			ppctContextTable[cContext]->sa.qwCol |=
+				pAstRVAS->accrued_sa.qwCol;
+		}
+
+	//reset snoop map and aggregate
+	memset((void *) pAstRVAS->accrued_sm, 0, sizeof(pAstRVAS->accrued_sm));
+	memset((void *) &pAstRVAS->accrued_sa, 0x00,
+		sizeof(pAstRVAS->accrued_sa));
+}
+
+static u32 setup_tfe_cr(struct FetchOperation *pfo)
+{
+	u32 dwTFECR = 0;
+
+	if (pfo->bEnableRLE)
+		dwTFECR = (pfo->byRLETripletCode << 24)
+			| (pfo->byRLERepeatCode << 16);
+
+	dwTFECR &= TFCTL_DESCRIPTOR_IN_DDR_MASK;
+	dwTFECR |= 1;
+	dwTFECR |= 1 << 1; // enabled IRQ
+	VIDEO_DBG("dwTFECR: %#x\n", dwTFECR);
+	return dwTFECR;
+}
+
+static void start_skip_mode_skip(struct Descriptor *pDescriptorVirtualAddr,
+	u32 dwDescPhysicalAddr, u32 dwSourceAddr, u32 dwDestAddr, u16 wStride,
+	u8 bytesPerPixel, u32 dwFetchWidthPixels, u32 dwFetchHeight,
+	bool bRLEOverFLow)
+{
+
+	struct Descriptor *pVirtDesc = pDescriptorVirtualAddr;
+
+	// Fetch Skipping data to a temp buffer
+	prepare_tfe_descriptor(pVirtDesc, dwSourceAddr, dwDestAddr, true, 1,
+		false, wStride, bytesPerPixel, dwFetchWidthPixels, dwFetchHeight,
+		LowByteMode, bRLEOverFLow, 0);
+
+	dwDestAddr += dwFetchWidthPixels * dwFetchHeight;
+	pVirtDesc++;
+
+	if (bytesPerPixel == 3 || bytesPerPixel == 4) {
+		prepare_tfe_descriptor(pVirtDesc, dwSourceAddr, dwDestAddr,
+			true, 1, false, wStride, bytesPerPixel,
+			dwFetchWidthPixels, dwFetchHeight, MiddleByteMode,
+			bRLEOverFLow, 0);
+
+		dwDestAddr += dwFetchWidthPixels * dwFetchHeight;
+		pVirtDesc++;
+	}
+
+	prepare_tfe_descriptor(pVirtDesc, dwSourceAddr, dwDestAddr, false, 1,
+		false, wStride, bytesPerPixel, dwFetchWidthPixels, dwFetchHeight,
+		TopByteMode, bRLEOverFLow, 1);
+}
+
+// calculate pure fetch size
+static u32 calculate_fetch_size(enum SelectedByteMode sbm, u8 bytesPerPixel,
+	u32 dwFetchWidthPixels, u32 dwFetchHeight)
+{
+	u32 dwFetchSize = 0;
+
+	switch (sbm) {
+	case AllBytesMode:
+		dwFetchSize = dwFetchWidthPixels * dwFetchHeight
+			* bytesPerPixel;
+		break;
+
+	case SkipMode:
+		if (bytesPerPixel == 3 || bytesPerPixel == 4)
+			dwFetchSize = dwFetchWidthPixels * dwFetchHeight * 3;
+		else
+			dwFetchSize = dwFetchWidthPixels * dwFetchHeight
+				* bytesPerPixel;
+		break;
+
+	case PlanarToPackedMode:
+		dwFetchSize = (dwFetchWidthPixels * dwFetchHeight);
+		break;
+
+	case PackedToPackedMode:
+		break;
+
+	default:
+		VIDEO_DBG("Mode= %d is not supported\n", sbm);
+		break;
+	} //switch
+	return dwFetchSize;
+}
+
+static void display_fetch_info(struct FetchVideoTilesArg *pFVTDescriptor, u32 dwCD)
+{
+	struct FetchRegion *pfr = NULL;
+
+	pfr = &(pFVTDescriptor->pfo[dwCD].fr);
+	VIDEO_DBG("FETCH - 1 dwCD: %u\n", dwCD);
+	VIDEO_DBG("pfr->wLeftX :%d\n", pfr->wLeftX);
+	VIDEO_DBG("pfr->wTopY :%d\n", pfr->wTopY);
+	VIDEO_DBG("pfr->wRightX :%d\n", pfr->wRightX);
+	VIDEO_DBG("pfr->wBottomY :%d\n", pfr->wBottomY);
+	VIDEO_DBG(" bEanbleRLE %d\n", pFVTDescriptor->pfo[dwCD].bEnableRLE);
+	VIDEO_DBG("Stride : %d\n", pFVTDescriptor->vg.wStride);
+}
+
+
+void ioctl_fetch_video_tiles(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	struct FetchVideoTilesArg *pFVTDescriptor;
+	u32 dwCD = 0;
+	struct Descriptor *pDescriptorVirtualAddr;
+	u32 dwDescPhysicalAddr;
+	u32 dwSourcePhyAddr;
+	u32 dwDestinationPhyAddr;
+	u8 bytesPerPixel;
+	struct FetchRegion *pfr;
+	bool bNotLastEntry = false;
+	u32 dwTFECR = 0;
+	u32 dwTotalFetchSize = 0;
+	u32 dwRLESize = 0;
+	bool bRLEOverFLow = false;
+	u32 dwFetchWidthPixels = 0;
+	u32 dwFetchHeight = 0;
+	u32 arg_phys = 0;
+	u32 data_phys_out = 0;
+	u32 data_phys_temp = 0;
+	u16 stride = 0;
+	bool bSkippingMode = false;
+	void *desc_virt = NULL;
+	u32 desc_phy = 0;
+	struct ContextTable *ctx_entry = NULL;
+
+	VIDEO_DBG("DRIVER:::: TILE FETCH CHAINING\n");
+	ctx_entry = get_context_entry(ri->rc, pAstRVAS);
+
+	if (ctx_entry) {
+		desc_virt = ctx_entry->desc_virt;
+		desc_phy = ctx_entry->desc_phy;
+	} else {
+		VIDEO_DBG("Returning with invalid Context handle: 0x%p\n", ri->rc);
+		ri->rs = InvalidContextHandle;
+		return;
+	}
+
+	ri->rs = SuccessStatus;
+	//struct FetchVideoTilesArg buffer
+	arg_phys = get_phys_add_rsvd_mem((u32) ri->rmh, pAstRVAS);
+	//Fetch final dest buffer
+	data_phys_out = get_phys_add_rsvd_mem((u32) ri->rmh1, pAstRVAS);
+	//Intermediate Buffer
+	data_phys_temp = get_phys_add_rsvd_mem((u32) ri->rmh2, pAstRVAS);
+
+	dwDestinationPhyAddr = data_phys_out;
+	pFVTDescriptor = (struct FetchVideoTilesArg *) get_virt_add_rsvd_mem((u32)ri->rmh, pAstRVAS);
+	VIDEO_DBG("Destination virtual Add: 0x%p\n", get_virt_add_rsvd_mem((u32)ri->rmh, pAstRVAS));
+	VIDEO_DBG("Destination Physical Add: %#x\n", dwDestinationPhyAddr);
+	memset(desc_virt, 0x00, PAGE_SIZE);
+
+	if (arg_phys && data_phys_out && data_phys_temp) {
+		pDescriptorVirtualAddr = (struct Descriptor *) desc_virt;
+		dwDescPhysicalAddr = desc_phy;
+		VIDEO_DBG("Descriptor Virtual Addr: %#x\n",
+			(u32)pDescriptorVirtualAddr);
+		VIDEO_DBG("Descriptor Physical Addr: %#x\n", dwDescPhysicalAddr);
+		stride = pFVTDescriptor->vg.wStride;
+
+		if (pFVTDescriptor->vg.byBitsPerPixel == 4) {
+			bytesPerPixel = 1;
+			stride >>= 1;
+		} else
+			bytesPerPixel = pFVTDescriptor->vg.byBitsPerPixel >> 3;
+
+		VIDEO_DBG("u8 per pixel:%u\n", bytesPerPixel);
+		// fetch all data to Destination 1 without RLE
+		VIDEO_DBG("FETCH - 0\n");
+		VIDEO_DBG("COUNT OF Operation: %u\n", pFVTDescriptor->cfo);
+
+		for (dwCD = 0; dwCD < pFVTDescriptor->cfo; dwCD++) {
+			display_fetch_info(pFVTDescriptor, dwCD);
+			// Set up Control Register.
+			dwTFECR = setup_tfe_cr(&pFVTDescriptor->pfo[dwCD]);
+			pfr = &(pFVTDescriptor->pfo[dwCD].fr);
+			// find Source Address
+			if (pFVTDescriptor->vg.byBitsPerPixel == 4) {
+				dwSourcePhyAddr = get_phy_fb_start_address(pAstRVAS)
+								+ ((pfr->wLeftX * bytesPerPixel)>>1)
+								+ pfr->wTopY * stride
+								* bytesPerPixel;
+
+				dwFetchWidthPixels = (pfr->wRightX - pfr->wLeftX + 1)>>1;
+			} else {
+				dwSourcePhyAddr = get_phy_fb_start_address(pAstRVAS)
+						+ pfr->wLeftX * bytesPerPixel
+						+ pfr->wTopY * stride
+						* bytesPerPixel;
+
+				dwFetchWidthPixels = (pfr->wRightX - pfr->wLeftX + 1);
+			}
+			VIDEO_DBG("dwCD: %u dwSourcePhyAddr: %#x\n", dwCD,
+				dwSourcePhyAddr);
+			dwFetchHeight = pfr->wBottomY - pfr->wTopY + 1;
+
+			VIDEO_DBG("DESCRIPTOR virtual ADDRESS: 0x%p\n",
+				pDescriptorVirtualAddr);
+			if (pFVTDescriptor->vg.byBitsPerPixel == 4)
+				pFVTDescriptor->pfo[dwCD].sbm =
+					PlanarToPackedMode;
+
+			pFVTDescriptor->pfo[dwCD].dwFetchSize =
+				calculate_fetch_size(
+					pFVTDescriptor->pfo[dwCD].sbm,
+					bytesPerPixel, dwFetchWidthPixels,
+					dwFetchHeight);
+			bSkippingMode =
+				(pFVTDescriptor->pfo[dwCD].sbm == SkipMode) ?
+				true : false;
+
+			if (bSkippingMode && bytesPerPixel > 1) {
+				u32 skipSrcAddr = dwSourcePhyAddr;
+				u32 skipDestAddr = dwDestinationPhyAddr;
+				u8 byPostBytesPerPixel =
+					(bytesPerPixel == 2) ? 2 : 3;
+				VIDEO_DBG("In SkippingMode...\n");
+
+				if (pFVTDescriptor->pfo[dwCD].bEnableRLE) {
+					//skip data to intermediate buffer
+					skipDestAddr = data_phys_temp;
+				}
+
+				start_skip_mode_skip(pDescriptorVirtualAddr,
+					dwDescPhysicalAddr, skipSrcAddr,
+					skipDestAddr,
+					pFVTDescriptor->vg.wStride,
+					bytesPerPixel, dwFetchWidthPixels,
+					dwFetchHeight, bRLEOverFLow);
+
+				if (pFVTDescriptor->pfo[dwCD].bEnableRLE) {
+					u32 rleSrcAddr = skipDestAddr;
+					u32 rleDesAddr = dwDestinationPhyAddr;
+
+					///// take second look at skip mode for using map single
+					if (sleep_on_tfe_busy(pAstRVAS,
+						dwDescPhysicalAddr, // Descriptor physical Address
+						dwTFECR, // control register value
+						pFVTDescriptor->pfo[dwCD].dwFetchSize, // bandwidth limitor value
+						&dwRLESize,    // out:: rle size
+						&pFVTDescriptor->pfo[dwCD].dwCheckSum
+						) == false) { // out:: cs size
+						ri->rs = GenericError;
+						return;
+					}
+
+					// perform RLE from Temp buffer to dwDestinationPhyAddr
+					//VIDEO_DBG("skip rle\n");
+					prepare_tfe_descriptor(
+						pDescriptorVirtualAddr,
+						rleSrcAddr, rleDesAddr,
+						bNotLastEntry, 1,
+						pFVTDescriptor->pfo[dwCD].bEnableRLE,
+						dwFetchWidthPixels,
+						byPostBytesPerPixel,
+						dwFetchWidthPixels,
+						dwFetchHeight, AllBytesMode,
+						bRLEOverFLow, 1);
+				}
+			} else {
+				VIDEO_DBG(
+					"Preparing TFE Descriptor with no skipping...\n");
+				prepare_tfe_descriptor(pDescriptorVirtualAddr,
+					dwSourcePhyAddr, dwDestinationPhyAddr,
+					bNotLastEntry, 1,
+					pFVTDescriptor->pfo[dwCD].bEnableRLE,
+					stride, bytesPerPixel,
+					dwFetchWidthPixels, dwFetchHeight,
+					pFVTDescriptor->pfo[dwCD].sbm,
+					bRLEOverFLow, 1);
+				VIDEO_DBG(
+					"Successfully prepared TFE Descriptor with no skipping\n");
+			}
+			VIDEO_DBG("Sleeping while TFE is busy...\n");
+
+			if (sleep_on_tfe_busy(pAstRVAS, dwDescPhysicalAddr, // Descriptor physical Address
+				dwTFECR,               // control register value
+				pFVTDescriptor->pfo[dwCD].dwFetchSize, // bandwidth limitor value
+				&dwRLESize,                    // out:: rle size
+				&pFVTDescriptor->pfo[dwCD].dwCheckSum
+				) == false) {  // out:: cs size
+				ri->rs = GenericError;
+				return;
+			}
+
+			VIDEO_DBG("After sleep where TFE was busy\n");
+
+			//VIDEO_DBG("skip rle end\n");
+			if (!pFVTDescriptor->pfo[dwCD].bEnableRLE) { // RLE not enabled
+				VIDEO_DBG("RLE is off\n");
+				pFVTDescriptor->pfo[dwCD].bRLEFailed = false;
+				dwRLESize =
+					pFVTDescriptor->pfo[dwCD].dwFetchSize;
+				dwTotalFetchSize +=
+					pFVTDescriptor->pfo[dwCD].dwFetchSize;
+			} else { // RLE enabled
+				VIDEO_DBG("RLE Enabled\n");
+				if (dwRLESize
+					>= pFVTDescriptor->pfo[dwCD].dwFetchSize) { // FAILED
+					VIDEO_DBG(
+						"DRVIER:: RLE failed RLE: %u > %u\n",
+						dwRLESize,
+						pFVTDescriptor->pfo[dwCD].dwFetchSize);
+					pFVTDescriptor->pfo[dwCD].bRLEFailed =
+						true;
+
+					if (bSkippingMode) {
+						u32 skipSrcAddr =
+							dwSourcePhyAddr;
+						u32 skipDestAddr =
+							dwDestinationPhyAddr;
+
+						start_skip_mode_skip(
+							pDescriptorVirtualAddr,
+							dwDescPhysicalAddr,
+							skipSrcAddr,
+							skipDestAddr,
+							pFVTDescriptor->vg.wStride,
+							bytesPerPixel,
+							dwFetchWidthPixels,
+							dwFetchHeight,
+							bRLEOverFLow);
+					} else {
+						VIDEO_DBG(" FETCH - 4\n");
+						prepare_tfe_descriptor(
+							pDescriptorVirtualAddr,
+							dwSourcePhyAddr,
+							dwDestinationPhyAddr,
+							bNotLastEntry, 1, false,
+							pFVTDescriptor->vg.wStride,
+							bytesPerPixel,
+							dwFetchWidthPixels,
+							dwFetchHeight,
+							pFVTDescriptor->pfo[dwCD].sbm,
+							bRLEOverFLow, 1);
+					}
+
+					if (sleep_on_tfe_busy(pAstRVAS,
+						dwDescPhysicalAddr, // Descriptor physical Address
+						dwTFECR, // control register value
+						pFVTDescriptor->pfo[dwCD].dwFetchSize, // bandwidth limitor value
+						&dwRLESize,    // out:: rle size
+						&pFVTDescriptor->pfo[dwCD].dwCheckSum
+						) == false) {  // out:: cs size
+						ri->rs = GenericError;
+						return;
+					}
+
+					dwTotalFetchSize +=
+						pFVTDescriptor->pfo[dwCD].dwFetchSize;
+					dwRLESize =
+						pFVTDescriptor->pfo[dwCD].dwFetchSize;
+				}  // RLE Failed
+				else { //RLE successful
+					pFVTDescriptor->pfo[dwCD].bRLEFailed =
+						false;
+					dwTotalFetchSize += dwRLESize;
+					dwTotalFetchSize = (dwTotalFetchSize
+						+ 0x3) & 0xfffffffc;
+				}
+			} //RLE Enabled
+
+			pFVTDescriptor->pfo[dwCD].dwFetchRLESize = dwRLESize;
+			VIDEO_DBG("DRIVER:: RLE: %u, nonRLE: %u\n", dwRLESize,
+				pFVTDescriptor->pfo[dwCD].dwFetchSize);
+			VIDEO_DBG("FETCH:: loop FETCH size: %u\n", dwTotalFetchSize);
+			dwDestinationPhyAddr = data_phys_out + dwTotalFetchSize;
+		} //for TFE
+
+		pFVTDescriptor->dwTotalOutputSize = dwTotalFetchSize;
+		VIDEO_DBG("Fetch Size: %#x\n", dwTotalFetchSize);
+	} else {
+		dev_err(pAstRVAS->pdev, "Memory allocation failure\n");
+		ri->rs = InvalidMemoryHandle;
+	}
+} // End - ioctl_fetch_video_tiles
+
+void prepare_ldma_descriptor(struct Descriptor *pDAddress, u32 dwSourceAddress,
+	u32 dwDestAddress, u32 dwLDMASize, u8 byNotLastEntry)
+{
+	u8 byInterrupt = 0;
+
+	VIDEO_DBG("pDAddress: 0x%p\n", pDAddress);
+
+	// initialize to 0
+	pDAddress->dw0General = 0x00;
+	pDAddress->dw1FetchWidthLine = 0x00;
+	pDAddress->dw2SourceAddr = 0x00;
+	pDAddress->dw3DestinationAddr = 0x00;
+
+	// initialize to 0
+	if (!byNotLastEntry)
+		byInterrupt = 0x1;
+
+	pDAddress->dw0General = ((dwLDMASize - 1) << 8) | (byNotLastEntry << 1)
+		| byInterrupt;
+	pDAddress->dw2SourceAddr = dwSourceAddress;
+	pDAddress->dw3DestinationAddr = dwDestAddress;
+
+	VIDEO_DBG("u32 0: 0x%x\n", pDAddress->dw0General);
+	VIDEO_DBG("u32 1: 0x%x\n", pDAddress->dw1FetchWidthLine);
+	VIDEO_DBG("u32 2: 0x%x\n", pDAddress->dw2SourceAddr);
+	VIDEO_DBG("u32 3: 0x%x\n", pDAddress->dw3DestinationAddr);
+}
+
+//
+// ioctl_run_length_encode_data - encode buffer data
+//
+void ioctl_run_length_encode_data(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	struct Descriptor *pDescriptorAdd = NULL;
+	struct Descriptor *pDescriptorAddPhys = NULL;
+	u8 bytesPerPixel;
+	bool bNotLastEntry = true;
+	u32 dwTFECR = 0;
+	bool bRLEOverFLow = false;
+	u32 dwFetchWidthPixels = 0;
+	u32 dwFetchHeight = 0;
+	u32 dwPhysAddIn;
+	u32 dwPhysAddOut;
+	u32 data_size = 0;
+	void *desc_virt = NULL;
+	u32 desc_phy = 0;
+	struct ContextTable *ctx_entry = NULL;
+
+	ctx_entry = get_context_entry(ri->rc, pAstRVAS);
+	if (ctx_entry) {
+		desc_virt = ctx_entry->desc_virt;
+		desc_phy = ctx_entry->desc_phy;
+	} else {
+		ri->rs = InvalidContextHandle;
+		return;
+	}
+
+	ri->rs = SuccessStatus;
+
+	dwPhysAddIn = get_phys_add_rsvd_mem((u32) ri->rmh, pAstRVAS);
+	dwPhysAddOut = get_phys_add_rsvd_mem((u32) ri->rmh1, pAstRVAS);
+
+	data_size = ri->rmh_mem_size;
+	pDescriptorAdd = (struct Descriptor *) ctx_entry->desc_virt;
+	pDescriptorAddPhys = (struct Descriptor *) ctx_entry->desc_phy;
+
+	VIDEO_DBG("pDescriptorAdd=%#x, phy=%#x\n", (u32)pDescriptorAdd,
+		(u32)pDescriptorAddPhys);
+
+	if (dwPhysAddIn && dwPhysAddOut) {
+		// Enable TFE
+		dwTFECR = (ri->encode & 0xffff0000) << 16;
+		dwTFECR |= 1;
+		dwTFECR &= TFCTL_DESCRIPTOR_IN_DDR_MASK;
+
+		// triplet code and repeat code
+		bNotLastEntry = false;
+		bRLEOverFLow = true;
+		dwFetchWidthPixels = TILE_SIZE;
+		dwFetchHeight = data_size / TILE_SIZE;
+		bytesPerPixel = 1;
+
+		prepare_tfe_descriptor(pDescriptorAdd, dwPhysAddIn,
+			dwPhysAddOut, bNotLastEntry, 1, 1, dwFetchWidthPixels,
+			bytesPerPixel, dwFetchWidthPixels, dwFetchHeight,
+			AllBytesMode, bRLEOverFLow, 1);
+
+		if (sleep_on_tfe_busy(pAstRVAS, (u32) pDescriptorAddPhys,
+			dwTFECR, data_size, &ri->rle_len,
+			&ri->rle_checksum) == false) {
+			ri->rs = GenericError;
+			dev_err(pAstRVAS->pdev, "%s sleep_on_tfe_busy ERROR\n", __func__);
+			return;
+		}
+	} else
+		ri->rs = InvalidMemoryHandle;
+}
+
+static u32 get_video_slice_fetch_width(u8 cBuckets)
+{
+	u32 dwFetchWidthPixels = 0;
+
+	switch (cBuckets) {
+	case 3:
+		dwFetchWidthPixels = ((TILE_SIZE << 5) * 3) >> 3;
+		break;
+
+	case 8:
+		dwFetchWidthPixels = TILE_SIZE << 5;
+		break;
+
+	case 16:
+		dwFetchWidthPixels = (TILE_SIZE << 5) * 2;
+		break;
+
+	case 24:
+		dwFetchWidthPixels = (TILE_SIZE << 5) * 3;
+		break;
+
+	default:
+		dwFetchWidthPixels = TILE_SIZE << 2;
+		break;
+	}
+
+	return dwFetchWidthPixels;
+}
+
+void ioctl_fetch_video_slices(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	struct FetchVideoSlicesArg *pFVSA;
+	u32 dwCD;
+	struct Descriptor *pDescriptorVirtualAddr;
+	u32 dwDescPhysicalAddr;
+	u32 dwSourceAddress;
+	u32 dwDestinationAddress1Index;
+	u8 bytesPerPixel;
+	bool bNotLastEntry = true;
+	bool bInterrupt = false;
+	u32 dwTFECR = 0;
+	u32 dwFetchSize = 0;
+	bool bRLEOverFLow = false;
+	u32 dwFetchWidthPixels = 0;
+	u32 dwFetchHeight = 0;
+	u32 arg_phys = 0;
+	u32 data_phys_out = 0;
+	u32 data_phys_rle = 0;
+	struct BSEAggregateRegister aBSEAR;
+	struct Descriptor *pNextDescriptor = 0;
+	u32 dwNexDestAddr = 0;
+	u32 dwBucketSizeIter = 0;
+	bool bBucketSizeEnable = 0;
+	u32 addrBSCR = pAstRVAS->fg_reg_base + BSE_Command_Register;
+	void *desc_virt = NULL;
+	u32 desc_phy = 0;
+	struct ContextTable *ctx_entry = get_context_entry(ri->rc, pAstRVAS);
+
+	VIDEO_DBG("Start\n");
+
+	if (ctx_entry) {
+		desc_virt = ctx_entry->desc_virt;
+		desc_phy = ctx_entry->desc_phy;
+	} else {
+		pr_err("BSE: Cannot get valid context\n");
+		ri->rs = InvalidContextHandle;
+		return;
+	}
+
+	arg_phys = get_phys_add_rsvd_mem((u32) ri->rmh, pAstRVAS);
+	data_phys_out = get_phys_add_rsvd_mem((u32) ri->rmh1, pAstRVAS);
+	data_phys_rle = get_phys_add_rsvd_mem((u32) ri->rmh2, pAstRVAS);
+
+	if (!arg_phys || !data_phys_out || !data_phys_rle) {
+		pr_err("BSE: Invalid memory handle\n");
+		ri->rs = InvalidMemoryHandle;
+		return;
+	}
+	ri->rs = SuccessStatus;
+	dwDestinationAddress1Index = data_phys_out;
+	pFVSA = (struct FetchVideoSlicesArg *) get_virt_add_rsvd_mem((u32)ri->rmh, pAstRVAS);
+
+	VIDEO_DBG("bEnableRLE: %d cBuckets: %u cfr: %u\n", pFVSA->bEnableRLE,
+		pFVSA->cBuckets, pFVSA->cfr);
+
+	if (pFVSA->cfr > 1) {
+		writel(readl((void *)addrBSCR)|BSE_ENABLE_MULT_BUCKET_SZS, (void *)addrBSCR);
+		bBucketSizeEnable = 1;
+	} else {
+		writel(readl((void *)addrBSCR)&(~BSE_ENABLE_MULT_BUCKET_SZS), (void *)addrBSCR);
+		bBucketSizeEnable = 0;
+	}
+
+	VIDEO_DBG("*pdwBSCR: %#x bBucketSizeEnable: %d\n", readl((void *)addrBSCR),
+		bBucketSizeEnable);
+
+	pDescriptorVirtualAddr = ctx_entry->desc_virt;
+	dwDescPhysicalAddr = ctx_entry->desc_phy;
+	bytesPerPixel = pFVSA->vg.byBitsPerPixel >> 3;
+
+	VIDEO_DBG("BSE:: u8 per pixel: %d\n", bytesPerPixel);
+	VIDEO_DBG("BSE:: cfr: %u bucket size: %d\n", pFVSA->cfr, pFVSA->cBuckets);
+
+	pNextDescriptor = pDescriptorVirtualAddr;
+	dwNexDestAddr = dwDestinationAddress1Index;
+	// Prepare BSE Descriptors for all Regions
+	VIDEO_DBG("pNextDescriptor 0x%p dwNexDestAddr: %#x\n", pNextDescriptor,
+		dwNexDestAddr);
+
+	for (dwCD = 0; dwCD < pFVSA->cfr; dwCD++) {
+		VIDEO_DBG("dwCD: %u\n", dwCD);
+		VIDEO_DBG("pfr->wLeftX :%d\n", pFVSA->pfr[dwCD].wLeftX);
+		VIDEO_DBG("pfr->wTopY :%d\n", pFVSA->pfr[dwCD].wTopY);
+		VIDEO_DBG("pfr->wRightX :%d\n", pFVSA->pfr[dwCD].wRightX);
+		VIDEO_DBG("pfr->wBottomY :%d\n", pFVSA->pfr[dwCD].wBottomY);
+
+		dwSourceAddress = get_phy_fb_start_address(pAstRVAS)
+			+ pFVSA->pfr[dwCD].wLeftX * bytesPerPixel
+			+ pFVSA->pfr[dwCD].wTopY * pFVSA->vg.wStride
+			* bytesPerPixel;
+		dwFetchWidthPixels = (pFVSA->pfr[dwCD].wRightX
+			- pFVSA->pfr[dwCD].wLeftX + 1);
+		dwFetchHeight = pFVSA->pfr[dwCD].wBottomY
+			- pFVSA->pfr[dwCD].wTopY + 1;
+
+		VIDEO_DBG("BSE Width in Pixel: %d\n", dwFetchWidthPixels);
+		VIDEO_DBG("BSE Height: %d bBucketSizeEnable: %d\n", dwFetchHeight,
+			bBucketSizeEnable);
+
+		if (!bBucketSizeEnable) {
+			bNotLastEntry = false;
+			bInterrupt = true;
+			prepare_bse_descriptor(pDescriptorVirtualAddr,
+				dwSourceAddress, dwDestinationAddress1Index,
+				bNotLastEntry, pFVSA->vg.wStride, bytesPerPixel,
+				dwFetchWidthPixels, dwFetchHeight, bInterrupt);
+			dwFetchSize += (pFVSA->cBuckets
+				* (dwFetchWidthPixels * dwFetchHeight) >> 3);
+			aBSEAR = setUp_bse_bucket(pFVSA->abyBitIndexes,
+				pFVSA->cBuckets, bytesPerPixel,
+				dwFetchWidthPixels, dwFetchHeight);
+
+		} else {
+			if (dwCD == pFVSA->cfr - 1) {
+				bNotLastEntry = false;
+				bInterrupt = true;
+			} else {
+				bNotLastEntry = true;
+				bInterrupt = false;
+			}
+
+			prepare_bse_descriptor_2(pNextDescriptor,
+				dwSourceAddress, dwNexDestAddr, bNotLastEntry,
+				pFVSA->vg.wStride, bytesPerPixel,
+				dwFetchWidthPixels, dwFetchHeight, bInterrupt,
+				arrBuckSizeRegIndex[dwBucketSizeIter]);
+
+			aBSEAR = set_up_bse_bucket_2(pAstRVAS,
+				pFVSA->abyBitIndexes, pFVSA->cBuckets,
+				bytesPerPixel, dwFetchWidthPixels,
+				dwFetchHeight,
+				arrBuckSizeRegIndex[dwBucketSizeIter]);
+
+			dwBucketSizeIter++;
+			pNextDescriptor++;
+			dwFetchSize += pFVSA->cBuckets
+				* ((dwFetchWidthPixels * dwFetchHeight) >> 3); //each bucket size
+			dwNexDestAddr = dwDestinationAddress1Index
+				+ dwFetchSize;
+		}
+	}
+
+	//bse now
+	if (pFVSA->cBuckets <= FULL_BUCKETS_COUNT) {
+		if (bBucketSizeEnable)
+			aBSEAR.dwBSDBS = 0x80000000;
+
+		VIDEO_DBG("Sleeping on BSE to complete\n");
+
+		if (sleep_on_bse_busy(pAstRVAS, dwDescPhysicalAddr, aBSEAR,
+			dwFetchSize) == false) {
+			dev_err(pAstRVAS->pdev, ".....BSE Timeout\n");
+			ri->rs = GenericError;
+			return;
+		}
+	}
+	VIDEO_DBG("Fetched the bit slices\n");
+	//RLE
+	pFVSA->dwSlicedSize = dwFetchSize;
+	pFVSA->dwSlicedRLESize = pFVSA->dwSlicedSize;
+
+	// do RLE if RLE is on. Fetch from Destination 1 to Destination 2 with RLE on
+	bNotLastEntry = false;
+
+	if (pFVSA->bEnableRLE) {
+		VIDEO_DBG("BSE - 3 (RLE Enabled)\n");
+		// Enable TFE
+		dwTFECR = ((pFVSA->byRLETripletCode << 24)
+			| (pFVSA->byRLERepeatCode << 16));
+		dwTFECR |= ((0x1 << 1) | 1);
+		dwTFECR &= TFCTL_DESCRIPTOR_IN_DDR_MASK;
+
+		bRLEOverFLow = true;
+		bytesPerPixel = 1;
+
+		dwFetchWidthPixels = get_video_slice_fetch_width(
+			pFVSA->cBuckets);
+		dwFetchHeight = dwFetchSize / dwFetchWidthPixels;
+
+		prepare_tfe_descriptor(pDescriptorVirtualAddr, data_phys_out,
+			data_phys_rle, bNotLastEntry, 1, pFVSA->bEnableRLE,
+			dwFetchWidthPixels, bytesPerPixel, dwFetchWidthPixels,
+			dwFetchHeight, 0, bRLEOverFLow, 1);
+
+		VIDEO_DBG("TFE-RLE Control Register value: 0x%x\n", dwTFECR);
+
+		if (sleep_on_tfe_busy(pAstRVAS, dwDescPhysicalAddr, // Descriptor physical Address
+			dwTFECR,               // control register value
+			dwFetchSize,          // bandwidth limiter value
+			&pFVSA->dwSlicedRLESize,       // out:: rle size
+			&pFVSA->dwCheckSum
+			) == false) {
+			ri->rs = GenericError;
+			return;
+		}
+
+		VIDEO_DBG("Finishing RLE Fetching\n");
+
+		if (pFVSA->dwSlicedRLESize >= pFVSA->dwSlicedSize)
+			pFVSA->bRLEFailed = true;
+		else
+			pFVSA->bRLEFailed = false;
+	}        // RLE enabled
+
+	memcpy((void *) &dwFetchSize, (void *) &pFVSA->dwSlicedRLESize, 4);
+
+}
+
+void ioctl_fetch_text_data(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	bool bRLEOn = ri->tfm.bEnableRLE;
+
+	ri->rs = SuccessStatus;
+
+	// first time fetch
+	on_fetch_text_data(ri, bRLEOn, pAstRVAS);
+}
+
+void on_fetch_text_data(struct RvasIoctl *ri, bool bRLEOn, struct AstRVAS *pAstRVAS)
+{
+	struct Descriptor *pDescriptorAdd;
+	struct Descriptor *pDescriptorAddPhys;
+	u32 dwScreenOffset = 0x00;
+	u32 dwSourceAddress = get_phy_fb_start_address(pAstRVAS);
+	u32 dwDestinationAddress;
+	bool bRLEOverFlow = false;
+	bool bInterrupt = true;
+	u32 wFetchLines = 0;
+	u8 byCharacterPerLine = 0;
+	u16 wFetchWidthInBytes = 0;
+	u32 data_phys = 0;
+	u32 data_phys_rle = 0;
+	u32 data_phys_temp = 0;
+	u32 dwCtrlRegValue = 0;
+	u32 dwMinBufSize = 0;
+	void *desc_virt = NULL;
+	u32 desc_phy = 0;
+	struct ContextTable *ctx_entry = NULL;
+
+	VIDEO_DBG("Start\n");
+	ctx_entry = get_context_entry(ri->rc, pAstRVAS);
+	if (ctx_entry) {
+		desc_virt = ctx_entry->desc_virt;
+		desc_phy = ctx_entry->desc_phy;
+	} else {
+		ri->rs = InvalidContextHandle;
+		return;
+	}
+
+	wFetchLines = get_text_mode_fetch_lines(pAstRVAS, ri->vg.wScreenHeight);
+	byCharacterPerLine = get_text_mode_character_per_line(pAstRVAS,
+		ri->vg.wScreenWidth);
+
+	data_phys = get_phys_add_rsvd_mem((u32) ri->rmh, pAstRVAS);
+	data_phys_rle = get_phys_add_rsvd_mem((u32) ri->rmh1, pAstRVAS);
+
+	if (!data_phys || !data_phys_rle) {
+		ri->rs = InvalidMemoryHandle;
+		dev_err(pAstRVAS->pdev, "Fetch Text: Invalid Memoryhandle\n");
+		return;
+	}
+
+	dwMinBufSize = (byCharacterPerLine * wFetchLines) << 1;
+
+	if (ri->rmh_mem_size < dwMinBufSize) {
+		//either buffer is too small or invalid data in registers
+		ri->rs = GenericError;
+		dev_err(pAstRVAS->pdev, "Fetch Text: required buffer len:0x%x\n", dwMinBufSize);
+		return;
+	}
+	memset(desc_virt, 0x00, MAX_DESC_SIZE);
+	pDescriptorAdd = desc_virt;
+	pDescriptorAddPhys = (struct Descriptor *) desc_phy;
+	dwDestinationAddress = data_phys;
+
+	// Enable TFE
+	dwCtrlRegValue |= 1;
+	dwCtrlRegValue &= TFCTL_DESCRIPTOR_IN_DDR_MASK;
+	// set up the text alignment
+	dwScreenOffset = get_screen_offset(pAstRVAS);
+	dwSourceAddress += dwScreenOffset;
+	VIDEO_DBG("screen offset:%#x, Source start Addr: %#x\n", dwScreenOffset,
+		dwSourceAddress);
+	if (ri->tfm.dpm == AttrMode) { // ATTR and ASCII
+		data_phys_temp = data_phys_rle;
+		wFetchWidthInBytes = byCharacterPerLine << 3;
+		// must fetch both ascii & attr
+		VIDEO_DBG("Attribute and ASCII\n");
+		prepare_tfe_text_descriptor(desc_virt, dwSourceAddress,
+			data_phys_temp,
+			false, wFetchWidthInBytes, wFetchLines, ri->tfm.dpm,
+			bRLEOverFlow, bInterrupt);
+		ri->tfm.dwFetchSize = (byCharacterPerLine * wFetchLines) << 1;
+	} else if (ri->tfm.dpm == AsciiOnlyMode) {
+		wFetchWidthInBytes = byCharacterPerLine << 3;
+		VIDEO_DBG("ASCII Only\n");
+		prepare_tfe_text_descriptor(desc_virt, dwSourceAddress,
+			dwDestinationAddress,
+			false, wFetchWidthInBytes, wFetchLines, ri->tfm.dpm,
+			bRLEOverFlow, bInterrupt);
+		ri->tfm.dwFetchSize = byCharacterPerLine * wFetchLines;
+	} else if (ri->tfm.dpm == FontFetchMode) {
+		wFetchWidthInBytes = byCharacterPerLine << 2;
+		VIDEO_DBG("Font Only\n");
+		prepare_tfe_text_descriptor(desc_virt, dwSourceAddress,
+			dwDestinationAddress,
+			false, wFetchWidthInBytes, wFetchLines + 256,
+			ri->tfm.dpm, bRLEOverFlow, bInterrupt);
+
+		ri->tfm.dwFetchSize = MAX_TEXT_DATA_SIZE;
+	}
+	dwCtrlRegValue |= 1 << 1; // enabled IRQ
+	if (ri->tfm.dpm == AttrMode) {
+		if (sleep_on_tfe_text_busy(pAstRVAS, desc_phy, dwCtrlRegValue, // control register value
+			ri->tfm.dwFetchSize,        // bandwidth limitor value
+			&ri->tfm.dwFetchRLESize,        // out:: rle size
+			&ri->tfm.dwCheckSum) == false) {
+			dev_err(pAstRVAS->pdev, "Could not sleep_on_tfe_busy for attributes\n");
+			ri->rs = GenericError;
+			return;
+		}
+	} else {
+		if (sleep_on_tfe_text_busy(pAstRVAS, desc_phy, dwCtrlRegValue,
+			ri->tfm.dwFetchSize, &ri->tfm.dwFetchRLESize,
+			&ri->tfm.dwCheckSum) == false) {
+			ri->rs = GenericError;
+			dev_err(pAstRVAS->pdev, "Could not sleep_on_tfe_busy for others\n");
+			return;
+		}
+	}
+
+	if (ri->tfm.dpm == AttrMode) {
+		//separate ATTR from ATTR+ASCII
+		dwSourceAddress = data_phys_temp;
+		dwDestinationAddress = data_phys;
+		prepare_tfe_descriptor(desc_virt, data_phys_temp, data_phys,
+			false,        //not last entry?
+			1,        //checksum
+			false,        //RLE?
+			byCharacterPerLine,
+			2,        //byBpp,
+			byCharacterPerLine, wFetchLines, TopByteMode,
+			bRLEOverFlow, bInterrupt);
+
+		ri->tfm.dwFetchSize = byCharacterPerLine * wFetchLines;
+
+		dwCtrlRegValue |= 1 << 1;        // enabled IRQ
+		if (sleep_on_tfe_text_busy(pAstRVAS, (u32) pDescriptorAddPhys,
+			dwCtrlRegValue, ri->tfm.dwFetchSize,
+			&ri->tfm.dwFetchRLESize, &ri->tfm.dwCheckSum) == false) {
+			dev_err(pAstRVAS->pdev, "Could not sleep_on_tfe_busy for attributes # 2\n");
+			ri->rs = GenericError;
+			return;
+		}
+	}
+	// RLE enabled
+	if (bRLEOn) {
+		bRLEOverFlow = true;
+		dwCtrlRegValue = 1;
+		dwCtrlRegValue |= (ri->tfm.byRLETripletCode << 24)
+			| (ri->tfm.byRLERepeatCode << 16);
+		dwSourceAddress = dwDestinationAddress;
+		dwDestinationAddress = data_phys_rle;
+
+		// RLE only
+		prepare_tfe_descriptor(pDescriptorAdd, dwSourceAddress,
+			dwDestinationAddress,
+			false,        //not last entry?
+			1,        //checksum
+			bRLEOn,        //RLE?
+			ri->tfm.dwFetchSize / wFetchLines, 1,
+			ri->tfm.dwFetchSize / wFetchLines, wFetchLines,
+			AllBytesMode, bRLEOverFlow, bInterrupt);
+
+		dwCtrlRegValue |= 1 << 1;        // enabled IRQ
+
+		if (sleep_on_tfe_busy(pAstRVAS, (u32) pDescriptorAddPhys, // Descriptor physical Address
+			dwCtrlRegValue,        // control register value
+			ri->tfm.dwFetchSize,        // bandwidth limitor value
+			&ri->tfm.dwFetchRLESize,        // out:: rle size
+			&ri->tfm.dwCheckSum) == false) { // out:: cs size
+			dev_err(pAstRVAS->pdev, "Could not sleep_on_tfe_busy for RLE for Text Mode\n");
+			ri->rs = GenericError;
+			return;
+		}     //sleeponTFEBusy
+	}
+	if (bRLEOn) {
+		ri->tfm.bRLEFailed =
+			(ri->tfm.dwFetchRLESize < ri->tfm.dwFetchSize) ?
+			false : true;
+	}
+}
+
+u8 get_text_mode_character_per_line(struct AstRVAS *pAstRVAS, u16 wScreenWidth)
+{
+	u8 byCharPerLine = 0x00;
+	u8 byCharWidth = 0;
+	u8 byVGASR1 = readb((void *)(pAstRVAS->grce_reg_base + GRCE_SEQ + 0x1));
+
+	byCharWidth = (byVGASR1 & 0x1) ? 8 : 9;
+	byCharPerLine = wScreenWidth / byCharWidth;
+
+	return byCharPerLine;
+}
+
+u16 get_text_mode_fetch_lines(struct AstRVAS *pAstRVAS, u16 wScreenHeight)
+{
+	u8 byVGACR9 = readb((void *)(pAstRVAS->grce_reg_base + GRCE_CRTC + 0x9));
+	u8 byFontHeight = (byVGACR9 & 0x1F) + 1;
+	u16 wFetchLines;
+
+	wFetchLines = wScreenHeight / byFontHeight;
+
+	return wFetchLines;
+}
+
+//
+// HELPER Functions
+//
+
+void prepare_bse_descriptor(struct Descriptor *pDAddress, u32 dwSourceAddress,
+	u32 dwDestAddress, bool bNotLastEntry, u16 wStride, u8 bytesPerPixel,
+	u32 dwFetchWidthPixels, u32 dwFetchHeight, bool bInterrupt)
+{
+	u16 wDestinationStride;
+
+	// initialize to 0
+	pDAddress->dw0General = 0x00;
+	pDAddress->dw1FetchWidthLine = 0x00;
+	pDAddress->dw2SourceAddr = 0x00;
+	pDAddress->dw3DestinationAddr = 0x00;
+
+	wDestinationStride = dwFetchWidthPixels >> 3;
+
+	// initialize to 0
+	pDAddress->dw0General = ((wStride * bytesPerPixel) << 16)
+		| (wDestinationStride << 8) | (bNotLastEntry << 1) | bInterrupt;
+	pDAddress->dw1FetchWidthLine = ((dwFetchHeight - 1) << 16)
+		| (dwFetchWidthPixels * bytesPerPixel - 1);
+	pDAddress->dw2SourceAddr = dwSourceAddress & 0xfffffffc;
+	pDAddress->dw3DestinationAddr = dwDestAddress & 0xfffffffc;
+
+	VIDEO_DBG("After SETTING BSE Descriptor\n");
+	VIDEO_DBG("u32 0: 0x%x\n", pDAddress->dw0General);
+	VIDEO_DBG("u32 1: 0x%x\n", pDAddress->dw1FetchWidthLine);
+	VIDEO_DBG("u32 2: 0x%x\n", pDAddress->dw2SourceAddr);
+	VIDEO_DBG("u32 3: 0x%x\n", pDAddress->dw3DestinationAddr);
+}
+
+//for descriptor chaining
+void prepare_bse_descriptor_2(struct Descriptor *pDAddress, u32 dwSourceAddress,
+	u32 dwDestAddress, bool bNotLastEntry, u16 wStride, u8 bytesPerPixel,
+	u32 dwFetchWidthPixels, u32 dwFetchHeight,
+	bool bInterrupt, u8 byBuckSizeRegIndex)
+{
+	u16 wDestinationStride;
+
+	// initialize to 0
+	pDAddress->dw0General = 0x00;
+	pDAddress->dw1FetchWidthLine = 0x00;
+	pDAddress->dw2SourceAddr = 0x00;
+	pDAddress->dw3DestinationAddr = 0x00;
+
+	wDestinationStride = dwFetchWidthPixels >> 3;
+
+	// initialize to 0
+	pDAddress->dw0General = ((wStride * bytesPerPixel) << 16)
+		| (wDestinationStride << 8)
+		| (byBuckSizeRegIndex << BSE_BUCK_SZ_INDEX_POS)
+		| (bNotLastEntry << 1) | bInterrupt;
+	pDAddress->dw1FetchWidthLine = ((dwFetchHeight - 1) << 16)
+		| (dwFetchWidthPixels * bytesPerPixel - 1);
+	pDAddress->dw2SourceAddr = dwSourceAddress & 0xfffffffc;
+	pDAddress->dw3DestinationAddr = dwDestAddress & 0xfffffffc;
+
+	VIDEO_DBG("AFter SETTING BSE Descriptor\n");
+	VIDEO_DBG("u32 0: 0x%x\n", pDAddress->dw0General);
+	VIDEO_DBG("u32 1: 0x%x\n", pDAddress->dw1FetchWidthLine);
+	VIDEO_DBG("u32 2: 0x%x\n", pDAddress->dw2SourceAddr);
+	VIDEO_DBG("u32 3: 0x%x\n", pDAddress->dw3DestinationAddr);
+}
+
+struct BSEAggregateRegister set_up_bse_bucket_2(struct AstRVAS *pAstRVAS, u8 *abyBitIndexes,
+	u8 byTotalBucketCount, u8 byBSBytesPerPixel, u32 dwFetchWidthPixels,
+	u32 dwFetchHeight, u32 dwBucketSizeIndex)
+{
+	struct BSEAggregateRegister aBSEAR = { 0 };
+	u32 addrBSDBS = 0;
+	u32 addrBSCR = pAstRVAS->fg_reg_base + BSE_Command_Register;
+
+	if (dwBucketSizeIndex >= BSE_MAX_BUCKET_SIZE_REGS) {
+		dev_err(pAstRVAS->pdev, "Video::BSE bucket size index %d too big!",
+			dwBucketSizeIndex);
+		return aBSEAR;
+	}
+
+	addrBSDBS = pAstRVAS->fg_reg_base + BSE_REG_BASE + dwBucketSizeRegOffset[dwBucketSizeIndex];
+
+	// initialize
+	memset((void *) &aBSEAR, 0x00, sizeof(struct BSEAggregateRegister));
+	aBSEAR = setUp_bse_bucket(abyBitIndexes, byTotalBucketCount,
+		byBSBytesPerPixel, dwFetchWidthPixels, dwFetchHeight);
+
+	writel(aBSEAR.dwBSDBS, (void *)addrBSDBS);
+	aBSEAR.dwBSCR |= readl((void *)addrBSCR) & (BSE_ENABLE_MULT_BUCKET_SZS);
+	VIDEO_DBG("BSE Bucket size register index %d, [%#x], readback 0x%x\n",
+		dwBucketSizeIndex, aBSEAR.dwBSDBS, readl((void *)addrBSCR));
+
+	return aBSEAR;
+}
+
+struct BSEAggregateRegister setUp_bse_bucket(u8 *abyBitIndexes, u8 byTotalBucketCount,
+	u8 byBSBytesPerPixel, u32 dwFetchWidthPixels, u32 dwFetchHeight)
+{
+	struct BSEAggregateRegister aBSEAR;
+	u32 dwSrcBucketSize = MAX_LMEM_BUCKET_SIZE;
+	u32 dwDestBucketSize = dwFetchWidthPixels * dwFetchHeight >> 3; //each bucket size
+	u8 byRegisterPosition = 0;
+	u8 cBucket;
+
+	// initialize
+	memset((void *) &aBSEAR, 0x00, sizeof(struct BSEAggregateRegister));
+
+	for (cBucket = 0; cBucket < byTotalBucketCount; cBucket++) {
+		if (cBucket < 6) {
+			VIDEO_DBG("BUCKET: 0x%x, Bit Position: 0x%x\n", cBucket,
+				abyBitIndexes[cBucket]);
+			VIDEO_DBG("BSBPS0 Position: 0x%x\n", byRegisterPosition);
+			aBSEAR.adwBSBPS[0] |= abyBitIndexes[cBucket]
+				<< byRegisterPosition;
+
+			byRegisterPosition += 5;
+		} else if (cBucket >= 6 && cBucket < 12) {
+			if (cBucket == 6)
+				byRegisterPosition = 0;
+
+			VIDEO_DBG("BUCKET: 0x%x, Bit Position: 0x%x\n", cBucket,
+				abyBitIndexes[cBucket]);
+			VIDEO_DBG("BSBPS1 Position: 0x%x\n", byRegisterPosition);
+			aBSEAR.adwBSBPS[1] |= abyBitIndexes[cBucket]
+				<< byRegisterPosition;
+			byRegisterPosition += 5;
+		} else {
+			if (cBucket == 12)
+				byRegisterPosition = 0;
+
+			VIDEO_DBG("BUCKET: 0x%x, Bit Position: 0x%x\n", cBucket,
+				abyBitIndexes[cBucket]);
+			VIDEO_DBG("BSBPS2 Position: 0x%x\n", byRegisterPosition);
+			aBSEAR.adwBSBPS[2] |= abyBitIndexes[cBucket]
+				<< byRegisterPosition;
+			byRegisterPosition += 5;
+		}
+	}
+
+	aBSEAR.dwBSCR = (((byTotalBucketCount - 1) << 8)
+			| ((byBSBytesPerPixel - 1) << 4) | (0x0 << 3)
+			| (0x1 << 1) | 0x1) & BSCMD_MASK;
+	aBSEAR.dwBSDBS = ((dwSrcBucketSize << 24) | dwDestBucketSize)
+		& 0xfcfffffc;
+
+	VIDEO_DBG("dwFetchWidthPixels [%#x], dwFetchHeight [%#x]\n",
+		dwFetchWidthPixels, dwFetchHeight);
+	VIDEO_DBG("BSE Destination Bucket Size [%#x]\n", dwDestBucketSize);
+	VIDEO_DBG("BSE Control [%#x]\n", aBSEAR.dwBSCR);
+	VIDEO_DBG("BSE BSDBS [%#x]\n", aBSEAR.dwBSDBS);
+	VIDEO_DBG("BSE BSBPS0 [%#x]\n", aBSEAR.adwBSBPS[0]);
+	VIDEO_DBG("BSE BSBPS1 [%#x]\n", aBSEAR.adwBSBPS[1]);
+	VIDEO_DBG("BSE BSBPS2 [%#x]\n", aBSEAR.adwBSBPS[2]);
+
+	return aBSEAR;
+}
+
+void prepare_tfe_descriptor(struct Descriptor *pDAddress, u32 dwSourceAddress,
+	u32 dwDestAddress, bool bNotLastEntry, u8 bCheckSum,
+	bool bEnabledRLE, u16 wStride, u8 bytesPerPixel, u32 dwFetchWidthPixels,
+	u32 dwFetchHeight, enum SelectedByteMode sbm,
+	bool bRLEOverFLow, bool bInterrupt)
+{
+	enum SkipByteMode skipBM = NoByteSkip;
+	enum DataProccessMode dpm = NormalTileMode;
+	enum StartBytePosition sbp = StartFromByte0;
+
+	VIDEO_DBG("BEFORE SETTING TFE Descriptor\n");
+	// initialize to 0
+	pDAddress->dw0General = 0x00;
+	pDAddress->dw1FetchWidthLine = 0x00;
+	pDAddress->dw2SourceAddr = 0x00;
+	pDAddress->dw3DestinationAddr = 0x00;
+
+	if (dwFetchHeight & 0x3)
+		dwFetchHeight = ((dwFetchHeight + 3) >> 2) << 2;
+
+	switch (sbm) {
+	case AllBytesMode:
+		break;
+
+	case LowByteMode:
+		dpm = SplitByteMode;
+		if (bytesPerPixel == 2)
+			skipBM = SkipOneByte;
+		else if (bytesPerPixel == 3)
+			skipBM = SkipTwoByte;
+		else if (bytesPerPixel == 4)
+			skipBM = SkipThreeByte;
+		break;
+
+	case MiddleByteMode:
+		dpm = SplitByteMode;
+		if (bytesPerPixel == 2) {
+			skipBM = SkipOneByte;
+			sbp = StartFromByte1;
+		} else if (bytesPerPixel == 3) {
+			skipBM = SkipTwoByte;
+			sbp = StartFromByte1;
+		} else if (bytesPerPixel == 4) {
+			skipBM = SkipThreeByte;
+			sbp = StartFromByte1;
+		}
+		break;
+
+	case TopByteMode:
+		dpm = SplitByteMode;
+		if (bytesPerPixel == 2) {
+			skipBM = SkipOneByte;
+			sbp = StartFromByte1;
+		} else if (bytesPerPixel == 3) {
+			skipBM = SkipTwoByte;
+			sbp = StartFromByte2;
+		} else if (bytesPerPixel == 4) {
+			skipBM = SkipThreeByte;
+			sbp = StartFromByte2;
+		}
+		break;
+
+	case PlanarToPackedMode:
+		dpm = FourBitPlanarMode;
+		break;
+
+	case PackedToPackedMode:
+		dpm = FourBitPackedMode;
+		break;
+
+	default:
+		break;
+	}
+
+	if (dwFetchWidthPixels > wStride)
+		wStride = dwFetchWidthPixels;
+
+	pDAddress->dw0General = ((wStride * bytesPerPixel) << 16) | (dpm << 13)
+		| (sbp << 10) | (skipBM << 8) | (bRLEOverFLow << 7)
+		| (bCheckSum << 5) | (bEnabledRLE << 4) | (bNotLastEntry << 1)
+		| bInterrupt;
+	pDAddress->dw1FetchWidthLine = ((dwFetchHeight - 1) << 16)
+		| (dwFetchWidthPixels * bytesPerPixel - 1);
+	pDAddress->dw2SourceAddr = dwSourceAddress & 0xfffffffc;
+	pDAddress->dw3DestinationAddr = dwDestAddress & 0xfffffffc;
+
+	VIDEO_DBG("After SETTING TFE Descriptor\n");
+	VIDEO_DBG("u32 0: 0x%x\n", pDAddress->dw0General);
+	VIDEO_DBG("u32 1: 0x%x\n", pDAddress->dw1FetchWidthLine);
+	VIDEO_DBG("u32 2: 0x%x\n", pDAddress->dw2SourceAddr);
+	VIDEO_DBG("u32 3: 0x%x\n", pDAddress->dw3DestinationAddr);
+}
+
+void prepare_tfe_text_descriptor(struct Descriptor *pDAddress, u32 dwSourceAddress,
+	u32 dwDestAddress, bool bEnabledRLE, u32 dwFetchWidth,
+	u32 dwFetchHeight, enum DataProccessMode dpm, bool bRLEOverFLow,
+	bool bInterrupt)
+{
+	// initialize to 0
+	pDAddress->dw0General = 0x00;
+	pDAddress->dw1FetchWidthLine = 0x00;
+	pDAddress->dw2SourceAddr = 0x00;
+	pDAddress->dw3DestinationAddr = 0x00;
+
+	if (dwFetchHeight & 0x3)
+		dwFetchHeight = ((dwFetchHeight + 3) >> 2) << 2;
+
+	pDAddress->dw0General = (dwFetchWidth << 16) | (dpm << 13)
+		| (bRLEOverFLow << 7) | (1 << 5) | (bEnabledRLE << 4)
+		| bInterrupt;
+	pDAddress->dw1FetchWidthLine = ((dwFetchHeight - 1) << 16)
+		| (dwFetchWidth - 1);
+	pDAddress->dw2SourceAddr = dwSourceAddress & 0xfffffffc;
+	pDAddress->dw3DestinationAddr = dwDestAddress & 0xfffffffc;
+
+	VIDEO_DBG("u32 0: 0x%x\n", pDAddress->dw0General);
+	VIDEO_DBG("u32 1: 0x%x\n", pDAddress->dw1FetchWidthLine);
+	VIDEO_DBG("u32 2: 0x%x\n", pDAddress->dw2SourceAddr);
+	VIDEO_DBG("u32 3: 0x%x\n", pDAddress->dw3DestinationAddr);
+}
+
+void on_fetch_mode_13_data(struct AstRVAS *pAstRVAS, struct RvasIoctl *ri, bool bRLEOn)
+{
+	struct Descriptor *pDescriptorAdd;
+	struct Descriptor *pDescriptorAddPhys;
+	u32 dwSourceAddress = get_phy_fb_start_address(pAstRVAS);
+	u32 dwDestinationAddress;
+	bool bRLEOverFlow = false;
+	bool bNotLastEntry = false;
+	bool bInterrupt = 1;
+	u32 dwFetchHeight = MODE13_HEIGHT;
+	u32 dwFetchWidth = MODE13_WIDTH;
+	u32 data_phys = 0;
+	u32 data_phys_rle = 0;
+	u32 dwCtrlRegValue = 0x55AA0080;
+	void *desc_virt = NULL;
+	u32 desc_phy = 0;
+	struct ContextTable *ctx_entry = NULL;
+
+	VIDEO_DBG("Start, bRLEOn: %d\n", bRLEOn);
+
+	ctx_entry = get_context_entry(ri->rc, pAstRVAS);
+
+	if (ctx_entry) {
+		desc_virt = ctx_entry->desc_virt;
+		desc_phy = ctx_entry->desc_phy;
+	} else {
+		pr_err("Mode 13: Failed to get context\n");
+		ri->rs = InvalidContextHandle;
+		return;
+	}
+
+	ri->tfm.dwFetchSize = MODE13_HEIGHT * MODE13_WIDTH;
+
+	data_phys = get_phys_add_rsvd_mem((u32) ri->rmh, pAstRVAS);
+	data_phys_rle = get_phys_add_rsvd_mem((u32) ri->rmh1, pAstRVAS);
+
+	if (!data_phys || !data_phys_rle) {
+		ri->rs = InvalidMemoryHandle;
+		dev_err(pAstRVAS->pdev, "Fetch Text: Invalid Memoryhandle\n");
+		return;
+	}
+	if (!data_phys || (bRLEOn && !data_phys_rle)) {
+		pr_err("Mode 13: Invalid memory handle\n");
+		ri->rs = InvalidMemoryHandle;
+		return;
+	}
+
+	pDescriptorAdd = desc_virt;
+	pDescriptorAddPhys = (struct Descriptor *) desc_phy;
+
+	VIDEO_DBG("\n===========MODE 13 FETCHED DATA===========\n");
+
+	// Enable TFE
+	dwCtrlRegValue |= 1;
+	dwCtrlRegValue &= TFCTL_DESCRIPTOR_IN_DDR_MASK;
+	dwDestinationAddress = data_phys;
+	prepare_tfe_descriptor(pDescriptorAdd, dwSourceAddress,
+		dwDestinationAddress,
+		false, //is last entry
+		1,     //checksum
+		false, //No RLE
+		dwFetchWidth,
+		1,		//bytes per pixel
+		dwFetchWidth, dwFetchHeight, PackedToPackedMode, bRLEOverFlow,
+		1);
+
+	dwCtrlRegValue |= 1 << 1; // enabled IRQ
+
+	if (sleep_on_tfe_busy(pAstRVAS, (u32) pDescriptorAddPhys, // Descriptor physical Address
+		dwCtrlRegValue,           // control register value
+		ri->tfm.dwFetchSize,         // bandwidth limitor value
+		&ri->tfm.dwFetchRLESize,     // out:: rle size
+		&ri->tfm.dwCheckSum) == false) {     // out:: cs size
+		ri->rs = GenericError;
+		return;
+	}
+
+	// RLE enabled
+	if (bRLEOn) {
+		bRLEOverFlow = true;
+		dwCtrlRegValue = 1;
+		dwCtrlRegValue |= (ri->tfm.byRLETripletCode << 24)
+		| (ri->tfm.byRLERepeatCode << 16);
+		dwSourceAddress = data_phys;
+		dwDestinationAddress = data_phys_rle;
+		VIDEO_DBG("RLE is on\n");
+
+		prepare_tfe_descriptor(pDescriptorAdd, dwSourceAddress,
+			dwDestinationAddress,
+			bNotLastEntry,  //not last entry?
+			1,				//checksum
+			bRLEOn,		//RLE?
+			dwFetchWidth, 1, dwFetchWidth, dwFetchHeight,
+			AllBytesMode, bRLEOverFlow, bInterrupt);
+
+		dwCtrlRegValue |= 1 << 1; // enabled IRQ
+
+		if (sleep_on_tfe_busy(pAstRVAS, (u32) pDescriptorAddPhys, // Descriptor physical Address
+			dwCtrlRegValue,           // control register value
+			ri->tfm.dwFetchSize,          // bandwidth limitor value
+			&ri->tfm.dwFetchRLESize,     // out:: rle size
+			&ri->tfm.dwCheckSum) == false) {    // out:: cs size
+			ri->rs = GenericError;
+			return;
+		}    //sleeponTFEBusy
+	}
+
+	if (bRLEOn)
+		ri->tfm.bRLEFailed =
+			(ri->tfm.dwFetchRLESize < ri->tfm.dwFetchSize) ?
+			false : true;
+}
+
+void ioctl_fetch_mode_13_data(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	bool bRLEOn = ri->tfm.bEnableRLE;
+
+	ri->rs = SuccessStatus;
+
+	// first time fetch
+	on_fetch_mode_13_data(pAstRVAS, ri, bRLEOn);
+
+	if (ri->rs != SuccessStatus)
+		return;
+
+	//if RLE fail. need to TFE without RLE to first buffer
+	if (ri->tfm.bEnableRLE & (ri->tfm.bRLEFailed)) {
+		bRLEOn = false;
+		on_fetch_mode_13_data(pAstRVAS, ri, bRLEOn);
+	}
+}
+
+u32 get_phy_fb_start_address(struct AstRVAS *pAstRVAS)
+{
+	u32 dw_offset = get_screen_offset(pAstRVAS);
+
+	pAstRVAS->FBInfo.dwFBPhysStart = DDR_BASE + pAstRVAS->FBInfo.dwDRAMSize - pAstRVAS->FBInfo.dwVGASize + dw_offset;
+
+	VIDEO_DBG("Frame buffer start address: %#x, dram size: %#x, vga size: %#x\n",
+		pAstRVAS->FBInfo.dwFBPhysStart,
+		pAstRVAS->FBInfo.dwDRAMSize,
+		pAstRVAS->FBInfo.dwVGASize);
+
+	return pAstRVAS->FBInfo.dwFBPhysStart;
+}
+
+
+// Enable Snoop Interrupts and TSE, Disable FIQ
+static void enable_tse_interrupt(struct AstRVAS *pAstRVAS)
+{
+	u32 reg_val = 0;
+	u32 reg_addr = pAstRVAS->fg_reg_base
+		+ TSE_SnoopCommand_Register_Offset;
+
+	reg_val = readl((void *)reg_addr);
+	reg_val |= SNOOP_IRQ_MASK;
+	reg_val &= ~SNOOP_FIQ_MASK;
+
+	VIDEO_DBG("Enabled TSE Interrupts[%#X]\n", reg_val);
+	writel(reg_val, (void *)reg_addr);
+	pAstRVAS->tse_tsicr = TSE_INTR_COUNT;
+	reg_addr = pAstRVAS->fg_reg_base
+			+ TSE_TileSnoop_Interrupt_Count;
+	//set max wait time before interrupt
+	writel(pAstRVAS->tse_tsicr, (void *)reg_addr);
+}
+
+//disable tse interrupt
+static void disable_tse_interrupt(struct AstRVAS *pAstRVAS)
+{
+	u32 reg_val = 0;
+	u32 reg_addr = pAstRVAS->fg_reg_base + TSE_SnoopCommand_Register_Offset;
+
+	// Disable Snoop Interrupts and TSE, Disable FIQ
+	reg_val = readl((void *)reg_addr);
+	VIDEO_DBG("disable interrupt\n");
+	reg_val &= ~(SNOOP_IRQ_MASK | SNOOP_FIQ_MASK);
+	writel(reg_val, (void *)reg_addr);
+}
+
+static void enable_grce_interrupt(struct AstRVAS *pAstRVAS)
+{
+	u32 reg_val = 0;
+	u32 reg_addr = pAstRVAS->grce_reg_base + GRCE_CTL0;
+
+	reg_val = readl((void *)reg_addr);
+	reg_val |= GRC_IRQ_MASK;
+	writel(reg_val, (void *)reg_addr);
+	VIDEO_DBG("Enabled GRC Interrupts[%#X]\n", reg_val);
+}
+
+//enable all interrupts
+void enable_grce_tse_interrupt(struct AstRVAS *pAstRVAS)
+{
+	enable_grce_interrupt(pAstRVAS);
+	enable_tse_interrupt(pAstRVAS);
+}
+
+void disable_grce_tse_interrupt(struct AstRVAS *pAstRVAS)
+{
+	u32 reg_val = 0;
+
+	VIDEO_DBG("disable_interrupts- grce_reg_base: %#x GRCE_CTL0: %#x\n",
+		pAstRVAS->grce_reg_base, GRCE_CTL0);
+	reg_val = readl((void *)(pAstRVAS->grce_reg_base + GRCE_CTL0));
+	writel(reg_val&(~GRC_IRQ_MASK), (void *)(pAstRVAS->grce_reg_base + GRCE_CTL0));
+	disable_tse_interrupt(pAstRVAS);
+}
+
+u32 clear_tse_interrupt(struct AstRVAS *pAstRVAS)
+{
+	u32 tse_sts = 0;
+	u32 tse_tile_status = 0;
+	u32 tse_snoop_ctrl = 0;
+	u32 tse_ctrl_addr = pAstRVAS->fg_reg_base + TSE_SnoopCommand_Register_Offset;
+
+	VIDEO_DBG("clear tse inerrupt");
+	tse_sts = readl((void *)(pAstRVAS->fg_reg_base + TSE_Status_Register_Offset));
+	tse_snoop_ctrl = readl((void *)(pAstRVAS->fg_reg_base + TSE_SnoopCommand_Register_Offset));
+
+	if (tse_sts & (TSSTS_TC_SCREEN0|TSSTS_TC_SCREEN1)) {
+		if (tse_sts & TSSTS_TC_SCREEN0) {
+			VIDEO_DBG("Snoop** Update Screen 0\n");
+			 // clear interrupt and switch to screen 1
+			tse_snoop_ctrl |= TSCMD_SCREEN_OWNER;
+			writel(tse_sts, (void *)(pAstRVAS->fg_reg_base + TSE_Status_Register_Offset));
+			writel(tse_snoop_ctrl, (void *)tse_ctrl_addr);
+
+		} else if (tse_sts & TSSTS_TC_SCREEN1) {
+			VIDEO_DBG("Snoop** Update Screen 1\n");
+			tse_snoop_ctrl &= ~TSCMD_SCREEN_OWNER; // snap shutter
+			// clear status
+			writel(tse_sts, (void *)(pAstRVAS->fg_reg_base + TSE_Status_Register_Offset));
+			 // clear interrupt and switch to screen 1
+			writel(tse_snoop_ctrl, (void *)tse_ctrl_addr);
+		}
+		// read clear interrupt
+		tse_tile_status = readl((void *)(pAstRVAS->fg_reg_base
+				+ TSE_TileCount_Register_Offset));
+
+		if (tse_sts & TSSTS_FIFO_OVFL) {
+			//need to send full frame
+			dev_err(pAstRVAS->pdev, "TSE snoop fifo overflow\n");
+			writel(TSSTS_FIFO_OVFL, (void *)(pAstRVAS->fg_reg_base + TSE_Status_Register_Offset));
+			memset((void *) pAstRVAS->accrued_sm, 0xff, sizeof(pAstRVAS->accrued_sm));
+			memset((void *) &pAstRVAS->accrued_sa, 0xff,
+				sizeof(pAstRVAS->accrued_sa));
+		} else {
+			get_snoop_map_data(pAstRVAS);
+		}
+	}
+	return tse_sts;
+}
+// LDMA interrupt
+bool clear_ldma_interrupt(struct AstRVAS *pAstRVAS)
+{
+	u32 ldma_sts = 0;
+
+	ldma_sts = readl((void *)(pAstRVAS->fg_reg_base + LDMA_Status_Register));
+
+	if (ldma_sts & 0x02) {
+		//VIDEO_DBG("Got a LDMA interrupt\n");
+		// write 1 to clear the interrupt
+		writel(0x2, (void *)(pAstRVAS->fg_reg_base + LDMA_Status_Register));
+		return true;
+	}
+	return false;
+}
+
+bool clear_tfe_interrupt(struct AstRVAS *pAstRVAS)
+{
+	u32 tfe_sts = 0;
+
+	tfe_sts = readl((void *)(pAstRVAS->fg_reg_base + TFE_Status_Register));
+
+	if (tfe_sts & 0x02) {
+		// VIDEO_DBG("Debug: TFSTS Interrupt is triggered\n");
+		writel(0x2, (void *)(pAstRVAS->fg_reg_base + TFE_Status_Register));
+		return true;
+	}
+	return false;
+}
+
+bool clear_bse_interrupt(struct AstRVAS *pAstRVAS)
+{
+	u32 bse_sts = 0;
+
+	bse_sts = readl((void *)(pAstRVAS->fg_reg_base + BSE_Status_Register));
+
+	if (bse_sts & 0x02) {
+		writel(0x2, (void *)(pAstRVAS->fg_reg_base + BSE_Status_Register));
+		return true;
+	}
+	return false;
+}
+
+void setup_lmem(struct AstRVAS *pAstRVAS)
+{
+	writel(0x0, (void *)(pAstRVAS->fg_reg_base + LMEM_BASE_REG_3));
+	writel(0x2000, (void *)(pAstRVAS->fg_reg_base + LMEM_LIMIT_REG_3));
+	writel(0x9c89c8, (void *)(pAstRVAS->fg_reg_base + LMEM11_P0));
+	writel(0x9c89c8, (void *)(pAstRVAS->fg_reg_base + LMEM12_P0));
+	writel(0xf3cf3c, (void *)(pAstRVAS->fg_reg_base + LMEM11_P1));
+	writel(0x067201, (void *)(pAstRVAS->fg_reg_base + LMEM11_P2));
+	writel(0x00F3CF3C, (void *)(pAstRVAS->fg_reg_base + LMEM10_P1));
+	writel(0x00067201, (void *)(pAstRVAS->fg_reg_base + LMEM10_P2));
+}
+
+bool host_suspended(struct AstRVAS *pAstRVAS)
+{
+	u32 GRCE18 = readl((void *)(pAstRVAS->grce_reg_base + GRCE_ATTR_VGAIR0_OFFSET));
+
+	// VGAER is GRCE19
+	// VGAER bit[0]:0 - vga disabled (host suspended)
+	// 1 - vga enabled
+	VIDEO_DBG("GRCE18:%#x\n", GRCE18);
+	if (GRCE18 & 0x100)
+		return false;
+	else
+		return true;
+}
+
diff --git a/drivers/soc/aspeed/rvas/hardware_engines.h b/drivers/soc/aspeed/rvas/hardware_engines.h
new file mode 100644
index 000000000000..d1069704c736
--- /dev/null
+++ b/drivers/soc/aspeed/rvas/hardware_engines.h
@@ -0,0 +1,500 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * This file is part of the ASPEED Linux Device Driver for ASPEED Baseboard Management Controller.
+ * Refer to the README file included with this package for driver version and adapter compatibility.
+ *
+ * Copyright (C) 2019-2021 ASPEED Technology Inc. All rights reserved.
+ */
+
+#ifndef __HARDWAREENGINES_H__
+#define __HARDWAREENGINES_H__
+
+#include <linux/semaphore.h>
+#include "video_ioctl.h"
+
+#define MAX_NUM_CONTEXT				(8)
+#define MAX_NUM_MEM_TBL				(24)//each context has 3
+
+#define MAX_DESC_SIZE				(PAGE_SIZE) // (0x400)
+
+#define ENGINE_TIMEOUT_IN_SECONDS		(3)
+#define TFE_TIMEOUT_IN_MS			(750)
+#define DESCRIPTOR_SIZE				(16)
+#define TILE_SIZE				(32)
+#define MAX_LMEM_BUCKET_SIZE			(0x80)
+
+#define EIGHT_BYTE_ALIGNMENT_MASK		(0xfffffff7)
+#define SIXTEEN_BYTE_ALIGNMENT_MASK		(0x8)
+#define TFCTL_DESCRIPTOR_IN_DDR_MASK		(0xffffff7f)
+#define BSCMD_MASK				(0xffff0f37)
+
+#define TEXT_MODE_BUFFER_ALIGNMENT		(16)
+#define MODE_13_CHAR_WIDTH			(32)
+#define BSE_MEMORY_ACCESS_MASK			(0x00ffffff)
+#define MEM_TABLE_SIZE_INCR			(8)
+#define MEMORY_TABLE_GROW_INCR			(8)
+
+#define MAX_TEXT_DATA_SIZE			(8192)
+
+
+//SCU
+#define SCU000_Protection_Key_Register	(0x000)
+#define SCU040_Module_Reset_Control_Register_Set_1 (0x040)
+#define SCU044_Module_Reset_Control_Clear_Register_1 (0x044)
+#define SCU080_Clock_Stop_Control_Register_Set_1 (0x080)
+#define SCU084_Clock_Stop_Control_Clear_Register (0x084)
+#define SCU500_Hardware_Strap1_Register (0x500)
+#define SCU418_Pin_Ctrl (0x418)
+#define SCU0C0_Misc1_Ctrl (0x0C0)
+#define SCU0D0_Misc3_Ctrl (0x0D0)
+//SCU418
+#define VGAVS_ENBL			(1<<31)
+#define VGAHS_ENBL			(1<<30)
+//SCU0C0
+#define VGA_CRT_DISBL			(1<<6)
+//SCU0D0
+#define PWR_OFF_VDAC			(1<<3)
+
+#define SCU_UNLOCK_PWD			(0x1688A8A8)
+#define SCU_RVAS_ENGINE_BIT		BIT(9)
+#define SCU_RVAS_STOP_CLOCK_BIT		BIT(25)
+
+//MCR -edac
+#define MCR_CONF	0x04 /* configuration register */
+
+//DP
+#define DPTX_Configuration_Register			(0x100)
+#define DPTX_PHY_Configuration_Register		(0x104)
+//DPTX100
+#define AUX_RESETN							(24)
+//DPTX104
+#define DP_TX_I_MAIN_ON						(8)
+
+//TOP REG
+#define TOP_REG_OFFSET				(0x0)
+#define TOP_REG_CTL				(TOP_REG_OFFSET + 0x00)
+#define TOP_REG_STS				(TOP_REG_OFFSET + 0x04)
+#define LMEM_BASE_REG_3				(TOP_REG_OFFSET + 0x2c)
+#define LMEM_LIMIT_REG_3			(TOP_REG_OFFSET + 0x3c)
+#define LMEM11_P0				(TOP_REG_OFFSET + 0x4c)
+#define LMEM12_P0				(TOP_REG_OFFSET + 0x50)
+#define LMEM10_P1				(TOP_REG_OFFSET + 0x80)
+#define LMEM11_P1				(TOP_REG_OFFSET + 0x84)
+#define LMEM10_P2				(TOP_REG_OFFSET + 0xA0)
+#define LMEM11_P2				(TOP_REG_OFFSET + 0xA4)
+
+#define TSE_SnoopCommand_Register_Offset	(0x0400)
+#define TSE_TileCount_Register_Offset		(0x0418)
+#define TSE_Status_Register_Offset		(0x0404)
+#define TSE_CS0Reg				(0x0408)
+#define TSE_CS1Reg				(0x040c)
+#define TSE_RS0Reg				(0x0410)
+#define TSE_RS1Reg				(0x0414)
+#define TSE_TileSnoop_Interrupt_Count		(0x0420)
+#define TSE_FrameBuffer_Offset			(0x041c)
+#define TSE_UpperLimit_Offset		(0x0424)
+#define TSE_SnoopMap_Offset			(0x0600)
+
+
+#define TFE_Descriptor_Table_Offset		(0x0108)
+#define TFE_Descriptor_Control_Resgister	(0x0100)
+#define TFE_Status_Register			(0x0104)
+#define TFE_RLE_CheckSum			(0x010C)
+#define TFE_RLE_Byte_Count			(0x0110)
+#define TFE_RLE_LIMITOR				(0x0114)
+
+#define BSE_REG_BASE				(0x0200)
+#define BSE_Command_Register			(0x0200)
+#define BSE_Status_Register			(0x0204)
+#define BSE_Descriptor_Table_Base_Register	(0x0208)
+#define BSE_Destination_Buket_Size_Resgister	(0x020c)
+#define BSE_Bit_Position_Register_0		(0x0210)
+#define BSE_Bit_Position_Register_1		(0x0214)
+#define BSE_Bit_Position_Register_2		(0x0218)
+#define BSE_LMEM_Temp_Buffer_Offset		(0x0000)
+#define BSE_ENABLE_MULT_BUCKET_SZS		(1<<12)
+#define BSE_BUCK_SZ_INDEX_POS			(4)
+#define BSE_MAX_BUCKET_SIZE_REGS		(16)
+#define BSE_BIT_MASK_Register_Offset		(0x54)
+
+#define LDMA_Control_Register			(0x0300)
+#define LDMA_Status_Register			(0x0304)
+#define LDMA_Descriptor_Table_Base_Register	(0x0308)
+#define LDMA_CheckSum_Register			(0x030c)
+#define LDMA_LMEM_Descriptor_Offset		(0x4000)
+
+//Shadow
+#define GRCE_SIZE				(0x800)
+#define GRCE_ATTR_OFFSET			(0x0)
+#define GRCE_ATTR_VGAIR0_OFFSET	(0x18)
+#define GRCE_SEQ_OFFSET				(0x20)
+#define GRCE_GCTL_OFFSET			(0x30)
+#define GRCE_GRCCTL0_OFFSET			(0x58)
+#define GRCE_GRCSTS_OFFSET			(0x5c)
+#define GRCE_CRTC_OFFSET			(0x60)
+#define GRCE_CRTCEXT_OFFSET			(0x80)
+#define GRCE_XCURCTL_OFFSET			(0xc8)
+#define GRCE_PAL_OFFSET				(0x400)
+//size
+#define GRCELT_RAM_SIZE				(0x400)
+#define GRCE_XCURCOL_SIZE			(0x40)
+#define GRCE_XCURCTL_SIZE			(0x40)
+#define GRCE_CRTC_SIZE				(0x40)
+#define GRCE_CRTCEXT_SIZE			(0x8)
+#define GRCE_SEQ_SIZE				(0x8)
+#define GRCE_GCTL_SIZE				(0x8)
+#define GRCE_ATTR_SIZE				(0x20)
+
+#define GRCELT_RAM				(GRCE_PAL_OFFSET)
+#define GRCE_XCURCTL				(GRCE_XCURCTL_OFFSET)
+#define GRCE_CRTC				(GRCE_CRTC_OFFSET)
+#define GRCE_CRTCEXT				(GRCE_CRTCEXT_OFFSET)
+#define GRCE_SEQ				(GRCE_SEQ_OFFSET)
+#define GRCE_GCTL				(GRCE_GCTL_OFFSET)
+#define GRCE_CTL0				(GRCE_GRCCTL0_OFFSET)
+#define GRCE_STATUS_REGISTER			(GRCE_GRCSTS_OFFSET)
+#define GRCE_ATTR				(GRCE_ATTR_OFFSET)
+#define AST_VIDEO_SCRATCH_34C			(0x8c)
+#define AST_VIDEO_SCRATCH_350			(0x90)
+#define AST_VIDEO_SCRATCH_354			(0x94)
+#define MODE_GET_INFO_DE			(0xA8)
+
+//GRC interrupt
+#define GRC_FIQ_MASK				(0x000003ff)
+#define GRC_IRQ_MASK				(0x000003ff)
+#define GRC_INT_STS_MASK			(0x000003ff)
+#define GRCSTS_XCUR_POS				(1<<9)
+#define GRCSTS_XCUR_DDR				(1<<8)
+#define GRCSTS_XCUR_CTL				(1<<7)
+#define GRCSTS_PLT_RAM				(1<<6)
+#define GRCSTS_XCRTC				(1<<5)
+#define GRCSTS_CRTC				(1<<4)
+#define GRCSTS_GCTL				(1<<3)
+#define GRCSTS_SEQ				(1<<2)
+#define GRCSTS_ATTR1				(1<<1)
+#define GRCSTS_ATTR0				(1<<0)
+#define SNOOP_RESTART (GRCSTS_XCUR_CTL|GRCSTS_XCRTC|GRCSTS_CRTC|GRCSTS_GCTL)
+
+//snoop TSE
+#define SNOOP_TSE_MASK				(0x00000001)
+#define SNOOP_IRQ_MASK				(0x00000100)
+#define SNOOP_FIQ_MASK				(0x00000200)
+#define	TSCMD_SCREEN_OWNER			(1<<15)
+#define TSCMD_PITCH_BIT				(16)
+#define TSCMD_INT_ENBL_BIT			(8)
+#define TSCMD_CPT_BIT				(6)
+#define TSCMD_RPT_BIT				(4)
+#define TSCMD_BPP_BIT				(2)
+#define TSCMD_VGA_MODE_BIT			(1)
+#define TSCMD_TSE_ENBL_BIT			(0)
+#define TSSTS_FIFO_OVFL				(1<<5)
+#define TSSTS_FONT				(1<<4)
+#define TSSTS_ATTR				(1<<3)
+#define TSSTS_ASCII				(1<<2)
+#define TSSTS_TC_SCREEN1			(1<<1)
+#define TSSTS_TC_SCREEN0			(1<<0)
+#define TSSTS_ALL				(0x3f)
+
+
+
+#define TSE_INTR_COUNT				(0xCB700)	//50MHz clock ~1/60 sec
+//#define TSE_INTR_COUNT			(0x196E00)	//50MHz clock ~1/30 sec
+#define TIMER_INTR_COUNT			(0x65000)	// 25MHz clock ~1/60 sec
+
+//Timer
+/* Register byte offsets */
+// AST2600 Timer registers
+#define TIMER_STATUS_BIT(x)			(1 << ((x) - 1))
+
+#define OFFSET_TIMER1         0x00                      /* * timer 1 offset */
+#define OFFSET_TIMER2         0x10                      /* * timer 2 offset */
+#define OFFSET_TIMER3         0x20                      /* * timer 3 offset */
+#define OFFSET_TIMER4         0x40                      /* * timer 4 offset */
+#define OFFSET_TIMER5         0x50                      /* * timer 5 offset */
+#define OFFSET_TIMER6         0x60                      /* * timer 6 offset */
+#define OFFSET_TIMER7         0x70                      /* * timer 7 offset */
+#define OFFSET_TIMER8         0x80                      /* * timer 8 offset */
+
+#define OFF_TIMER_REG_CURR_CNT   0x00
+#define OFF_TIMER_REG_LOAD_CNT   0x04
+#define OFF_TIMER_REG_EO0        0x08                    /* Read to clear interrupt */
+#define OFF_TIMER_REG_EOI        0x0c                    /* Read to clear interrupt */
+#define OFF_TIMER_REG_STAT       0x10                    /* Timer Interrupt Status */
+#define OFF_TIMER_REG_CONTROL    0x30							/* Control Register */
+#define OFF_TIMER_REG_STATUS     0x34							/* Status Register */
+#define OFF_TIMER_REG_CLEAR_CONTROL    0x3C							/* Control Register */
+#define RB_OFF_TIMERS_STAT       0xA0                    /* * timers status offset */
+
+#define CTRL_TIMER1           (0)
+#define CTRL_TIMER2           (4)
+#define CTRL_TIMER3           (8)
+#define CTRL_TIMER4           (12)
+#define CTRL_TIMER5           (16)
+#define CTRL_TIMER6           (20)
+#define CTRL_TIMER7           (24)
+#define CTRL_TIMER8           (28)
+#define BIT_TIMER_ENBL           (1 << 0)
+#define BIT_TIMER_CLK_SEL        (1 << 1)
+#define BIT_INTERRUPT_ENBL       (1 << 2)
+#define BIT_TIMER_STAT           (1 << 0)
+
+#define SNOOP_MAP_QWORD_COUNT			(64)
+#define BSE_UPPER_LIMIT				(0x900000) //(0x540000)
+#define FULL_BUCKETS_COUNT			(16)
+#define MODE13_HEIGHT				(200)
+#define MODE13_WIDTH				(320)
+
+#define NUM_SNOOP_ROWS				(64)
+
+//vga memory information
+#define SCU500						(0x500)
+#define DDR_SIZE_CONFIG_BITS				(0x3)
+#define VGA_MEM_SIZE_CONFIG_BITS			(0x3)
+#define VGA_MEM_SIZE_CONFIG_BIT_POS			(13)
+#define DDR_BASE					(0x80000000)
+
+
+//grce
+#define VGACR0_REG					(0x60)
+#define VGACR9F_REG					(0x9F)
+
+
+struct ContextTable {
+	struct inode *pin;
+	struct file *pf;
+	struct SnoopAggregate sa;
+	u64 aqwSnoopMap[NUM_SNOOP_ROWS];
+	void *rc;
+	struct EventMap emEventWaiting;
+	struct EventMap emEventReceived;
+	u32 dwEventWaitInMs;
+	void *desc_virt;
+	u32 desc_phy;
+};
+
+struct MemoryMapTable {
+	struct file *pf;
+	void *pvVirtualAddr;
+	u32 dwPhysicalAddr;
+	u32 dwLength;
+	u8 byDmaAlloc;
+	u8 byReserved[3];
+};
+
+union EmDwordUnion {
+	struct EventMap em;
+	u32 dw;
+};
+
+struct Descriptor {
+	u32 dw0General;
+	u32 dw1FetchWidthLine;
+	u32 dw2SourceAddr;
+	u32 dw3DestinationAddr;
+};
+
+struct BSEAggregateRegister {
+	u32 dwBSCR;
+	u32 dwBSDBS;
+	u32 adwBSBPS[3];
+};
+
+enum SkipByteMode {
+	NoByteSkip = 0, SkipOneByte = 1, SkipTwoByte = 2, SkipThreeByte = 3
+};
+
+enum StartBytePosition {
+	StartFromByte0 = 0,
+	StartFromByte1 = 1,
+	StartFromByte2 = 2,
+	StartFromByte3 = 3
+};
+
+struct VGAMemInfo {
+	u32 dwVGASize;
+	u32 dwDRAMSize;
+	u32 dwFBPhysStart;
+};
+
+struct VideoDataBufferInfo {
+	u32 dwSize;
+	u32 dwPhys;
+	u32 dwVirt;
+};
+
+enum ColorMode {
+	MODE_EGA = 0x0, //4bpp eg. mode 12/6A
+	MODE_VGA = 0x1, //mode 13
+	MODE_BPP15 = 0x2,
+	MODE_BPP16 = 0x3,
+	MODE_BPP32 = 0x4,
+	MODE_TEXT = 0xE,
+	MODE_CGA = 0xF
+};
+
+struct ModeInfo {
+	u8 byColorMode;
+	u8 byRefreshRateIndex;
+	u8 byModeID;
+	u8 byScanLines;
+};
+
+struct NewModeInfoHeader {
+	u8 byReserved;
+	u8 byDisplayInfo;
+	u8 byColorDepth;
+	u8 byMhzPixelClock;
+};
+
+struct DisplayEnd {
+	u16 HDE;
+	u16 VDE;
+};
+
+struct Resolution {
+	u16 wWidth;
+	u16 wHeight;
+};
+
+struct Video_OsSleepStruct {
+	wait_queue_head_t queue;
+	struct timer_list tim;
+	u8 Timeout;
+};
+
+struct EngineInfo {
+	struct semaphore sem;
+	struct Video_OsSleepStruct wait;
+	u8 finished;
+};
+
+
+struct AstRVAS {
+	struct miscdevice *rvas_dev;
+	void *pdev;
+	int irq_fge;	//FrameGrabber IRQ number
+	int irq_vga; // VGA IRQ number
+	int irq_video;
+	u32 fg_reg_base;
+	u32 grce_reg_base;
+	u32 video_reg_base;
+	struct regmap *scu;
+	struct reset_control *rvas_reset;
+	struct reset_control *video_engine_reset;
+	struct VGAMemInfo FBInfo;
+	u64 accrued_sm[SNOOP_MAP_QWORD_COUNT];
+	struct SnoopAggregate accrued_sa;
+	struct VideoGeometry current_vg;
+	u32 snoop_stride;
+	u32 tse_tsicr;
+	struct EngineInfo tfe_engine;
+	struct EngineInfo bse_engine;
+	struct EngineInfo ldma_engine;
+	struct EngineInfo video_engine;
+	struct semaphore mem_sem;
+	struct semaphore context_sem;
+	struct Video_OsSleepStruct video_wait;
+	u8 video_intr_occurred;
+	u8 timer_irq_requested;
+	u8 reserved[2];
+	struct ContextTable *ppctContextTable[MAX_NUM_CONTEXT];
+	u32 dwMemoryTableSize;
+	u32 dwScreenOffset;
+	struct MemoryMapTable *ppmmtMemoryTable[MAX_NUM_MEM_TBL];
+	struct completion  video_compression_complete;
+	struct completion  video_capture_complete;
+	struct clk *vclk;
+	struct clk *eclk;
+	struct clk *rvasclk;
+};
+
+//
+// IOCTL function
+//
+void ioctl_get_video_geometry(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_wait_for_video_event(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_get_grc_register(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_read_snoop_map(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_read_snoop_aggregate(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_set_tse_tsicr(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_get_tse_tsicr(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_reset_video_engine(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+
+
+void ioctl_fetch_video_tiles(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_fetch_video_slices(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_run_length_encode_data(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_fetch_text_data(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_fetch_mode_13_data(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+u32 get_phy_fb_start_address(struct AstRVAS *ast_rvas);
+bool video_geometry_change(struct AstRVAS *ast_rvas, u32 dwGRCEStatus);
+void update_video_geometry(struct AstRVAS *ast_rvas);
+//interrupts
+void enable_grce_tse_interrupt(struct AstRVAS *ast_rvas);
+void disable_grce_tse_interrupt(struct AstRVAS *ast_rvas);
+u32 clear_tse_interrupt(struct AstRVAS *ast_rvas);
+bool clear_ldma_interrupt(struct AstRVAS *ast_rvas);
+bool clear_tfe_interrupt(struct AstRVAS *ast_rvas);
+bool clear_bse_interrupt(struct AstRVAS *ast_rvas);
+u32 get_screen_offset(struct AstRVAS *ast_rvas);
+//
+void setup_lmem(struct AstRVAS *ast_rvas);
+//
+// helper functions
+//
+
+struct BSEAggregateRegister setUp_bse_bucket(u8 *abyBitIndexes, u8 byTotalBucketCount,
+	u8 byBSBytesPerPixel, u32 dwFetchWidthPixels, u32 dwFetchHeight);
+void prepare_bse_descriptor(struct Descriptor *pDAddress, u32 dwSourceAddress,
+	u32 dwDestAddress, bool bNotLastEntry, u16 wStride, u8 bytesPerPixel,
+	u32 dwFetchWidthPixels, u32 dwFetchHeight,
+	bool bInterrupt);
+
+void prepare_tfe_descriptor(struct Descriptor *pDAddress, u32 dwSourceAddress,
+	u32 dwDestAddress, bool bNotLastEntry, u8 bCheckSum,
+	bool bEnabledRLE, u16 wStride, u8 bytesPerPixel, u32 dwFetchWidthPixels,
+	u32 dwFetchHeight, enum SelectedByteMode sbm,
+	bool bRLEOverFLow, bool bInterrupt);
+void prepare_tfe_text_descriptor(struct Descriptor *pDAddress, u32 dwSourceAddress,
+	u32 dwDestAddress, bool bEnabledRLE, u32 dwFetchWidth,
+	u32 dwFetchHeight, enum DataProccessMode dpm, bool bRLEOverFLow,
+	bool bInterrupt);
+void prepare_ldma_descriptor(struct Descriptor *pDAddress, u32 dwSourceAddress,
+	u32 dwDestAddress, u32 dwLDMASize, u8 byNotLastEntry);
+
+void OnFetchVideoTileChaining(struct RvasIoctl *ri);
+void OnFetchVideoTileNoChainingWithRLE(struct RvasIoctl *ri);
+void WaitWhileEngineBusy(u32 theAddress);
+u8 get_text_mode_character_per_line(struct AstRVAS *ast_rvas, u16 wScreenWidth);
+u16 get_text_mode_fetch_lines(struct AstRVAS *ast_rvas, u16 wScreenHeight);
+void on_fetch_text_data(struct RvasIoctl *ri, bool bRLEOn, struct AstRVAS *ast_rvas);
+
+void reset_snoop_engine(struct AstRVAS *ast_rvas);
+void set_snoop_engine(bool b_geom_chg, struct AstRVAS *ast_rvas);
+u64 reinterpret_32bpp_snoop_row_as_24bpp(u64 theSnoopRow);
+
+void convert_snoop_map(struct AstRVAS *ast_rvas);
+void update_all_snoop_context(struct AstRVAS *ast_rvas);
+void get_snoop_map_data(struct AstRVAS *ast_rvas);
+void get_snoop_aggregate(struct AstRVAS *ast_rvas);
+
+void sleep_on_ldma_busy(struct AstRVAS *ast_rvas, u32 dwAddress);
+bool sleep_on_tfe_busy(struct AstRVAS *ast_rvas, u32 dwTFEDescriptorAddr,
+	u32 dwTFEControlR, u32 dwTFERleLimitor, u32 *pdwRLESize,
+	u32 *pdwCheckSum);
+
+bool sleep_on_tfe_text_busy(struct AstRVAS *ast_rvas, u32 dwTFEDescriptorAddr,
+	u32 dwTFEControlR, u32 dwTFERleLimitor, u32 *pdwRLESize,
+	u32 *pdwCheckSum);
+
+bool sleep_on_bse_busy(struct AstRVAS *ast_rvas, u32 dwBSEDescriptorAddr,
+	struct BSEAggregateRegister aBSEAR, u32 size);
+
+void enable_grce_tse_interrupt(struct AstRVAS *ast_rvas);
+void disable_grce_tse_interrupt(struct AstRVAS *ast_rvas);
+
+void disable_interrupts(struct AstRVAS *ast_rvas);
+void enable_interrupts(struct AstRVAS *ast_rvas);
+
+bool host_suspended(struct AstRVAS *pAstRVAS);
+#endif // __HARDWAREENGINES_H__
diff --git a/drivers/soc/aspeed/rvas/video.h b/drivers/soc/aspeed/rvas/video.h
new file mode 100644
index 000000000000..4f23564b7dfd
--- /dev/null
+++ b/drivers/soc/aspeed/rvas/video.h
@@ -0,0 +1,43 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/******************************************************************************
+ * video.h
+ *
+ * This file is part of the ASPEED Linux Device Driver for ASPEED Baseboard Management Controller.
+ * Refer to the README file included with this package for driver version and adapter compatibility.
+ *
+ * Copyright (C) 2019-2021 ASPEED Technology Inc. All rights reserved.
+ */
+
+#ifndef __RVAS_VIDEO_H__
+#define __RVAS_VIDEO_H__
+
+#define RVAS_DRIVER_NAME "rvas"
+#define Stringify(x) #x
+
+//
+//functions
+//
+
+
+void ioctl_new_context(struct file *file, struct RvasIoctl *pri, struct AstRVAS *pAstRVAS);
+void ioctl_delete_context(struct RvasIoctl *pri, struct AstRVAS *pAstRVAS);
+void ioctl_alloc(struct file *file, struct RvasIoctl *pri, struct AstRVAS *pAstRVAS);
+void ioctl_free(struct RvasIoctl *pri, struct AstRVAS *pAstRVAS);
+void ioctl_update_lms(u8 lms_on, struct AstRVAS *ast_rvas);
+u32 ioctl_get_lm_status(struct AstRVAS *ast_rvas);
+
+
+//void* get_from_rsvd_mem(u32 size, u32 *phys_add, struct AstRVAS *pAstRVAS);
+void *get_virt_add_rsvd_mem(u32 index, struct AstRVAS *pAstRVAS);
+u32 get_phys_add_rsvd_mem(u32 index, struct AstRVAS *pAstRVAS);
+u32 get_len_rsvd_mem(u32 index, struct AstRVAS *pAstRVAS);
+
+//int release_rsvd_mem(u32 size, u32 phys_add);
+bool virt_is_valid_rsvd_mem(u32 index, u32 size, struct AstRVAS *pAstRVAS);
+
+
+struct ContextTable *get_new_context_table_entry(struct AstRVAS *pAstRVAS);
+struct ContextTable *get_context_entry(const void *crc, struct AstRVAS *pAstRVAS);
+bool remove_context_table_entry(const void *crmh, struct AstRVAS *pAstRVAS);
+
+#endif // __RVAS_VIDEO_H__
diff --git a/drivers/soc/aspeed/rvas/video_debug.h b/drivers/soc/aspeed/rvas/video_debug.h
new file mode 100644
index 000000000000..66df480357cc
--- /dev/null
+++ b/drivers/soc/aspeed/rvas/video_debug.h
@@ -0,0 +1,32 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2019-2021  ASPEED Technology Inc.
+ */
+
+#ifndef AST_VIDEO_DEBUG_H_
+#define AST_VIDEO_DEBUG_H_
+
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/fcntl.h>
+
+
+//#define RVAS_VIDEO_DEBUG
+//#define VIDEO_ENGINE_DEBUG
+//#define HARDWARE_ENGINE_DEBUG
+
+
+#ifdef RVAS_VIDEO_DEBUG
+#define VIDEO_DBG(fmt, args...) ({ dev_printk(KERNEL_INFO, pAstRVAS->pdev, "%s() " fmt, __func__, ## args); })
+#else
+#define VIDEO_DBG(fmt, args...) do; while (0)
+#endif // RVAS_VIDEO_DEBUG
+
+#ifdef VIDEO_ENGINE_DEBUG
+#define VIDEO_ENG_DBG(fmt, args...) ({ dev_printk(KERNEL_INFO, pAstRVAS->pdev, "%s() " fmt, __func__, ## args); })
+#else
+#define VIDEO_ENG_DBG(fmt, args...) do; while (0)
+#endif // RVAS_VIDEO_DEBUG
+
+
+#endif // AST_VIDEO_DEBUG_H_
diff --git a/drivers/soc/aspeed/rvas/video_engine.c b/drivers/soc/aspeed/rvas/video_engine.c
new file mode 100644
index 000000000000..7f319fb12565
--- /dev/null
+++ b/drivers/soc/aspeed/rvas/video_engine.c
@@ -0,0 +1,1205 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * File Name     : video_engines.c
+ * Description   : AST2600 video  engines
+ *
+ * Copyright (C) 2019-2021 ASPEED Technology Inc. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+#include <linux/poll.h>
+#include <linux/slab.h>
+#include <linux/sched.h>
+#include <linux/clk.h>
+#include <linux/reset.h>
+
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/platform_device.h>
+#include <linux/types.h>
+#include <linux/interrupt.h>
+#include <linux/mm.h>
+#include <linux/delay.h>
+#include <linux/miscdevice.h>
+#include <linux/hwmon-sysfs.h>
+#include <linux/regmap.h>
+#include <linux/mfd/syscon.h>
+#include <linux/dma-mapping.h>
+#include <asm/io.h>
+#include <linux/of.h>
+#include <linux/of_reserved_mem.h>
+#include <asm/uaccess.h>
+
+
+#include "video_ioctl.h"
+#include "video_engine.h"
+#include "video_debug.h"
+#include "hardware_engines.h"
+
+
+static struct VideoEngineMem vem;
+
+//
+//functions
+//
+
+static inline void video_write(struct AstRVAS *pAstRVAS, u32 val, u32 reg);
+static inline u32 video_read(struct AstRVAS *pAstRVAS, u32 reg);
+
+static u32 get_vga_mem_base(struct AstRVAS *pAstRVAS);
+static int reserve_video_engine_memory(struct AstRVAS *pAstRVAS);
+static void init_jpeg_table(void);
+static void video_set_scaling(struct AstRVAS *pAstRVAS);
+static int video_capture_trigger(struct AstRVAS *pAstRVAS);
+static void dump_buffer(u32 dwPhyStreamAddress, u32 size);
+
+
+
+//
+// function definitions
+//
+void ioctl_get_video_engine_config(struct VideoConfig *pVideoConfig, struct AstRVAS *pAstRVAS)
+{
+	u32 VR004_SeqCtrl = video_read(pAstRVAS, AST_VIDEO_SEQ_CTRL);
+	u32 VR060_ComCtrl = video_read(pAstRVAS, AST_VIDEO_COMPRESS_CTRL);
+
+	// status
+	pVideoConfig->rs = SuccessStatus;
+
+	pVideoConfig->engine = 0;	// engine = 1 is Video Management
+	pVideoConfig->capture_format = 0;
+	pVideoConfig->compression_mode = 0;
+
+	pVideoConfig->compression_format = (VR004_SeqCtrl >> 13) & 0x1;
+	pVideoConfig->YUV420_mode = (VR004_SeqCtrl >> 10) & 0x3;
+	pVideoConfig->AutoMode = (VR004_SeqCtrl >> 5) & 0x1;
+
+	pVideoConfig->rc4_enable = (VR060_ComCtrl >> 5) & 0x1;
+	pVideoConfig->Visual_Lossless = (VR060_ComCtrl >> 16) & 0x1;
+	pVideoConfig->Y_JPEGTableSelector = VIDEO_GET_DCT_LUM(VR060_ComCtrl);
+	pVideoConfig->AdvanceTableSelector = (VR060_ComCtrl >> 27) & 0xf;
+
+}
+
+
+void ioctl_set_video_engine_config(struct VideoConfig  *pVideoConfig, struct AstRVAS *pAstRVAS)
+{
+
+	int i, base = 0;
+	u32 ctrl = 0;	//for VR004, VR204
+	u32 compress_ctrl = 0x00080000;
+	u32 *tlb_table = vem.jpegTable.pVirt;
+
+
+	// status
+	pVideoConfig->rs = SuccessStatus;
+
+	VIDEO_ENG_DBG("\n");
+
+	ctrl = video_read(pAstRVAS, AST_VIDEO_SEQ_CTRL);
+
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_PASS_CTRL) &
+				~(G6_VIDEO_FRAME_CT_MASK | G6_VIDEO_MULTI_JPEG_MODE | G6_VIDEO_MULTI_JPEG_FLAG_MODE), AST_VIDEO_PASS_CTRL);
+
+	ctrl &= ~VIDEO_AUTO_COMPRESS;
+	ctrl |= G5_VIDEO_COMPRESS_JPEG_MODE;
+	ctrl &= ~VIDEO_COMPRESS_FORMAT_MASK; //~(3<<10) bit 4 is set to 0
+
+	if (pVideoConfig->YUV420_mode)
+		ctrl |= VIDEO_COMPRESS_FORMAT(YUV420);
+
+	if (pVideoConfig->rc4_enable)
+		compress_ctrl |= VIDEO_ENCRYP_ENABLE;
+
+	switch (pVideoConfig->compression_mode) {
+	case 0:	//DCT only
+			compress_ctrl |= VIDEO_DCT_ONLY_ENCODE;
+			break;
+	case 1:	//DCT VQ mix 2-color
+			compress_ctrl &= ~(VIDEO_4COLOR_VQ_ENCODE | VIDEO_DCT_ONLY_ENCODE);
+			break;
+	case 2:	//DCT VQ mix 4-color
+			compress_ctrl |= VIDEO_4COLOR_VQ_ENCODE;
+			break;
+	default:
+			dev_err(pAstRVAS->pdev, "unknown compression mode:%d\n", pVideoConfig->compression_mode);
+			break;
+	}
+
+	if (pVideoConfig->Visual_Lossless) {
+		compress_ctrl |= VIDEO_HQ_ENABLE;
+		compress_ctrl |= VIDEO_HQ_DCT_LUM(pVideoConfig->AdvanceTableSelector);
+		compress_ctrl |= VIDEO_HQ_DCT_CHROM((pVideoConfig->AdvanceTableSelector + 16));
+	} else
+		compress_ctrl &= ~VIDEO_HQ_ENABLE;
+
+
+	video_write(pAstRVAS, ctrl, AST_VIDEO_SEQ_CTRL);
+	// we are using chrominance quantization table instead of luminance quantization table
+	video_write(pAstRVAS, compress_ctrl | VIDEO_DCT_LUM(pVideoConfig->Y_JPEGTableSelector) | VIDEO_DCT_CHROM(pVideoConfig->Y_JPEGTableSelector + 16), AST_VIDEO_COMPRESS_CTRL);
+	VIDEO_ENG_DBG("VR04: %#X\n", video_read(pAstRVAS, AST_VIDEO_SEQ_CTRL));
+	VIDEO_ENG_DBG("VR60: %#X\n", video_read(pAstRVAS, AST_VIDEO_COMPRESS_CTRL));
+
+	// chose a table for JPEG or multi-JPEG
+	if (pVideoConfig->compression_format >= 1) {
+		VIDEO_ENG_DBG("Choose a JPEG Table\n");
+		for (i = 0; i < 12; i++) {
+			base = (1024 * i);
+			//base = (256 * i);
+			if (pVideoConfig->YUV420_mode)	//yuv420
+				tlb_table[base + 46] = 0x00220103; //for YUV420 mode
+			else
+				tlb_table[base + 46] = 0x00110103; //for YUV444 mode)
+		}
+	}
+
+	video_set_scaling(pAstRVAS);
+}
+
+//
+void ioctl_get_video_engine_data(struct MultiJpegConfig *pArrayMJConfig, struct AstRVAS *pAstRVAS, u32 dwPhyStreamAddress)
+{
+	u32 yuv_shift;
+	u32 yuv_msk;
+	u32 scan_lines;
+	int timeout = 0;
+	u32 x0;
+	u32 y0;
+	int i = 0;
+	u32 dw_w_h;
+	u32 start_addr;
+	u32 multi_jpeg_data = 0;
+	u32 VR044;
+	u32 nextFrameOffset = 0;
+
+	pArrayMJConfig->rs = SuccessStatus;
+
+	VIDEO_ENG_DBG("\n");
+	VIDEO_ENG_DBG("before Stream buffer:\n");
+	//dump_buffer(dwPhyStreamAddress,100);
+
+	video_write(pAstRVAS, dwPhyStreamAddress, AST_VIDEO_STREAM_BUFF);
+
+	if (host_suspended(pAstRVAS)) {
+		pArrayMJConfig->rs = HostSuspended;
+		VIDEO_ENG_DBG("HostSuspended Timeout\n");
+		return;
+	}
+
+	if (video_capture_trigger(pAstRVAS) == 0) {
+		pArrayMJConfig->rs = CaptureTimedOut;
+		VIDEO_ENG_DBG("Capture Timeout\n");
+		return;
+	}
+	//dump_buffer(dwPhyStreamAddress,100);
+
+	init_completion(&pAstRVAS->video_compression_complete);
+	VIDEO_ENG_DBG("capture complete buffer:\n");
+
+	//dump_buffer(vem.captureBuf0.phy,100);
+	VR044 = video_read(pAstRVAS, AST_VIDEO_SOURCE_BUFF0);
+
+	scan_lines = video_read(pAstRVAS, AST_VIDEO_SOURCE_SCAN_LINE);
+	VIDEO_ENG_DBG("scan_lines: %#x\n", scan_lines);
+
+
+	if (video_read(pAstRVAS, AST_VIDEO_SEQ_CTRL) & VIDEO_COMPRESS_FORMAT(YUV420)) {
+		// YUV 420
+		VIDEO_ENG_DBG("Debug: YUV420\n");
+		yuv_shift = 4;
+		yuv_msk = 0xf;
+	} else {
+		// YUV 444
+		VIDEO_ENG_DBG("Debug: YUV444\n");
+		yuv_shift = 3;
+		yuv_msk = 0x7;
+	}
+
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_PASS_CTRL) | G6_VIDEO_MULTI_JPEG_FLAG_MODE |
+			(G6_VIDEO_JPEG__COUNT(pArrayMJConfig->multi_jpeg_frames - 1) | G6_VIDEO_MULTI_JPEG_MODE), AST_VIDEO_PASS_CTRL);
+
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_BCD_CTRL) & ~VIDEO_BCD_CHG_EN, AST_VIDEO_BCD_CTRL);
+
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_CTRL) | VIDEO_CTRL_ADDRESS_MAP_MULTI_JPEG, AST_VIDEO_CTRL);
+
+	for (i = 0; i < pArrayMJConfig->multi_jpeg_frames; i++) {
+		VIDEO_ENG_DBG("Debug: Before: [%d]: x: %#x y: %#x w: %#x h: %#x\n", i,
+			pArrayMJConfig->frame[i].wXPixels, pArrayMJConfig->frame[i].wYPixels,
+			pArrayMJConfig->frame[i].wWidthPixels, pArrayMJConfig->frame[i].wHeightPixels);
+		x0 = pArrayMJConfig->frame[i].wXPixels;
+		y0 = pArrayMJConfig->frame[i].wYPixels;
+		dw_w_h = SET_FRAME_W_H(pArrayMJConfig->frame[i].wWidthPixels, pArrayMJConfig->frame[i].wHeightPixels);
+
+		start_addr = VR044 + (scan_lines * y0) + ((256 * x0) / (1 << yuv_shift));
+
+		VIDEO_ENG_DBG("VR%x dw_w_h: %#x, VR%x : addr : %#x, x0 %d, y0 %d\n",
+				AST_VIDEO_MULTI_JPEG_SRAM + (8 * i), dw_w_h,
+				AST_VIDEO_MULTI_JPEG_SRAM + (8 * i) + 4, start_addr, x0, y0);
+		video_write(pAstRVAS, dw_w_h, AST_VIDEO_MULTI_JPEG_SRAM + (8 * i));
+		video_write(pAstRVAS, start_addr, AST_VIDEO_MULTI_JPEG_SRAM + (8 * i) + 4);
+	}
+
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_SEQ_CTRL) & ~(VIDEO_CAPTURE_TRIGGER | VIDEO_COMPRESS_FORCE_IDLE | VIDEO_COMPRESS_TRIGGER), AST_VIDEO_SEQ_CTRL);
+
+	//set mode for multi-jpeg mode VR004[5:3]
+	video_write(pAstRVAS, (video_read(pAstRVAS, AST_VIDEO_SEQ_CTRL) & ~VIDEO_AUTO_COMPRESS)
+				| VIDEO_CAPTURE_MULTI_FRAME | G5_VIDEO_COMPRESS_JPEG_MODE, AST_VIDEO_SEQ_CTRL);
+
+	//If CPU is too fast, pleas read back and trigger
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_SEQ_CTRL) | VIDEO_COMPRESS_TRIGGER, AST_VIDEO_SEQ_CTRL);
+	VIDEO_ENG_DBG("wait_for_completion_interruptible_timeout...\n");
+
+	timeout = wait_for_completion_interruptible_timeout(&pAstRVAS->video_compression_complete, HZ / 2);
+
+	if (timeout == 0) {
+		dev_err(pAstRVAS->pdev, "multi compression timeout sts %x\n", video_read(pAstRVAS, AST_VIDEO_INT_STS));
+		pArrayMJConfig->multi_jpeg_frames = 0;
+		pArrayMJConfig->rs = CompressionTimedOut;
+	} else {
+		VIDEO_ENG_DBG("400 %x , 404 %x\n", video_read(pAstRVAS, AST_VIDEO_MULTI_JPEG_SRAM), video_read(pAstRVAS, AST_VIDEO_MULTI_JPEG_SRAM + 4));
+		VIDEO_ENG_DBG("408 %x , 40c %x\n", video_read(pAstRVAS, AST_VIDEO_MULTI_JPEG_SRAM + 8), video_read(pAstRVAS, AST_VIDEO_MULTI_JPEG_SRAM + 0xC));
+		VIDEO_ENG_DBG("done reading 408\n");
+
+		for (i = 0; i < pArrayMJConfig->multi_jpeg_frames; i++) {
+
+			pArrayMJConfig->frame[i].dwOffsetInBytes = nextFrameOffset;
+
+			multi_jpeg_data = video_read(pAstRVAS, AST_VIDEO_MULTI_JPEG_SRAM + (8 * i) + 4);
+			if (multi_jpeg_data & BIT(7)) {
+				pArrayMJConfig->frame[i].dwSizeInBytes = video_read(pAstRVAS, AST_VIDEO_MULTI_JPEG_SRAM + (8 * i)) & 0xffffff;
+				nextFrameOffset = (multi_jpeg_data & ~BIT(7)) >> 1;
+			} else {
+				pArrayMJConfig->frame[i].dwSizeInBytes = 0;
+				nextFrameOffset = 0;
+			}
+			VIDEO_ENG_DBG("[%d] size %d, dwOffsetInBytes %x\n", i, pArrayMJConfig->frame[i].dwSizeInBytes, pArrayMJConfig->frame[i].dwOffsetInBytes);
+		} //for
+	}
+
+	video_write(pAstRVAS, (video_read(pAstRVAS, AST_VIDEO_SEQ_CTRL) & ~(G5_VIDEO_COMPRESS_JPEG_MODE | VIDEO_CAPTURE_MULTI_FRAME))
+			| VIDEO_AUTO_COMPRESS, AST_VIDEO_SEQ_CTRL);
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_PASS_CTRL) &
+			~(G6_VIDEO_FRAME_CT_MASK | G6_VIDEO_MULTI_JPEG_MODE), AST_VIDEO_PASS_CTRL);
+
+	//VIDEO_ENG_DBG("after Stream buffer:\n");
+	//dump_buffer(dwPhyStreamAddress,100);
+}
+
+
+irqreturn_t ast_video_isr(int this_irq, void *dev_id)
+{
+	u32 status;
+	//u32 swap0, swap1;
+	struct AstRVAS *pAstRVAS = dev_id;
+
+	status = video_read(pAstRVAS, AST_VIDEO_INT_STS);
+
+	VIDEO_ENG_DBG("%x\n", status);
+
+
+	if (status & VIDEO_COMPRESS_COMPLETE) {
+		video_write(pAstRVAS, VIDEO_COMPRESS_COMPLETE, AST_VIDEO_INT_STS);
+		VIDEO_ENG_DBG("compress complete swap\n");
+		// no need to swap for better performance
+		// swap0 = video_read(pAstRVAS, AST_VIDEO_SOURCE_BUFF0);
+		// swap1 = video_read(pAstRVAS, AST_VIDEO_SOURCE_BUFF1);
+		// video_write(pAstRVAS, swap1, AST_VIDEO_SOURCE_BUFF0);
+		// video_write(pAstRVAS, swap0, AST_VIDEO_SOURCE_BUFF1);
+		complete(&pAstRVAS->video_compression_complete);
+	}
+	if (status & VIDEO_CAPTURE_COMPLETE) {
+		video_write(pAstRVAS, VIDEO_CAPTURE_COMPLETE, AST_VIDEO_INT_STS);
+		VIDEO_ENG_DBG("capture complete\n");
+		complete(&pAstRVAS->video_capture_complete);
+	}
+
+	return IRQ_HANDLED;
+}
+
+void enable_video_interrupt(struct AstRVAS *pAstRVAS)
+{
+	u32 intCtrReg = video_read(pAstRVAS, AST_VIDEO_INT_EN);
+
+	intCtrReg = (VIDEO_COMPRESS_COMPLETE | VIDEO_CAPTURE_COMPLETE);
+	video_write(pAstRVAS, intCtrReg, AST_VIDEO_INT_EN);
+}
+
+void disable_video_interrupt(struct AstRVAS *pAstRVAS)
+{
+	video_write(pAstRVAS, 0, AST_VIDEO_INT_EN);
+}
+
+void video_engine_rc4Reset(struct AstRVAS *pAstRVAS)
+{
+	//rc4 init reset ..
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_CTRL) | VIDEO_CTRL_RC4_RST, AST_VIDEO_CTRL);
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_CTRL) & ~VIDEO_CTRL_RC4_RST, AST_VIDEO_CTRL);
+}
+
+// setup functions
+int video_engine_reserveMem(struct AstRVAS *pAstRVAS)
+{
+	int result = 0;
+
+	// reserve mem
+	result = reserve_video_engine_memory(pAstRVAS);
+	if (result < 0) {
+		dev_err(pAstRVAS->pdev, "Error Reserving Video Engine Memory\n");
+		return result;
+	}
+	return 0;
+}
+
+
+
+int free_video_engine_memory(struct AstRVAS *pAstRVAS)
+{
+	int size = vem.captureBuf0.size + vem.captureBuf1.size + vem.jpegTable.size;
+
+	if (size && vem.captureBuf0.pVirt) {
+		dma_free_coherent(pAstRVAS->pdev, size,
+				vem.captureBuf0.pVirt,
+				vem.captureBuf0.phy);
+	} else {
+		return -1;
+	}
+	VIDEO_ENG_DBG("After dma_free_coherent\n");
+
+	return 0;
+}
+
+// this function needs to be called when graphic mode change
+void video_set_Window(struct AstRVAS *pAstRVAS)
+{
+	u32 scan_line;
+
+	VIDEO_ENG_DBG("\n");
+
+	//compression x,y
+	video_write(pAstRVAS, VIDEO_COMPRESS_H(pAstRVAS->current_vg.wStride) | VIDEO_COMPRESS_V(pAstRVAS->current_vg.wScreenHeight), AST_VIDEO_COMPRESS_WIN);
+	VIDEO_ENG_DBG("reg offset[%#x]: %#x\n", AST_VIDEO_COMPRESS_WIN, video_read(pAstRVAS, AST_VIDEO_COMPRESS_WIN));
+
+	if (pAstRVAS->current_vg.wStride == 1680)
+		video_write(pAstRVAS, VIDEO_CAPTURE_H(1728) | VIDEO_CAPTURE_V(pAstRVAS->current_vg.wScreenHeight), AST_VIDEO_CAPTURE_WIN);
+	else
+		video_write(pAstRVAS, VIDEO_CAPTURE_H(pAstRVAS->current_vg.wStride) | VIDEO_CAPTURE_V(pAstRVAS->current_vg.wScreenHeight), AST_VIDEO_CAPTURE_WIN);
+
+	VIDEO_ENG_DBG("reg offset[%#x]: %#x\n", AST_VIDEO_CAPTURE_WIN, video_read(pAstRVAS, AST_VIDEO_CAPTURE_WIN));
+
+	// set scan_line VR048
+	if ((pAstRVAS->current_vg.wStride % 8) == 0)
+		video_write(pAstRVAS, pAstRVAS->current_vg.wStride * 4, AST_VIDEO_SOURCE_SCAN_LINE);
+	else {
+		scan_line = pAstRVAS->current_vg.wStride;
+		scan_line = scan_line + 16 - (scan_line % 16);
+		scan_line = scan_line * 4;
+		video_write(pAstRVAS, scan_line, AST_VIDEO_SOURCE_SCAN_LINE);
+	}
+	VIDEO_ENG_DBG("reg offset[%#x]: %#x\n", AST_VIDEO_SOURCE_SCAN_LINE, video_read(pAstRVAS, AST_VIDEO_SOURCE_SCAN_LINE));
+}
+
+void set_direct_mode(struct AstRVAS *pAstRVAS)
+{
+	int Direct_Mode = 0;
+	u32 ColorDepthIndex;
+	u32 VGA_Scratch_Register_350, VGA_Scratch_Register_354, VGA_Scratch_Register_34C, Color_Depth;
+
+	VIDEO_ENG_DBG("\n");
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_PASS_CTRL) & ~(VIDEO_AUTO_FETCH | VIDEO_DIRECT_FETCH), AST_VIDEO_PASS_CTRL);
+
+	VGA_Scratch_Register_350 = video_read(pAstRVAS, AST_VIDEO_E_SCRATCH_350);
+	VGA_Scratch_Register_34C = video_read(pAstRVAS, AST_VIDEO_E_SCRATCH_34C);
+	VGA_Scratch_Register_354 = video_read(pAstRVAS, AST_VIDEO_E_SCRATCH_354);
+
+	if (((VGA_Scratch_Register_350 & 0xff00) >> 8) == 0xA8) {
+
+		Color_Depth = ((VGA_Scratch_Register_350 & 0xff0000) >> 16);
+
+		if (Color_Depth < 15)
+			Direct_Mode = 0;
+		else
+			Direct_Mode = 1;
+
+	} else { //Original mode information
+		ColorDepthIndex = (VGA_Scratch_Register_34C >> 4) & 0x0F;
+
+		if ((ColorDepthIndex == 0xe) || (ColorDepthIndex == 0xf)) {
+			Direct_Mode = 0;
+		} else {
+			if (ColorDepthIndex > 2) {
+				Direct_Mode = 1;
+			} else {
+				Direct_Mode = 0;
+			}
+		}
+	}
+
+	if (Direct_Mode) {
+		VIDEO_ENG_DBG("Direct Mode\n");
+		video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_PASS_CTRL) | VIDEO_AUTO_FETCH | VIDEO_DIRECT_FETCH, AST_VIDEO_PASS_CTRL);
+		video_write(pAstRVAS, get_vga_mem_base(pAstRVAS), AST_VIDEO_DIRECT_BASE);
+		video_write(pAstRVAS, VIDEO_FETCH_TIMING(0) | VIDEO_FETCH_LINE_OFFSET(pAstRVAS->current_vg.wStride * 4), AST_VIDEO_DIRECT_CTRL);
+	} else {
+		VIDEO_ENG_DBG("Sync None Direct Mode\n");
+		video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_PASS_CTRL) & ~(VIDEO_AUTO_FETCH | VIDEO_DIRECT_FETCH), AST_VIDEO_PASS_CTRL);
+	}
+}
+
+// return timeout 0 - timeout; non 0 is successful
+static int video_capture_trigger(struct AstRVAS *pAstRVAS)
+{
+	int timeout = 0;
+
+	VIDEO_ENG_DBG("\n");
+
+	init_completion(&pAstRVAS->video_capture_complete);
+
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_BCD_CTRL) & ~VIDEO_BCD_CHG_EN, AST_VIDEO_BCD_CTRL);
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_SEQ_CTRL) & ~(VIDEO_CAPTURE_TRIGGER | VIDEO_COMPRESS_FORCE_IDLE | VIDEO_COMPRESS_TRIGGER | VIDEO_AUTO_COMPRESS), AST_VIDEO_SEQ_CTRL);
+	//If CPU is too fast, pleas read back and trigger
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_SEQ_CTRL) | VIDEO_CAPTURE_TRIGGER, AST_VIDEO_SEQ_CTRL);
+
+	timeout = wait_for_completion_interruptible_timeout(&pAstRVAS->video_capture_complete, HZ / 2);
+
+	if (timeout == 0)
+		dev_err(pAstRVAS->pdev, "Capture timeout sts %x\n", video_read(pAstRVAS, AST_VIDEO_INT_STS));
+
+	//dump_buffer(vem.captureBuf0.phy, 1024);
+	return timeout;
+}
+
+//
+// static functions
+//
+static u32 get_vga_mem_base(struct AstRVAS *pAstRVAS)
+{
+	u32 vga_mem_size, mem_size;
+
+	mem_size = pAstRVAS->FBInfo.dwDRAMSize;
+	vga_mem_size = pAstRVAS->FBInfo.dwVGASize;
+	VIDEO_ENG_DBG("VGA Info : MEM Size %dMB, VGA Mem Size %dMB\n", mem_size/1024/1024, vga_mem_size/1024/1024);
+	return (mem_size - vga_mem_size);
+}
+
+static void dump_buffer(u32 dwPhyStreamAddress, u32 size)
+{
+	u32 iC;
+	u32 val = 0;
+
+	for (iC = 0; iC < size; iC += 4) {
+		val = readl((void *)(dwPhyStreamAddress + iC));
+		VIDEO_ENG_DBG("%#x, ", val);
+	}
+
+}
+
+
+static void video_set_scaling(struct AstRVAS *pAstRVAS)
+{
+	u32 ctrl = video_read(pAstRVAS, AST_VIDEO_CTRL);
+	//no scaling
+	ctrl &= ~VIDEO_CTRL_DWN_SCALING_MASK;
+
+	VIDEO_ENG_DBG("Scaling Disable\n");
+	video_write(pAstRVAS, 0x00200000, AST_VIDEO_SCALING0);
+	video_write(pAstRVAS, 0x00200000, AST_VIDEO_SCALING1);
+	video_write(pAstRVAS, 0x00200000, AST_VIDEO_SCALING2);
+	video_write(pAstRVAS, 0x00200000, AST_VIDEO_SCALING3);
+
+	video_write(pAstRVAS, 0x10001000, AST_VIDEO_SCAL_FACTOR);
+	video_write(pAstRVAS, ctrl, AST_VIDEO_CTRL);
+
+	video_set_Window(pAstRVAS);
+}
+
+
+
+void video_ctrl_init(struct AstRVAS *pAstRVAS)
+{
+	VIDEO_ENG_DBG("\n");
+	VIDEO_ENG_DBG("reg address: %#x\n", pAstRVAS->video_reg_base);
+	video_write(pAstRVAS, (u32)vem.captureBuf0.phy, AST_VIDEO_SOURCE_BUFF0);//44h
+	video_write(pAstRVAS, (u32)vem.captureBuf1.phy, AST_VIDEO_SOURCE_BUFF1);//4Ch
+	video_write(pAstRVAS, (u32)vem.jpegTable.phy, AST_VIDEO_JPEG_HEADER_BUFF); //40h
+	video_write(pAstRVAS, 0, AST_VIDEO_COMPRESS_READ); //3Ch
+
+	//clr int sts
+	video_write(pAstRVAS, 0xffffffff, AST_VIDEO_INT_STS);
+	video_write(pAstRVAS, 0, AST_VIDEO_BCD_CTRL);
+
+	// =============================  JPEG init ===========================================
+	init_jpeg_table();
+	VIDEO_ENG_DBG("JpegTable in Memory:%#x\n", vem.jpegTable.pVirt);
+	dump_buffer(vem.jpegTable.phy, 80);
+
+	// ===================================================================================
+	//Specification define bit 12:13 must always 0;
+	video_write(pAstRVAS, (video_read(pAstRVAS, AST_VIDEO_PASS_CTRL) &
+								~(VIDEO_DUAL_EDGE_MODE | VIDEO_18BIT_SINGLE_EDGE)) |
+								VIDEO_DVO_INPUT_DELAY(0x4),
+								AST_VIDEO_PASS_CTRL);
+
+	video_write(pAstRVAS, VIDEO_STREAM_PKT_N(STREAM_32_PKTS) |
+					VIDEO_STREAM_PKT_SIZE(STREAM_128KB), AST_VIDEO_STREAM_SIZE);
+
+
+	//rc4 init reset ..
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_CTRL) | VIDEO_CTRL_RC4_RST, AST_VIDEO_CTRL);
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_CTRL) & ~VIDEO_CTRL_RC4_RST, AST_VIDEO_CTRL);
+
+	//CRC/REDUCE_BIT register clear
+	video_write(pAstRVAS, 0, AST_VIDEO_CRC1);
+	video_write(pAstRVAS, 0, AST_VIDEO_CRC2);
+	video_write(pAstRVAS, 0, AST_VIDEO_DATA_TRUNCA);
+	video_write(pAstRVAS, 0, AST_VIDEO_COMPRESS_READ);
+}
+
+
+static int reserve_video_engine_memory(struct AstRVAS *pAstRVAS)
+{
+	u32 size;
+	u32 phys_add = 0;
+	u32 virt_add = 0;
+
+	memset(&vem, 0, sizeof(struct VideoEngineMem));
+	vem.captureBuf0.size = VIDEO_CAPTURE_BUFFER_SIZE; //size 10M
+	vem.captureBuf1.size = VIDEO_CAPTURE_BUFFER_SIZE; //size 10M
+	vem.jpegTable.size =  VIDEO_JPEG_TABLE_SIZE; //size 1M
+
+	size = vem.captureBuf0.size + vem.captureBuf1.size + vem.jpegTable.size;
+	VIDEO_ENG_DBG("Allocating memory size: 0x%x\n", size);
+	virt_add = (u32)dma_alloc_coherent(pAstRVAS->pdev, size, &phys_add,
+				  GFP_KERNEL);
+
+	if (!virt_add) {
+		pr_err("Cannot alloc buffer for video engine\n");
+		return -ENOMEM;
+	}
+
+	vem.captureBuf0.phy =  phys_add;
+	vem.captureBuf1.phy =  phys_add + vem.captureBuf0.size;
+	vem.jpegTable.phy = phys_add + vem.captureBuf0.size + vem.captureBuf1.size;
+
+	vem.captureBuf0.pVirt = (void *)virt_add;
+	vem.captureBuf1.pVirt = (void *)(virt_add + vem.captureBuf0.size);
+	vem.jpegTable.pVirt = (void *)(virt_add + vem.captureBuf0.size + vem.captureBuf1.size);
+
+	VIDEO_ENG_DBG("Allocated: phys: %#x\n", phys_add);
+	VIDEO_ENG_DBG("Phy: Buf0:%#x; Buf1:%#x; jpegT:%#x\n", vem.captureBuf0.phy, vem.captureBuf1.phy, vem.jpegTable.phy);
+	VIDEO_ENG_DBG("Virt: Buf0:%#x; Buf1:%#x; JpegT:%#x\n", vem.captureBuf0.pVirt, vem.captureBuf1.pVirt, vem.jpegTable.pVirt);
+
+	return 0;
+
+}
+
+
+
+/************************************************ JPEG ***************************************************************************************/
+static void init_jpeg_table(void)
+{
+	int i = 0;
+	int base = 0;
+	u32 *tlb_table = vem.jpegTable.pVirt;
+
+	//JPEG header default value:
+	for (i = 0; i < 12; i++) {
+		base = (256 * i);
+		tlb_table[base + 0] = 0xE0FFD8FF;
+		tlb_table[base + 1] = 0x464A1000;
+		tlb_table[base + 2] = 0x01004649;
+		tlb_table[base + 3] = 0x60000101;
+		tlb_table[base + 4] = 0x00006000;
+		tlb_table[base + 5] = 0x0F00FEFF;
+		tlb_table[base + 6] = 0x00002D05;
+		tlb_table[base + 7] = 0x00000000;
+		tlb_table[base + 8] = 0x00000000;
+		tlb_table[base + 9] = 0x00DBFF00;
+		tlb_table[base + 44] = 0x081100C0;
+		tlb_table[base + 45] = 0x00000000;
+		tlb_table[base + 47] = 0x03011102;
+		tlb_table[base + 48] = 0xC4FF0111;
+		tlb_table[base + 49] = 0x00001F00;
+		tlb_table[base + 50] = 0x01010501;
+		tlb_table[base + 51] = 0x01010101;
+		tlb_table[base + 52] = 0x00000000;
+		tlb_table[base + 53] = 0x00000000;
+		tlb_table[base + 54] = 0x04030201;
+		tlb_table[base + 55] = 0x08070605;
+		tlb_table[base + 56] = 0xFF0B0A09;
+		tlb_table[base + 57] = 0x10B500C4;
+		tlb_table[base + 58] = 0x03010200;
+		tlb_table[base + 59] = 0x03040203;
+		tlb_table[base + 60] = 0x04040505;
+		tlb_table[base + 61] = 0x7D010000;
+		tlb_table[base + 62] = 0x00030201;
+		tlb_table[base + 63] = 0x12051104;
+		tlb_table[base + 64] = 0x06413121;
+		tlb_table[base + 65] = 0x07615113;
+		tlb_table[base + 66] = 0x32147122;
+		tlb_table[base + 67] = 0x08A19181;
+		tlb_table[base + 68] = 0xC1B14223;
+		tlb_table[base + 69] = 0xF0D15215;
+		tlb_table[base + 70] = 0x72623324;
+		tlb_table[base + 71] = 0x160A0982;
+		tlb_table[base + 72] = 0x1A191817;
+		tlb_table[base + 73] = 0x28272625;
+		tlb_table[base + 74] = 0x35342A29;
+		tlb_table[base + 75] = 0x39383736;
+		tlb_table[base + 76] = 0x4544433A;
+		tlb_table[base + 77] = 0x49484746;
+		tlb_table[base + 78] = 0x5554534A;
+		tlb_table[base + 79] = 0x59585756;
+		tlb_table[base + 80] = 0x6564635A;
+		tlb_table[base + 81] = 0x69686766;
+		tlb_table[base + 82] = 0x7574736A;
+		tlb_table[base + 83] = 0x79787776;
+		tlb_table[base + 84] = 0x8584837A;
+		tlb_table[base + 85] = 0x89888786;
+		tlb_table[base + 86] = 0x9493928A;
+		tlb_table[base + 87] = 0x98979695;
+		tlb_table[base + 88] = 0xA3A29A99;
+		tlb_table[base + 89] = 0xA7A6A5A4;
+		tlb_table[base + 90] = 0xB2AAA9A8;
+		tlb_table[base + 91] = 0xB6B5B4B3;
+		tlb_table[base + 92] = 0xBAB9B8B7;
+		tlb_table[base + 93] = 0xC5C4C3C2;
+		tlb_table[base + 94] = 0xC9C8C7C6;
+		tlb_table[base + 95] = 0xD4D3D2CA;
+		tlb_table[base + 96] = 0xD8D7D6D5;
+		tlb_table[base + 97] = 0xE2E1DAD9;
+		tlb_table[base + 98] = 0xE6E5E4E3;
+		tlb_table[base + 99] = 0xEAE9E8E7;
+		tlb_table[base + 100] = 0xF4F3F2F1;
+		tlb_table[base + 101] = 0xF8F7F6F5;
+		tlb_table[base + 102] = 0xC4FFFAF9;
+		tlb_table[base + 103] = 0x00011F00;
+		tlb_table[base + 104] = 0x01010103;
+		tlb_table[base + 105] = 0x01010101;
+		tlb_table[base + 106] = 0x00000101;
+		tlb_table[base + 107] = 0x00000000;
+		tlb_table[base + 108] = 0x04030201;
+		tlb_table[base + 109] = 0x08070605;
+		tlb_table[base + 110] = 0xFF0B0A09;
+		tlb_table[base + 111] = 0x11B500C4;
+		tlb_table[base + 112] = 0x02010200;
+		tlb_table[base + 113] = 0x04030404;
+		tlb_table[base + 114] = 0x04040507;
+		tlb_table[base + 115] = 0x77020100;
+		tlb_table[base + 116] = 0x03020100;
+		tlb_table[base + 117] = 0x21050411;
+		tlb_table[base + 118] = 0x41120631;
+		tlb_table[base + 119] = 0x71610751;
+		tlb_table[base + 120] = 0x81322213;
+		tlb_table[base + 121] = 0x91421408;
+		tlb_table[base + 122] = 0x09C1B1A1;
+		tlb_table[base + 123] = 0xF0523323;
+		tlb_table[base + 124] = 0xD1726215;
+		tlb_table[base + 125] = 0x3424160A;
+		tlb_table[base + 126] = 0x17F125E1;
+		tlb_table[base + 127] = 0x261A1918;
+		tlb_table[base + 128] = 0x2A292827;
+		tlb_table[base + 129] = 0x38373635;
+		tlb_table[base + 130] = 0x44433A39;
+		tlb_table[base + 131] = 0x48474645;
+		tlb_table[base + 132] = 0x54534A49;
+		tlb_table[base + 133] = 0x58575655;
+		tlb_table[base + 134] = 0x64635A59;
+		tlb_table[base + 135] = 0x68676665;
+		tlb_table[base + 136] = 0x74736A69;
+		tlb_table[base + 137] = 0x78777675;
+		tlb_table[base + 138] = 0x83827A79;
+		tlb_table[base + 139] = 0x87868584;
+		tlb_table[base + 140] = 0x928A8988;
+		tlb_table[base + 141] = 0x96959493;
+		tlb_table[base + 142] = 0x9A999897;
+		tlb_table[base + 143] = 0xA5A4A3A2;
+		tlb_table[base + 144] = 0xA9A8A7A6;
+		tlb_table[base + 145] = 0xB4B3B2AA;
+		tlb_table[base + 146] = 0xB8B7B6B5;
+		tlb_table[base + 147] = 0xC3C2BAB9;
+		tlb_table[base + 148] = 0xC7C6C5C4;
+		tlb_table[base + 149] = 0xD2CAC9C8;
+		tlb_table[base + 150] = 0xD6D5D4D3;
+		tlb_table[base + 151] = 0xDAD9D8D7;
+		tlb_table[base + 152] = 0xE5E4E3E2;
+		tlb_table[base + 153] = 0xE9E8E7E6;
+		tlb_table[base + 154] = 0xF4F3F2EA;
+		tlb_table[base + 155] = 0xF8F7F6F5;
+		tlb_table[base + 156] = 0xDAFFFAF9;
+		tlb_table[base + 157] = 0x01030C00;
+		tlb_table[base + 158] = 0x03110200;
+		tlb_table[base + 159] = 0x003F0011;
+
+		//Table 0
+		if (i == 0) {
+			tlb_table[base + 10] = 0x0D140043;
+			tlb_table[base + 11] = 0x0C0F110F;
+			tlb_table[base + 12] = 0x11101114;
+			tlb_table[base + 13] = 0x17141516;
+			tlb_table[base + 14] = 0x1E20321E;
+			tlb_table[base + 15] = 0x3D1E1B1B;
+			tlb_table[base + 16] = 0x32242E2B;
+			tlb_table[base + 17] = 0x4B4C3F48;
+			tlb_table[base + 18] = 0x44463F47;
+			tlb_table[base + 19] = 0x61735A50;
+			tlb_table[base + 20] = 0x566C5550;
+			tlb_table[base + 21] = 0x88644644;
+			tlb_table[base + 22] = 0x7A766C65;
+			tlb_table[base + 23] = 0x4D808280;
+			tlb_table[base + 24] = 0x8C978D60;
+			tlb_table[base + 25] = 0x7E73967D;
+			tlb_table[base + 26] = 0xDBFF7B80;
+			tlb_table[base + 27] = 0x1F014300;
+			tlb_table[base + 28] = 0x272D2121;
+			tlb_table[base + 29] = 0x3030582D;
+			tlb_table[base + 30] = 0x697BB958;
+			tlb_table[base + 31] = 0xB8B9B97B;
+			tlb_table[base + 32] = 0xB9B8A6A6;
+			tlb_table[base + 33] = 0xB9B9B9B9;
+			tlb_table[base + 34] = 0xB9B9B9B9;
+			tlb_table[base + 35] = 0xB9B9B9B9;
+			tlb_table[base + 36] = 0xB9B9B9B9;
+			tlb_table[base + 37] = 0xB9B9B9B9;
+			tlb_table[base + 38] = 0xB9B9B9B9;
+			tlb_table[base + 39] = 0xB9B9B9B9;
+			tlb_table[base + 40] = 0xB9B9B9B9;
+			tlb_table[base + 41] = 0xB9B9B9B9;
+			tlb_table[base + 42] = 0xB9B9B9B9;
+			tlb_table[base + 43] = 0xFFB9B9B9;
+		}
+		//Table 1
+		if (i == 1) {
+			tlb_table[base + 10] = 0x0C110043;
+			tlb_table[base + 11] = 0x0A0D0F0D;
+			tlb_table[base + 12] = 0x0F0E0F11;
+			tlb_table[base + 13] = 0x14111213;
+			tlb_table[base + 14] = 0x1A1C2B1A;
+			tlb_table[base + 15] = 0x351A1818;
+			tlb_table[base + 16] = 0x2B1F2826;
+			tlb_table[base + 17] = 0x4142373F;
+			tlb_table[base + 18] = 0x3C3D373E;
+			tlb_table[base + 19] = 0x55644E46;
+			tlb_table[base + 20] = 0x4B5F4A46;
+			tlb_table[base + 21] = 0x77573D3C;
+			tlb_table[base + 22] = 0x6B675F58;
+			tlb_table[base + 23] = 0x43707170;
+			tlb_table[base + 24] = 0x7A847B54;
+			tlb_table[base + 25] = 0x6E64836D;
+			tlb_table[base + 26] = 0xDBFF6C70;
+			tlb_table[base + 27] = 0x1B014300;
+			tlb_table[base + 28] = 0x22271D1D;
+			tlb_table[base + 29] = 0x2A2A4C27;
+			tlb_table[base + 30] = 0x5B6BA04C;
+			tlb_table[base + 31] = 0xA0A0A06B;
+			tlb_table[base + 32] = 0xA0A0A0A0;
+			tlb_table[base + 33] = 0xA0A0A0A0;
+			tlb_table[base + 34] = 0xA0A0A0A0;
+			tlb_table[base + 35] = 0xA0A0A0A0;
+			tlb_table[base + 36] = 0xA0A0A0A0;
+			tlb_table[base + 37] = 0xA0A0A0A0;
+			tlb_table[base + 38] = 0xA0A0A0A0;
+			tlb_table[base + 39] = 0xA0A0A0A0;
+			tlb_table[base + 40] = 0xA0A0A0A0;
+			tlb_table[base + 41] = 0xA0A0A0A0;
+			tlb_table[base + 42] = 0xA0A0A0A0;
+			tlb_table[base + 43] = 0xFFA0A0A0;
+		}
+		//Table 2
+		if (i == 2) {
+			tlb_table[base + 10] = 0x090E0043;
+			tlb_table[base + 11] = 0x090A0C0A;
+			tlb_table[base + 12] = 0x0C0B0C0E;
+			tlb_table[base + 13] = 0x110E0F10;
+			tlb_table[base + 14] = 0x15172415;
+			tlb_table[base + 15] = 0x2C151313;
+			tlb_table[base + 16] = 0x241A211F;
+			tlb_table[base + 17] = 0x36372E34;
+			tlb_table[base + 18] = 0x31322E33;
+			tlb_table[base + 19] = 0x4653413A;
+			tlb_table[base + 20] = 0x3E4E3D3A;
+			tlb_table[base + 21] = 0x62483231;
+			tlb_table[base + 22] = 0x58564E49;
+			tlb_table[base + 23] = 0x385D5E5D;
+			tlb_table[base + 24] = 0x656D6645;
+			tlb_table[base + 25] = 0x5B536C5A;
+			tlb_table[base + 26] = 0xDBFF595D;
+			tlb_table[base + 27] = 0x16014300;
+			tlb_table[base + 28] = 0x1C201818;
+			tlb_table[base + 29] = 0x22223F20;
+			tlb_table[base + 30] = 0x4B58853F;
+			tlb_table[base + 31] = 0x85858558;
+			tlb_table[base + 32] = 0x85858585;
+			tlb_table[base + 33] = 0x85858585;
+			tlb_table[base + 34] = 0x85858585;
+			tlb_table[base + 35] = 0x85858585;
+			tlb_table[base + 36] = 0x85858585;
+			tlb_table[base + 37] = 0x85858585;
+			tlb_table[base + 38] = 0x85858585;
+			tlb_table[base + 39] = 0x85858585;
+			tlb_table[base + 40] = 0x85858585;
+			tlb_table[base + 41] = 0x85858585;
+			tlb_table[base + 42] = 0x85858585;
+			tlb_table[base + 43] = 0xFF858585;
+		}
+		//Table 3
+		if (i == 3) {
+			tlb_table[base + 10] = 0x070B0043;
+			tlb_table[base + 11] = 0x07080A08;
+			tlb_table[base + 12] = 0x0A090A0B;
+			tlb_table[base + 13] = 0x0D0B0C0C;
+			tlb_table[base + 14] = 0x11121C11;
+			tlb_table[base + 15] = 0x23110F0F;
+			tlb_table[base + 16] = 0x1C141A19;
+			tlb_table[base + 17] = 0x2B2B2429;
+			tlb_table[base + 18] = 0x27282428;
+			tlb_table[base + 19] = 0x3842332E;
+			tlb_table[base + 20] = 0x313E302E;
+			tlb_table[base + 21] = 0x4E392827;
+			tlb_table[base + 22] = 0x46443E3A;
+			tlb_table[base + 23] = 0x2C4A4A4A;
+			tlb_table[base + 24] = 0x50565137;
+			tlb_table[base + 25] = 0x48425647;
+			tlb_table[base + 26] = 0xDBFF474A;
+			tlb_table[base + 27] = 0x12014300;
+			tlb_table[base + 28] = 0x161A1313;
+			tlb_table[base + 29] = 0x1C1C331A;
+			tlb_table[base + 30] = 0x3D486C33;
+			tlb_table[base + 31] = 0x6C6C6C48;
+			tlb_table[base + 32] = 0x6C6C6C6C;
+			tlb_table[base + 33] = 0x6C6C6C6C;
+			tlb_table[base + 34] = 0x6C6C6C6C;
+			tlb_table[base + 35] = 0x6C6C6C6C;
+			tlb_table[base + 36] = 0x6C6C6C6C;
+			tlb_table[base + 37] = 0x6C6C6C6C;
+			tlb_table[base + 38] = 0x6C6C6C6C;
+			tlb_table[base + 39] = 0x6C6C6C6C;
+			tlb_table[base + 40] = 0x6C6C6C6C;
+			tlb_table[base + 41] = 0x6C6C6C6C;
+			tlb_table[base + 42] = 0x6C6C6C6C;
+			tlb_table[base + 43] = 0xFF6C6C6C;
+		}
+		//Table 4
+		if (i == 4) {
+			tlb_table[base + 10] = 0x06090043;
+			tlb_table[base + 11] = 0x05060706;
+			tlb_table[base + 12] = 0x07070709;
+			tlb_table[base + 13] = 0x0A09090A;
+			tlb_table[base + 14] = 0x0D0E160D;
+			tlb_table[base + 15] = 0x1B0D0C0C;
+			tlb_table[base + 16] = 0x16101413;
+			tlb_table[base + 17] = 0x21221C20;
+			tlb_table[base + 18] = 0x1E1F1C20;
+			tlb_table[base + 19] = 0x2B332824;
+			tlb_table[base + 20] = 0x26302624;
+			tlb_table[base + 21] = 0x3D2D1F1E;
+			tlb_table[base + 22] = 0x3735302D;
+			tlb_table[base + 23] = 0x22393A39;
+			tlb_table[base + 24] = 0x3F443F2B;
+			tlb_table[base + 25] = 0x38334338;
+			tlb_table[base + 26] = 0xDBFF3739;
+			tlb_table[base + 27] = 0x0D014300;
+			tlb_table[base + 28] = 0x11130E0E;
+			tlb_table[base + 29] = 0x15152613;
+			tlb_table[base + 30] = 0x2D355026;
+			tlb_table[base + 31] = 0x50505035;
+			tlb_table[base + 32] = 0x50505050;
+			tlb_table[base + 33] = 0x50505050;
+			tlb_table[base + 34] = 0x50505050;
+			tlb_table[base + 35] = 0x50505050;
+			tlb_table[base + 36] = 0x50505050;
+			tlb_table[base + 37] = 0x50505050;
+			tlb_table[base + 38] = 0x50505050;
+			tlb_table[base + 39] = 0x50505050;
+			tlb_table[base + 40] = 0x50505050;
+			tlb_table[base + 41] = 0x50505050;
+			tlb_table[base + 42] = 0x50505050;
+			tlb_table[base + 43] = 0xFF505050;
+		}
+		//Table 5
+		if (i == 5) {
+			tlb_table[base + 10] = 0x04060043;
+			tlb_table[base + 11] = 0x03040504;
+			tlb_table[base + 12] = 0x05040506;
+			tlb_table[base + 13] = 0x07060606;
+			tlb_table[base + 14] = 0x09090F09;
+			tlb_table[base + 15] = 0x12090808;
+			tlb_table[base + 16] = 0x0F0A0D0D;
+			tlb_table[base + 17] = 0x16161315;
+			tlb_table[base + 18] = 0x14151315;
+			tlb_table[base + 19] = 0x1D221B18;
+			tlb_table[base + 20] = 0x19201918;
+			tlb_table[base + 21] = 0x281E1514;
+			tlb_table[base + 22] = 0x2423201E;
+			tlb_table[base + 23] = 0x17262726;
+			tlb_table[base + 24] = 0x2A2D2A1C;
+			tlb_table[base + 25] = 0x25222D25;
+			tlb_table[base + 26] = 0xDBFF2526;
+			tlb_table[base + 27] = 0x09014300;
+			tlb_table[base + 28] = 0x0B0D0A0A;
+			tlb_table[base + 29] = 0x0E0E1A0D;
+			tlb_table[base + 30] = 0x1F25371A;
+			tlb_table[base + 31] = 0x37373725;
+			tlb_table[base + 32] = 0x37373737;
+			tlb_table[base + 33] = 0x37373737;
+			tlb_table[base + 34] = 0x37373737;
+			tlb_table[base + 35] = 0x37373737;
+			tlb_table[base + 36] = 0x37373737;
+			tlb_table[base + 37] = 0x37373737;
+			tlb_table[base + 38] = 0x37373737;
+			tlb_table[base + 39] = 0x37373737;
+			tlb_table[base + 40] = 0x37373737;
+			tlb_table[base + 41] = 0x37373737;
+			tlb_table[base + 42] = 0x37373737;
+			tlb_table[base + 43] = 0xFF373737;
+		}
+		//Table 6
+		if (i == 6) {
+			tlb_table[base + 10] = 0x02030043;
+			tlb_table[base + 11] = 0x01020202;
+			tlb_table[base + 12] = 0x02020203;
+			tlb_table[base + 13] = 0x03030303;
+			tlb_table[base + 14] = 0x04040704;
+			tlb_table[base + 15] = 0x09040404;
+			tlb_table[base + 16] = 0x07050606;
+			tlb_table[base + 17] = 0x0B0B090A;
+			tlb_table[base + 18] = 0x0A0A090A;
+			tlb_table[base + 19] = 0x0E110D0C;
+			tlb_table[base + 20] = 0x0C100C0C;
+			tlb_table[base + 21] = 0x140F0A0A;
+			tlb_table[base + 22] = 0x1211100F;
+			tlb_table[base + 23] = 0x0B131313;
+			tlb_table[base + 24] = 0x1516150E;
+			tlb_table[base + 25] = 0x12111612;
+			tlb_table[base + 26] = 0xDBFF1213;
+			tlb_table[base + 27] = 0x04014300;
+			tlb_table[base + 28] = 0x05060505;
+			tlb_table[base + 29] = 0x07070D06;
+			tlb_table[base + 30] = 0x0F121B0D;
+			tlb_table[base + 31] = 0x1B1B1B12;
+			tlb_table[base + 32] = 0x1B1B1B1B;
+			tlb_table[base + 33] = 0x1B1B1B1B;
+			tlb_table[base + 34] = 0x1B1B1B1B;
+			tlb_table[base + 35] = 0x1B1B1B1B;
+			tlb_table[base + 36] = 0x1B1B1B1B;
+			tlb_table[base + 37] = 0x1B1B1B1B;
+			tlb_table[base + 38] = 0x1B1B1B1B;
+			tlb_table[base + 39] = 0x1B1B1B1B;
+			tlb_table[base + 40] = 0x1B1B1B1B;
+			tlb_table[base + 41] = 0x1B1B1B1B;
+			tlb_table[base + 42] = 0x1B1B1B1B;
+			tlb_table[base + 43] = 0xFF1B1B1B;
+		}
+		//Table 7
+		if (i == 7) {
+			tlb_table[base + 10] = 0x01020043;
+			tlb_table[base + 11] = 0x01010101;
+			tlb_table[base + 12] = 0x01010102;
+			tlb_table[base + 13] = 0x02020202;
+			tlb_table[base + 14] = 0x03030503;
+			tlb_table[base + 15] = 0x06030202;
+			tlb_table[base + 16] = 0x05030404;
+			tlb_table[base + 17] = 0x07070607;
+			tlb_table[base + 18] = 0x06070607;
+			tlb_table[base + 19] = 0x090B0908;
+			tlb_table[base + 20] = 0x080A0808;
+			tlb_table[base + 21] = 0x0D0A0706;
+			tlb_table[base + 22] = 0x0C0B0A0A;
+			tlb_table[base + 23] = 0x070C0D0C;
+			tlb_table[base + 24] = 0x0E0F0E09;
+			tlb_table[base + 25] = 0x0C0B0F0C;
+			tlb_table[base + 26] = 0xDBFF0C0C;
+			tlb_table[base + 27] = 0x03014300;
+			tlb_table[base + 28] = 0x03040303;
+			tlb_table[base + 29] = 0x04040804;
+			tlb_table[base + 30] = 0x0A0C1208;
+			tlb_table[base + 31] = 0x1212120C;
+			tlb_table[base + 32] = 0x12121212;
+			tlb_table[base + 33] = 0x12121212;
+			tlb_table[base + 34] = 0x12121212;
+			tlb_table[base + 35] = 0x12121212;
+			tlb_table[base + 36] = 0x12121212;
+			tlb_table[base + 37] = 0x12121212;
+			tlb_table[base + 38] = 0x12121212;
+			tlb_table[base + 39] = 0x12121212;
+			tlb_table[base + 40] = 0x12121212;
+			tlb_table[base + 41] = 0x12121212;
+			tlb_table[base + 42] = 0x12121212;
+			tlb_table[base + 43] = 0xFF121212;
+		}
+		//Table 8
+		if (i == 8) {
+			tlb_table[base + 10] = 0x01020043;
+			tlb_table[base + 11] = 0x01010101;
+			tlb_table[base + 12] = 0x01010102;
+			tlb_table[base + 13] = 0x02020202;
+			tlb_table[base + 14] = 0x03030503;
+			tlb_table[base + 15] = 0x06030202;
+			tlb_table[base + 16] = 0x05030404;
+			tlb_table[base + 17] = 0x07070607;
+			tlb_table[base + 18] = 0x06070607;
+			tlb_table[base + 19] = 0x090B0908;
+			tlb_table[base + 20] = 0x080A0808;
+			tlb_table[base + 21] = 0x0D0A0706;
+			tlb_table[base + 22] = 0x0C0B0A0A;
+			tlb_table[base + 23] = 0x070C0D0C;
+			tlb_table[base + 24] = 0x0E0F0E09;
+			tlb_table[base + 25] = 0x0C0B0F0C;
+			tlb_table[base + 26] = 0xDBFF0C0C;
+			tlb_table[base + 27] = 0x02014300;
+			tlb_table[base + 28] = 0x03030202;
+			tlb_table[base + 29] = 0x04040703;
+			tlb_table[base + 30] = 0x080A0F07;
+			tlb_table[base + 31] = 0x0F0F0F0A;
+			tlb_table[base + 32] = 0x0F0F0F0F;
+			tlb_table[base + 33] = 0x0F0F0F0F;
+			tlb_table[base + 34] = 0x0F0F0F0F;
+			tlb_table[base + 35] = 0x0F0F0F0F;
+			tlb_table[base + 36] = 0x0F0F0F0F;
+			tlb_table[base + 37] = 0x0F0F0F0F;
+			tlb_table[base + 38] = 0x0F0F0F0F;
+			tlb_table[base + 39] = 0x0F0F0F0F;
+			tlb_table[base + 40] = 0x0F0F0F0F;
+			tlb_table[base + 41] = 0x0F0F0F0F;
+			tlb_table[base + 42] = 0x0F0F0F0F;
+			tlb_table[base + 43] = 0xFF0F0F0F;
+		}
+		//Table 9
+		if (i == 9) {
+			tlb_table[base + 10] = 0x01010043;
+			tlb_table[base + 11] = 0x01010101;
+			tlb_table[base + 12] = 0x01010101;
+			tlb_table[base + 13] = 0x01010101;
+			tlb_table[base + 14] = 0x02020302;
+			tlb_table[base + 15] = 0x04020202;
+			tlb_table[base + 16] = 0x03020303;
+			tlb_table[base + 17] = 0x05050405;
+			tlb_table[base + 18] = 0x05050405;
+			tlb_table[base + 19] = 0x07080606;
+			tlb_table[base + 20] = 0x06080606;
+			tlb_table[base + 21] = 0x0A070505;
+			tlb_table[base + 22] = 0x09080807;
+			tlb_table[base + 23] = 0x05090909;
+			tlb_table[base + 24] = 0x0A0B0A07;
+			tlb_table[base + 25] = 0x09080B09;
+			tlb_table[base + 26] = 0xDBFF0909;
+			tlb_table[base + 27] = 0x02014300;
+			tlb_table[base + 28] = 0x02030202;
+			tlb_table[base + 29] = 0x03030503;
+			tlb_table[base + 30] = 0x07080C05;
+			tlb_table[base + 31] = 0x0C0C0C08;
+			tlb_table[base + 32] = 0x0C0C0C0C;
+			tlb_table[base + 33] = 0x0C0C0C0C;
+			tlb_table[base + 34] = 0x0C0C0C0C;
+			tlb_table[base + 35] = 0x0C0C0C0C;
+			tlb_table[base + 36] = 0x0C0C0C0C;
+			tlb_table[base + 37] = 0x0C0C0C0C;
+			tlb_table[base + 38] = 0x0C0C0C0C;
+			tlb_table[base + 39] = 0x0C0C0C0C;
+			tlb_table[base + 40] = 0x0C0C0C0C;
+			tlb_table[base + 41] = 0x0C0C0C0C;
+			tlb_table[base + 42] = 0x0C0C0C0C;
+			tlb_table[base + 43] = 0xFF0C0C0C;
+		}
+		//Table 10
+		if (i == 10) {
+			tlb_table[base + 10] = 0x01010043;
+			tlb_table[base + 11] = 0x01010101;
+			tlb_table[base + 12] = 0x01010101;
+			tlb_table[base + 13] = 0x01010101;
+			tlb_table[base + 14] = 0x01010201;
+			tlb_table[base + 15] = 0x03010101;
+			tlb_table[base + 16] = 0x02010202;
+			tlb_table[base + 17] = 0x03030303;
+			tlb_table[base + 18] = 0x03030303;
+			tlb_table[base + 19] = 0x04050404;
+			tlb_table[base + 20] = 0x04050404;
+			tlb_table[base + 21] = 0x06050303;
+			tlb_table[base + 22] = 0x06050505;
+			tlb_table[base + 23] = 0x03060606;
+			tlb_table[base + 24] = 0x07070704;
+			tlb_table[base + 25] = 0x06050706;
+			tlb_table[base + 26] = 0xDBFF0606;
+			tlb_table[base + 27] = 0x01014300;
+			tlb_table[base + 28] = 0x01020101;
+			tlb_table[base + 29] = 0x02020402;
+			tlb_table[base + 30] = 0x05060904;
+			tlb_table[base + 31] = 0x09090906;
+			tlb_table[base + 32] = 0x09090909;
+			tlb_table[base + 33] = 0x09090909;
+			tlb_table[base + 34] = 0x09090909;
+			tlb_table[base + 35] = 0x09090909;
+			tlb_table[base + 36] = 0x09090909;
+			tlb_table[base + 37] = 0x09090909;
+			tlb_table[base + 38] = 0x09090909;
+			tlb_table[base + 39] = 0x09090909;
+			tlb_table[base + 40] = 0x09090909;
+			tlb_table[base + 41] = 0x09090909;
+			tlb_table[base + 42] = 0x09090909;
+			tlb_table[base + 43] = 0xFF090909;
+		}
+		//Table 11
+		if (i == 11) {
+			tlb_table[base + 10] = 0x01010043;
+			tlb_table[base + 11] = 0x01010101;
+			tlb_table[base + 12] = 0x01010101;
+			tlb_table[base + 13] = 0x01010101;
+			tlb_table[base + 14] = 0x01010101;
+			tlb_table[base + 15] = 0x01010101;
+			tlb_table[base + 16] = 0x01010101;
+			tlb_table[base + 17] = 0x01010101;
+			tlb_table[base + 18] = 0x01010101;
+			tlb_table[base + 19] = 0x02020202;
+			tlb_table[base + 20] = 0x02020202;
+			tlb_table[base + 21] = 0x03020101;
+			tlb_table[base + 22] = 0x03020202;
+			tlb_table[base + 23] = 0x01030303;
+			tlb_table[base + 24] = 0x03030302;
+			tlb_table[base + 25] = 0x03020303;
+			tlb_table[base + 26] = 0xDBFF0403;
+			tlb_table[base + 27] = 0x01014300;
+			tlb_table[base + 28] = 0x01010101;
+			tlb_table[base + 29] = 0x01010201;
+			tlb_table[base + 30] = 0x03040602;
+			tlb_table[base + 31] = 0x06060604;
+			tlb_table[base + 32] = 0x06060606;
+			tlb_table[base + 33] = 0x06060606;
+			tlb_table[base + 34] = 0x06060606;
+			tlb_table[base + 35] = 0x06060606;
+			tlb_table[base + 36] = 0x06060606;
+			tlb_table[base + 37] = 0x06060606;
+			tlb_table[base + 38] = 0x06060606;
+			tlb_table[base + 39] = 0x06060606;
+			tlb_table[base + 40] = 0x06060606;
+			tlb_table[base + 41] = 0x06060606;
+			tlb_table[base + 42] = 0x06060606;
+			tlb_table[base + 43] = 0xFF060606;
+		}
+	}
+
+
+}
+
+static inline void
+video_write(struct AstRVAS *pAstRVAS, u32 val, u32 reg)
+{
+	VIDEO_ENG_DBG("write offset: %x, val: %x\n", reg, val);
+	//Video is lock after reset, need always unlock
+	//unlock
+	writel(VIDEO_PROTECT_UNLOCK, (void *)pAstRVAS->video_reg_base);
+	writel(val, (void *)(pAstRVAS->video_reg_base + reg));
+
+}
+
+static inline u32
+video_read(struct AstRVAS *pAstRVAS, u32 reg)
+{
+	u32 val = readl((void *)(pAstRVAS->video_reg_base + reg));
+
+	VIDEO_ENG_DBG("read offset: %x, val: %x\n", reg, val);
+	return val;
+}
+
diff --git a/drivers/soc/aspeed/rvas/video_engine.h b/drivers/soc/aspeed/rvas/video_engine.h
new file mode 100644
index 000000000000..0b6670816fa3
--- /dev/null
+++ b/drivers/soc/aspeed/rvas/video_engine.h
@@ -0,0 +1,293 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * File Name     : video_engines.h
+ * Description   : AST2600 video  engines
+ *
+ * Copyright (C) 2019-2021 ASPEED Technology Inc. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+#ifndef __VIDEO_ENGINE_H__
+#define __VIDEO_ENGINE_H__
+
+#include "video_ioctl.h"
+#include "hardware_engines.h"
+
+
+#define VIDEO_STREAM_BUFFER_SIZE	(0x400000) //4M
+#define VIDEO_CAPTURE_BUFFER_SIZE	(0xA00000) //10M
+#define VIDEO_JPEG_TABLE_SIZE		(0x100000) //1M
+
+#define SCU_VIDEO_ENGINE_BIT						BIT(6)
+#define SCU_VIDEO_CAPTURE_STOP_CLOCK_BIT			BIT(3)
+#define SCU_VIDEO_ENGINE_STOP_CLOCK_BIT				BIT(1)
+/***********************************************************************/
+/* Register for VIDEO */
+#define AST_VIDEO_PROTECT			0x000		/*	protection key register	*/
+#define AST_VIDEO_SEQ_CTRL			0x004		/*	Video Sequence Control register	*/
+#define AST_VIDEO_PASS_CTRL		0x008		/*	Video Pass 1 Control register	*/
+
+//VR008[5]=1
+#define AST_VIDEO_DIRECT_BASE		0x00C		/*	Video Direct Frame buffer mode control Register VR008[5]=1 */
+#define AST_VIDEO_DIRECT_CTRL		0x010		/*	Video Direct Frame buffer mode control Register VR008[5]=1 */
+
+//VR008[5]=0
+#define AST_VIDEO_TIMING_H			0x00C		/*	Video Timing Generation Setting Register */
+#define AST_VIDEO_TIMING_V			0x010		/*	Video Timing Generation Setting Register */
+#define AST_VIDEO_SCAL_FACTOR		0x014		/*	Video Scaling Factor Register */
+
+#define AST_VIDEO_SCALING0		0x018		/*	Video Scaling Filter Parameter Register #0 */
+#define AST_VIDEO_SCALING1		0x01C		/*	Video Scaling Filter Parameter Register #1 */
+#define AST_VIDEO_SCALING2		0x020		/*	Video Scaling Filter Parameter Register #2 */
+#define AST_VIDEO_SCALING3		0x024		/*	Video Scaling Filter Parameter Register #3 */
+
+
+#define AST_VIDEO_BCD_CTRL			0x02C		/*	Video BCD Control Register */
+#define AST_VIDEO_CAPTURE_WIN		0x030		/*	 Video Capturing Window Setting Register */
+#define AST_VIDEO_COMPRESS_WIN	0x034		/*	 Video Compression Window Setting Register */
+
+
+
+#define AST_VIDEO_COMPRESS_PRO	0x038		/* Video Compression Stream Buffer Processing Offset Register */
+#define AST_VIDEO_COMPRESS_READ	0x03C		/* Video Compression Stream Buffer Read Offset Register */
+
+#define AST_VIDEO_JPEG_HEADER_BUFF	0x040		/*	Video Based Address of JPEG Header Buffer Register */
+#define AST_VIDEO_SOURCE_BUFF0		0x044		/*	Video Based Address of Video Source Buffer #1 Register */
+#define AST_VIDEO_SOURCE_SCAN_LINE	0x048		/*	Video Scan Line Offset of Video Source Buffer Register */
+#define AST_VIDEO_SOURCE_BUFF1		0x04C		/*	Video Based Address of Video Source Buffer #2 Register */
+#define AST_VIDEO_BCD_BUFF				0x050		/*	Video Base Address of BCD Flag Buffer Register */
+#define AST_VIDEO_STREAM_BUFF			0x054		/*	Video Base Address of Compressed Video Stream Buffer Register */
+#define AST_VIDEO_STREAM_SIZE			0x058		/*	Video Stream Buffer Size Register */
+
+
+#define AST_VIDEO_COMPRESS_CTRL				0x060		/* Video Compression Control Register */
+
+#define AST_VIDEO_COMPRESS_DATA_COUNT		0x070		/* Video Total Size of Compressed Video Stream Read Back Register */
+#define AST_VIDEO_COMPRESS_BLOCK_COUNT		0x074		/* Video Total Number of Compressed Video Block Read Back Register */
+#define AST_VIDEO_COMPRESS_FRAME_END		0x078		/* Video Frame-end offset of compressed video stream buffer read back Register */
+
+
+#define AST_VIDEO_CTRL			0x300		/* Video Control Register */
+#define AST_VIDEO_INT_EN		0x304		/* Video interrupt Enable */
+#define AST_VIDEO_INT_STS		0x308		/* Video interrupt status */
+#define AST_VIDEO_MODE_DETECT	0x30C		/* Video Mode Detection Parameter Register */
+
+#define AST_VIDEO_CRC1			0x320		/* Primary CRC Parameter Register */
+#define AST_VIDEO_CRC2			0x324		/* Second CRC Parameter Register */
+#define AST_VIDEO_DATA_TRUNCA	0x328		/* Video Data Truncation Register */
+
+#define AST_VIDEO_E_SCRATCH_34C	0x34C		/* Video Scratch Remap Read Back */
+#define AST_VIDEO_E_SCRATCH_350	0x350		/* Video Scratch Remap Read Back */
+#define AST_VIDEO_E_SCRATCH_354	0x354		/* Video Scratch Remap Read Back */
+
+//multi jpeg
+#define AST_VIDEO_ENCRYPT_SRAM		0x400		/* Video RC4/AES128 Encryption Key Register #0 ~ #63 */
+#define AST_VIDEO_MULTI_JPEG_SRAM	(AST_VIDEO_ENCRYPT_SRAM)		/* Multi JPEG registers */
+
+#define REG_32_BIT_SZ_IN_BYTES (sizeof(u32))
+
+#define SET_FRAME_W_H(w, h) ((((u32) (h)) & 0x1fff) | ((((u32) (w)) & 0x1fff) << 13))
+#define SET_FRAME_START_ADDR(addr) ((addr) & 0x7fffff80)
+
+/////////////////////////////////////////////////////////////////////////////
+
+/*	AST_VIDEO_PROTECT: 0x000  - protection key register */
+#define VIDEO_PROTECT_UNLOCK				0x1A038AA8
+
+/*	AST_VIDEO_SEQ_CTRL		0x004		Video Sequence Control register	*/
+#define VIDEO_HALT_ENG_STS					(1 << 21)
+#define VIDEO_COMPRESS_BUSY				(1 << 18)
+#define VIDEO_CAPTURE_BUSY					(1 << 16)
+#define VIDEO_HALT_ENG_TRIGGER			(1 << 12)
+#define VIDEO_COMPRESS_FORMAT_MASK		(3 << 10)
+#define VIDEO_GET_COMPRESS_FORMAT(x)		((x >> 10) & 0x3)   // 0 YUV444
+#define VIDEO_COMPRESS_FORMAT(x)			(x << 10)	// 0 YUV444
+#define YUV420		1
+
+#define G5_VIDEO_COMPRESS_JPEG_MODE		(1 << 13)
+#define VIDEO_YUV2RGB_DITHER_EN			(1 << 8)
+
+#define VIDEO_COMPRESS_JPEG_MODE			(1 << 8)
+
+//if bit 0 : 1
+#define VIDEO_INPUT_MODE_CHG_WDT			(1 << 7)
+#define VIDEO_INSERT_FULL_COMPRESS		(1 << 6)
+#define VIDEO_AUTO_COMPRESS				(1 << 5)
+#define VIDEO_COMPRESS_TRIGGER			(1 << 4)
+#define VIDEO_CAPTURE_MULTI_FRAME		(1 << 3)
+#define VIDEO_COMPRESS_FORCE_IDLE		(1 << 2)
+#define VIDEO_CAPTURE_TRIGGER				(1 << 1)
+#define VIDEO_DETECT_TRIGGER				(1 << 0)
+
+
+#define VIDEO_HALT_ENG_RB					(1 << 21)
+
+#define VIDEO_ABCD_CHG_EN					(1 << 1)
+#define VIDEO_BCD_CHG_EN					(1)
+
+/*	AST_VIDEO_PASS_CTRL			0x008		Video Pass1 Control register	*/
+#define G6_VIDEO_MULTI_JPEG_FLAG_MODE	(1 << 31)
+#define G6_VIDEO_MULTI_JPEG_MODE			(1 << 30)
+#define G6_VIDEO_JPEG__COUNT(x)			((x) << 24)
+#define G6_VIDEO_FRAME_CT_MASK			(0x3f << 24)
+
+/*	AST_VIDEO_DIRECT_CTRL	0x010		Video Direct Frame buffer mode control Register VR008[5]=1 */
+#define VIDEO_FETCH_TIMING(x)			((x) << 16)
+#define VIDEO_FETCH_LINE_OFFSET(x)		(x & 0xffff)
+
+//x * source frame rate / 60
+#define VIDEO_FRAME_RATE_CTRL(x)			(x << 16)
+#define VIDEO_HSYNC_POLARITY_CTRL		(1 << 15)
+#define VIDEO_INTERLANCE_MODE				(1 << 14)
+#define VIDEO_DUAL_EDGE_MODE				(1 << 13)	//0 : Single edage
+#define VIDEO_18BIT_SINGLE_EDGE			(1 << 12)	//0: 24bits
+#define VIDEO_DVO_INPUT_DELAY_MASK		(7 << 9)
+#define VIDEO_DVO_INPUT_DELAY(x)			(x << 9) //0 : no delay , 1: 1ns, 2: 2ns, 3:3ns, 4: inversed clock but no delay
+// if bit 5 : 0
+#define VIDEO_HW_CURSOR_DIS				(1 << 8)
+// if bit 5 : 1
+#define VIDEO_AUTO_FETCH					(1 << 8)	//
+#define VIDEO_CAPTURE_FORMATE_MASK		(3 << 6)
+
+#define VIDEO_SET_CAPTURE_FORMAT(x)		(x << 6)
+#define JPEG_MODE		1
+#define RGB_MODE		2
+#define GRAY_MODE		3
+#define VIDEO_DIRECT_FETCH				(1 << 5)
+// if bit 5 : 0
+#define VIDEO_INTERNAL_DE				(1 << 4)
+#define VIDEO_EXT_ADC_ATTRIBUTE		(1 << 3)
+
+
+/*	 AST_VIDEO_CAPTURE_WIN	0x030		Video Capturing Window Setting Register */
+#define VIDEO_CAPTURE_V(x)				(x & 0x7ff)
+#define VIDEO_CAPTURE_H(x)				((x & 0x7ff) << 16)
+
+/*	 AST_VIDEO_COMPRESS_WIN	0x034		Video Compression Window Setting Register */
+#define VIDEO_COMPRESS_V(x)			(x & 0x7ff)
+#define VIDEO_GET_COMPRESS_V(x)		(x & 0x7ff)
+#define VIDEO_COMPRESS_H(x)			((x & 0x7ff) << 16)
+#define VIDEO_GET_COMPRESS_H(x)		((x >> 16) & 0x7ff)
+
+/*	AST_VIDEO_STREAM_SIZE	0x058		Video Stream Buffer Size Register */
+#define VIDEO_STREAM_PKT_N(x)			(x << 3)
+#define STREAM_4_PKTS		0
+#define STREAM_8_PKTS		1
+#define STREAM_16_PKTS		2
+#define STREAM_32_PKTS		3
+#define STREAM_64_PKTS		4
+#define STREAM_128_PKTS		5
+
+#define VIDEO_STREAM_PKT_SIZE(x)		(x)
+#define STREAM_1KB		0
+#define STREAM_2KB		1
+#define STREAM_4KB		2
+#define STREAM_8KB		3
+#define STREAM_16KB		4
+#define STREAM_32KB		5
+#define STREAM_64KB		6
+#define STREAM_128KB	7
+
+
+/* AST_VIDEO_COMPRESS_CTRL	0x060		Video Compression Control Register */
+#define VIDEO_DCT_CQT_SELECTION			(0xf<<6)  // bit 6-9, bit 10 for which quantization is referred
+#define VIDEO_DCT_HQ_CQT_SELECTION		(0xf<<27) // bit 27-30, bit 31 for which quantization is referred
+
+#define VIDEO_HQ_DCT_LUM(x)				((x) << 27)
+#define VIDEO_GET_HQ_DCT_LUM(x)			((x >> 27) & 0x1f)
+#define VIDEO_HQ_DCT_CHROM(x)				((x) << 22)
+#define VIDEO_GET_HQ_DCT_CHROM(x)			((x >> 22) & 0x1f)
+#define VIDEO_HQ_DCT_MASK					(0x3ff << 22)
+#define VIDEO_DCT_HUFFMAN_ENCODE(x)		((x) << 20)
+#define VIDEO_DCT_RESET						(1 << 17)
+#define VIDEO_HQ_ENABLE						(1 << 16)
+#define VIDEO_GET_HQ_ENABLE(x)			((x >> 16) & 0x1)
+#define VIDEO_DCT_LUM(x)					((x) << 11)
+#define VIDEO_GET_DCT_LUM(x)				((x >> 11) & 0x1f)
+#define VIDEO_DCT_CHROM(x)					((x) << 6)
+#define VIDEO_GET_DCT_CHROM(x)			((x >> 6) & 0x1f)
+#define VIDEO_DCT_MASK						(0x3ff << 6)
+#define VIDEO_ENCRYP_ENABLE				(1 << 5)
+#define VIDEO_COMPRESS_QUANTIZ_MODE		(1 << 2)
+#define VIDEO_4COLOR_VQ_ENCODE			(1 << 1)
+#define VIDEO_DCT_ONLY_ENCODE				(1)
+#define VIDEO_DCT_VQ_MASK					(0x3)
+
+#define VIDEO_CTRL_RC4_TEST_MODE		(1 << 9)
+#define VIDEO_CTRL_RC4_RST				(1 << 8)
+
+#define VIDEO_CTRL_ADDRESS_MAP_MULTI_JPEG	(0x3 << 30)
+
+#define VIDEO_CTRL_DWN_SCALING_MASK		(0x3 << 4)
+#define VIDEO_CTRL_DWN_SCALING_ENABLE_LINE_BUFFER		(0x1 << 4)
+
+/* AST_VIDEO_INT_EN			0x304		Video interrupt Enable */
+/* AST_VIDEO_INT_STS		0x308		Video interrupt status */
+#define VM_COMPRESS_COMPLETE			(1 << 17)
+#define VM_CAPTURE_COMPLETE			(1 << 16)
+
+#define VIDEO_FRAME_COMPLETE			(1 << 5)
+#define VIDEO_MODE_DETECT_RDY			(1 << 4)
+#define VIDEO_COMPRESS_COMPLETE		(1 << 3)
+#define VIDEO_COMPRESS_PKT_COMPLETE	(1 << 2)
+#define VIDEO_CAPTURE_COMPLETE		(1 << 1)
+#define VIDEO_MODE_DETECT_WDT			(1 << 0)
+
+/***********************************************************************/
+struct VideoMem {
+	u32	phy;
+	void *pVirt;
+	u32 size;
+};
+
+struct VideoEngineMem {
+	struct VideoMem captureBuf0;
+	struct VideoMem captureBuf1;
+	struct VideoMem jpegTable;
+};
+
+struct ast_capture_mode {
+	u8	engine_idx;					//set 0: engine 0, engine 1
+	u8	differential;					//set 0: full, 1:diff frame
+	u8	mode_change;				//get 0: no, 1:change
+};
+
+struct ast_compression_mode {
+	u8	engine_idx;					//set 0: engine 0, engine 1
+	u8	mode_change;				//get 0: no, 1:change
+	u32	total_size;					//get
+	u32	block_count;					//get
+};
+
+
+/***********************************************************************/
+struct INTERNAL_MODE {
+	u16 HorizontalActive;
+	u16 VerticalActive;
+	u16 RefreshRateIndex;
+	u32 PixelClock;
+};
+
+
+
+// ioctl functions
+void ioctl_get_video_engine_config(struct VideoConfig  *pVideoConfig, struct AstRVAS *pAstRVAS);
+void ioctl_set_video_engine_config(struct VideoConfig  *pVideoConfig, struct AstRVAS *pAstRVAS);
+void ioctl_get_video_engine_data(struct MultiJpegConfig *pArrayMJConfig, struct AstRVAS *pAstRVAS,  u32 dwPhyStreamAddress);
+
+//local functions
+irqreturn_t ast_video_isr(int this_irq, void *dev_id);
+int video_engine_reserveMem(struct AstRVAS *pAstRVAS);
+void enable_video_interrupt(struct AstRVAS *pAstRVAS);
+void disable_video_interrupt(struct AstRVAS *pAstRVAS);
+void video_set_Window(struct AstRVAS *pAstRVAS);
+int free_video_engine_memory(struct AstRVAS *pAstRVAS);
+void video_ctrl_init(struct AstRVAS *pAstRVAS);
+void video_engine_rc4Reset(struct AstRVAS *pAstRVAS);
+void set_direct_mode(struct AstRVAS *pAstRVAS);
+
+
+#endif // __VIDEO_ENGINE_H__
diff --git a/drivers/soc/aspeed/rvas/video_ioctl.h b/drivers/soc/aspeed/rvas/video_ioctl.h
new file mode 100644
index 000000000000..e49d37ed7db1
--- /dev/null
+++ b/drivers/soc/aspeed/rvas/video_ioctl.h
@@ -0,0 +1,275 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * This file is part of the ASPEED Linux Device Driver for ASPEED Baseboard Management Controller.
+ * Refer to the README file included with this package for driver version and adapter compatibility.
+ *
+ * Copyright (C) 2019-2021 ASPEED Technology Inc. All rights reserved.
+ *
+ */
+
+#ifndef _VIDEO_IOCTL_H
+#define _VIDEO_IOCTL_H
+
+#include <linux/types.h>
+
+#define RVAS_MAGIC				('b')
+#define CMD_IOCTL_TURN_LOCAL_MONITOR_ON		_IOR(RVAS_MAGIC, IOCTL_TURN_LOCAL_MONITOR_ON, struct RvasIoctl)
+#define CMD_IOCTL_TURN_LOCAL_MONITOR_OFF	_IOR(RVAS_MAGIC, IOCTL_TURN_LOCAL_MONITOR_OFF, struct RvasIoctl)
+#define CMD_IOCTL_IS_LOCAL_MONITOR_ENABLED	_IOR(RVAS_MAGIC, IOCTL_IS_LOCAL_MONITOR_ENABLED, struct RvasIoctl)
+#define CMD_IOCTL_GET_VIDEO_GEOMETRY		_IOWR(RVAS_MAGIC, IOCTL_GET_VIDEO_GEOMETRY, struct RvasIoctl)
+#define CMD_IOCTL_WAIT_FOR_VIDEO_EVENT		_IOWR(RVAS_MAGIC, IOCTL_WAIT_FOR_VIDEO_EVENT, struct RvasIoctl)
+#define CMD_IOCTL_GET_GRC_REGIESTERS		_IOWR(RVAS_MAGIC, IOCTL_GET_GRC_REGIESTERS, struct RvasIoctl)
+#define CMD_IOCTL_READ_SNOOP_MAP		_IOWR(RVAS_MAGIC, IOCTL_READ_SNOOP_MAP, struct RvasIoctl)
+#define CMD_IOCTL_READ_SNOOP_AGGREGATE		_IOWR(RVAS_MAGIC, IOCTL_READ_SNOOP_AGGREGATE, struct RvasIoctl)
+#define CMD_IOCTL_FETCH_VIDEO_TILES		_IOWR(RVAS_MAGIC, IOCTL_FETCH_VIDEO_TILES, struct RvasIoctl)
+#define CMD_IOCTL_FETCH_VIDEO_SLICES		_IOWR(RVAS_MAGIC, IOCTL_FETCH_VIDEO_SLICES, struct RvasIoctl)
+#define CMD_IOCTL_RUN_LENGTH_ENCODE_DATA	_IOWR(RVAS_MAGIC, IOCTL_RUN_LENGTH_ENCODE_DATA, struct RvasIoctl)
+#define CMD_IOCTL_FETCH_TEXT_DATA		_IOWR(RVAS_MAGIC, IOCTL_FETCH_TEXT_DATA, struct RvasIoctl)
+#define CMD_IOCTL_FETCH_MODE13_DATA		_IOWR(RVAS_MAGIC, IOCTL_FETCH_MODE13_DATA, struct RvasIoctl)
+#define CMD_IOCTL_NEW_CONTEXT			_IOWR(RVAS_MAGIC, IOCTL_NEW_CONTEXT, struct RvasIoctl)
+#define CMD_IOCTL_DEL_CONTEXT			_IOWR(RVAS_MAGIC, IOCTL_DEL_CONTEXT, struct RvasIoctl)
+#define CMD_IOCTL_ALLOC				_IOWR(RVAS_MAGIC, IOCTL_ALLOC, struct RvasIoctl)
+#define CMD_IOCTL_FREE				_IOWR(RVAS_MAGIC, IOCTL_FREE, struct RvasIoctl)
+#define CMD_IOCTL_SET_TSE_COUNTER		_IOWR(RVAS_MAGIC, IOCTL_SET_TSE_COUNTER, struct RvasIoctl)
+#define CMD_IOCTL_GET_TSE_COUNTER		_IOWR(RVAS_MAGIC, IOCTL_GET_TSE_COUNTER, struct RvasIoctl)
+#define CMD_IOCTL_VIDEO_ENGINE_RESET		_IOWR(RVAS_MAGIC, IOCTL_VIDEO_ENGINE_RESET, struct RvasIoctl)
+//jpeg
+#define CMD_IOCTL_SET_VIDEO_ENGINE_CONFIG	_IOW(RVAS_MAGIC, IOCTL_SET_VIDEO_ENGINE_CONFIG, struct VideoConfig*)
+#define CMD_IOCTL_GET_VIDEO_ENGINE_CONFIG	_IOW(RVAS_MAGIC, IOCTL_GET_VIDEO_ENGINE_CONFIG, struct VideoConfig*)
+#define CMD_IOCTL_GET_VIDEO_ENGINE_DATA		_IOWR(RVAS_MAGIC, IOCTL_GET_VIDEO_ENGINE_DATA, struct MultiJpegConfig*)
+
+enum  HARD_WARE_ENGINE_IOCTL {
+	IOCTL_TURN_LOCAL_MONITOR_ON = 20, //REMOTE VIDEO GENERAL IOCTL
+	IOCTL_TURN_LOCAL_MONITOR_OFF,
+	IOCTL_IS_LOCAL_MONITOR_ENABLED,
+
+	IOCTL_GET_VIDEO_GEOMETRY = 40, // REMOTE VIDEO
+	IOCTL_WAIT_FOR_VIDEO_EVENT,
+	IOCTL_GET_GRC_REGIESTERS,
+	IOCTL_READ_SNOOP_MAP,
+	IOCTL_READ_SNOOP_AGGREGATE,
+	IOCTL_FETCH_VIDEO_TILES,
+	IOCTL_FETCH_VIDEO_SLICES,
+	IOCTL_RUN_LENGTH_ENCODE_DATA,
+	IOCTL_FETCH_TEXT_DATA,
+	IOCTL_FETCH_MODE13_DATA,
+	IOCTL_NEW_CONTEXT,
+	IOCTL_DEL_CONTEXT,
+	IOCTL_ALLOC,
+	IOCTL_FREE,
+	IOCTL_SET_TSE_COUNTER,
+	IOCTL_GET_TSE_COUNTER,
+	IOCTL_VIDEO_ENGINE_RESET,
+	IOCTL_SET_VIDEO_ENGINE_CONFIG,
+	IOCTL_GET_VIDEO_ENGINE_CONFIG,
+	IOCTL_GET_VIDEO_ENGINE_DATA,
+};
+
+enum GraphicsModeType {
+	InvalidMode = 0, TextMode = 1, VGAGraphicsMode = 2, AGAGraphicsMode = 3
+};
+
+enum RVASStatus {
+	SuccessStatus = 0,
+	GenericError = 1,
+	MemoryAllocError = 2,
+	InvalidMemoryHandle = 3,
+	CannotMapMemory = 4,
+	CannotUnMapMemory = 5,
+	TimedOut = 6,
+	InvalidContextHandle = 7,
+	CaptureTimedOut = 8,
+	CompressionTimedOut = 9,
+	HostSuspended
+};
+
+enum SelectedByteMode {
+	AllBytesMode = 0,
+	SkipMode = 1,
+	PlanarToPackedMode,
+	PackedToPackedMode,
+	LowByteMode,
+	MiddleByteMode,
+	TopByteMode
+};
+
+enum DataProccessMode {
+	NormalTileMode = 0,
+	FourBitPlanarMode = 1,
+	FourBitPackedMode = 2,
+	AttrMode = 3,
+	AsciiOnlyMode = 4,
+	FontFetchMode = 5,
+	SplitByteMode = 6
+};
+
+enum ResetEngineMode {
+	ResetAll = 0,
+	ResetRvasEngine = 1,
+	ResetVeEngine = 2
+};
+
+struct VideoGeometry {
+	u16 wScreenWidth;
+	u16 wScreenHeight;
+	u16 wStride;
+	u8 byBitsPerPixel;
+	u8 byModeID;
+	enum GraphicsModeType gmt;
+};
+
+struct EventMap {
+	u32 bPaletteChanged :1;
+	u32 bATTRChanged :1;
+	u32 bSEQChanged :1;
+	u32 bGCTLChanged :1;
+	u32 bCRTCChanged :1;
+	u32 bCRTCEXTChanged :1;
+	u32 bPLTRAMChanged :1;
+	u32 bXCURCOLChanged :1;
+	u32 bXCURCTLChanged :1;
+	u32 bXCURPOSChanged :1;
+	u32 bDoorbellA :1;
+	u32 bDoorbellB :1;
+	u32 bGeometryChanged :1;
+	u32 bSnoopChanged :1;
+	u32 bTextFontChanged :1;
+	u32 bTextATTRChanged :1;
+	u32 bTextASCIIChanged :1;
+};
+
+struct FetchMap {
+	//in parameters
+	bool bEnableRLE;
+	u8 bTextAlignDouble; // 0 - 8 byte, 1 - 16 byte
+	u8 byRLETripletCode;
+	u8 byRLERepeatCode;
+	enum DataProccessMode dpm;
+	//out parameters
+	u32 dwFetchSize;
+	u32 dwFetchRLESize;
+	u32 dwCheckSum;
+	bool bRLEFailed;
+	u8 rsvd[3];
+};
+
+struct SnoopAggregate {
+	u64 qwRow;
+	u64 qwCol;
+};
+
+struct FetchRegion {
+	u16 wTopY;
+	u16 wLeftX;
+	u16 wBottomY;
+	u16 wRightX;
+};
+
+struct FetchOperation {
+	struct FetchRegion fr;
+	enum SelectedByteMode sbm;
+	u32 dwFetchSize;
+	u32 dwFetchRLESize;
+	u32 dwCheckSum;
+	bool bRLEFailed;
+	bool bEnableRLE;
+	u8 byRLETripletCode;
+	u8 byRLERepeatCode;
+	u8 byVGATextAlignment; //0-8bytes, 1-16bytes.
+};
+
+struct FetchVideoTilesArg {
+	struct VideoGeometry vg;
+	u32 dwTotalOutputSize;
+	u32 cfo;
+	struct FetchOperation pfo[4];
+};
+
+struct FetchVideoSlicesArg {
+	struct VideoGeometry vg;
+	u32 dwSlicedSize;
+	u32 dwSlicedRLESize;
+	u32 dwCheckSum;
+	bool bEnableRLE;
+	bool bRLEFailed;
+	u8 byRLETripletCode;
+	u8 byRLERepeatCode;
+	u8 cBuckets;
+	u8 abyBitIndexes[24];
+	u32 cfr;
+	struct FetchRegion pfr[4];
+};
+
+struct RVASBuffer {
+	void *pv;
+	size_t cb;
+};
+
+
+struct RvasIoctl {
+	enum RVASStatus rs;
+	void *rc;
+	struct RVASBuffer rvb;
+	void *rmh;
+	void *rmh1;
+	void *rmh2;
+	u32 rmh_mem_size;
+	u32 rmh1_mem_size;
+	u32 rmh2_mem_size;
+	struct VideoGeometry vg;
+	struct EventMap em;
+	struct SnoopAggregate sa;
+	union {
+		u32 tse_counter;
+		u32 req_mem_size;
+		u32 encode;
+		u32 time_out;
+	};
+	u32 rle_len;  // RLE Length
+	u32 rle_checksum;
+	struct FetchMap tfm;
+	u8 flag;
+	u8 lms;
+	u8 resetMode;
+	u8 rsvd[1];
+};
+
+
+//
+// Video Engine
+//
+
+#define MAX_MULTI_FRAME_CT (32)
+
+struct VideoConfig {
+	u8 engine;					//0: engine 0 - normal engine, engine 1 - VM legacy engine
+	u8 compression_mode;	//0:DCT, 1:DCT_VQ mix VQ-2 color, 2:DCT_VQ mix VQ-4 color		9:
+	u8 compression_format;	//0:ASPEED 1:JPEG
+	u8 capture_format;		//0:CCIR601-2 YUV, 1:JPEG YUV, 2:RGB for ASPEED mode only, 3:Gray
+	u8 rc4_enable;				//0:disable 1:enable
+	u8 YUV420_mode;			//0:YUV444, 1:YUV420
+	u8 Visual_Lossless;
+	u8 Y_JPEGTableSelector;
+	u8 AdvanceTableSelector;
+	u8 AutoMode;
+	u8 rsvd[2];
+	enum RVASStatus rs;
+};
+
+struct MultiJpegFrame {
+	u32 dwSizeInBytes;			// Image size in bytes
+	u32 dwOffsetInBytes;			// Offset in bytes
+	u16 wXPixels;					// In: X coordinate
+	u16 wYPixels;					// In: Y coordinate
+	u16 wWidthPixels;				// In: Width for Fetch
+	u16 wHeightPixels;			// In: Height for Fetch
+};
+
+struct MultiJpegConfig {
+	unsigned char multi_jpeg_frames;				// frame count
+	struct MultiJpegFrame frame[MAX_MULTI_FRAME_CT];	// The Multi Frames
+	void *aStreamHandle;
+	enum RVASStatus rs;
+};
+
+#endif // _VIDEO_IOCTL_H
diff --git a/drivers/soc/aspeed/rvas/video_main.c b/drivers/soc/aspeed/rvas/video_main.c
new file mode 100644
index 000000000000..b90309a8db4a
--- /dev/null
+++ b/drivers/soc/aspeed/rvas/video_main.c
@@ -0,0 +1,1622 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * File Name     : video_main.c
+ * Description   : AST2600 RVAS hardware engines
+ *
+ * Copyright (C) ASPEED Technology Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/version.h>
+#include <linux/interrupt.h>
+#include <linux/device.h>
+#include <linux/reset.h>
+#include <asm/uaccess.h>
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/fs.h>
+#include <linux/platform_device.h>
+#include <linux/cdev.h>
+#include <linux/dma-mapping.h>
+#include <linux/miscdevice.h>
+#include <linux/slab.h>
+#include <linux/wait.h>
+#include <linux/sched.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/mm.h>
+#include <stdbool.h>
+#include <linux/of_reserved_mem.h>
+#include <linux/regmap.h>
+#include <linux/mfd/syscon.h>
+#include <linux/clk.h>
+
+#include "video_ioctl.h"
+#include "hardware_engines.h"
+#include "video.h"
+#include "video_debug.h"
+#include "video_engine.h"
+
+
+#define TEST_GRCE_DETECT_RESOLUTION_CHG
+
+static long video_ioctl(struct file *file, unsigned int cmd, unsigned long arg);
+static int video_open(struct inode *pInode, struct file *pFile);
+static int video_release(struct inode *pInode, struct file *pFile);
+static irqreturn_t fge_handler(int irq, void *dev_id);
+
+static void video_os_init_sleep_struct(struct Video_OsSleepStruct *Sleep);
+static void video_ss_wakeup_on_timeout(struct Video_OsSleepStruct *Sleep);
+static void enable_rvas_engines(struct AstRVAS *pAstRVAS);
+static void video_engine_init(void);
+static void rvas_init(void);
+static void reset_rvas_engine(struct AstRVAS *pAstRVAS);
+static void reset_video_engine(struct AstRVAS *pAstRVAS);
+static void set_FBInfo_size(struct AstRVAS *pAstRVAS, void __iomem *mcr_base);
+
+static long video_os_sleep_on_timeout(struct Video_OsSleepStruct *Sleep, u8 *Var, long msecs);
+
+static struct AstRVAS *pAstRVAS;
+static void __iomem *dp_base;
+
+static long video_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	int iResult = 0;
+	struct RvasIoctl ri;
+	struct VideoConfig video_config;
+	struct MultiJpegConfig multi_jpeg;
+	u8 bVideoCmd = 0;
+	u32 dw_phys = 0;
+
+	VIDEO_DBG("Start\n");
+	VIDEO_DBG("pAstRVAS: 0x%p\n", pAstRVAS);
+	memset(&ri, 0, sizeof(ri));
+
+	if ((cmd != CMD_IOCTL_SET_VIDEO_ENGINE_CONFIG) &&
+		(cmd != CMD_IOCTL_GET_VIDEO_ENGINE_CONFIG) &&
+		(cmd != CMD_IOCTL_GET_VIDEO_ENGINE_DATA)) {
+		if (raw_copy_from_user(&ri, (void *) arg, sizeof(struct RvasIoctl))) {
+			dev_err(pAstRVAS->pdev, "Copy from user buffer Failed\n");
+			return -EINVAL;
+		}
+
+		ri.rs = SuccessStatus;
+		bVideoCmd = 0;
+	} else {
+		bVideoCmd = 1;
+	}
+
+
+	VIDEO_DBG(" Command = 0x%x\n", cmd);
+
+	switch (cmd) {
+	case CMD_IOCTL_TURN_LOCAL_MONITOR_ON:
+		ioctl_update_lms(0x1, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_TURN_LOCAL_MONITOR_OFF:
+		ioctl_update_lms(0x0, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_IS_LOCAL_MONITOR_ENABLED:
+		if (ioctl_get_lm_status(pAstRVAS))
+			ri.lms = 0x1;
+		else
+			ri.lms = 0x0;
+		break;
+
+	case CMD_IOCTL_GET_VIDEO_GEOMETRY:
+		VIDEO_DBG(" Command CMD_IOCTL_GET_VIDEO_GEOMETRY\n");
+		ioctl_get_video_geometry(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_WAIT_FOR_VIDEO_EVENT:
+		VIDEO_DBG(" Command CMD_IOCTL_WAIT_FOR_VIDEO_EVENT\n");
+		ioctl_wait_for_video_event(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_GET_GRC_REGIESTERS:
+		VIDEO_DBG(" Command CMD_IOCTL_GET_GRC_REGIESTERS\n");
+		ioctl_get_grc_register(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_READ_SNOOP_MAP:
+		VIDEO_DBG(" Command CMD_IOCTL_READ_SNOOP_MAP\n");
+		ioctl_read_snoop_map(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_READ_SNOOP_AGGREGATE:
+		VIDEO_DBG(" Command CMD_IOCTL_READ_SNOOP_AGGREGATE\n");
+		ioctl_read_snoop_aggregate(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_FETCH_VIDEO_TILES: ///
+		VIDEO_DBG("CMD_IOCTL_FETCH_VIDEO_TILES\n");
+		ioctl_fetch_video_tiles(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_FETCH_VIDEO_SLICES:
+		VIDEO_DBG(" Command CMD_IOCTL_FETCH_VIDEO_SLICES\n");
+		ioctl_fetch_video_slices(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_RUN_LENGTH_ENCODE_DATA:
+		VIDEO_DBG(" Command CMD_IOCTL_RUN_LENGTH_ENCODE_DATA\n");
+		ioctl_run_length_encode_data(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_FETCH_TEXT_DATA:
+		VIDEO_DBG(" Command CMD_IOCTL_FETCH_TEXT_DATA\n");
+		ioctl_fetch_text_data(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_FETCH_MODE13_DATA:
+		VIDEO_DBG(" Command CMD_IOCTL_FETCH_MODE13_DATA\n");
+		ioctl_fetch_mode_13_data(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_ALLOC:
+		VIDEO_DBG(" Command CMD_IOCTL_ALLOC\n");
+		ioctl_alloc(file, &ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_FREE:
+		VIDEO_DBG(" Command CMD_IOCTL_FREE\n");
+		ioctl_free(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_NEW_CONTEXT:
+		VIDEO_DBG(" Command CMD_IOCTL_NEW_CONTEXT\n");
+		ioctl_new_context(file, &ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_DEL_CONTEXT:
+		VIDEO_DBG(" Command CMD_IOCTL_DEL_CONTEXT\n");
+		ioctl_delete_context(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_SET_TSE_COUNTER:
+		VIDEO_DBG(" Command CMD_IOCTL_SET_TSE_COUNTER\n");
+		ioctl_set_tse_tsicr(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_GET_TSE_COUNTER:
+		VIDEO_DBG(" Command CMD_IOCTL_GET_TSE_COUNTER\n");
+		ioctl_get_tse_tsicr(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_VIDEO_ENGINE_RESET:
+		VIDEO_ENG_DBG(" Command CMD_IOCTL_VIDEO_ENGINE_RESET\n");
+		ioctl_reset_video_engine(&ri, pAstRVAS);
+		break;
+	case CMD_IOCTL_GET_VIDEO_ENGINE_CONFIG:
+		VIDEO_DBG(" Command CMD_IOCTL_GET_VIDEO_ENGINE_CONFIG\n");
+		ioctl_get_video_engine_config(&video_config, pAstRVAS);
+
+		iResult = raw_copy_to_user((void *) arg, &video_config, sizeof(video_config));
+		break;
+	case CMD_IOCTL_SET_VIDEO_ENGINE_CONFIG:
+		VIDEO_DBG(" Command CMD_IOCTL_SET_VIDEO_ENGINE_CONFIG\n");
+		iResult = raw_copy_from_user(&video_config, (void *) arg, sizeof(video_config));
+
+		ioctl_set_video_engine_config(&video_config, pAstRVAS);
+		break;
+	case CMD_IOCTL_GET_VIDEO_ENGINE_DATA:
+		VIDEO_DBG(" Command CMD_IOCTL_GET_VIDEO_ENGINE_DATA\n");
+		iResult = raw_copy_from_user(&multi_jpeg, (void *) arg, sizeof(multi_jpeg));
+		dw_phys = get_phys_add_rsvd_mem((u32)multi_jpeg.aStreamHandle, pAstRVAS);
+		VIDEO_DBG("physical stream address: %#x\n", dw_phys);
+
+		if (dw_phys == 0)
+			dev_err(pAstRVAS->pdev, "Error of getting stream buffer address\n");
+		else
+			ioctl_get_video_engine_data(&multi_jpeg, pAstRVAS, dw_phys);
+
+		iResult = raw_copy_to_user((void *) arg, &multi_jpeg, sizeof(multi_jpeg));
+		break;
+	default:
+		dev_err(pAstRVAS->pdev, "Unknown Ioctl: %#x\n", cmd);
+		iResult = -EINVAL;
+		break;
+	}
+
+	if (!iResult && !bVideoCmd)
+		if (raw_copy_to_user((void *) arg, &ri, sizeof(struct RvasIoctl))) {
+			dev_err(pAstRVAS->pdev, "Copy to user buffer Failed\n");
+			iResult = -EINVAL;
+		}
+
+	return iResult;
+}
+
+static int video_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	size_t size;
+	u32 dw_index;
+	u8 found = 0;
+
+	struct MemoryMapTable **pmmt = pAstRVAS->ppmmtMemoryTable;
+
+	size = vma->vm_end - vma->vm_start;
+	vma->vm_private_data = pAstRVAS;
+	VIDEO_DBG("vma->vm_start 0x%lx, vma->vm_end 0x%lx, vma->vm_pgoff=0x%x\n",
+			vma->vm_start,
+			vma->vm_end,
+			(u32) vma->vm_pgoff);
+	VIDEO_DBG("(vma->vm_pgoff << PAGE_SHIFT) = 0x%lx\n", (vma->vm_pgoff << PAGE_SHIFT));
+	for (dw_index = 0; dw_index < MAX_NUM_MEM_TBL; ++dw_index) {
+		if (pmmt[dw_index]) {
+			VIDEO_DBG("index %d, phys_addr=0x%x, virt_addr0x%x, length=0x%x\n",
+					dw_index,
+					pmmt[dw_index]->dwPhysicalAddr,
+					(u32)pmmt[dw_index]->pvVirtualAddr,
+					pmmt[dw_index]->dwLength);
+			if ((vma->vm_pgoff<<PAGE_SHIFT) == (u32)pmmt[dw_index]->pvVirtualAddr) {
+				found = 1;
+				if (size > pmmt[dw_index]->dwLength) {
+					pr_err("required size exceed alloc size\n");
+					return -EAGAIN;
+				}
+				break;
+			}
+		}
+	}
+	if (!found) {
+		pr_err("no match mem entry\n");
+		return -EAGAIN;
+	}
+
+	vma->vm_flags |= VM_IO;
+	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+
+	if (io_remap_pfn_range(vma, vma->vm_start, ((u32) vma->vm_pgoff), size,
+		vma->vm_page_prot)) {
+		pr_err("remap_pfn_range fail at %s()\n", __func__);
+		return -EAGAIN;
+	}
+
+	return 0;
+}
+
+static int video_open(struct inode *pin, struct file *pf)
+{
+	VIDEO_DBG("%s Called\n", __func__);
+
+	// make sure the rvas clk is running.
+	//	 if it's already enabled, clk_enable will just return.
+	clk_enable(pAstRVAS->rvasclk);
+
+	return 0;
+}
+
+void free_all_mem_entries(struct AstRVAS *pAstRVAS)
+{
+	u32 dw_index;
+	struct MemoryMapTable **pmmt = pAstRVAS->ppmmtMemoryTable;
+	void *virt_add;
+	u32 dw_phys, len;
+
+	VIDEO_DBG("Removing mem map entries...\n");
+	for (dw_index = 0; dw_index < MAX_NUM_MEM_TBL; ++dw_index) {
+		if (pmmt[dw_index]) {
+			if (pmmt[dw_index]->dwPhysicalAddr) {
+				virt_add = get_virt_add_rsvd_mem(dw_index, pAstRVAS);
+				dw_phys = get_phys_add_rsvd_mem(dw_index, pAstRVAS);
+				len = get_len_rsvd_mem(dw_index, pAstRVAS);
+				dma_free_coherent(pAstRVAS->pdev, len, virt_add, dw_phys);
+			}
+			pmmt[dw_index]->pf = NULL;
+			kfree(pmmt[dw_index]);
+			pmmt[dw_index] = NULL;
+		}
+	}
+}
+
+static int video_release(struct inode *inode, struct file *filp)
+{
+	u32 dw_index;
+	struct ContextTable **ppctContextTable = pAstRVAS->ppctContextTable;
+
+	VIDEO_DBG("Start\n");
+
+	free_all_mem_entries(pAstRVAS);
+
+	VIDEO_DBG("ppctContextTable: 0x%p\n", ppctContextTable);
+
+	disable_grce_tse_interrupt(pAstRVAS);
+
+	for (dw_index = 0; dw_index < MAX_NUM_CONTEXT; ++dw_index) {
+
+		if (ppctContextTable[dw_index]) {
+			VIDEO_DBG("Releasing Context dw_index: %u\n", dw_index);
+			kfree(ppctContextTable[dw_index]);
+			ppctContextTable[dw_index] = NULL;
+		}
+	}
+	enable_grce_tse_interrupt(pAstRVAS);
+	VIDEO_DBG("End\n");
+
+	return 0;
+}
+
+static struct file_operations video_module_ops = { .compat_ioctl = video_ioctl,
+	.unlocked_ioctl = video_ioctl, .open = video_open, .release =
+		video_release, .mmap = video_mmap, .owner = THIS_MODULE, };
+
+static struct miscdevice video_misc = { .minor = MISC_DYNAMIC_MINOR, .name =
+	RVAS_DRIVER_NAME, .fops = &video_module_ops, };
+
+void ioctl_new_context(struct file *file, struct RvasIoctl *pri, struct AstRVAS *pAstRVAS)
+{
+	struct ContextTable *pct;
+
+	VIDEO_DBG("Start\n");
+	pct = get_new_context_table_entry(pAstRVAS);
+
+	if (pct) {
+		pct->desc_virt = dma_alloc_coherent(pAstRVAS->pdev, PAGE_SIZE, (dma_addr_t *) &pct->desc_phy, GFP_KERNEL);
+		if (!pct->desc_virt) {
+			pri->rs = MemoryAllocError;
+			return;
+		}
+		pri->rc = pct->rc;
+
+	} else {
+		pri->rs = MemoryAllocError;
+	}
+	VIDEO_DBG("end: return status: %d\n", pri->rs);
+
+}
+
+void ioctl_delete_context(struct RvasIoctl *pri, struct AstRVAS *pAstRVAS)
+{
+	VIDEO_DBG("Start\n");
+
+	VIDEO_DBG("pri->rc: %d\n", pri->rc);
+	if (remove_context_table_entry(pri->rc, pAstRVAS)) {
+		VIDEO_DBG("Success in removing\n");
+		pri->rs = SuccessStatus;
+	} else {
+		VIDEO_DBG("Failed in removing\n");
+		pri->rs = InvalidMemoryHandle;
+	}
+}
+
+int get_mem_entry(struct AstRVAS *pAstRVAS)
+{
+	int index = 0;
+	bool found = false;
+
+	down(&pAstRVAS->mem_sem);
+	do {
+		if (pAstRVAS->ppmmtMemoryTable[index])
+			index++;
+		else {
+			found = true;
+			break;
+		}
+
+	} while (!found && (index < MAX_NUM_MEM_TBL));
+
+	if (found) {
+		pAstRVAS->ppmmtMemoryTable[index] = kmalloc(sizeof(struct MemoryMapTable), GFP_KERNEL);
+		if (!pAstRVAS->ppmmtMemoryTable[index])
+			index = -1;
+	} else
+		index = -1;
+
+	up(&pAstRVAS->mem_sem);
+	return index;
+}
+
+bool delete_mem_entry(const void *crmh, struct AstRVAS *pAstRVAS)
+{
+	bool b_ret = false;
+	u32 dw_index = (u32) crmh;
+
+	VIDEO_DBG("Start, dw_index: %#x\n", dw_index);
+
+	down(&pAstRVAS->mem_sem);
+	if ((dw_index < MAX_NUM_MEM_TBL) && pAstRVAS->ppmmtMemoryTable[dw_index]) {
+		VIDEO_DBG("mem: 0x%p\n", pAstRVAS->ppmmtMemoryTable[dw_index]);
+		kfree(pAstRVAS->ppmmtMemoryTable[dw_index]);
+		pAstRVAS->ppmmtMemoryTable[dw_index] = NULL;
+		b_ret = true;
+	}
+	up(&pAstRVAS->mem_sem);
+	VIDEO_DBG("End\n");
+	return b_ret;
+}
+
+void *get_virt_add_rsvd_mem(u32 index, struct AstRVAS *pAstRVAS)
+{
+	if (index < MAX_NUM_MEM_TBL && pAstRVAS->ppmmtMemoryTable[index])
+		return pAstRVAS->ppmmtMemoryTable[index]->pvVirtualAddr;
+
+	return 0;
+}
+
+u32 get_phys_add_rsvd_mem(u32 index, struct AstRVAS *pAstRVAS)
+{
+	if (index < MAX_NUM_MEM_TBL && pAstRVAS->ppmmtMemoryTable[index])
+		return pAstRVAS->ppmmtMemoryTable[index]->dwPhysicalAddr;
+
+	return 0;
+}
+
+u32 get_len_rsvd_mem(u32 index, struct AstRVAS *pAstRVAS)
+{
+	u32 len = 0;
+
+	if (index < MAX_NUM_MEM_TBL && pAstRVAS->ppmmtMemoryTable[index])
+		len = pAstRVAS->ppmmtMemoryTable[index]->dwLength;
+
+	return len;
+}
+
+bool virt_is_valid_rsvd_mem(u32 index, u32 size, struct AstRVAS *pAstRVAS)
+{
+	if (index < MAX_NUM_MEM_TBL &&
+		pAstRVAS->ppmmtMemoryTable[index] &&
+		pAstRVAS->ppmmtMemoryTable[index]->dwLength)
+		return true;
+
+	return false;
+}
+
+void ioctl_alloc(struct file *pfile, struct RvasIoctl *pri, struct AstRVAS *pAstRVAS)
+{
+	u32 size;
+	u32 phys_add = 0;
+	u32 virt_add = 0;
+	u32 index = get_mem_entry(pAstRVAS);
+
+	if (index < 0 || index >= MAX_NUM_MEM_TBL) {
+		pri->rs = MemoryAllocError;
+		return;
+	}
+	if (pri->req_mem_size < PAGE_SIZE)
+		pri->req_mem_size = PAGE_SIZE;
+
+	size = pri->req_mem_size;
+
+	VIDEO_DBG("Allocating memory size: 0x%x\n", size);
+	virt_add = (u32)dma_alloc_coherent(pAstRVAS->pdev, size, &phys_add,
+			GFP_KERNEL);
+	if (virt_add) {
+		pri->rmh = (void *)index;
+		pri->rvb.pv = (void *) phys_add;
+		pri->rvb.cb = size;
+		pri->rs = SuccessStatus;
+		pAstRVAS->ppmmtMemoryTable[index]->pf = pfile;
+		pAstRVAS->ppmmtMemoryTable[index]->dwPhysicalAddr = phys_add;
+		pAstRVAS->ppmmtMemoryTable[index]->pvVirtualAddr = (void *)virt_add;
+		pAstRVAS->ppmmtMemoryTable[index]->dwLength = size;
+		pAstRVAS->ppmmtMemoryTable[index]->byDmaAlloc = 1;
+	} else {
+		if (pAstRVAS->ppmmtMemoryTable[index])
+			delete_mem_entry((void *)index, pAstRVAS);
+
+		pr_err("Cannot alloc video destination data buffer\n");
+		pri->rs = MemoryAllocError;
+	}
+
+	VIDEO_DBG("Allocated: index: 0x%x phys: %#x cb: 0x%x\n", index,
+			phys_add, pri->rvb.cb);
+}
+
+void ioctl_free(struct RvasIoctl *pri, struct AstRVAS *pAstRVAS)
+{
+	void *virt_add = get_virt_add_rsvd_mem((u32)pri->rmh, pAstRVAS);
+	u32 dw_phys = get_phys_add_rsvd_mem((u32) pri->rmh, pAstRVAS);
+	u32 len = get_len_rsvd_mem((u32) pri->rmh, pAstRVAS);
+
+	VIDEO_DBG("Start\n");
+	VIDEO_DBG("Freeing: rmh: 0x%p, phys: 0x%x, size 0x%x virt_add: 0x%p len: %u\n",
+		pri->rmh, dw_phys, pri->rvb.cb, virt_add, len);
+
+	delete_mem_entry(pri->rmh, pAstRVAS);
+	VIDEO_DBG("After delete_mem_entry\n");
+
+	dma_free_coherent(pAstRVAS->pdev, len,
+			virt_add,
+			dw_phys);
+	VIDEO_DBG("After dma_free_coherent\n");
+}
+
+
+void ioctl_update_lms(u8 lms_on, struct AstRVAS *pAstRVAS)
+{
+	u32 reg_scu418 = 0;
+	u32 reg_scu0C0 = 0;
+	u32 reg_scu0D0 = 0;
+	u32 reg_dptx100 = 0;
+	u32 reg_dptx104 = 0;
+
+	regmap_read(pAstRVAS->scu, SCU418_Pin_Ctrl, &reg_scu418);
+	regmap_read(pAstRVAS->scu, SCU0C0_Misc1_Ctrl, &reg_scu0C0);
+	regmap_read(pAstRVAS->scu, SCU0D0_Misc3_Ctrl, &reg_scu0D0);
+	if (dp_base) {
+		reg_dptx100 = readl(dp_base + DPTX_Configuration_Register);
+		reg_dptx104 = readl(dp_base + DPTX_PHY_Configuration_Register);
+	}
+
+	if (lms_on) {
+		if (!(reg_scu418 & (VGAVS_ENBL|VGAHS_ENBL))) {
+			reg_scu418 |= (VGAVS_ENBL|VGAHS_ENBL);
+			regmap_write(pAstRVAS->scu, SCU418_Pin_Ctrl, reg_scu418);
+		}
+		if (reg_scu0C0 & VGA_CRT_DISBL) {
+			reg_scu0C0 &= ~VGA_CRT_DISBL;
+			regmap_write(pAstRVAS->scu, SCU0C0_Misc1_Ctrl, reg_scu0C0);
+		}
+		if (reg_scu0D0 & PWR_OFF_VDAC) {
+			reg_scu0D0 &= ~PWR_OFF_VDAC;
+			regmap_write(pAstRVAS->scu, SCU0D0_Misc3_Ctrl, reg_scu0D0);
+		}
+		//dp output
+		if (dp_base) {
+			reg_dptx100 |= 1 << AUX_RESETN;
+			writel(reg_dptx100, dp_base + DPTX_Configuration_Register);
+		}
+	} else { //turn off
+		if (reg_scu418 & (VGAVS_ENBL|VGAHS_ENBL)) {
+			reg_scu418 &= ~(VGAVS_ENBL|VGAHS_ENBL);
+			regmap_write(pAstRVAS->scu, SCU418_Pin_Ctrl, reg_scu418);
+		}
+		if (!(reg_scu0C0 & VGA_CRT_DISBL)) {
+			reg_scu0C0 |= VGA_CRT_DISBL;
+			regmap_write(pAstRVAS->scu, SCU0C0_Misc1_Ctrl, reg_scu0C0);
+		}
+		if (!(reg_scu0D0 & PWR_OFF_VDAC)) {
+			reg_scu0D0 |= PWR_OFF_VDAC;
+			regmap_write(pAstRVAS->scu, SCU0D0_Misc3_Ctrl, reg_scu0D0);
+		}
+		//dp output
+		if (dp_base) {
+			reg_dptx100 &= ~(1 << AUX_RESETN);
+			writel(reg_dptx100, dp_base + DPTX_Configuration_Register);
+			reg_dptx104 &= ~(1 << DP_TX_I_MAIN_ON);
+			writel(reg_dptx104, dp_base + DPTX_PHY_Configuration_Register);
+		}
+	}
+}
+
+u32 ioctl_get_lm_status(struct AstRVAS *pAstRVAS)
+{
+	u32 reg_val = 0;
+
+	regmap_read(pAstRVAS->scu, SCU418_Pin_Ctrl, &reg_val);
+	if (reg_val & (VGAVS_ENBL|VGAHS_ENBL)) {
+		regmap_read(pAstRVAS->scu, SCU0C0_Misc1_Ctrl, &reg_val);
+		if (!(reg_val & VGA_CRT_DISBL)) {
+			regmap_read(pAstRVAS->scu, SCU0D0_Misc3_Ctrl, &reg_val);
+			if (!(reg_val & PWR_OFF_VDAC))
+				return 1;
+
+		}
+	}
+	return 0;
+}
+
+void init_osr_es(struct AstRVAS *pAstRVAS)
+{
+	VIDEO_DBG("Start\n");
+	sema_init(&pAstRVAS->mem_sem, 1);
+	sema_init(&pAstRVAS->context_sem, 1);
+
+	video_os_init_sleep_struct(&pAstRVAS->video_wait);
+
+	memset(&pAstRVAS->tfe_engine, 0x00, sizeof(struct EngineInfo));
+	memset(&pAstRVAS->bse_engine, 0x00, sizeof(struct EngineInfo));
+	memset(&pAstRVAS->ldma_engine, 0x00, sizeof(struct EngineInfo));
+	sema_init(&pAstRVAS->tfe_engine.sem, 1);
+	sema_init(&pAstRVAS->bse_engine.sem, 1);
+	sema_init(&pAstRVAS->ldma_engine.sem, 1);
+	video_os_init_sleep_struct(&pAstRVAS->tfe_engine.wait);
+	video_os_init_sleep_struct(&pAstRVAS->bse_engine.wait);
+	video_os_init_sleep_struct(&pAstRVAS->ldma_engine.wait);
+
+	memset(pAstRVAS->ppctContextTable, 0x00, MAX_NUM_CONTEXT * sizeof(u32));
+	pAstRVAS->dwMemoryTableSize = MAX_NUM_MEM_TBL;
+	memset(pAstRVAS->ppmmtMemoryTable, 0x00, MAX_NUM_MEM_TBL * sizeof(u32));
+	VIDEO_DBG("End\n");
+}
+
+void release_osr_es(struct AstRVAS *pAstRVAS)
+{
+	u32 dw_index;
+	struct ContextTable **ppctContextTable = pAstRVAS->ppctContextTable;
+
+	VIDEO_DBG("Removing contexts...\n");
+	for (dw_index = 0; dw_index < MAX_NUM_CONTEXT; ++dw_index) {
+		//if (ppctContextTable[dw_index]) {
+		kfree(ppctContextTable[dw_index]);
+		ppctContextTable[dw_index] = NULL;
+		//} // kfree(NULL) is safe and this check is probably not require
+	}
+
+	free_all_mem_entries(pAstRVAS);
+}
+
+//Retrieve a context entry
+struct ContextTable *get_context_entry(const void *crc, struct AstRVAS *pAstRVAS)
+{
+	struct ContextTable *pct = NULL;
+	u32 dw_index = (u32) crc;
+	struct ContextTable **ppctContextTable = pAstRVAS->ppctContextTable;
+
+	if ((dw_index < MAX_NUM_CONTEXT) && ppctContextTable[dw_index]
+		&& (ppctContextTable[dw_index]->rc == crc))
+		pct = ppctContextTable[dw_index];
+
+	return pct;
+}
+
+struct ContextTable *get_new_context_table_entry(struct AstRVAS *pAstRVAS)
+{
+	struct ContextTable *pct = NULL;
+	u32 dw_index = 0;
+	bool b_found = false;
+	struct ContextTable **ppctContextTable = pAstRVAS->ppctContextTable;
+
+	disable_grce_tse_interrupt(pAstRVAS);
+	down(&pAstRVAS->context_sem);
+	while (!b_found && (dw_index < MAX_NUM_CONTEXT)) {
+		if (!(ppctContextTable[dw_index]))
+			b_found = true;
+		else
+			++dw_index;
+	}
+	if (b_found) {
+		pct = kmalloc(sizeof(struct ContextTable), GFP_KERNEL);
+
+		if (pct) {
+			memset(pct, 0x00, sizeof(struct ContextTable));
+			pct->rc = (void *) dw_index;
+			memset(&(pct->aqwSnoopMap), 0xff,
+				sizeof(pct->aqwSnoopMap));
+			memset(&(pct->sa), 0xff, sizeof(pct->sa));
+			ppctContextTable[dw_index] = pct;
+		}
+	}
+	up(&pAstRVAS->context_sem);
+	enable_grce_tse_interrupt(pAstRVAS);
+
+	return pct;
+}
+
+bool remove_context_table_entry(const void *crc, struct AstRVAS *pAstRVAS)
+{
+	bool b_ret = false;
+	u32 dw_index = (u32) crc;
+	struct ContextTable *ctx_entry;
+
+	VIDEO_DBG("Start\n");
+
+	VIDEO_DBG("dw_index: %u\n", dw_index);
+
+	if (dw_index < MAX_NUM_CONTEXT) {
+		ctx_entry = pAstRVAS->ppctContextTable[dw_index];
+		VIDEO_DBG("ctx_entry: 0x%p\n", ctx_entry);
+
+		if (ctx_entry) {
+			disable_grce_tse_interrupt(pAstRVAS);
+			if (!ctx_entry->desc_virt) {
+				VIDEO_DBG("Removing memory, virt: 0x%p phys: %#x\n",
+					ctx_entry->desc_virt,
+					ctx_entry->desc_phy);
+
+				dma_free_coherent(pAstRVAS->pdev, PAGE_SIZE, ctx_entry->desc_virt, ctx_entry->desc_phy);
+			}
+			VIDEO_DBG("Removing memory: 0x%p\n", ctx_entry);
+			pAstRVAS->ppctContextTable[dw_index] = NULL;
+			kfree(ctx_entry);
+			b_ret = true;
+			enable_grce_tse_interrupt(pAstRVAS);
+		}
+	}
+	return b_ret;
+}
+
+void display_event_map(const struct EventMap *pem)
+{
+	VIDEO_DBG("EM:\n");
+	VIDEO_DBG("*************************\n");
+	VIDEO_DBG("  bATTRChanged=      %u\n", pem->bATTRChanged);
+	VIDEO_DBG("  bCRTCChanged=      %u\n", pem->bCRTCChanged);
+	VIDEO_DBG("  bCRTCEXTChanged=   %u\n", pem->bCRTCEXTChanged);
+	VIDEO_DBG("  bDoorbellA=        %u\n", pem->bDoorbellA);
+	VIDEO_DBG("  bDoorbellB=        %u\n", pem->bDoorbellB);
+	VIDEO_DBG("  bGCTLChanged=      %u\n", pem->bGCTLChanged);
+	VIDEO_DBG("  bGeometryChanged=  %u\n", pem->bGeometryChanged);
+	VIDEO_DBG("  bPLTRAMChanged=    %u\n", pem->bPLTRAMChanged);
+	VIDEO_DBG("  bPaletteChanged=   %u\n", pem->bPaletteChanged);
+	VIDEO_DBG("  bSEQChanged=       %u\n", pem->bSEQChanged);
+	VIDEO_DBG("  bSnoopChanged=     %u\n", pem->bSnoopChanged);
+	VIDEO_DBG("  bTextASCIIChanged= %u\n", pem->bTextASCIIChanged);
+	VIDEO_DBG("  bTextATTRChanged=  %u\n", pem->bTextATTRChanged);
+	VIDEO_DBG("  bTextFontChanged=  %u\n", pem->bTextFontChanged);
+	VIDEO_DBG("  bXCURCOLChanged=   %u\n", pem->bXCURCOLChanged);
+	VIDEO_DBG("  bXCURCTLChanged=   %u\n", pem->bXCURCTLChanged);
+	VIDEO_DBG("  bXCURPOSChanged=   %u\n", pem->bXCURPOSChanged);
+	VIDEO_DBG("*************************\n");
+}
+
+void ioctl_wait_for_video_event(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	union EmDwordUnion eduRequested;
+	union EmDwordUnion eduReturned;
+	union EmDwordUnion eduChanged;
+	struct EventMap anEm;
+	u32 result = 1;
+	int iTimerRemaining = ri->time_out;
+	unsigned long ulTimeStart, ulTimeEnd, ulElapsedTime;
+	struct ContextTable **ppctContextTable = pAstRVAS->ppctContextTable;
+
+
+	memset(&anEm, 0x0, sizeof(struct EventMap));
+
+	VIDEO_DBG("Calling VideoSleepOnTimeout\n");
+
+	eduRequested.em = ri->em;
+	VIDEO_DBG("eduRequested.em:\n");
+	display_event_map(&eduRequested.em);
+	eduChanged.em = ppctContextTable[(int) ri->rc]->emEventReceived;
+	VIDEO_DBG("eduChanged.em:\n");
+	display_event_map(&eduChanged.em);
+
+	// While event has not occurred and there is still time remaining for wait
+	while (!(eduChanged.dw & eduRequested.dw) && (iTimerRemaining > 0)
+		&& result) {
+		pAstRVAS->video_intr_occurred = 0;
+		ulTimeStart = jiffies_to_msecs(jiffies);
+		result = video_os_sleep_on_timeout(&pAstRVAS->video_wait,
+			&pAstRVAS->video_intr_occurred, iTimerRemaining);
+		ulTimeEnd = jiffies_to_msecs(jiffies);
+		ulElapsedTime = (ulTimeEnd - ulTimeStart);
+		iTimerRemaining -= (int) ulElapsedTime;
+		eduChanged.em = ppctContextTable[(int) ri->rc]->emEventReceived;
+//    VIDEO_DBG("Elapsedtime [%u], timestart[%u], timeend[%u]\n", dwElapsedTime, dwTimeStart, dwTimeEnd);
+
+		VIDEO_DBG("ulElapsedTime [%lu], ulTimeStart[%lu], ulTimeEnd[%lu]\n",
+			ulElapsedTime, ulTimeStart, ulTimeEnd);
+		VIDEO_DBG("HZ [%ul]\n", HZ);
+		VIDEO_DBG("result [%u], iTimerRemaining [%d]\n", result,
+			iTimerRemaining);
+	}
+
+	if (result == 0 && ri->time_out != 0) {
+		VIDEO_DBG("IOCTL Timedout\n");
+		ri->rs = TimedOut;
+		memset(&(ri->em), 0x0, sizeof(struct EventMap));
+	} else {
+		eduChanged.em = ppctContextTable[(int) ri->rc]->emEventReceived;
+		VIDEO_DBG("Event Received[%X]\n", eduChanged.dw);
+		// Mask out the changes we are waiting on
+		eduReturned.dw = eduChanged.dw & eduRequested.dw;
+
+		// Reset flags of changes that have been returned
+		eduChanged.dw &= ~(eduReturned.dw);
+		VIDEO_DBG("Event Reset[%X]\n", eduChanged.dw);
+		ppctContextTable[(int) ri->rc]->emEventReceived = eduChanged.em;
+
+		// Copy changes back to ri
+		ri->em = eduReturned.em;
+		VIDEO_DBG("ri->em:\n");
+		display_event_map(&ri->em);
+		ri->rs = SuccessStatus;
+		VIDEO_DBG("Success [%x]\n",
+			eduReturned.dw);
+	}
+}
+
+static void update_context_events(struct AstRVAS *pAstRVAS,
+		union EmDwordUnion eduFge_status)
+{
+	union EmDwordUnion eduEmReceived;
+	u32 dwIter = 0;
+	struct ContextTable **ppctContextTable = pAstRVAS->ppctContextTable;
+	// VIDEO_DBG("Setting up context\n");
+	for (dwIter = 0; dwIter < MAX_NUM_CONTEXT; ++dwIter) {
+		if (ppctContextTable[dwIter] != NULL) {
+			//          VIDEO_DBG ("Copying EventMap to RVAS Context\n");
+			memcpy((void *) &eduEmReceived,
+					(void *) &(ppctContextTable[dwIter]->emEventReceived),
+					sizeof(union EmDwordUnion));
+			eduEmReceived.dw |= eduFge_status.dw;
+			memcpy(
+					(void *) &(ppctContextTable[dwIter]->emEventReceived),
+					(void *) &eduEmReceived,
+					sizeof(union EmDwordUnion));
+		}
+	}
+	pAstRVAS->video_intr_occurred = 1;
+	video_ss_wakeup_on_timeout(&pAstRVAS->video_wait);
+}
+
+static irqreturn_t fge_handler(int irq, void *dev_id)
+{
+	union EmDwordUnion eduFge_status;
+	u32 tse_sts = 0;
+	u32 dwGRCEStatus = 0;
+	bool bFgeItr = false;
+	bool bTfeItr = false;
+	bool bBSEItr = false;
+	bool bLdmaItr = false;
+	bool vg_changed = false;
+	u32 dw_screen_offset = 0;
+	struct AstRVAS *pAstRVAS = (struct AstRVAS *) dev_id;
+	struct VideoGeometry *cur_vg = NULL;
+
+	memset(&eduFge_status, 0x0, sizeof(union EmDwordUnion));
+	bFgeItr = false;
+	VIDEO_DBG("fge_handler");
+	// Checking for GRC status changes
+	dwGRCEStatus = readl((void *)(pAstRVAS->grce_reg_base + GRCE_STATUS_REGISTER));
+	if (dwGRCEStatus & GRC_INT_STS_MASK) {
+		VIDEO_DBG("GRC Status Changed: %#x\n", dwGRCEStatus);
+		eduFge_status.dw |= dwGRCEStatus & GRC_INT_STS_MASK;
+		bFgeItr = true;
+
+		if (dwGRCEStatus & 0x30) {
+			dw_screen_offset = get_screen_offset(pAstRVAS);
+
+			if (pAstRVAS->dwScreenOffset != dw_screen_offset) {
+				pAstRVAS->dwScreenOffset = dw_screen_offset;
+				vg_changed = true;
+			}
+		}
+	}
+	vg_changed |= video_geometry_change(pAstRVAS, dwGRCEStatus);
+	if (vg_changed) {
+		eduFge_status.em.bGeometryChanged = true;
+		bFgeItr = true;
+		set_snoop_engine(vg_changed, pAstRVAS);
+		video_set_Window(pAstRVAS);
+		VIDEO_DBG("Geometry has changed\n");
+		VIDEO_DBG("Reconfigure TSE\n");
+	}
+	// Checking and clear TSE Intr Status
+	tse_sts = clear_tse_interrupt(pAstRVAS);
+
+	if (tse_sts & TSSTS_ALL) {
+		bFgeItr = true;
+		if (tse_sts & (TSSTS_TC_SCREEN0|TSSTS_TC_SCREEN1)) {
+			eduFge_status.em.bSnoopChanged = 1;
+			cur_vg = &(pAstRVAS->current_vg);
+
+			if (cur_vg->gmt == TextMode) {
+				eduFge_status.em.bTextASCIIChanged = 1;
+				eduFge_status.em.bTextATTRChanged = 1;
+				eduFge_status.em.bTextFontChanged = 1;
+			}
+		}
+		if (tse_sts & TSSTS_ASCII) {
+			//VIDEO_DBG("Text Ascii Changed\n");
+			eduFge_status.em.bTextASCIIChanged = 1;
+		}
+
+		if (tse_sts & TSSTS_ATTR) {
+			//VIDEO_DBG("Text Attr Changed\n");
+			eduFge_status.em.bTextATTRChanged = 1;
+		}
+
+		if (tse_sts & TSSTS_FONT) {
+			//VIDEO_DBG("Text Font Changed\n");
+			eduFge_status.em.bTextFontChanged = 1;
+		}
+	}
+
+	if (clear_ldma_interrupt(pAstRVAS)) {
+		bLdmaItr = true;
+		pAstRVAS->ldma_engine.finished = 1;
+		video_ss_wakeup_on_timeout(&pAstRVAS->ldma_engine.wait);
+	}
+
+	if (clear_tfe_interrupt(pAstRVAS)) {
+		bTfeItr = true;
+		pAstRVAS->tfe_engine.finished = 1;
+		video_ss_wakeup_on_timeout(&pAstRVAS->tfe_engine.wait);
+	}
+
+	if (clear_bse_interrupt(pAstRVAS)) {
+		bBSEItr = true;
+		pAstRVAS->bse_engine.finished = 1;
+		video_ss_wakeup_on_timeout(&pAstRVAS->bse_engine.wait);
+	}
+
+	if ((!bFgeItr) && (!bTfeItr) && (!bBSEItr) && (!bLdmaItr)) {
+		//VIDEO_DBG(" Unknown Interrupt\n");
+//      VIDEO_DBG("TFE CRT [%#x].", *fge_intr);
+		return IRQ_NONE;
+	}
+
+	if (bFgeItr) {
+		update_context_events(pAstRVAS, eduFge_status);
+		pAstRVAS->video_intr_occurred = 1;
+		video_ss_wakeup_on_timeout(&pAstRVAS->video_wait);
+	}
+
+	return IRQ_HANDLED;
+}
+
+/*Sleep and Wakeup Functions*/
+
+void video_os_init_sleep_struct(struct Video_OsSleepStruct *Sleep)
+{
+	init_waitqueue_head(&(Sleep->queue));
+	Sleep->Timeout = 0;
+}
+
+void video_ss_wakeup_on_timeout(struct Video_OsSleepStruct *Sleep)
+{
+	/* Wakeup Process and Kill timeout handler */
+	wake_up(&(Sleep->queue));
+}
+
+long video_os_sleep_on_timeout(struct Video_OsSleepStruct *Sleep, u8 *Var, long msecs)
+{
+	long timeout; /* In jiffies */
+	u8 *Condition = Var;
+	/* Sleep on the Condition for a wakeup */
+	timeout = wait_event_interruptible_timeout(Sleep->queue,
+		(*Condition == 1), msecs_to_jiffies(msecs));
+
+	return timeout;
+}
+
+void disable_video_engines(struct AstRVAS *pAstRVAS)
+{
+	clk_disable(pAstRVAS->eclk);
+	clk_disable(pAstRVAS->vclk);
+}
+
+void enable_video_engines(struct AstRVAS *pAstRVAS)
+{
+	clk_enable(pAstRVAS->eclk);
+	clk_enable(pAstRVAS->vclk);
+}
+
+
+void disable_rvas_engines(struct AstRVAS *pAstRVAS)
+{
+	clk_disable(pAstRVAS->rvasclk);
+}
+
+void enable_rvas_engines(struct AstRVAS *pAstRVAS)
+{
+	// clk enable does
+	//	reset engine reset at SCU040
+	//	delay 100 us
+	//	enable clock at SCU080
+	//	delay 10ms
+	//	disable engine reset at SCU040
+	clk_enable(pAstRVAS->rvasclk);
+}
+
+
+static void reset_rvas_engine(struct AstRVAS *pAstRVAS)
+{
+	disable_rvas_engines(pAstRVAS);
+	enable_rvas_engines(pAstRVAS);
+	rvas_init();
+}
+
+static void reset_video_engine(struct AstRVAS *pAstRVAS)
+{
+	disable_video_engines(pAstRVAS);
+	enable_video_engines(pAstRVAS);
+	video_engine_init();
+}
+
+void ioctl_reset_video_engine(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	enum ResetEngineMode resetMode = ri->resetMode;
+
+	switch (resetMode) {
+	case  ResetAll:
+		VIDEO_ENG_DBG("reset all engine\n");
+		reset_rvas_engine(pAstRVAS);
+		reset_video_engine(pAstRVAS);
+		break;
+	case ResetRvasEngine:
+		VIDEO_ENG_DBG("reset rvas engine\n");
+		reset_rvas_engine(pAstRVAS);
+		break;
+	case ResetVeEngine:
+		VIDEO_ENG_DBG("reset video engine\n");
+		reset_video_engine(pAstRVAS);
+		break;
+	default:
+		dev_err(pAstRVAS->pdev, "Error resetting: no such mode: %d\n", resetMode);
+		break;
+	}
+
+	if (ri)
+		ri->rs = SuccessStatus;
+
+}
+
+static ssize_t rvas_reset_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct AstRVAS *pAstRVAS = dev_get_drvdata(dev);
+	u32 val = kstrtoul(buf, 10, NULL);
+
+	if (val)
+		ioctl_reset_video_engine(NULL, pAstRVAS);
+
+	return count;
+}
+
+static DEVICE_ATTR_WO(rvas_reset);
+
+static struct attribute *ast_rvas_attributes[] = {
+	&dev_attr_rvas_reset.attr,
+	NULL
+};
+
+static const struct attribute_group rvas_attribute_group = {
+	.attrs = ast_rvas_attributes
+};
+
+bool sleep_on_tfe_busy(struct AstRVAS *pAstRVAS, u32 dwTFEDescriptorAddr,
+	u32 dwTFEControlR, u32 dwTFERleLimitor, u32 *pdwRLESize,
+	u32 *pdwCheckSum)
+{
+	u32 addrTFEDTBR = pAstRVAS->fg_reg_base + TFE_Descriptor_Table_Offset;
+	u32 addrTFECR = pAstRVAS->fg_reg_base + TFE_Descriptor_Control_Resgister;
+	u32 addrTFERleL = pAstRVAS->fg_reg_base + TFE_RLE_LIMITOR;
+	u32 addrTFERSTS = pAstRVAS->fg_reg_base + TFE_Status_Register;
+	bool bResult = true;
+
+	down(&pAstRVAS->tfe_engine.sem);
+	VIDEO_DBG("In Busy Semaphore......\n");
+
+	VIDEO_DBG("Before change, TFECR: %#x\n", readl((void *)addrTFECR));
+	writel(dwTFEControlR, (void *)addrTFECR);
+	VIDEO_DBG("After change, TFECR: %#x\n", readl((void *)addrTFECR));
+	writel(dwTFERleLimitor, (void *)addrTFERleL);
+	VIDEO_DBG("dwTFEControlR: %#x\n", dwTFEControlR);
+	VIDEO_DBG("dwTFERleLimitor: %#x\n", dwTFERleLimitor);
+	VIDEO_DBG("dwTFEDescriptorAddr: %#x\n", dwTFEDescriptorAddr);
+	// put descriptor add to TBR and Fetch start
+	writel(dwTFEDescriptorAddr, (void *)addrTFEDTBR);
+	//wTFETiles = 1;
+	pAstRVAS->tfe_engine.finished = 0;
+	video_os_sleep_on_timeout(&pAstRVAS->tfe_engine.wait,
+		&pAstRVAS->tfe_engine.finished, TFE_TIMEOUT_IN_MS);
+
+	if (!pAstRVAS->tfe_engine.finished) {
+		dev_err(pAstRVAS->pdev, "Video TFE failed\n");
+		writel(0x00, (void *)addrTFERSTS);
+		pAstRVAS->tfe_engine.finished = 1;
+		bResult = false;
+	}
+
+	writel((readl((void *)addrTFECR)&(~0x3)), (void *)addrTFECR); // Disable IRQ and Turn off TFE when done
+	*pdwRLESize = readl((void *)(pAstRVAS->fg_reg_base + TFE_RLE_Byte_Count));
+	*pdwCheckSum = readl((void *)(pAstRVAS->fg_reg_base + TFE_RLE_CheckSum));
+
+	up(&pAstRVAS->tfe_engine.sem);
+	VIDEO_DBG("Done Busy: bResult: %d\n", bResult);
+
+	return bResult;
+}
+
+bool sleep_on_tfe_text_busy(struct AstRVAS *pAstRVAS, u32 dwTFEDescriptorAddr,
+	u32 dwTFEControlR, u32 dwTFERleLimitor, u32 *pdwRLESize,
+	u32 *pdwCheckSum)
+{
+	u32 addrTFEDTBR = pAstRVAS->fg_reg_base + TFE_Descriptor_Table_Offset;
+	u32 addrTFECR = pAstRVAS->fg_reg_base + TFE_Descriptor_Control_Resgister;
+	u32 addrTFERleL = pAstRVAS->fg_reg_base + TFE_RLE_LIMITOR;
+	u32 addrTFERSTS = pAstRVAS->fg_reg_base + TFE_Status_Register;
+	bool bResult = true;
+
+	down(&pAstRVAS->tfe_engine.sem);
+	VIDEO_DBG("In Busy Semaphore......\n");
+
+	VIDEO_DBG("Before change, TFECR: %#x\n", readl((void *)addrTFECR));
+	writel(dwTFEControlR, (void *)addrTFECR);
+	VIDEO_DBG("After change, TFECR: %#x\n", readl((void *)addrTFECR));
+	writel(dwTFERleLimitor, (void *)addrTFERleL);
+	VIDEO_DBG("dwTFEControlR: %#x\n", dwTFEControlR);
+	VIDEO_DBG("dwTFERleLimitor: %#x\n", dwTFERleLimitor);
+	VIDEO_DBG("dwTFEDescriptorAddr: %#x\n", dwTFEDescriptorAddr);
+	// put descriptor add to TBR and Fetch start
+	writel(dwTFEDescriptorAddr, (void *)addrTFEDTBR);
+	//wTFETiles = 1;
+	pAstRVAS->tfe_engine.finished = 0;
+	video_os_sleep_on_timeout(&pAstRVAS->tfe_engine.wait,
+		&pAstRVAS->tfe_engine.finished, TFE_TIMEOUT_IN_MS);
+
+	if (!pAstRVAS->tfe_engine.finished) {
+		dev_err(pAstRVAS->pdev, "Video TFE failed\n");
+		writel(0x00, (void *)addrTFERSTS);
+		pAstRVAS->tfe_engine.finished = 1;
+		bResult = false;
+	}
+
+	writel((readl((void *)addrTFECR)&(~0x3)), (void *)addrTFECR);// Disable IRQ and Turn off TFE when done
+	writel((readl((void *)addrTFERSTS)|0x2), (void *)addrTFERSTS); // clear status bit
+	*pdwRLESize = readl((void *)(pAstRVAS->fg_reg_base + TFE_RLE_Byte_Count));
+	*pdwCheckSum = readl((void *)(pAstRVAS->fg_reg_base + TFE_RLE_CheckSum));
+
+	up(&pAstRVAS->tfe_engine.sem);
+	VIDEO_DBG("Done Busy: bResult: %d\n", bResult);
+
+	return bResult;
+}
+
+bool sleep_on_bse_busy(struct AstRVAS *pAstRVAS, u32 dwBSEDescriptorAddr,
+		struct BSEAggregateRegister aBSEAR, u32 size)
+{
+	u32 addrBSEDTBR = pAstRVAS->fg_reg_base + BSE_Descriptor_Table_Base_Register;
+	u32 addrBSCR = pAstRVAS->fg_reg_base + BSE_Command_Register;
+	u32 addrBSDBS = pAstRVAS->fg_reg_base + BSE_Destination_Buket_Size_Resgister;
+	u32 addrBSBPS0 = pAstRVAS->fg_reg_base + BSE_Bit_Position_Register_0;
+	u32 addrBSBPS1 = pAstRVAS->fg_reg_base + BSE_Bit_Position_Register_1;
+	u32 addrBSBPS2 = pAstRVAS->fg_reg_base + BSE_Bit_Position_Register_2;
+	u32 addrBSESSTS = pAstRVAS->fg_reg_base + BSE_Status_Register;
+	u8 byCounter = 0;
+	bool bResult = true;
+
+	down(&pAstRVAS->bse_engine.sem);
+	pAstRVAS->bse_engine.finished = 0;
+
+    // Set BSE Temp buffer address, and clear lower u16
+	writel(BSE_LMEM_Temp_Buffer_Offset << 16, (void *)addrBSCR);
+	writel(readl((void *)addrBSCR)|(aBSEAR.dwBSCR & 0X00000FFF), (void *)addrBSCR);
+	writel(aBSEAR.dwBSDBS, (void *)addrBSDBS);
+	writel(aBSEAR.adwBSBPS[0], (void *)addrBSBPS0);
+	writel(aBSEAR.adwBSBPS[1], (void *)addrBSBPS1);
+	writel(aBSEAR.adwBSBPS[2], (void *)addrBSBPS2);
+
+	writel(dwBSEDescriptorAddr, (void *)addrBSEDTBR);
+
+	while (!pAstRVAS->bse_engine.finished) {
+		VIDEO_DBG("BSE Sleeping...\n");
+		video_os_sleep_on_timeout(&pAstRVAS->bse_engine.wait,
+			&pAstRVAS->bse_engine.finished, 1000); // loop if bse timedout
+		byCounter++;
+		VIDEO_DBG("Back from BSE Sleeping, finished: %u\n",
+			pAstRVAS->bse_engine.finished);
+
+		if (byCounter == ENGINE_TIMEOUT_IN_SECONDS) {
+			writel(0x00, (void *)addrBSESSTS);
+			pAstRVAS->bse_engine.finished = 1;
+			dev_err(pAstRVAS->pdev, "TIMEOUT::Waiting BSE\n");
+			bResult = false;
+		}
+	}
+
+	VIDEO_DBG("*pdwBSESSTS = %#x\n", readl((void *)addrBSESSTS));
+	writel(readl((void *)addrBSCR)&(~0x3), (void *)addrBSCR);
+
+	up(&pAstRVAS->bse_engine.sem);
+
+	return bResult;
+}
+
+void sleep_on_ldma_busy(struct AstRVAS *pAstRVAS, u32 dwDescriptorAddress)
+{
+	u32 addrLDMADTBR = pAstRVAS->fg_reg_base + LDMA_Descriptor_Table_Base_Register;
+	u32 addrLDMAControlR = pAstRVAS->fg_reg_base + LDMA_Control_Register;
+
+	VIDEO_DBG("In sleepONldma busy\n");
+
+	down(&pAstRVAS->ldma_engine.sem);
+
+	pAstRVAS->ldma_engine.finished = 0;
+
+	writel(0x83, (void *)addrLDMAControlR);// descriptor can only in LMEM FOR LDMA
+	writel(dwDescriptorAddress, (void *)addrLDMADTBR);
+	VIDEO_DBG("LDMA: control [%#x]\n", readl((void *)addrLDMAControlR));
+	VIDEO_DBG("LDMA:  DTBR  [%#x]\n", readl((void *)addrLDMADTBR));
+
+	while (!pAstRVAS->ldma_engine.finished)
+		video_os_sleep_on_timeout(&pAstRVAS->ldma_engine.wait, (u8 *)&pAstRVAS->ldma_engine.finished, 1000); // loop if bse timedout
+
+	VIDEO_DBG("LDMA wake up\n");
+	writel(readl((void *)addrLDMAControlR)&(~0x3), (void *)addrLDMAControlR);
+	up(&pAstRVAS->ldma_engine.sem);
+}
+
+static int video_drv_get_resources(struct platform_device *pdev)
+{
+	int result = 0;
+
+	struct resource *io_fg;
+	struct resource *io_grc;
+	struct resource *io_video;
+
+	//get resources from platform
+	io_fg = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	VIDEO_DBG("io_fg: 0x%p\n", io_fg);
+
+	if (io_fg == NULL) {
+		dev_err(&pdev->dev, "No Frame Grabber IORESOURCE_MEM entry\n");
+		return -ENOENT;
+	}
+	io_grc = platform_get_resource(pdev, IORESOURCE_MEM, 1);
+	VIDEO_DBG("io_grc: 0x%p\n", io_grc);
+	if (io_grc == NULL) {
+		dev_err(&pdev->dev, "No GRCE IORESOURCE_MEM entry\n");
+		return -ENOENT;
+	}
+	io_video = platform_get_resource(pdev, IORESOURCE_MEM, 2);
+	VIDEO_DBG("io_video: 0x%p\n", io_video);
+	if (io_video == NULL) {
+		dev_err(&pdev->dev, "No video compression IORESOURCE_MEM entry\n");
+		return -ENOENT;
+	}
+
+	//map resource by device
+	pAstRVAS->fg_reg_base = (u32) devm_ioremap_resource(&pdev->dev, io_fg);
+	VIDEO_DBG("fg_reg_base: %#x\n", pAstRVAS->fg_reg_base);
+	if (IS_ERR((void *) pAstRVAS->fg_reg_base)) {
+		result = PTR_ERR((void *) pAstRVAS->fg_reg_base);
+		dev_err(&pdev->dev, "Cannot map FG registers\n");
+		pAstRVAS->fg_reg_base = 0;
+		return result;
+	}
+	pAstRVAS->grce_reg_base = (u32) devm_ioremap_resource(&pdev->dev,
+			  io_grc);
+	VIDEO_DBG("grce_reg_base: %#x\n", pAstRVAS->grce_reg_base);
+	if (IS_ERR((void *) pAstRVAS->grce_reg_base)) {
+		result = PTR_ERR((void *) pAstRVAS->grce_reg_base);
+		dev_err(&pdev->dev, "Cannot map GRC registers\n");
+		pAstRVAS->grce_reg_base = 0;
+		return result;
+	}
+	pAstRVAS->video_reg_base = (u32) devm_ioremap_resource(&pdev->dev,
+				  io_video);
+	VIDEO_DBG("video_reg_base: %#x\n", pAstRVAS->video_reg_base);
+	if (IS_ERR((void *) pAstRVAS->video_reg_base)) {
+		result = PTR_ERR((void *) pAstRVAS->video_reg_base);
+		dev_err(&pdev->dev, "Cannot map video registers\n");
+		pAstRVAS->video_reg_base = 0;
+		return result;
+	}
+	return 0;
+}
+
+static int video_drv_get_irqs(struct platform_device *pdev)
+{
+	pAstRVAS->irq_fge = platform_get_irq(pdev, 0);
+	VIDEO_DBG("irq_fge: %#x\n", pAstRVAS->irq_fge);
+	if (pAstRVAS->irq_fge < 0) {
+		dev_err(&pdev->dev, "NO FGE irq entry\n");
+		return -ENOENT;
+	}
+	pAstRVAS->irq_vga = platform_get_irq(pdev, 1);
+	VIDEO_DBG("irq_vga: %#x\n", pAstRVAS->irq_vga);
+		if (pAstRVAS->irq_vga < 0) {
+			dev_err(&pdev->dev, "NO VGA irq entry\n");
+			return -ENOENT;
+		}
+	pAstRVAS->irq_video = platform_get_irq(pdev, 2);
+	VIDEO_DBG("irq_video: %#x\n", pAstRVAS->irq_video);
+	if (pAstRVAS->irq_video < 0) {
+		dev_err(&pdev->dev, "NO video compression entry\n");
+		return -ENOENT;
+	}
+	return 0;
+}
+
+static int video_drv_get_clock(struct platform_device *pdev)
+{
+	//enable Video Engine clocks:
+	//vclk enable will not reset
+	//eclk enable will reset the Video Engine
+	//so enable vclk first then eclk
+	pAstRVAS->vclk = devm_clk_get(&pdev->dev, "vclk");
+	if (IS_ERR(pAstRVAS->vclk)) {
+		dev_err(&pdev->dev, "no vclk clock defined\n");
+		return PTR_ERR(pAstRVAS->vclk);
+	}
+
+	clk_prepare_enable(pAstRVAS->vclk);
+
+	pAstRVAS->eclk = devm_clk_get(&pdev->dev, "eclk");
+	if (IS_ERR(pAstRVAS->eclk)) {
+		dev_err(&pdev->dev, "no eclk clock defined\n");
+		return PTR_ERR(pAstRVAS->eclk);
+	}
+
+	clk_prepare_enable(pAstRVAS->eclk);
+
+	//enable RVAS engine clock
+	pAstRVAS->rvasclk = devm_clk_get(&pdev->dev, "rvasclk-gate");
+	if (IS_ERR(pAstRVAS->rvasclk)) {
+		dev_err(&pdev->dev, "no rvasclk clock defined\n");
+		return PTR_ERR(pAstRVAS->rvasclk);
+	}
+
+	clk_prepare_enable(pAstRVAS->rvasclk);
+	return 0;
+}
+
+static int video_drv_map_irqs(struct platform_device *pdev)
+{
+	int result = 0;
+	//Map IRQS to handler
+	VIDEO_DBG("Requesting IRQs, irq_fge: %d, irq_vga: %d, irq_video: %d\n",
+			  pAstRVAS->irq_fge, pAstRVAS->irq_vga, pAstRVAS->irq_video);
+
+	result = devm_request_irq(&pdev->dev, pAstRVAS->irq_fge, fge_handler, 0,
+			  dev_name(&pdev->dev), pAstRVAS);
+	if (result) {
+		pr_err("Error in requesting IRQ\n");
+		pr_err("RVAS: Failed request FGE irq %d\n", pAstRVAS->irq_fge);
+		misc_deregister(&video_misc);
+		return result;
+	}
+
+	result = devm_request_irq(&pdev->dev, pAstRVAS->irq_vga, fge_handler, 0,
+				  dev_name(&pdev->dev), pAstRVAS);
+	if (result) {
+		pr_err("Error in requesting IRQ\n");
+		pr_err("RVAS: Failed request vga irq %d\n", pAstRVAS->irq_vga);
+		misc_deregister(&video_misc);
+		return result;
+	}
+
+	result = devm_request_irq(&pdev->dev, pAstRVAS->irq_video, ast_video_isr, 0,
+				  dev_name(&pdev->dev), pAstRVAS);
+	if (result) {
+		pr_err("Error in requesting IRQ\n");
+		pr_err("RVAS: Failed request video irq %d\n", pAstRVAS->irq_video);
+		misc_deregister(&video_misc);
+		return result;
+	}
+
+	return result;
+}
+//
+//
+//
+static int video_drv_probe(struct platform_device *pdev)
+{
+	int result = 0;
+
+	struct regmap *sdram_scu;
+	struct device_node *dp_node;
+	struct device_node *edac_node;
+	void __iomem *mcr_base;
+
+	VIDEO_DBG("RVAS driver probe\n");
+	pAstRVAS = devm_kzalloc(&pdev->dev, sizeof(struct AstRVAS), GFP_KERNEL);
+	VIDEO_DBG("pAstRVAS: 0x%p\n", pAstRVAS);
+
+	if (!pAstRVAS) {
+		dev_err(pAstRVAS->pdev, "Cannot allocate device structure\n");
+		return -ENOMEM;
+	}
+	pAstRVAS->pdev = (void *)&pdev->dev;
+
+
+	// Get resources
+	result = video_drv_get_resources(pdev);
+	if (result < 0) {
+		dev_err(pAstRVAS->pdev, "video_probe: Error getting resources\n");
+		return result;
+	}
+
+	//get irqs
+	result = video_drv_get_irqs(pdev);
+	if (result < 0) {
+		dev_err(pAstRVAS->pdev, "video_probe: Error getting irqs\n");
+		return result;
+	}
+
+
+	pAstRVAS->rvas_reset = devm_reset_control_get_by_index(&pdev->dev, 0);
+	if (IS_ERR(pAstRVAS->rvas_reset)) {
+		dev_err(&pdev->dev, "can't get rvas reset\n");
+		return -ENOENT;
+	}
+
+	pAstRVAS->video_engine_reset = devm_reset_control_get_by_index(&pdev->dev, 1);
+	if (IS_ERR(pAstRVAS->video_engine_reset)) {
+		dev_err(&pdev->dev, "can't get video engine reset\n");
+		return -ENOENT;
+	}
+
+	//prepare video engine clock
+	result = video_drv_get_clock(pdev);
+	if (result < 0) {
+		dev_err(pAstRVAS->pdev, "video_probe: Error getting clocks\n");
+		return result;
+	}
+
+	dp_node = of_find_compatible_node(NULL, NULL, "aspeed,ast2600-displayport");
+	if (!dp_node)
+		dev_err(&pdev->dev, "cannot find dp node\n");
+	else {
+		dp_base = of_iomap(dp_node, 0);
+		if (!dp_base)
+			dev_err(&pdev->dev, "failed to iomem of display port\n");
+	}
+
+	edac_node = of_find_compatible_node(NULL, NULL, "aspeed,ast2600-sdram-edac");
+	if (!edac_node)
+		dev_err(&pdev->dev, "cannot find edac node\n");
+	else {
+		mcr_base = of_iomap(edac_node, 0);
+		if (!mcr_base)
+			dev_err(&pdev->dev, "failed to iomem of MCR\n");
+	}
+
+	set_FBInfo_size(pAstRVAS, mcr_base);
+
+	//scu
+	sdram_scu = syscon_regmap_lookup_by_compatible("aspeed,ast2600-scu");
+	VIDEO_DBG("sdram_scu: 0x%p\n", sdram_scu);
+	if (IS_ERR(sdram_scu)) {
+		dev_err(&pdev->dev, "failed to find ast2600-scu regmap\n");
+		return PTR_ERR(sdram_scu);
+	}
+	pAstRVAS->scu = sdram_scu;
+
+	result = misc_register(&video_misc);
+	if (result) {
+		pr_err("Failed in miscellaneous register (err: %d)\n", result);
+		return result;
+	}
+	pr_info("Video misc minor %d\n", video_misc.minor);
+
+	if (sysfs_create_group(&pdev->dev.kobj, &rvas_attribute_group)) {
+		pr_err("Failed in creating group\n");
+		return -1;
+	}
+
+	VIDEO_DBG("Disabling interrupts...\n");
+	disable_grce_tse_interrupt(pAstRVAS);
+
+	//reserve memory
+	of_reserved_mem_device_init(&pdev->dev);
+	result = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));
+	if (result) {
+		dev_err(&pdev->dev, "Failed to set DMA mask\n");
+		of_reserved_mem_device_release(&pdev->dev);
+	}
+
+	// map irqs to irq_handlers
+	result = video_drv_map_irqs(pdev);
+	if (result < 0) {
+		dev_err(pAstRVAS->pdev, "video_probe: Error mapping irqs\n");
+		return result;
+	}
+	VIDEO_DBG("After IRQ registration\n");
+
+
+	platform_set_drvdata(pdev, pAstRVAS);
+	pAstRVAS->rvas_dev = &video_misc;
+	VIDEO_DBG("pdev: 0x%p dev: 0x%p pAstRVAS: 0x%p rvas_dev: 0x%p\n", pdev,
+		&pdev->dev, pAstRVAS, pAstRVAS->rvas_dev);
+
+	init_osr_es(pAstRVAS);
+	rvas_init();
+	video_engine_reserveMem(pAstRVAS);
+	video_engine_init();
+
+
+	pr_info("RVAS: driver successfully loaded.\n");
+	return result;
+}
+
+static void rvas_init(void)
+{
+	VIDEO_ENG_DBG("\n");
+
+	reset_snoop_engine(pAstRVAS);
+	update_video_geometry(pAstRVAS);
+
+	set_snoop_engine(true, pAstRVAS);
+	enable_grce_tse_interrupt(pAstRVAS);
+}
+
+static void video_engine_init(void)
+{
+	VIDEO_ENG_DBG("\n");
+	// video engine
+	disable_video_interrupt(pAstRVAS);
+	video_ctrl_init(pAstRVAS);
+	video_engine_rc4Reset(pAstRVAS);
+	set_direct_mode(pAstRVAS);
+	video_set_Window(pAstRVAS);
+	enable_video_interrupt(pAstRVAS);
+}
+
+static int video_drv_remove(struct platform_device *pdev)
+{
+	struct AstRVAS *pAstRVAS = NULL;
+
+	VIDEO_DBG("\n");
+	pAstRVAS = platform_get_drvdata(pdev);
+
+	VIDEO_DBG("disable_grce_tse_interrupt...\n");
+	disable_grce_tse_interrupt(pAstRVAS);
+	disable_video_interrupt(pAstRVAS);
+
+	sysfs_remove_group(&pdev->dev.kobj, &rvas_attribute_group);
+
+	VIDEO_DBG("misc_deregister...\n");
+	misc_deregister(&video_misc);
+
+	VIDEO_DBG("Releasing OSRes...\n");
+	release_osr_es(pAstRVAS);
+	pr_info("RVAS: driver successfully unloaded.\n");
+	free_video_engine_memory(pAstRVAS);
+	return 0;
+}
+
+static const u32 ast2400_dram_table[] = {
+	0x04000000,     //64MB
+	0x08000000,     //128MB
+	0x10000000,     //256MB
+	0x20000000,     //512MB
+};
+
+static const u32 ast2500_dram_table[] = {
+	0x08000000,     //128MB
+	0x10000000,     //256MB
+	0x20000000,     //512MB
+	0x40000000,     //1024MB
+};
+
+static const u32 ast2600_dram_table[] = {
+	0x10000000,     //256MB
+	0x20000000,     //512MB
+	0x40000000,     //1024MB
+	0x80000000,     //2048MB
+};
+
+static const u32 aspeed_vga_table[] = {
+	0x800000,       //8MB
+	0x1000000,      //16MB
+	0x2000000,      //32MB
+	0x4000000,      //64MB
+};
+
+static void set_FBInfo_size(struct AstRVAS *pAstRVAS, void __iomem *mcr_base)
+{
+	u32 reg_mcr004 = readl(mcr_base + MCR_CONF);
+
+#if defined(CONFIG_MACH_ASPEED_G6)
+	pAstRVAS->FBInfo.dwDRAMSize = ast2600_dram_table[reg_mcr004 & 0x3];
+#elif defined(CONFIG_MACH_ASPEED_G5)
+	pAstRVAS->FBInfo.dwDRAMSize = ast2500_dram_table[reg_mcr004 & 0x3];
+#else
+	pAstRVAS->FBInfo.dwDRAMSize = ast2400_dram_table[reg_mcr004 & 0x3];
+#endif
+
+	pAstRVAS->FBInfo.dwVGASize = aspeed_vga_table[((reg_mcr004 & 0xC) >> 2)];
+
+}
+
+static const struct of_device_id ast_rvas_match[] = { { .compatible =
+	"aspeed,ast2600-rvas", }, { }, };
+
+MODULE_DEVICE_TABLE(of, ast_rvas_match);
+
+static struct platform_driver video_driver = {
+	.probe = video_drv_probe,
+	.remove = video_drv_remove,
+	.driver = { .of_match_table = of_match_ptr(ast_rvas_match), .name =
+		RVAS_DRIVER_NAME, .owner = THIS_MODULE, }, };
+
+module_platform_driver(video_driver);
+
+MODULE_AUTHOR("ASPEED Technology");
+MODULE_DESCRIPTION("RVAS video driver module for AST2600");
+MODULE_LICENSE("GPL");
diff --git a/include/linux/aspeed-mctp.h b/include/linux/aspeed-mctp.h
new file mode 100644
index 000000000000..7fbbf301be3b
--- /dev/null
+++ b/include/linux/aspeed-mctp.h
@@ -0,0 +1,155 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/* Copyright (c) 2020 Intel Corporation */
+
+#ifndef __LINUX_ASPEED_MCTP_H
+#define __LINUX_ASPEED_MCTP_H
+
+#include <linux/types.h>
+
+struct mctp_client;
+struct aspeed_mctp;
+
+struct pcie_transport_hdr {
+	u8 fmt_type;
+	u8 mbz;
+	u8 mbz_attr_len_hi;
+	u8 len_lo;
+	u16 requester;
+	u8 tag;
+	u8 code;
+	u16 target;
+	u16 vendor;
+} __packed;
+
+struct mctp_protocol_hdr {
+	u8 ver;
+	u8 dest;
+	u8 src;
+	u8 flags_seq_tag;
+} __packed;
+
+#define PCIE_VDM_HDR_SIZE 16
+#define MCTP_BTU_SIZE 64
+/* The MTU of the ASPEED MCTP can be 64/128/256 */
+#define ASPEED_MCTP_MTU MCTP_BTU_SIZE
+#define PCIE_VDM_DATA_SIZE_DW (ASPEED_MCTP_MTU / 4)
+#define PCIE_VDM_HDR_SIZE_DW (PCIE_VDM_HDR_SIZE / 4)
+
+#define PCIE_MCTP_MIN_PACKET_SIZE (PCIE_VDM_HDR_SIZE + 4)
+
+struct mctp_pcie_packet_data_2500 {
+	u32 data[32];
+};
+
+struct mctp_pcie_packet_data {
+	u32 hdr[PCIE_VDM_HDR_SIZE_DW];
+	u32 payload[PCIE_VDM_DATA_SIZE_DW];
+};
+
+struct mctp_pcie_packet {
+	struct mctp_pcie_packet_data data;
+	u32 size;
+};
+
+/**
+ * aspeed_mctp_add_type_handler() - register for the given MCTP message type
+ * @client: pointer to the existing mctp_client context
+ * @mctp_type: message type code according to DMTF DSP0239 spec.
+ * @pci_vendor_id: vendor ID (non-zero if msg_type is Vendor Defined PCI,
+ * otherwise it should be set to 0)
+ * @vdm_type: vendor defined message type (it should be set to 0 for non-Vendor
+ * Defined PCI message type)
+ * @vdm_mask: vendor defined message mask (it should be set to 0 for non-Vendor
+ * Defined PCI message type)
+ *
+ * Return:
+ * * 0 - success,
+ * * -EINVAL - arguments passed are incorrect,
+ * * -ENOMEM - cannot alloc a new handler,
+ * * -EBUSY - given message has already registered handler.
+ */
+
+int aspeed_mctp_add_type_handler(struct mctp_client *client, u8 mctp_type,
+				 u16 pci_vendor_id, u16 vdm_type, u16 vdm_mask);
+
+/**
+ * aspeed_mctp_create_client() - create mctp_client context
+ * @priv pointer to aspeed-mctp context
+ *
+ * Returns struct mctp_client or NULL.
+ */
+struct mctp_client *aspeed_mctp_create_client(struct aspeed_mctp *priv);
+
+/**
+ * aspeed_mctp_delete_client()- delete mctp_client context
+ * @client: pointer to existing mctp_client context
+ */
+void aspeed_mctp_delete_client(struct mctp_client *client);
+
+/**
+ * aspeed_mctp_send_packet() - send mctp_packet
+ * @client: pointer to existing mctp_client context
+ * @tx_packet: the allocated packet that needs to be send via aspeed-mctp
+ *
+ * After the function returns success, the packet is no longer owned by the
+ * caller, and as such, the caller should not attempt to free it.
+ *
+ * Return:
+ * * 0 - success,
+ * * -ENOSPC - failed to send packet due to lack of available space.
+ */
+int aspeed_mctp_send_packet(struct mctp_client *client,
+			    struct mctp_pcie_packet *tx_packet);
+
+/**
+ * aspeed_mctp_receive_packet() - receive mctp_packet
+ * @client: pointer to existing mctp_client context
+ * @timeout: timeout, in jiffies
+ *
+ * The function will sleep for up to @timeout if no packet is ready to read.
+ *
+ * After the function returns valid packet, the caller takes its ownership and
+ * is responsible for freeing it.
+ *
+ * Returns struct mctp_pcie_packet from or ERR_PTR in case of error or the
+ * @timeout elapsed.
+ */
+struct mctp_pcie_packet *aspeed_mctp_receive_packet(struct mctp_client *client,
+						    unsigned long timeout);
+
+/**
+ * aspeed_mctp_flush_rx_queue() - remove all mctp_packets from rx queue
+ * @client: pointer to existing mctp_client context
+ */
+void aspeed_mctp_flush_rx_queue(struct mctp_client *client);
+
+/**
+ * aspeed_mctp_get_eid_bdf() - return PCIe address for requested endpoint ID
+ * @client: pointer to existing mctp_client context
+ * @eid: requested eid
+ * @bdf: pointer to store BDF value
+ *
+ * Return:
+ * * 0 - success,
+ * * -ENOENT - there is no record for requested endpoint id.
+ */
+int aspeed_mctp_get_eid_bdf(struct mctp_client *client, u8 eid, u16 *bdf);
+
+/**
+ * aspeed_mctp_get_eid() - return EID for requested BDF and domainId.
+ * @client: pointer to existing mctp_client context
+ * @bdf: requested BDF value
+ * @domain_id: requested domainId
+ * @eid: pointer to store EID value
+ *
+ * Return:
+ * * 0 - success,
+ * * -ENOENT - there is no record for requested bdf/domainId.
+ */
+int aspeed_mctp_get_eid(struct mctp_client *client, u16 bdf,
+			u8 domain_id, u8 *eid);
+
+void *aspeed_mctp_packet_alloc(gfp_t flags);
+void aspeed_mctp_packet_free(void *packet);
+
+#endif /* __LINUX_ASPEED_MCTP_H */
diff --git a/include/linux/soc/aspeed/aspeed-udma.h b/include/linux/soc/aspeed/aspeed-udma.h
new file mode 100644
index 000000000000..33acea745f1c
--- /dev/null
+++ b/include/linux/soc/aspeed/aspeed-udma.h
@@ -0,0 +1,30 @@
+#ifndef __ASPEED_UDMA_H__
+#define __ASPEED_UDMA_H__
+
+#include <linux/circ_buf.h>
+
+typedef void (*aspeed_udma_cb_t)(int rb_rwptr, void *id);
+
+enum aspeed_udma_ops {
+	ASPEED_UDMA_OP_ENABLE,
+	ASPEED_UDMA_OP_DISABLE,
+	ASPEED_UDMA_OP_RESET,
+};
+
+void aspeed_udma_set_tx_wptr(u32 ch_no, u32 wptr);
+void aspeed_udma_set_rx_rptr(u32 ch_no, u32 rptr);
+
+void aspeed_udma_tx_chan_ctrl(u32 ch_no, enum aspeed_udma_ops op);
+void aspeed_udma_rx_chan_ctrl(u32 ch_no, enum aspeed_udma_ops op);
+
+int aspeed_udma_request_tx_chan(u32 ch_no, dma_addr_t addr,
+				struct circ_buf *rb, u32 rb_sz,
+				aspeed_udma_cb_t cb, void *id, bool en_tmout);
+int aspeed_udma_request_rx_chan(u32 ch_no, dma_addr_t addr,
+				struct circ_buf *rb, u32 rb_sz,
+				aspeed_udma_cb_t cb, void *id, bool en_tmout);
+
+int aspeed_udma_free_tx_chan(u32 ch_no);
+int aspeed_udma_free_rx_chan(u32 ch_no);
+
+#endif
diff --git a/include/uapi/linux/aspeed-mctp.h b/include/uapi/linux/aspeed-mctp.h
new file mode 100644
index 000000000000..ffa90009d258
--- /dev/null
+++ b/include/uapi/linux/aspeed-mctp.h
@@ -0,0 +1,136 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/* Copyright (c) 2020 Intel Corporation */
+
+#ifndef _UAPI_LINUX_ASPEED_MCTP_H
+#define _UAPI_LINUX_ASPEED_MCTP_H
+
+#include <linux/ioctl.h>
+#include <linux/types.h>
+
+/*
+ * aspeed-mctp is a simple device driver exposing a read/write interface:
+ *  +----------------------+
+ *  | PCIe VDM Header      | 16 bytes (Big Endian)
+ *  +----------------------+
+ *  | MCTP Message Payload | 64/128/256/512 bytes (Big Endian)
+ *  +----------------------+
+ *
+ * MCTP packet description can be found in DMTF DSP0238,
+ * MCTP PCIe VDM Transport Specification.
+ */
+
+#define ASPEED_MCTP_PCIE_VDM_HDR_SIZE 16
+
+/*
+ * uevents generated by aspeed-mctp driver
+ */
+#define ASPEED_MCTP_READY "PCIE_READY"
+
+/*
+ * maximum possible number of struct eid_info elements stored in list
+ */
+#define ASPEED_MCTP_EID_INFO_MAX 256
+
+/*
+ * MCTP operations
+ * @ASPEED_MCTP_IOCTL_FILTER_EID: enable/disable filter incoming packets based
+ * on Endpoint ID (BROKEN)
+ * @ASPEED_MCTP_IOCTL_GET_BDF: read PCI bus/device/function of MCTP Controller
+ * @ASPEED_MCTP_IOCTL_GET_MEDIUM_ID: read MCTP physical medium identifier
+ * related to PCIe revision
+ * @ASPEED_MCTP_IOCTL_GET_MTU: read max transmission unit (in bytes)
+ * @ASPEED_MCTP_IOCTL_REGISTER_DEFAULT_HANDLER Register client as default
+ * handler that receives all MCTP messages that were not dispatched to other
+ * clients
+ * @ASPEED_MCTP_IOCTL_REGISTER_TYPE_HANDLER Register client to receive all
+ * messages of specified MCTP type or PCI vendor defined type
+ * @ASPEED_MCTP_IOCTL_UNREGISTER_TYPE_HANDLER Unregister client as handler
+ * for specified MCTP type or PCI vendor defined message type
+ * @ASPEED_MCTP_GET_EID_INFO - deprecated, use ASPEED_MCTP_GET_EID_EXT instead
+ * @ASPEED_MCTP_SET_EID_INFO - deprecated, use ASPEED_MCTP_SET_EID_EXT instead
+ * @ASPEED_MCTP_GET_EID_EXT_INFO: read list of existing CPU EID and Domain ID
+ * mappings and return count which is lesser of the two: requested count and existing count
+ * @ASPEED_MCTP_SET_EID_EXT_INFO: write or overwrite already existing list of
+ * CPU EID and Domain ID mappings
+ * @ASPEED_MCTP_SET_OWN_EID: write/overwrite own EID information
+ */
+
+struct aspeed_mctp_filter_eid {
+	__u8 eid;
+	bool enable;
+};
+
+struct aspeed_mctp_get_bdf {
+	__u16 bdf;
+};
+
+struct aspeed_mctp_get_medium_id {
+	__u8 medium_id;
+};
+
+struct aspeed_mctp_get_mtu {
+	__u16 mtu;
+};
+
+struct aspeed_mctp_type_handler_ioctl {
+	__u8 mctp_type;		/* MCTP message type as per DSP239*/
+	/* Below params must be 0 if mctp_type is not Vendor Defined PCI */
+	__u16 pci_vendor_id;	/* PCI Vendor ID */
+	__u16 vendor_type;	/* Vendor specific type */
+	__u16 vendor_type_mask; /* Mask applied to vendor type */
+};
+
+struct aspeed_mctp_eid_info {
+	__u8 eid;
+	__u16 bdf;
+};
+
+struct aspeed_mctp_eid_ext_info {
+	__u8 eid;
+	__u16 bdf;
+	__u8 domain_id;
+};
+
+struct aspeed_mctp_get_eid_info {
+	__u64 ptr;
+	__u16 count;
+	__u8 start_eid;
+};
+
+struct aspeed_mctp_set_eid_info {
+	__u64 ptr;
+	__u16 count;
+};
+
+struct aspeed_mctp_set_own_eid {
+	__u8 eid;
+};
+
+#define ASPEED_MCTP_IOCTL_BASE	0x4d
+
+#define ASPEED_MCTP_IOCTL_FILTER_EID \
+	_IOW(ASPEED_MCTP_IOCTL_BASE, 0, struct aspeed_mctp_filter_eid)
+#define ASPEED_MCTP_IOCTL_GET_BDF \
+	_IOR(ASPEED_MCTP_IOCTL_BASE, 1, struct aspeed_mctp_get_bdf)
+#define ASPEED_MCTP_IOCTL_GET_MEDIUM_ID \
+	_IOR(ASPEED_MCTP_IOCTL_BASE, 2, struct aspeed_mctp_get_medium_id)
+#define ASPEED_MCTP_IOCTL_GET_MTU \
+	_IOR(ASPEED_MCTP_IOCTL_BASE, 3, struct aspeed_mctp_get_mtu)
+#define ASPEED_MCTP_IOCTL_REGISTER_DEFAULT_HANDLER \
+	_IO(ASPEED_MCTP_IOCTL_BASE, 4)
+#define ASPEED_MCTP_IOCTL_REGISTER_TYPE_HANDLER \
+	_IOW(ASPEED_MCTP_IOCTL_BASE, 6, struct aspeed_mctp_type_handler_ioctl)
+#define ASPEED_MCTP_IOCTL_UNREGISTER_TYPE_HANDLER \
+	_IOW(ASPEED_MCTP_IOCTL_BASE, 7, struct aspeed_mctp_type_handler_ioctl)
+#define ASPEED_MCTP_IOCTL_GET_EID_INFO \
+	_IOWR(ASPEED_MCTP_IOCTL_BASE, 8, struct aspeed_mctp_get_eid_info) /* deprecated */
+#define ASPEED_MCTP_IOCTL_SET_EID_INFO \
+	_IOW(ASPEED_MCTP_IOCTL_BASE, 9, struct aspeed_mctp_set_eid_info) /* deprecated */
+#define ASPEED_MCTP_IOCTL_GET_EID_EXT_INFO \
+	_IOW(ASPEED_MCTP_IOCTL_BASE, 10, struct aspeed_mctp_get_eid_info)
+#define ASPEED_MCTP_IOCTL_SET_EID_EXT_INFO \
+	_IOW(ASPEED_MCTP_IOCTL_BASE, 11, struct aspeed_mctp_set_eid_info)
+#define ASPEED_MCTP_IOCTL_SET_OWN_EID \
+	_IOW(ASPEED_MCTP_IOCTL_BASE, 12, struct aspeed_mctp_set_own_eid)
+
+#endif /* _UAPI_LINUX_ASPEED_MCTP_H */
diff --git a/include/uapi/linux/aspeed-otp.h b/include/uapi/linux/aspeed-otp.h
new file mode 100644
index 000000000000..cbb4d26fc804
--- /dev/null
+++ b/include/uapi/linux/aspeed-otp.h
@@ -0,0 +1,39 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later WITH Linux-syscall-note */
+/*
+ * Copyright (C) 2021 ASPEED Technology Inc.
+ */
+
+#ifndef _UAPI_LINUX_ASPEED_OTP_H
+#define _UAPI_LINUX_ASPEED_OTP_H
+
+#include <linux/ioctl.h>
+#include <linux/types.h>
+
+struct otp_read {
+	unsigned int offset;
+	unsigned int len;
+	unsigned int *data;
+};
+
+struct otp_prog {
+	unsigned int dw_offset;
+	unsigned int bit_offset;
+	unsigned int value;
+};
+
+#define OTP_A0	0
+#define OTP_A1	1
+#define OTP_A2	2
+#define OTP_A3	3
+
+#define OTPIOC_BASE 'O'
+
+#define ASPEED_OTP_READ_DATA _IOR(OTPIOC_BASE, 0, struct otp_read)
+#define ASPEED_OTP_READ_CONF _IOR(OTPIOC_BASE, 1, struct otp_read)
+#define ASPEED_OTP_PROG_DATA _IOW(OTPIOC_BASE, 2, struct otp_prog)
+#define ASPEED_OTP_PROG_CONF _IOW(OTPIOC_BASE, 3, struct otp_prog)
+#define ASPEED_OTP_VER _IOR(OTPIOC_BASE, 4, unsigned int)
+#define ASPEED_OTP_SW_RID _IOR(OTPIOC_BASE, 5, u32*)
+#define ASPEED_SEC_KEY_NUM _IOR(OTPIOC_BASE, 6, u32*)
+
+#endif /* _UAPI_LINUX_ASPEED_JTAG_H */
-- 
2.25.1

