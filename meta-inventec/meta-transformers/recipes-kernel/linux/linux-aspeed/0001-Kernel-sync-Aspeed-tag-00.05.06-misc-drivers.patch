From 27075bf65d1295de6226bdff2042d5be4e2683d6 Mon Sep 17 00:00:00 2001
From: "liz.chiu" <Chiu.Liz@inventec.com>
Date: Thu, 13 Jul 2023 10:27:56 +0800
Subject: [PATCH] Kernel-sync - [Aspeed] tag 00.05.06 misc drivers

Sympton/Reason:
    Sync codebase with AspeedTech-BMC/linux with tag 00.05.06

Root Cause:
    N/A

Solution/Change:
    hwmon drivers
        - aspeed-chassis
        - tach-aspeed-ast2600

    pwm driver
       - pwm-aspeed-ast2600

    jtag drivers
        - jtag
	      - jtag-aspeed
        - jtag-aspeed-internal

    SPI drivers
        - spi-aspeed

    i3c drivers
        - i3cdev
        - i3c-ibi-mqueue (will be removed, and is changed to i3c-ast-bridge-ic)
        https://github.com/AspeedTech-BMC/linux/commit/0e9fedee6377f2235bcc210ace15e02563ff0d6e
        - i3c-ast-bridge-ic
        - i3c-mux-imx3102
        - i3c-slave-mqueue
        - master
        - master/aspeed-i3c-global
        - master/ast2600-i3c-master

    usb-vhub drivers
        - gadget/udc/aspeed-vhub

    uart driver
        - 8250_aspeed

    gpio
        - gpio-aspeed
	    - gpio-aspeed-sgpio (This time is not changed)

Entry Test:
    N/A
---
 .../bindings/hwmon/aspeed,ast2600-tach.yaml   |   68 +
 .../bindings/mfd/aspeed,ast2600-pwm-tach.yaml |   76 +
 .../bindings/pwm/aspeed,ast2600-pwm.yaml      |   64 +
 drivers/Kconfig                               |    2 +
 drivers/Makefile                              |    1 +
 drivers/gpio/gpio-aspeed.c                    |   46 +-
 drivers/hwmon/Kconfig                         |   19 +
 drivers/hwmon/Makefile                        |    2 +
 drivers/hwmon/aspeed-chassis.c                |  220 ++
 drivers/hwmon/tach-aspeed-ast2600.c           |  390 +++
 drivers/i3c/Kconfig                           |   54 +
 drivers/i3c/Makefile                          |    4 +
 drivers/i3c/device.c                          |  241 ++
 drivers/i3c/i3c-ast-bridge-ic.c               |  279 ++
 drivers/i3c/i3c-ibi-mqueue.c                  |  246 ++
 drivers/i3c/i3c-mux-imx3102.c                 |  227 ++
 drivers/i3c/i3c-slave-mqueue.c                |  206 ++
 drivers/i3c/i3cdev.c                          |  509 +++
 drivers/i3c/internals.h                       |   15 +
 drivers/i3c/master.c                          |  723 ++++-
 drivers/i3c/master/Kconfig                    |   28 +
 drivers/i3c/master/Makefile                   |    1 +
 drivers/i3c/master/ast2600-i3c-global.c       |  141 +
 drivers/i3c/master/ast2600-i3c-master.c       | 2882 +++++++++++++++++
 drivers/i3c/master/svc-i3c-master.c           |  340 +-
 drivers/jtag/Kconfig                          |   45 +
 drivers/jtag/Makefile                         |    3 +
 drivers/jtag/jtag-aspeed-internal.c           | 1083 +++++++
 drivers/jtag/jtag-aspeed.c                    | 1626 ++++++++++
 drivers/jtag/jtag.c                           |  384 +++
 drivers/pwm/Kconfig                           |   10 +
 drivers/pwm/Makefile                          |    1 +
 drivers/pwm/pwm-aspeed-ast2600.c              |  359 ++
 drivers/spi/Kconfig                           |    8 +
 drivers/spi/Makefile                          |    1 +
 drivers/spi/spi-aspeed.c                      | 1466 +++++++++
 drivers/tty/serial/8250/8250_aspeed.c         |  498 +++
 drivers/tty/serial/8250/Kconfig               |    8 +
 drivers/tty/serial/8250/Makefile              |    1 +
 drivers/usb/gadget/udc/aspeed-vhub/core.c     |    3 +-
 drivers/usb/gadget/udc/aspeed-vhub/dev.c      |   11 +-
 drivers/usb/gadget/udc/aspeed-vhub/ep0.c      |    2 +-
 drivers/usb/gadget/udc/aspeed-vhub/epn.c      |   48 +-
 drivers/usb/gadget/udc/aspeed-vhub/hub.c      |   18 +-
 drivers/usb/gadget/udc/aspeed-vhub/vhub.h     |    1 +
 include/linux/i3c/ccc.h                       |   22 +-
 include/linux/i3c/device.h                    |   79 +
 include/linux/i3c/master.h                    |   91 +-
 include/linux/i3c/target.h                    |   23 +
 include/linux/jtag.h                          |   49 +
 include/uapi/linux/i3c/i3cdev.h               |   37 +
 include/uapi/linux/jtag.h                     |  370 +++
 52 files changed, 12664 insertions(+), 367 deletions(-)
 create mode 100644 Documentation/devicetree/bindings/hwmon/aspeed,ast2600-tach.yaml
 create mode 100644 Documentation/devicetree/bindings/mfd/aspeed,ast2600-pwm-tach.yaml
 create mode 100644 Documentation/devicetree/bindings/pwm/aspeed,ast2600-pwm.yaml
 create mode 100644 drivers/hwmon/aspeed-chassis.c
 create mode 100644 drivers/hwmon/tach-aspeed-ast2600.c
 create mode 100644 drivers/i3c/i3c-ast-bridge-ic.c
 create mode 100644 drivers/i3c/i3c-ibi-mqueue.c
 create mode 100644 drivers/i3c/i3c-mux-imx3102.c
 create mode 100644 drivers/i3c/i3c-slave-mqueue.c
 create mode 100644 drivers/i3c/i3cdev.c
 create mode 100644 drivers/i3c/master/ast2600-i3c-global.c
 create mode 100644 drivers/i3c/master/ast2600-i3c-master.c
 create mode 100644 drivers/jtag/Kconfig
 create mode 100644 drivers/jtag/Makefile
 create mode 100644 drivers/jtag/jtag-aspeed-internal.c
 create mode 100644 drivers/jtag/jtag-aspeed.c
 create mode 100644 drivers/jtag/jtag.c
 create mode 100644 drivers/pwm/pwm-aspeed-ast2600.c
 create mode 100644 drivers/spi/spi-aspeed.c
 create mode 100644 drivers/tty/serial/8250/8250_aspeed.c
 create mode 100644 include/linux/i3c/target.h
 create mode 100644 include/linux/jtag.h
 create mode 100644 include/uapi/linux/i3c/i3cdev.h
 create mode 100644 include/uapi/linux/jtag.h

diff --git a/Documentation/devicetree/bindings/hwmon/aspeed,ast2600-tach.yaml b/Documentation/devicetree/bindings/hwmon/aspeed,ast2600-tach.yaml
new file mode 100644
index 000000000000..dded50a049fb
--- /dev/null
+++ b/Documentation/devicetree/bindings/hwmon/aspeed,ast2600-tach.yaml
@@ -0,0 +1,68 @@
+# SPDX-License-Identifier: (GPL-2.0-only OR BSD-2-Clause)
+# Copyright (C) 2021 Aspeed, Inc.
+%YAML 1.2
+---
+$id: http://devicetree.org/schemas/hwmon/aspeed,ast2600-tach.yaml#
+$schema: http://devicetree.org/meta-schemas/core.yaml#
+
+title: Aspeed Ast2600 Tach controller
+
+maintainers:
+  - Billy Tsai <billy_tsai@aspeedtech.com>
+
+description: |
+  The Aspeed Tach controller can support upto 16 fan input.
+  This module is part of the ast2600-pwm-tach multi-function device. For more
+  details see ../mfd/aspeed,ast2600-pwm-tach.yaml.
+
+properties:
+  compatible:
+    enum:
+      - aspeed,ast2600-tach
+
+  "#address-cells":
+    const: 1
+
+  "#size-cells":
+    const: 0
+
+  pinctrl-0: true
+
+  pinctrl-names:
+    const: default
+
+required:
+  - compatible
+  - "#address-cells"
+  - "#size-cells"
+
+additionalProperties:
+  type: object
+  properties:
+    aspeed,tach-ch:
+      description:
+        The tach channel used for this node.
+      maxItems: 1
+
+    aspeed,min-rpm:
+      description:
+        define the minimal revolutions per minute of the measure fan
+        used to calculate the sample period of tach
+      default: 1000
+
+    aspeed,pulse-pr:
+      description:
+        Value specifying the number of pulses per revolution of the
+        monitored FAN.
+      default: 2
+
+    aspeed,tach-div:
+      description:
+        define the tachometer clock divider as an integer. Formula of
+        tach clock = clock source / (2^tach-div)^2
+      minimum: 0
+      maximum: 15
+      default: 5
+
+  required:
+    - reg
diff --git a/Documentation/devicetree/bindings/mfd/aspeed,ast2600-pwm-tach.yaml b/Documentation/devicetree/bindings/mfd/aspeed,ast2600-pwm-tach.yaml
new file mode 100644
index 000000000000..1eaf6fab2752
--- /dev/null
+++ b/Documentation/devicetree/bindings/mfd/aspeed,ast2600-pwm-tach.yaml
@@ -0,0 +1,76 @@
+# SPDX-License-Identifier: (GPL-2.0-only OR BSD-2-Clause)
+# Copyright (C) 2021 Aspeed, Inc.
+%YAML 1.2
+---
+$id: http://devicetree.org/schemas/mfd/aspeed,ast2600-pwm-tach.yaml#
+$schema: http://devicetree.org/meta-schemas/core.yaml#
+
+title: PWM Tach controller Device Tree Bindings
+
+description: |
+  The PWM Tach controller is represented as a multi-function device which
+  includes:
+    PWM
+    Tach
+
+maintainers:
+  - Billy Tsai <billy_tsai@aspeedtech.com>
+
+properties:
+  compatible:
+    items:
+      - enum:
+          - aspeed,ast2600-pwm-tach
+      - const: syscon
+      - const: simple-mfd
+
+  reg:
+    maxItems: 1
+
+  clocks:
+    maxItems: 1
+
+  resets:
+    maxItems: 1
+
+required:
+  - compatible
+  - reg
+  - clocks
+  - resets
+
+patternProperties:
+  "^pwm(@[0-9a-f]+)?$":
+    $ref: ../pwm/aspeed,ast2600-pwm.yaml
+
+  "^tach(@[0-9a-f]+)?$":
+    $ref: ../hwmon/aspeed,ast2600-tach.yaml
+
+additionalProperties: false
+
+examples:
+  - |
+    #include <dt-bindings/clock/ast2600-clock.h>
+    pwm_tach: pwm_tach@1e610000 {
+      compatible = "aspeed,ast2600-pwm-tach", "syscon", "simple-mfd";
+      reg = <0x1e610000 0x100>;
+      clocks = <&syscon ASPEED_CLK_AHB>;
+      resets = <&syscon ASPEED_RESET_PWM>;
+
+      pwm: pwm {
+        compatible = "aspeed,ast2600-pwm";
+        #address-cells = <1>;
+        #size-cells = <0>;
+        #pwm-cells = <3>;
+        pinctrl-names = "default";
+        pinctrl-0 = <&pinctrl_pwm0_default>;
+      };
+
+      tach: tach {
+        compatible = "aspeed,ast2600-tach";
+        #address-cells = <1>;
+        #size-cells = <0>;
+        pinctrl-names = "default";
+        pinctrl-0 = <&pinctrl_tach0_default>;
+      };
+    };
diff --git a/Documentation/devicetree/bindings/pwm/aspeed,ast2600-pwm.yaml b/Documentation/devicetree/bindings/pwm/aspeed,ast2600-pwm.yaml
new file mode 100644
index 000000000000..f501f8a769df
--- /dev/null
+++ b/Documentation/devicetree/bindings/pwm/aspeed,ast2600-pwm.yaml
@@ -0,0 +1,64 @@
+# SPDX-License-Identifier: (GPL-2.0-only OR BSD-2-Clause)
+# Copyright (C) 2021 Aspeed, Inc.
+%YAML 1.2
+---
+$id: http://devicetree.org/schemas/pwm/aspeed,ast2600-pwm.yaml#
+$schema: http://devicetree.org/meta-schemas/core.yaml#
+
+title: Aspeed Ast2600 PWM controller
+
+maintainers:
+  - Billy Tsai <billy_tsai@aspeedtech.com>
+
+description: |
+  The Aspeed PWM controller can support upto 16 PWM outputs.
+  This module is part of the ast2600-pwm-tach multi-function device. For more
+  details see ../mfd/aspeed,ast2600-pwm-tach.yaml.
+
+properties:
+  compatible:
+    enum:
+      - aspeed,ast2600-pwm
+
+  "#pwm-cells":
+    const: 3
+
+  "#address-cells":
+    const: 1
+
+  "#size-cells":
+    const: 0
+
+  pinctrl-0: true
+
+  pinctrl-names:
+    const: default
+
+required:
+  - compatible
+  - "#pwm-cells"
+  - "#address-cells"
+  - "#size-cells"
+
+additionalProperties:
+  description: Set extend properties for each pwm channel.
+  type: object
+  properties:
+    reg:
+      description:
+        The pwm channel index.
+      maxItems: 1
+
+    aspeed,wdt-reload-enable:
+      type: boolean
+      description:
+        Enable the function of wdt reset reload duty point.
+
+    aspeed,wdt-reload-duty-point:
+      description:
+        Define the duty point after wdt reset, 0 = 100%
+      minimum: 0
+      maximum: 255
+
+  required:
+    - reg
diff --git a/drivers/Kconfig b/drivers/Kconfig
index 19ee995bd0ae..1995aff497bb 100644
--- a/drivers/Kconfig
+++ b/drivers/Kconfig
@@ -235,6 +235,8 @@ source "drivers/counter/Kconfig"
 
 source "drivers/most/Kconfig"
 
+source "drivers/jtag/Kconfig"
+
 source "drivers/peci/Kconfig"
 
 source "drivers/hte/Kconfig"
diff --git a/drivers/Makefile b/drivers/Makefile
index bdf1c66141c9..199128cdaded 100644
--- a/drivers/Makefile
+++ b/drivers/Makefile
@@ -187,5 +187,6 @@ obj-$(CONFIG_GNSS)		+= gnss/
 obj-$(CONFIG_INTERCONNECT)	+= interconnect/
 obj-$(CONFIG_COUNTER)		+= counter/
 obj-$(CONFIG_MOST)		+= most/
+obj-$(CONFIG_JTAG)		+= jtag/
 obj-$(CONFIG_PECI)		+= peci/
 obj-$(CONFIG_HTE)		+= hte/
diff --git a/drivers/gpio/gpio-aspeed.c b/drivers/gpio/gpio-aspeed.c
index e60eabe468cd..22a8dd311da2 100644
--- a/drivers/gpio/gpio-aspeed.c
+++ b/drivers/gpio/gpio-aspeed.c
@@ -16,7 +16,6 @@
 #include <linux/module.h>
 #include <linux/pinctrl/consumer.h>
 #include <linux/platform_device.h>
-#include <linux/seq_file.h>
 #include <linux/spinlock.h>
 #include <linux/string.h>
 
@@ -53,7 +52,7 @@ struct aspeed_gpio_config {
  */
 struct aspeed_gpio {
 	struct gpio_chip chip;
-	struct device *dev;
+	struct irq_chip irqc;
 	raw_spinlock_t lock;
 	void __iomem *base;
 	int irq;
@@ -405,6 +404,8 @@ static void __aspeed_gpio_set(struct gpio_chip *gc, unsigned int offset,
 	gpio->dcache[GPIO_BANK(offset)] = reg;
 
 	iowrite32(reg, addr);
+	// Dummy read
+	ioread32(addr);
 }
 
 static void aspeed_gpio_set(struct gpio_chip *gc, unsigned int offset,
@@ -566,10 +567,6 @@ static void aspeed_gpio_irq_set_mask(struct irq_data *d, bool set)
 
 	addr = bank_reg(gpio, bank, reg_irq_enable);
 
-	/* Unmasking the IRQ */
-	if (set)
-		gpiochip_enable_irq(&gpio->chip, irqd_to_hwirq(d));
-
 	raw_spin_lock_irqsave(&gpio->lock, flags);
 	copro = aspeed_gpio_copro_request(gpio, offset);
 
@@ -583,10 +580,6 @@ static void aspeed_gpio_irq_set_mask(struct irq_data *d, bool set)
 	if (copro)
 		aspeed_gpio_copro_release(gpio, offset);
 	raw_spin_unlock_irqrestore(&gpio->lock, flags);
-
-	/* Masking the IRQ */
-	if (!set)
-		gpiochip_disable_irq(&gpio->chip, irqd_to_hwirq(d));
 }
 
 static void aspeed_gpio_irq_mask(struct irq_data *d)
@@ -1088,30 +1081,6 @@ int aspeed_gpio_copro_release_gpio(struct gpio_desc *desc)
 }
 EXPORT_SYMBOL_GPL(aspeed_gpio_copro_release_gpio);
 
-static void aspeed_gpio_irq_print_chip(struct irq_data *d, struct seq_file *p)
-{
-	const struct aspeed_gpio_bank *bank;
-	struct aspeed_gpio *gpio;
-	u32 bit;
-	int rc, offset;
-
-	rc = irqd_to_aspeed_gpio_data(d, &gpio, &bank, &bit, &offset);
-	if (rc)
-		return;
-
-	seq_printf(p, dev_name(gpio->dev));
-}
-
-static const struct irq_chip aspeed_gpio_irq_chip = {
-	.irq_ack = aspeed_gpio_irq_ack,
-	.irq_mask = aspeed_gpio_irq_mask,
-	.irq_unmask = aspeed_gpio_irq_unmask,
-	.irq_set_type = aspeed_gpio_set_type,
-	.irq_print_chip = aspeed_gpio_irq_print_chip,
-	.flags = IRQCHIP_IMMUTABLE,
-	GPIOCHIP_IRQ_RESOURCE_HELPERS,
-};
-
 /*
  * Any banks not specified in a struct aspeed_bank_props array are assumed to
  * have the properties:
@@ -1181,8 +1150,6 @@ static int __init aspeed_gpio_probe(struct platform_device *pdev)
 	if (IS_ERR(gpio->base))
 		return PTR_ERR(gpio->base);
 
-	gpio->dev = &pdev->dev;
-
 	raw_spin_lock_init(&gpio->lock);
 
 	gpio_id = of_match_node(aspeed_gpio_of_table, pdev->dev.of_node);
@@ -1242,9 +1209,12 @@ static int __init aspeed_gpio_probe(struct platform_device *pdev)
 
 		gpio->irq = rc;
 		girq = &gpio->chip.irq;
-		gpio_irq_chip_set_chip(girq, &aspeed_gpio_irq_chip);
+		girq->chip = &gpio->irqc;
 		girq->chip->name = dev_name(&pdev->dev);
-
+		girq->chip->irq_ack = aspeed_gpio_irq_ack;
+		girq->chip->irq_mask = aspeed_gpio_irq_mask;
+		girq->chip->irq_unmask = aspeed_gpio_irq_unmask;
+		girq->chip->irq_set_type = aspeed_gpio_set_type;
 		girq->parent_handler = aspeed_gpio_irq_handler;
 		girq->num_parents = 1;
 		girq->parents = devm_kcalloc(&pdev->dev, 1,
diff --git a/drivers/hwmon/Kconfig b/drivers/hwmon/Kconfig
index a5253abb7ea7..6ab3347ae4b0 100644
--- a/drivers/hwmon/Kconfig
+++ b/drivers/hwmon/Kconfig
@@ -411,6 +411,25 @@ config SENSORS_ASPEED
 	  This driver can also be built as a module. If so, the module
 	  will be called aspeed_pwm_tacho.
 
+config SENSORS_ASPEED_CHASSIS
+	tristate "ASPEED CHASSIS Driver"
+	depends on ARCH_ASPEED || COMPILE_TEST
+	help
+	  This driver provides support for Aspeed ast2600 chassis intruded
+	  detect support.
+
+	  To compile this driver as a module, choose M here: the module
+	  will be called aspeed-chassis.
+
+config SENSORS_TACH_ASPEED_AST2600
+	tristate "ASPEED ast2600 Tachometer support"
+	select REGMAP
+	help
+	  This driver provides support for Aspeed ast2600 Tachometer.
+
+	  To compile this driver as a module, choose M here: the module
+	  will be called tach-aspeed-ast2600.
+
 config SENSORS_ATXP1
 	tristate "Attansic ATXP1 VID controller"
 	depends on I2C
diff --git a/drivers/hwmon/Makefile b/drivers/hwmon/Makefile
index c5cd7e3a67ff..da9ed3e18286 100644
--- a/drivers/hwmon/Makefile
+++ b/drivers/hwmon/Makefile
@@ -53,7 +53,9 @@ obj-$(CONFIG_SENSORS_ARM_SCMI)	+= scmi-hwmon.o
 obj-$(CONFIG_SENSORS_ARM_SCPI)	+= scpi-hwmon.o
 obj-$(CONFIG_SENSORS_AS370)	+= as370-hwmon.o
 obj-$(CONFIG_SENSORS_ASC7621)	+= asc7621.o
+obj-$(CONFIG_SENSORS_TACH_ASPEED_AST2600) += tach-aspeed-ast2600.o
 obj-$(CONFIG_SENSORS_ASPEED)	+= aspeed-pwm-tacho.o
+obj-$(CONFIG_SENSORS_ASPEED_CHASSIS) += aspeed-chassis.o
 obj-$(CONFIG_SENSORS_ATXP1)	+= atxp1.o
 obj-$(CONFIG_SENSORS_AXI_FAN_CONTROL) += axi-fan-control.o
 obj-$(CONFIG_SENSORS_BT1_PVT)	+= bt1-pvt.o
diff --git a/drivers/hwmon/aspeed-chassis.c b/drivers/hwmon/aspeed-chassis.c
new file mode 100644
index 000000000000..483c622c8545
--- /dev/null
+++ b/drivers/hwmon/aspeed-chassis.c
@@ -0,0 +1,220 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright (C) 2021 ASPEED Technology Inc.
+ *
+ * CHASSIS driver for the Aspeed SoC
+ */
+#include <linux/errno.h>
+#include <linux/delay.h>
+#include <linux/hwmon.h>
+#include <linux/hwmon-sysfs.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of_platform.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+#include <linux/sysfs.h>
+#include <linux/interrupt.h>
+
+/* #define USE_INTERRUPTS */
+/******************************************************************************/
+union chassis_ctrl_register {
+	uint32_t value;
+	struct {
+		uint32_t intrusion_status_clear : 1; /*[0]*/
+		uint32_t intrusion_int_enable : 1; /*[1]*/
+		uint32_t intrusion_status : 1; /*[2]*/
+		uint32_t battery_power_good : 1; /*[3]*/
+		uint32_t chassis_raw_status : 1; /*[4]*/
+		uint32_t reserved0 : 3; /*[5-7]*/
+		uint32_t io_power_status_clear : 1; /*[8]*/
+		uint32_t io_power_int_enable : 1; /*[9]*/
+		uint32_t core_power_status : 1; /*[10]*/
+		uint32_t reserved1 : 5; /*[11-15]*/
+		uint32_t core_power_status_clear : 1; /*[16]*/
+		uint32_t core_power_int_enable : 1; /*[17]*/
+		uint32_t io_power_status : 1; /*[18]*/
+		uint32_t reserved2 : 13; /*[19-31]*/
+	} fields;
+};
+
+struct aspeed_chassis {
+	struct device *dev;
+	void __iomem *base;
+	int irq;
+	/* for hwmon */
+	const struct attribute_group *groups[2];
+};
+
+static ssize_t
+intrusion_store(struct device *dev, struct device_attribute *attr,
+		       const char *buf, size_t count)
+{
+	unsigned long val;
+	struct aspeed_chassis *chassis = dev_get_drvdata(dev);
+	union chassis_ctrl_register chassis_ctrl;
+
+	if (kstrtoul(buf, 10, &val) < 0 || val != 0)
+		return -EINVAL;
+
+	chassis_ctrl.value = readl(chassis->base);
+	chassis_ctrl.fields.intrusion_status_clear = 1;
+	writel(chassis_ctrl.value, chassis->base);
+	chassis_ctrl.fields.intrusion_status_clear = 0;
+	writel(chassis_ctrl.value, chassis->base);
+	return count;
+}
+
+static ssize_t intrusion_show(struct device *dev, struct device_attribute *attr,
+			  char *buf)
+{
+	struct sensor_device_attribute *sensor_attr = to_sensor_dev_attr(attr);
+	int index = sensor_attr->index;
+	struct aspeed_chassis *chassis = dev_get_drvdata(dev);
+	union chassis_ctrl_register chassis_ctrl;
+	uint8_t ret;
+
+	chassis_ctrl.value = readl(chassis->base);
+
+	switch (index) {
+	case 0:
+		ret = chassis_ctrl.fields.core_power_status;
+		break;
+	case 1:
+		ret = chassis_ctrl.fields.io_power_status;
+		break;
+	case 2:
+		ret = chassis_ctrl.fields.intrusion_status;
+		break;
+	}
+
+	return sprintf(buf, "%d\n", ret);
+}
+
+static SENSOR_DEVICE_ATTR_RO(core_power, intrusion, 0);
+static SENSOR_DEVICE_ATTR_RO(io_power, intrusion, 1);
+static SENSOR_DEVICE_ATTR_RW(intrusion0_alarm, intrusion, 2);
+
+static struct attribute *intrusion_dev_attrs[] = {
+	&sensor_dev_attr_core_power.dev_attr.attr,
+	&sensor_dev_attr_io_power.dev_attr.attr,
+	&sensor_dev_attr_intrusion0_alarm.dev_attr.attr, NULL
+};
+
+static const struct attribute_group intrusion_dev_group = {
+	.attrs = intrusion_dev_attrs,
+	.is_visible = NULL,
+};
+#ifdef USE_INTERRUPTS
+static void aspeed_chassis_status_check(struct aspeed_chassis *chassis)
+{
+	union chassis_ctrl_register chassis_ctrl;
+
+	chassis_ctrl.value = readl(chassis->base);
+	if (chassis_ctrl.fields.intrusion_status) {
+		dev_info(chassis->dev, "CHASI# pin has been pulled low");
+		chassis_ctrl.fields.intrusion_status_clear = 1;
+		writel(chassis_ctrl.value, chassis->base);
+		chassis_ctrl.fields.intrusion_status_clear = 0;
+		writel(chassis_ctrl.value, chassis->base);
+	}
+
+	if (chassis_ctrl.fields.core_power_status) {
+		dev_info(chassis->dev, "Core power has been pulled low");
+		chassis_ctrl.fields.core_power_status_clear = 1;
+		writel(chassis_ctrl.value, chassis->base);
+		chassis_ctrl.fields.core_power_status_clear = 0;
+		writel(chassis_ctrl.value, chassis->base);
+	}
+
+	if (chassis_ctrl.fields.io_power_status) {
+		dev_info(chassis->dev, "IO power has been pulled low");
+		chassis_ctrl.fields.io_power_status_clear = 1;
+		writel(chassis_ctrl.value, chassis->base);
+		chassis_ctrl.fields.io_power_status_clear = 0;
+		writel(chassis_ctrl.value, chassis->base);
+	}
+}
+
+static irqreturn_t aspeed_chassis_isr(int this_irq, void *dev_id)
+{
+	struct aspeed_chassis *chassis = dev_id;
+
+	aspeed_chassis_status_check(chassis);
+	return IRQ_HANDLED;
+}
+#endif
+
+static void aspeed_chassis_int_ctrl(struct aspeed_chassis *chassis, bool ctrl)
+{
+	union chassis_ctrl_register chassis_ctrl;
+
+	chassis_ctrl.value = readl(chassis->base);
+	chassis_ctrl.fields.intrusion_int_enable = ctrl;
+	chassis_ctrl.fields.io_power_int_enable = ctrl;
+	chassis_ctrl.fields.core_power_int_enable = ctrl;
+	writel(chassis_ctrl.value, chassis->base);
+}
+
+static const struct of_device_id aspeed_chassis_of_table[] = {
+	{ .compatible = "aspeed,ast2600-chassis" },
+	{}
+};
+MODULE_DEVICE_TABLE(of, aspeed_chassis_of_table);
+
+static int aspeed_chassis_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct aspeed_chassis *priv;
+	struct device *hwmon;
+	int __maybe_unused ret;
+
+	priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	priv->dev = dev;
+	priv->base = devm_platform_ioremap_resource(pdev, 0);
+	if (IS_ERR(priv->base))
+		return PTR_ERR(priv->base);
+#ifdef USE_INTERRUPTS
+	priv->irq = platform_get_irq(pdev, 0);
+	if (priv->irq < 0) {
+		dev_err(dev, "no irq specified\n");
+		return -ENOENT;
+	}
+
+	ret = devm_request_irq(dev, priv->irq, aspeed_chassis_isr, 0,
+			       dev_name(dev), priv);
+	if (ret) {
+		dev_err(dev, "Chassis Unable to get IRQ");
+		return ret;
+	}
+	aspeed_chassis_int_ctrl(priv, true);
+#else
+	aspeed_chassis_int_ctrl(priv, false);
+#endif
+
+	priv->groups[0] = &intrusion_dev_group;
+	priv->groups[1] = NULL;
+
+	hwmon = devm_hwmon_device_register_with_groups(dev, "aspeed_chassis",
+						       priv, priv->groups);
+
+	return PTR_ERR_OR_ZERO(hwmon);
+}
+
+static struct platform_driver aspeed_chassis_driver = {
+	.probe		= aspeed_chassis_probe,
+	.driver		= {
+		.name	= KBUILD_MODNAME,
+		.of_match_table = aspeed_chassis_of_table,
+	},
+};
+
+module_platform_driver(aspeed_chassis_driver);
+
+MODULE_AUTHOR("Billy Tsai<billy_tsai@aspeedtech.com>");
+MODULE_DESCRIPTION("ASPEED CHASSIS Driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/hwmon/tach-aspeed-ast2600.c b/drivers/hwmon/tach-aspeed-ast2600.c
new file mode 100644
index 000000000000..27a46271f8d7
--- /dev/null
+++ b/drivers/hwmon/tach-aspeed-ast2600.c
@@ -0,0 +1,390 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright (C) ASPEED Technology Inc.
+ */
+
+#include <linux/bitfield.h>
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/errno.h>
+#include <linux/hwmon.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/mfd/syscon.h>
+#include <linux/module.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+#include <linux/regmap.h>
+#include <linux/reset.h>
+#include <linux/sysfs.h>
+
+/* The channel number of Aspeed tach controller */
+#define TACH_ASPEED_NR_TACHS		16
+/* TACH Control Register */
+#define TACH_ASPEED_CTRL(ch)		(((ch) * 0x10) + 0x08)
+#define TACH_ASPEED_IER			BIT(31)
+#define TACH_ASPEED_INVERS_LIMIT	BIT(30)
+#define TACH_ASPEED_LOOPBACK		BIT(29)
+#define TACH_ASPEED_ENABLE		BIT(28)
+#define TACH_ASPEED_DEBOUNCE_MASK	GENMASK(27, 26)
+#define TACH_ASPEED_DEBOUNCE_BIT	26
+#define TACH_ASPEED_IO_EDGE_MASK	GENMASK(25, 24)
+#define TACH_ASPEED_IO_EDGE_BIT		24
+#define TACH_ASPEED_CLK_DIV_T_MASK	GENMASK(23, 20)
+#define TACH_ASPEED_CLK_DIV_BIT		20
+#define TACH_ASPEED_THRESHOLD_MASK	GENMASK(19, 0)
+/* [27:26] */
+#define DEBOUNCE_3_CLK			0x00
+#define DEBOUNCE_2_CLK			0x01
+#define DEBOUNCE_1_CLK			0x02
+#define DEBOUNCE_0_CLK			0x03
+/* [25:24] */
+#define F2F_EDGES			0x00
+#define R2R_EDGES			0x01
+#define BOTH_EDGES			0x02
+/* [23:20] */
+/* divisor = 4 to the nth power, n = register value */
+#define DEFAULT_TACH_DIV		1024
+#define DIV_TO_REG(divisor)		(ilog2(divisor) >> 1)
+
+/* TACH Status Register */
+#define TACH_ASPEED_STS(ch)		(((ch) * 0x10) + 0x0C)
+
+/*PWM_TACH_STS */
+#define TACH_ASPEED_ISR			BIT(31)
+#define TACH_ASPEED_PWM_OUT		BIT(25)
+#define TACH_ASPEED_PWM_OEN		BIT(24)
+#define TACH_ASPEED_DEB_INPUT		BIT(23)
+#define TACH_ASPEED_RAW_INPUT		BIT(22)
+#define TACH_ASPEED_VALUE_UPDATE	BIT(21)
+#define TACH_ASPEED_FULL_MEASUREMENT	BIT(20)
+#define TACH_ASPEED_VALUE_MASK		GENMASK(19, 0)
+/**********************************************************
+ * Software setting
+ *********************************************************/
+#define DEFAULT_FAN_PULSE_PR		2
+struct aspeed_tach_channel_params {
+	int limited_inverse;
+	u16 threshold;
+	u8 tach_edge;
+	u8 tach_debounce;
+	u8 pulse_pr;
+	u32 divisor;
+	u32 sample_period; /* unit is us */
+	u32 polling_period; /* unit is us */
+};
+
+struct aspeed_tach_data {
+	struct device *dev;
+	struct regmap *regmap;
+	struct clk *clk;
+	struct reset_control *reset;
+	bool tach_present[TACH_ASPEED_NR_TACHS];
+	struct aspeed_tach_channel_params tach_channel[TACH_ASPEED_NR_TACHS];
+	unsigned long clk_source;
+};
+
+static void aspeed_tach_ch_enable(struct aspeed_tach_data *priv, u8 tach_ch,
+				  bool enable)
+{
+	if (enable)
+		regmap_set_bits(priv->regmap, TACH_ASPEED_CTRL(tach_ch),
+				TACH_ASPEED_ENABLE);
+	else
+		regmap_clear_bits(priv->regmap, TACH_ASPEED_CTRL(tach_ch),
+				  TACH_ASPEED_ENABLE);
+}
+
+static u64 aspeed_tach_val_to_rpm(struct aspeed_tach_data *priv, u8 fan_tach_ch,
+				  u32 tach_val)
+{
+	u64 rpm;
+	u32 tach_div;
+
+	/*
+	 * We need the mode to determine if the tach_val is double (from
+	 * counting both edges).
+	 */
+	if (priv->tach_channel[fan_tach_ch].tach_edge == BOTH_EDGES)
+		tach_val <<= 1;
+
+	tach_div = tach_val * (priv->tach_channel[fan_tach_ch].divisor) *
+		   (priv->tach_channel[fan_tach_ch].pulse_pr);
+
+	dev_dbg(priv->dev, "clk %ld, tach_val %d , tach_div %d\n",
+		priv->clk_source, tach_val, tach_div);
+
+	rpm = (u64)priv->clk_source * 60;
+	do_div(rpm, tach_div);
+
+	return rpm;
+}
+
+static int aspeed_get_fan_tach_ch_rpm(struct aspeed_tach_data *priv,
+				      u8 fan_tach_ch)
+{
+	u32 val;
+	u64 rpm;
+	int ret;
+
+	ret = regmap_read(priv->regmap, TACH_ASPEED_STS(fan_tach_ch), &val);
+	if (ret)
+		return ret;
+
+	if (!(val & TACH_ASPEED_FULL_MEASUREMENT))
+		return 0;
+	rpm = aspeed_tach_val_to_rpm(priv, fan_tach_ch,
+				     val & TACH_ASPEED_VALUE_MASK);
+
+	return rpm;
+}
+
+static int aspeed_tach_hwmon_read(struct device *dev,
+				  enum hwmon_sensor_types type, u32 attr,
+				  int channel, long *val)
+{
+	struct aspeed_tach_data *priv = dev_get_drvdata(dev);
+	u32 reg_val;
+	int ret;
+
+	switch (attr) {
+	case hwmon_fan_input:
+		ret = aspeed_get_fan_tach_ch_rpm(priv, channel);
+		if (ret < 0)
+			return ret;
+		*val = ret;
+		break;
+	case hwmon_fan_div:
+		regmap_read(priv->regmap, TACH_ASPEED_CTRL(channel), &reg_val);
+		reg_val = FIELD_GET(TACH_ASPEED_CLK_DIV_T_MASK, reg_val);
+		*val = BIT(reg_val << 1);
+		break;
+	case hwmon_fan_pulses:
+		*val = priv->tach_channel[channel].pulse_pr;
+		break;
+	default:
+		return -EOPNOTSUPP;
+	}
+	return 0;
+}
+
+static int aspeed_tach_hwmon_write(struct device *dev,
+				   enum hwmon_sensor_types type, u32 attr,
+				   int channel, long val)
+{
+	struct aspeed_tach_data *priv = dev_get_drvdata(dev);
+
+	switch (attr) {
+	case hwmon_fan_div:
+		if (!is_power_of_2(val) || (ilog2(val) % 2)) {
+			dev_err(dev, "fan_div value %ld not supported.\n", val);
+			return -EINVAL;
+		}
+		if (DIV_TO_REG(val) > 0xb)
+			return -ERANGE;
+		priv->tach_channel[channel].divisor = val;
+		regmap_write_bits(priv->regmap, TACH_ASPEED_CTRL(channel),
+				  TACH_ASPEED_CLK_DIV_T_MASK,
+				  DIV_TO_REG(priv->tach_channel[channel].divisor)
+					  << TACH_ASPEED_CLK_DIV_BIT);
+		break;
+	case hwmon_fan_pulses:
+		priv->tach_channel[channel].pulse_pr = val;
+		break;
+	default:
+		return -EOPNOTSUPP;
+	}
+
+	return 0;
+}
+
+static umode_t aspeed_tach_dev_is_visible(const void *drvdata,
+					  enum hwmon_sensor_types type,
+					  u32 attr, int channel)
+{
+	const struct aspeed_tach_data *priv = drvdata;
+
+	if (!priv->tach_present[channel])
+		return 0;
+	switch (attr) {
+	case hwmon_fan_input:
+		return 0444;
+	case hwmon_fan_div:
+	case hwmon_fan_pulses:
+		return 0644;
+	}
+	return 0;
+}
+
+static const struct hwmon_ops aspeed_tach_ops = {
+	.is_visible = aspeed_tach_dev_is_visible,
+	.read = aspeed_tach_hwmon_read,
+	.write = aspeed_tach_hwmon_write,
+};
+
+static const struct hwmon_channel_info *aspeed_tach_info[] = {
+	HWMON_CHANNEL_INFO(fan, HWMON_F_INPUT | HWMON_F_DIV | HWMON_F_PULSES,
+			   HWMON_F_INPUT | HWMON_F_DIV | HWMON_F_PULSES,
+			   HWMON_F_INPUT | HWMON_F_DIV | HWMON_F_PULSES,
+			   HWMON_F_INPUT | HWMON_F_DIV | HWMON_F_PULSES,
+			   HWMON_F_INPUT | HWMON_F_DIV | HWMON_F_PULSES,
+			   HWMON_F_INPUT | HWMON_F_DIV | HWMON_F_PULSES,
+			   HWMON_F_INPUT | HWMON_F_DIV | HWMON_F_PULSES,
+			   HWMON_F_INPUT | HWMON_F_DIV | HWMON_F_PULSES,
+			   HWMON_F_INPUT | HWMON_F_DIV | HWMON_F_PULSES,
+			   HWMON_F_INPUT | HWMON_F_DIV | HWMON_F_PULSES,
+			   HWMON_F_INPUT | HWMON_F_DIV | HWMON_F_PULSES,
+			   HWMON_F_INPUT | HWMON_F_DIV | HWMON_F_PULSES,
+			   HWMON_F_INPUT | HWMON_F_DIV | HWMON_F_PULSES,
+			   HWMON_F_INPUT | HWMON_F_DIV | HWMON_F_PULSES,
+			   HWMON_F_INPUT | HWMON_F_DIV | HWMON_F_PULSES,
+			   HWMON_F_INPUT | HWMON_F_DIV | HWMON_F_PULSES),
+	NULL
+};
+
+static const struct hwmon_chip_info aspeed_tach_chip_info = {
+	.ops = &aspeed_tach_ops,
+	.info = aspeed_tach_info,
+};
+
+static void aspeed_create_fan_tach_channel(struct aspeed_tach_data *priv,
+					   u32 tach_ch)
+{
+	priv->tach_present[tach_ch] = true;
+	priv->tach_channel[tach_ch].limited_inverse = 0;
+	regmap_write_bits(priv->regmap, TACH_ASPEED_CTRL(tach_ch),
+			  TACH_ASPEED_INVERS_LIMIT,
+			  priv->tach_channel[tach_ch].limited_inverse ?
+				  TACH_ASPEED_INVERS_LIMIT :
+				  0);
+
+	priv->tach_channel[tach_ch].tach_debounce = DEBOUNCE_3_CLK;
+	regmap_write_bits(priv->regmap, TACH_ASPEED_CTRL(tach_ch),
+			  TACH_ASPEED_DEBOUNCE_MASK,
+			  priv->tach_channel[tach_ch].tach_debounce
+				  << TACH_ASPEED_DEBOUNCE_BIT);
+
+	priv->tach_channel[tach_ch].tach_edge = F2F_EDGES;
+	regmap_write_bits(priv->regmap, TACH_ASPEED_CTRL(tach_ch),
+			  TACH_ASPEED_IO_EDGE_MASK,
+			  priv->tach_channel[tach_ch].tach_edge
+				  << TACH_ASPEED_IO_EDGE_BIT);
+
+	priv->tach_channel[tach_ch].divisor = DEFAULT_TACH_DIV;
+	regmap_write_bits(priv->regmap, TACH_ASPEED_CTRL(tach_ch),
+			  TACH_ASPEED_CLK_DIV_T_MASK,
+			  DIV_TO_REG(priv->tach_channel[tach_ch].divisor)
+				  << TACH_ASPEED_CLK_DIV_BIT);
+
+	priv->tach_channel[tach_ch].threshold = 0;
+	regmap_write_bits(priv->regmap, TACH_ASPEED_CTRL(tach_ch),
+			  TACH_ASPEED_THRESHOLD_MASK,
+			  priv->tach_channel[tach_ch].threshold);
+
+	priv->tach_channel[tach_ch].pulse_pr = DEFAULT_FAN_PULSE_PR;
+
+	aspeed_tach_ch_enable(priv, tach_ch, true);
+}
+
+static int aspeed_tach_create_fan(struct device *dev, struct device_node *child,
+				  struct aspeed_tach_data *priv)
+{
+	u32 tach_channel;
+	int ret;
+
+	ret = of_property_read_u32(child, "reg", &tach_channel);
+	if (ret)
+		return ret;
+
+	aspeed_create_fan_tach_channel(priv, tach_channel);
+
+	return 0;
+}
+
+static void aspeed_tach_reset_assert(void *data)
+{
+	struct reset_control *rst = data;
+
+	reset_control_assert(rst);
+}
+
+static int aspeed_tach_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct device_node *np, *child;
+	struct aspeed_tach_data *priv;
+	struct device *hwmon;
+	int ret;
+
+	np = dev->parent->of_node;
+	if (!of_device_is_compatible(np, "aspeed,ast2600-pwm-tach"))
+		return dev_err_probe(dev, -ENODEV,
+				     "Unsupported tach device binding\n");
+
+	priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+	priv->dev = &pdev->dev;
+	priv->regmap = syscon_node_to_regmap(np);
+	if (IS_ERR(priv->regmap))
+		return dev_err_probe(dev, PTR_ERR(priv->regmap),
+				     "Couldn't get regmap\n");
+	priv->clk = devm_clk_get_enabled(dev->parent, NULL);
+	if (IS_ERR(priv->clk))
+		return dev_err_probe(dev, PTR_ERR(priv->clk),
+				     "Couldn't get clock\n");
+
+	priv->clk_source = clk_get_rate(priv->clk);
+	priv->reset = devm_reset_control_get_shared(dev->parent, NULL);
+	if (IS_ERR(priv->reset))
+		return dev_err_probe(dev, PTR_ERR(priv->reset),
+				     "Couldn't get reset control\n");
+
+	ret = reset_control_deassert(priv->reset);
+	if (ret)
+		return dev_err_probe(dev, ret,
+				     "Couldn't deassert reset control\n");
+
+	ret = devm_add_action_or_reset(dev, aspeed_tach_reset_assert,
+				       priv->reset);
+	if (ret)
+		return ret;
+
+	for_each_child_of_node(dev->of_node, child) {
+		ret = aspeed_tach_create_fan(dev, child, priv);
+		if (ret) {
+			of_node_put(child);
+			return ret;
+		}
+	}
+
+	hwmon = devm_hwmon_device_register_with_info(dev, "aspeed_tach", priv,
+						     &aspeed_tach_chip_info, NULL);
+	ret = PTR_ERR_OR_ZERO(hwmon);
+	if (ret)
+		return dev_err_probe(dev, ret,
+				     "Failed to register hwmon device\n");
+	return 0;
+}
+
+static const struct of_device_id of_stach_match_table[] = {
+	{
+		.compatible = "aspeed,ast2600-tach",
+	},
+	{},
+};
+MODULE_DEVICE_TABLE(of, of_stach_match_table);
+
+static struct platform_driver aspeed_tach_driver = {
+	.probe		= aspeed_tach_probe,
+	.driver		= {
+		.name	= "aspeed_tach",
+		.of_match_table = of_stach_match_table,
+	},
+};
+
+module_platform_driver(aspeed_tach_driver);
+
+MODULE_AUTHOR("Billy Tsai <billy_tsai@aspeedtech.com>");
+MODULE_DESCRIPTION("Aspeed ast2600 TACH device driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/i3c/Kconfig b/drivers/i3c/Kconfig
index 30a441506f61..839561f38c9a 100644
--- a/drivers/i3c/Kconfig
+++ b/drivers/i3c/Kconfig
@@ -20,5 +20,59 @@ menuconfig I3C
 	  will be called i3c.
 
 if I3C
+
+config I3CDEV
+	tristate "I3C device interface"
+	depends on I3C
+	help
+	  Say Y here to use i3c-* device files, usually found in the /dev
+	  directory on your system.  They make it possible to have user-space
+	  programs use the I3C devices.
+
+	  This support is also available as a module.  If so, the module
+	  will be called i3cdev.
+
+	  Note that this application programming interface is EXPERIMENTAL
+	  and hence SUBJECT TO CHANGE WITHOUT NOTICE while it stabilizes.
+
+if I3CDEV
+config I3CDEV_FORCE_CREATE
+	bool "force create I3C device interface"
+	default y
+	help
+	  Say 'y' to force create I3C devices under /dev/bus/i3c/ regardless of
+	  driver binding.  This option is to help development so it shall be
+	  turned off in production.
+endif # I3CDEV
+
+config I3C_AST_BRIDGE_IC
+	bool "Aspeed Bridge IC driver"
+	default y
+	help
+	  Say y to enable Aspeed bridge IC device driver.
+
+config I3C_MUX_IMX3102
+	bool "IMX/IML3102 I3C multiplexer driver"
+	default y
+	select REGMAP_I3C
+	help
+	  Say y to enable Renesas IMX3102 I3C 2:1 multiplexer.
+
+choice
+	prompt "I3C secondary master / slave mode driver selection"
+	default I3C_SLAVE_MQUEUE
+
+config I3C_SLAVE_MQUEUE
+	bool "I3C mqueue (message queue) secondary master and slave driver"
+	help
+	  Some protocols over I3C are designed for bi-directional transferring
+	  messages by using I3C Master Write protocol. This driver is used to
+	  receive and queue messages from the remote I3C main master device.
+
+	  Userspace can get the messages by reading sysfs file that this driver
+	  exposes.
+
+endchoice
+
 source "drivers/i3c/master/Kconfig"
 endif # I3C
diff --git a/drivers/i3c/Makefile b/drivers/i3c/Makefile
index 11982efbc6d9..9e35176c2f3e 100644
--- a/drivers/i3c/Makefile
+++ b/drivers/i3c/Makefile
@@ -1,4 +1,8 @@
 # SPDX-License-Identifier: GPL-2.0
 i3c-y				:= device.o master.o
 obj-$(CONFIG_I3C)		+= i3c.o
+obj-$(CONFIG_I3CDEV)	+= i3cdev.o
 obj-$(CONFIG_I3C)		+= master/
+obj-$(CONFIG_I3C_AST_BRIDGE_IC) += i3c-ast-bridge-ic.o
+obj-$(CONFIG_I3C_SLAVE_MQUEUE) 	+= i3c-slave-mqueue.o
+obj-$(CONFIG_I3C_MUX_IMX3102) 	+= i3c-mux-imx3102.o
diff --git a/drivers/i3c/device.c b/drivers/i3c/device.c
index e92d3e9a52bd..26f2d745fc38 100644
--- a/drivers/i3c/device.c
+++ b/drivers/i3c/device.c
@@ -50,6 +50,73 @@ int i3c_device_do_priv_xfers(struct i3c_device *dev,
 }
 EXPORT_SYMBOL_GPL(i3c_device_do_priv_xfers);
 
+/**
+ * i3c_device_send_hdr_cmds() - send HDR commands to a specific device
+ *
+ * @dev: device to which these commands should be sent
+ * @cmds: array of commands
+ * @ncmds: number of commands
+ *
+ * Send one or several HDR commands to @dev.
+ *
+ * This function can sleep and thus cannot be called in atomic context.
+ *
+ * Return: 0 in case of success, a negative error core otherwise.
+ */
+int i3c_device_send_hdr_cmds(struct i3c_device *dev, struct i3c_hdr_cmd *cmds,
+			     int ncmds)
+{
+	struct i3c_master_controller *master;
+	enum i3c_hdr_mode mode;
+	int ret, i;
+
+	if (ncmds < 1)
+		return 0;
+
+	mode = cmds[0].mode;
+	for (i = 1; i < ncmds; i++) {
+		if (mode != cmds[i].mode)
+			return -EINVAL;
+	}
+
+	master = i3c_dev_get_master(dev->desc);
+	if (!master)
+		return -EINVAL;
+
+	i3c_bus_normaluse_lock(&master->bus);
+	for (i = 0; i < ncmds; i++)
+		cmds[i].addr = dev->desc->info.dyn_addr;
+
+	ret = i3c_master_send_hdr_cmds_locked(master, cmds, ncmds);
+	i3c_bus_normaluse_unlock(&master->bus);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_device_send_hdr_cmds);
+
+/**
+ * i3c_device_generate_ibi() - request In-Band Interrupt
+ *
+ * @dev: target device
+ * @data: IBI payload
+ * @len: payload length in bytes
+ *
+ * Request In-Band Interrupt with or without data payload.
+ *
+ * Return: 0 in case of success, a negative error code otherwise.
+ */
+int i3c_device_generate_ibi(struct i3c_device *dev, const u8 *data, int len)
+{
+	int ret;
+
+	i3c_bus_normaluse_lock(dev->bus);
+	ret = i3c_dev_generate_ibi_locked(dev->desc, data, len);
+	i3c_bus_normaluse_unlock(dev->bus);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_device_generate_ibi);
+
 /**
  * i3c_device_get_info() - get I3C device information
  *
@@ -176,6 +243,33 @@ void i3c_device_free_ibi(struct i3c_device *dev)
 }
 EXPORT_SYMBOL_GPL(i3c_device_free_ibi);
 
+/**
+ * i3c_device_send_ccc_cmd() - send ccc to the target device
+ * @dev: device on which you want to release IBI resources
+ * @ccc_id: CCC ID you want to send.  Only support SETAASA, RSTDAA for now.
+ *
+ * This function provides a interface to send CCC from high layer driver.
+ * This is needed for the bus topologic with I3C MUX or switch devices.
+ * The I3C MUX may not enable the local/slave port by default.  The master
+ * controller needs to attach the I3C MUX device, and program the mode
+ * registers to enable the local/slave port.  Then the devices hehind
+ * the MUX may need for CCC for initialization (e.g. SETAASA to bring them
+ * from I2C mode to I3C mode)
+ */
+int i3c_device_send_ccc_cmd(struct i3c_device *dev, u8 ccc_id)
+{
+	int ret;
+
+	if (dev->desc) {
+		i3c_bus_normaluse_lock(dev->bus);
+		ret = i3c_dev_send_ccc_cmd_locked(dev->desc, ccc_id);
+		i3c_bus_normaluse_unlock(dev->bus);
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_device_send_ccc_cmd);
+
 /**
  * i3cdev_to_dev() - Returns the device embedded in @i3cdev
  * @i3cdev: I3C device
@@ -283,3 +377,150 @@ void i3c_driver_unregister(struct i3c_driver *drv)
 	driver_unregister(&drv->driver);
 }
 EXPORT_SYMBOL_GPL(i3c_driver_unregister);
+
+/**
+ * i3c_device_getstatus_ccc() - receive device status
+ *
+ * @dev: I3C device to get the status for
+ * @info: I3C device info to fill the status in
+ *
+ * Receive I3C device status from I3C master device via corresponding CCC
+ * command
+ *
+ * Return: 0 in case of success, a negative error code otherwise.
+ */
+int i3c_device_getstatus_ccc(struct i3c_device *dev, struct i3c_device_info *info)
+{
+	int ret = -EINVAL;
+
+	i3c_bus_normaluse_lock(dev->bus);
+	if (dev->desc)
+		ret = i3c_dev_getstatus_locked(dev->desc, &dev->desc->info);
+	i3c_bus_normaluse_unlock(dev->bus);
+	i3c_device_get_info(dev, info);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_device_getstatus_ccc);
+
+/**
+ * i3c_device_control_pec() - enable or disable PEC support in HW
+ *
+ * @dev: I3C device to get the status for
+ * @pec: flag telling whether PEC support shall be enabled or disabled
+ *
+ * Try to enable or disable HW support for PEC (Packet Error Check).
+ * In case no HW support for PEC, software implementation could be used.
+ *
+ * Return: 0 in case of success, -EOPNOTSUPP in case PEC is not supported by HW,
+ *         other negative error codes when PEC enabling failed.
+ */
+int i3c_device_control_pec(struct i3c_device *dev, bool pec)
+{
+	return i3c_dev_control_pec(dev->desc, pec);
+}
+EXPORT_SYMBOL_GPL(i3c_device_control_pec);
+
+/**
+ * i3c_device_setmrl_ccc() - set maximum read length
+ *
+ * @dev: I3C device to set the length for
+ * @info: I3C device info to fill the length in
+ * @read_len: maximum read length value to be set
+ * @ibi_len: maximum ibi payload length to be set
+ *
+ * Set I3C device maximum read length from I3C master device via corresponding CCC command
+ *
+ * Return: 0 in case of success, a negative error code otherwise.
+ */
+int i3c_device_setmrl_ccc(struct i3c_device *dev, struct i3c_device_info *info, u16 read_len,
+			  u8 ibi_len)
+{
+	struct i3c_master_controller *master = i3c_dev_get_master(dev->desc);
+	int ret = -EINVAL;
+
+	i3c_bus_normaluse_lock(dev->bus);
+	if (master)
+		ret = i3c_master_setmrl_locked(master, &dev->desc->info, read_len, ibi_len);
+	i3c_bus_normaluse_unlock(dev->bus);
+	i3c_device_get_info(dev, info);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_device_setmrl_ccc);
+
+/**
+ * i3c_device_setmwl_ccc() - set maximum write length
+ *
+ * @dev: I3C device to set the length for
+ * @info: I3C device info to fill the length in
+ * @write_len: maximum write length value to be set
+ *
+ * Set I3C device maximum write length from I3C master device via corresponding CCC command
+ *
+ * Return: 0 in case of success, a negative error code otherwise.
+ */
+int i3c_device_setmwl_ccc(struct i3c_device *dev, struct i3c_device_info *info, u16 write_len)
+{
+	struct i3c_master_controller *master = i3c_dev_get_master(dev->desc);
+	int ret = -EINVAL;
+
+	i3c_bus_normaluse_lock(dev->bus);
+	if (master)
+		ret = i3c_master_setmwl_locked(master, &dev->desc->info, write_len);
+	i3c_bus_normaluse_unlock(dev->bus);
+	i3c_device_get_info(dev, info);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_device_setmwl_ccc);
+
+/**
+ * i3c_device_getmrl_ccc() - get maximum read length
+ *
+ * @dev: I3C device to get the length for
+ * @info: I3C device info to fill the length in
+ *
+ * Receive I3C device maximum read length from I3C master device via corresponding CCC command
+ *
+ * Return: 0 in case of success, a negative error code otherwise.
+ */
+int i3c_device_getmrl_ccc(struct i3c_device *dev, struct i3c_device_info *info)
+{
+	struct i3c_master_controller *master = i3c_dev_get_master(dev->desc);
+	int ret = -EINVAL;
+
+	i3c_bus_normaluse_lock(dev->bus);
+	if (master)
+		ret = i3c_master_getmrl_locked(master, &dev->desc->info);
+	i3c_bus_normaluse_unlock(dev->bus);
+	i3c_device_get_info(dev, info);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_device_getmrl_ccc);
+
+/**
+ * i3c_device_getmwl_ccc() - get maximum write length
+ *
+ * @dev: I3C device to get the length for
+ * @info: I3C device info to fill the length in
+ *
+ * Receive I3C device maximum write length from I3C master device via corresponding CCC command
+ *
+ * Return: 0 in case of success, a negative error code otherwise.
+ */
+int i3c_device_getmwl_ccc(struct i3c_device *dev, struct i3c_device_info *info)
+{
+	struct i3c_master_controller *master = i3c_dev_get_master(dev->desc);
+	int ret = -EINVAL;
+
+	i3c_bus_normaluse_lock(dev->bus);
+	if (master)
+		ret = i3c_master_getmwl_locked(master, &dev->desc->info);
+	i3c_bus_normaluse_unlock(dev->bus);
+	i3c_device_get_info(dev, info);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_device_getmwl_ccc);
diff --git a/drivers/i3c/i3c-ast-bridge-ic.c b/drivers/i3c/i3c-ast-bridge-ic.c
new file mode 100644
index 000000000000..cd2a8225ef3f
--- /dev/null
+++ b/drivers/i3c/i3c-ast-bridge-ic.c
@@ -0,0 +1,279 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (c) 2021 Aspeed Technology Inc.
+ *
+ * Aspeed Bridge IC driver
+ *
+ */
+
+#include <linux/i3c/device.h>
+#include <linux/i3c/master.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/sysfs.h>
+#include "internals.h"
+#include <linux/delay.h>
+#define MQ_MSGBUF_SIZE		256
+#define MQ_QUEUE_SIZE		4
+#define MQ_QUEUE_NEXT(x)	(((x) + 1) & (MQ_QUEUE_SIZE - 1))
+
+#define IBI_QUEUE_STATUS_PEC_ERR	BIT(30)
+#define IBI_STATUS_LAST_FRAG	BIT(24)
+#define PID_MANUF_ID_ASPEED	0x03f6
+#define POLLIING_INTERVAL_MS	2000
+
+struct mq_msg {
+	int len;
+	u8 *buf;
+};
+
+struct astbic {
+	struct bin_attribute bin;
+	struct kernfs_node *kn;
+
+	struct i3c_device *i3cdev;
+
+	spinlock_t lock;
+	int in;
+	int out;
+	struct mutex mq_lock;
+	struct mq_msg *curr;
+	int truncated;
+	struct mq_msg queue[MQ_QUEUE_SIZE];
+};
+
+static u8 mdb_table[] = {
+	0xbf, /* Aspeed BIC */
+	0,
+};
+
+static void i3c_ibi_mqueue_callback(struct i3c_device *dev,
+				    const struct i3c_ibi_payload *payload)
+{
+	struct astbic *mq = dev_get_drvdata(&dev->dev);
+	struct mq_msg *msg;
+	u8 *buf = (u8 *)payload->data;
+	struct i3c_device_info info;
+	u32 status;
+	const u8 *mdb;
+
+	mutex_lock(&mq->mq_lock);
+	i3c_device_get_info(dev, &info);
+	msg = mq->curr;
+
+	/* first DW is IBI status */
+	status = *(u32 *)buf;
+
+	/* then the raw data */
+	buf += sizeof(status);
+	memcpy(&msg->buf[msg->len], buf, payload->len - sizeof(status));
+	msg->len += payload->len - sizeof(status);
+	if (status & IBI_QUEUE_STATUS_PEC_ERR) {
+		for (mdb = mdb_table; *mdb != 0; mdb++)
+			if (buf[0] == *mdb)
+				break;
+		if (!(*mdb)) {
+			dev_err(&dev->dev, "ibi crc/pec error: mdb = %x", buf[0]);
+			mutex_unlock(&mq->mq_lock);
+			return;
+		}
+	}
+	/* if last fragment, notify and update pointers */
+	if (status & IBI_STATUS_LAST_FRAG) {
+		/* check pending-read-notification */
+		if (IS_MDB_PENDING_READ_NOTIFY(msg->buf[0])) {
+			struct i3c_priv_xfer xfers[1] = {
+				{
+					.rnw = true,
+					.len = info.max_read_len,
+					.data.in = msg->buf,
+				},
+			};
+
+			i3c_device_do_priv_xfers(dev, xfers, 1);
+
+			msg->len = xfers[0].len;
+		}
+
+		mq->in = MQ_QUEUE_NEXT(mq->in);
+		mq->curr = &mq->queue[mq->in];
+		mq->curr->len = 0;
+
+		if (mq->out == mq->in)
+			mq->out = MQ_QUEUE_NEXT(mq->out);
+		kernfs_notify(mq->kn);
+	}
+	mutex_unlock(&mq->mq_lock);
+}
+
+static ssize_t i3c_astbic_bin_read(struct file *filp, struct kobject *kobj,
+				   struct bin_attribute *attr, char *buf,
+				   loff_t pos, size_t count)
+{
+	struct astbic *mq;
+	struct mq_msg *msg;
+	unsigned long flags;
+	bool more = false;
+	ssize_t ret = 0;
+
+	mq = dev_get_drvdata(container_of(kobj, struct device, kobj));
+
+	spin_lock_irqsave(&mq->lock, flags);
+	if (mq->out != mq->in) {
+		msg = &mq->queue[mq->out];
+
+		if (msg->len <= count) {
+			ret = msg->len;
+			memcpy(buf, msg->buf, ret);
+		} else {
+			ret = -EOVERFLOW; /* Drop this HUGE one. */
+		}
+
+		mq->out = MQ_QUEUE_NEXT(mq->out);
+		if (mq->out != mq->in)
+			more = true;
+	}
+	spin_unlock_irqrestore(&mq->lock, flags);
+
+	if (more)
+		kernfs_notify(mq->kn);
+
+	return ret;
+}
+
+static ssize_t i3c_astbic_bin_write(struct file *filp, struct kobject *kobj,
+				    struct bin_attribute *attr, char *buf,
+				    loff_t pos, size_t count)
+{
+	struct astbic *astbic;
+	// struct i3c_device *i3c;
+	struct i3c_priv_xfer xfers = {
+		.rnw = false,
+		.len = count,
+	};
+	int ret = -EACCES;
+
+	astbic = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	if (!astbic) {
+		count = -1;
+		goto out;
+	}
+
+	xfers.data.out = buf;
+	ret = i3c_device_do_priv_xfers(astbic->i3cdev, &xfers, 1);
+out:
+	return (!ret) ? count : ret;
+}
+
+static void i3c_ast_bridgeic_remove(struct i3c_device *i3cdev)
+{
+	struct device *dev = &i3cdev->dev;
+	struct astbic *astbic;
+
+	astbic = dev_get_drvdata(dev);
+
+	i3c_device_disable_ibi(i3cdev);
+	i3c_device_free_ibi(i3cdev);
+
+	kernfs_put(astbic->kn);
+	sysfs_remove_bin_file(&dev->kobj, &astbic->bin);
+	devm_kfree(dev, astbic);
+}
+
+static int i3c_ast_bridgeic_probe(struct i3c_device *i3cdev)
+{
+	struct device *dev = &i3cdev->dev;
+	struct astbic *astbic;
+	struct i3c_ibi_setup ibireq = {};
+	int ret, i;
+	struct i3c_device_info info;
+	void *buf;
+
+	if (dev->type == &i3c_masterdev_type)
+		return -ENOTSUPP;
+
+	astbic = devm_kzalloc(dev, sizeof(*astbic), GFP_KERNEL);
+	if (!astbic)
+		return -ENOMEM;
+
+	BUILD_BUG_ON(!is_power_of_2(MQ_QUEUE_SIZE));
+
+	buf = devm_kmalloc_array(dev, MQ_QUEUE_SIZE, MQ_MSGBUF_SIZE,
+				 GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	for (i = 0; i < MQ_QUEUE_SIZE; i++) {
+		astbic->queue[i].buf = (u8 *)buf + i * MQ_MSGBUF_SIZE;
+		astbic->queue[i].len = 0;
+	}
+	spin_lock_init(&astbic->lock);
+	mutex_init(&astbic->mq_lock);
+	astbic->curr = &astbic->queue[0];
+
+	astbic->i3cdev = i3cdev;
+
+	sysfs_bin_attr_init(&astbic->bin);
+	astbic->bin.attr.name = "mqueue";
+	astbic->bin.attr.mode = 0600;
+	astbic->bin.read = i3c_astbic_bin_read;
+	astbic->bin.write = i3c_astbic_bin_write;
+	astbic->bin.size = MQ_MSGBUF_SIZE * MQ_QUEUE_SIZE;
+	ret = sysfs_create_bin_file(&dev->kobj, &astbic->bin);
+
+	astbic->kn = kernfs_find_and_get(dev->kobj.sd, astbic->bin.attr.name);
+	if (!astbic->kn) {
+		sysfs_remove_bin_file(&dev->kobj, &astbic->bin);
+		return -EFAULT;
+	}
+
+	i3c_device_get_info(i3cdev, &info);
+
+	ret = i3c_device_setmrl_ccc(i3cdev, &info, MQ_MSGBUF_SIZE,
+					    min(MQ_MSGBUF_SIZE, __UINT8_MAX__));
+	if (ret) {
+		ret = i3c_device_getmrl_ccc(i3cdev, &info);
+		if (ret)
+			return ret;
+	}
+
+	dev_set_drvdata(dev, astbic);
+
+	ibireq.handler = i3c_ibi_mqueue_callback;
+	ibireq.max_payload_len = MQ_MSGBUF_SIZE;
+	ibireq.num_slots = MQ_QUEUE_SIZE;
+
+	ret = i3c_device_request_ibi(astbic->i3cdev, &ibireq);
+	ret |= i3c_device_enable_ibi(astbic->i3cdev);
+	if (ret) {
+		kernfs_put(astbic->kn);
+		sysfs_remove_bin_file(&dev->kobj, &astbic->bin);
+		return ret;
+	}
+	return 0;
+}
+
+static const struct i3c_device_id i3c_ast_bridgeic_ids[] = {
+	I3C_DEVICE(0x3f6, 0x7341, (void *)0),
+	I3C_DEVICE(0x3f6, 0x8000, (void *)0),
+	I3C_DEVICE(0x3f6, 0x8001, (void *)0),
+	I3C_DEVICE(0x3f6, 0x0503, (void *)0),
+	I3C_DEVICE(0x3f6, 0xA001, (void *)0),
+	{ /* sentinel */ },
+};
+MODULE_DEVICE_TABLE(i3c, i3c_ast_bridgeic_ids);
+
+static struct i3c_driver astbic_driver = {
+	.driver = {
+		.name = "i3c-ast-bridgeic",
+	},
+	.probe = i3c_ast_bridgeic_probe,
+	.remove = i3c_ast_bridgeic_remove,
+	.id_table = i3c_ast_bridgeic_ids,
+};
+module_i3c_driver(astbic_driver);
+
+MODULE_AUTHOR("Andy Chung <Andy_Chung@wiwynn.com>");
+MODULE_DESCRIPTION("I3C Aspeed bridge IC driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/i3c/i3c-ibi-mqueue.c b/drivers/i3c/i3c-ibi-mqueue.c
new file mode 100644
index 000000000000..8f1ddb9fa8f8
--- /dev/null
+++ b/drivers/i3c/i3c-ibi-mqueue.c
@@ -0,0 +1,246 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (c) 2021 Aspeed Technology Inc.
+ */
+
+#include <linux/i3c/device.h>
+#include <linux/i3c/master.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/sysfs.h>
+#include <linux/delay.h>
+#include "internals.h"
+
+#define MQ_MSGBUF_SIZE		256
+#define MQ_QUEUE_SIZE		4
+#define MQ_QUEUE_NEXT(x)	(((x) + 1) & (MQ_QUEUE_SIZE - 1))
+
+#define IBI_QUEUE_STATUS_PEC_ERR	BIT(30)
+#define IBI_STATUS_LAST_FRAG	BIT(24)
+#define PID_MANUF_ID_ASPEED	0x03f6
+
+struct mq_msg {
+	int len;
+	u8 *buf;
+};
+
+struct mq_queue {
+	struct bin_attribute bin;
+	struct kernfs_node *kn;
+
+	spinlock_t lock;
+	int in;
+	int out;
+
+	struct mq_msg *curr;
+	int truncated;
+	struct mq_msg queue[MQ_QUEUE_SIZE];
+};
+
+static u8 mdb_table[] = {
+	0xbf, /* Aspeed BIC */
+	0,
+};
+
+static void i3c_ibi_mqueue_callback(struct i3c_device *dev,
+				    const struct i3c_ibi_payload *payload)
+{
+	struct mq_queue *mq = dev_get_drvdata(&dev->dev);
+	struct mq_msg *msg = mq->curr;
+	u8 *buf = (u8 *)payload->data;
+	struct i3c_device_info info;
+	u32 status;
+	const u8 *mdb;
+
+	i3c_device_get_info(dev, &info);
+	/* first DW is IBI status */
+	status = *(u32 *)buf;
+
+	/* then the raw data */
+	buf += sizeof(status);
+	memcpy(&msg->buf[msg->len], buf, payload->len - sizeof(status));
+	msg->len += payload->len - sizeof(status);
+	if (status & IBI_QUEUE_STATUS_PEC_ERR) {
+		for (mdb = mdb_table; *mdb != 0; mdb++)
+			if (buf[0] == *mdb)
+				break;
+		if (!(*mdb)) {
+			dev_err(&dev->dev, "ibi crc/pec error: mdb = %x", buf[0]);
+			return;
+		}
+	}
+	/* if last fragment, notify and update pointers */
+	if (status & IBI_STATUS_LAST_FRAG) {
+		/* check pending-read-notification */
+		if (IS_MDB_PENDING_READ_NOTIFY(msg->buf[0])) {
+			struct i3c_priv_xfer xfers[1] = {
+				{
+					.rnw = true,
+					.len = info.max_read_len,
+					.data.in = msg->buf,
+				},
+			};
+
+			i3c_device_do_priv_xfers(dev, xfers, 1);
+
+			msg->len = xfers[0].len;
+		}
+
+		spin_lock(&mq->lock);
+		mq->in = MQ_QUEUE_NEXT(mq->in);
+		mq->curr = &mq->queue[mq->in];
+		mq->curr->len = 0;
+
+		if (mq->out == mq->in)
+			mq->out = MQ_QUEUE_NEXT(mq->out);
+		spin_unlock(&mq->lock);
+		kernfs_notify(mq->kn);
+	}
+}
+
+static ssize_t i3c_ibi_mqueue_bin_read(struct file *filp, struct kobject *kobj,
+				       struct bin_attribute *attr, char *buf,
+				       loff_t pos, size_t count)
+{
+	struct mq_queue *mq;
+	struct mq_msg *msg;
+	unsigned long flags;
+	bool more = false;
+	ssize_t ret = 0;
+
+	mq = dev_get_drvdata(container_of(kobj, struct device, kobj));
+
+	spin_lock_irqsave(&mq->lock, flags);
+	if (mq->out != mq->in) {
+		msg = &mq->queue[mq->out];
+
+		if (msg->len <= count) {
+			ret = msg->len;
+			memcpy(buf, msg->buf, ret);
+		} else {
+			ret = -EOVERFLOW; /* Drop this HUGE one. */
+		}
+
+		mq->out = MQ_QUEUE_NEXT(mq->out);
+		if (mq->out != mq->in)
+			more = true;
+	}
+	spin_unlock_irqrestore(&mq->lock, flags);
+
+	if (more)
+		kernfs_notify(mq->kn);
+
+	return ret;
+}
+
+static int i3c_ibi_mqueue_probe(struct i3c_device *i3cdev)
+{
+	struct device *dev = &i3cdev->dev;
+	struct mq_queue *mq;
+	struct i3c_ibi_setup ibireq = {};
+	int ret, i;
+	struct i3c_device_info info;
+	void *buf;
+
+	if (dev->type == &i3c_masterdev_type)
+		return -ENOTSUPP;
+
+	mq = devm_kzalloc(dev, sizeof(*mq), GFP_KERNEL);
+	if (!mq)
+		return -ENOMEM;
+
+	BUILD_BUG_ON(!is_power_of_2(MQ_QUEUE_SIZE));
+
+	buf = devm_kmalloc_array(dev, MQ_QUEUE_SIZE, MQ_MSGBUF_SIZE,
+				 GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	for (i = 0; i < MQ_QUEUE_SIZE; i++) {
+		mq->queue[i].buf = buf + i * MQ_MSGBUF_SIZE;
+		mq->queue[i].len = 0;
+	}
+
+	i3c_device_get_info(i3cdev, &info);
+
+	ret = i3c_device_setmrl_ccc(i3cdev, &info, MQ_MSGBUF_SIZE,
+					    min(MQ_MSGBUF_SIZE, __UINT8_MAX__));
+	if (ret) {
+		ret = i3c_device_getmrl_ccc(i3cdev, &info);
+		if (ret)
+			return ret;
+	}
+
+	dev_set_drvdata(dev, mq);
+
+	spin_lock_init(&mq->lock);
+	mq->curr = &mq->queue[0];
+
+	sysfs_bin_attr_init(&mq->bin);
+	mq->bin.attr.name = "ibi-mqueue";
+	mq->bin.attr.mode = 0400;
+	mq->bin.read = i3c_ibi_mqueue_bin_read;
+	mq->bin.size = MQ_MSGBUF_SIZE * MQ_QUEUE_SIZE;
+
+	ret = sysfs_create_bin_file(&dev->kobj, &mq->bin);
+	if (ret)
+		return ret;
+
+	mq->kn = kernfs_find_and_get(dev->kobj.sd, mq->bin.attr.name);
+	if (!mq->kn) {
+		sysfs_remove_bin_file(&dev->kobj, &mq->bin);
+		return -EFAULT;
+	}
+
+	ibireq.handler = i3c_ibi_mqueue_callback;
+	ibireq.max_payload_len = MQ_MSGBUF_SIZE;
+	ibireq.num_slots = MQ_QUEUE_SIZE;
+
+	ret = i3c_device_request_ibi(i3cdev, &ibireq);
+	ret |= i3c_device_enable_ibi(i3cdev);
+
+	if (ret) {
+		kernfs_put(mq->kn);
+		sysfs_remove_bin_file(&dev->kobj, &mq->bin);
+		return ret;
+	}
+
+	return 0;
+}
+
+static void i3c_ibi_mqueue_remove(struct i3c_device *i3cdev)
+{
+	struct mq_queue *mq = dev_get_drvdata(&i3cdev->dev);
+
+	i3c_device_disable_ibi(i3cdev);
+	i3c_device_free_ibi(i3cdev);
+
+	kernfs_put(mq->kn);
+	sysfs_remove_bin_file(&i3cdev->dev.kobj, &mq->bin);
+}
+
+static const struct i3c_device_id i3c_ibi_mqueue_ids[] = {
+	I3C_DEVICE(0x3f6, 0x8000, (void *)0),
+	I3C_DEVICE(0x3f6, 0x8001, (void *)0),
+	I3C_DEVICE(0x3f6, 0x0503, (void *)0),
+	I3C_DEVICE(0x3f6, 0xA001, (void *)0),
+	{ /* sentinel */ },
+};
+MODULE_DEVICE_TABLE(i3c, i3c_ibi_mqueue_ids);
+
+static struct i3c_driver ibi_mqueue_driver = {
+	.driver = {
+		.name = "i3c-ibi-mqueue",
+	},
+	.probe = i3c_ibi_mqueue_probe,
+	.remove = i3c_ibi_mqueue_remove,
+	.id_table = i3c_ibi_mqueue_ids,
+};
+module_i3c_driver(ibi_mqueue_driver);
+
+MODULE_AUTHOR("Dylan Hung <dylan_hung@aspeedtech.com>");
+MODULE_DESCRIPTION("I3C IBI mqueue driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/i3c/i3c-mux-imx3102.c b/drivers/i3c/i3c-mux-imx3102.c
new file mode 100644
index 000000000000..b6972893cbd6
--- /dev/null
+++ b/drivers/i3c/i3c-mux-imx3102.c
@@ -0,0 +1,227 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (c) 2021 Aspeed Technology Inc.
+ *
+ * IMX3102: 2-to-1 multiplexier
+ *
+ * +------------------   +
+ * | SoC                 |
+ * |                     |
+ * | I3C controller #0 - | --+
+ * |                     |    \                  dev   dev
+ * |                     |     +---------+       |     |
+ * |                     |     | IMX3102 | ---+--+--+--+--- i3c bus
+ * |                     |     +---------+    |     |
+ * |                     |    /               dev   dev
+ * | I3C controller #1 - | --+
+ * |                     |
+ * +---------------------+
+ */
+
+#include <linux/i3c/device.h>
+#include <linux/i3c/master.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/sysfs.h>
+#include <linux/delay.h>
+#include <linux/regmap.h>
+#include "internals.h"
+
+#define IMX3102_DEVICE_TYPE_HI		0x0
+#define IMX3102_DEVICE_TYPE_LO		0x1
+
+#define IMX3102_PORT_CONF		0x40
+#define   IMX3102_PORT_CONF_M1_EN	BIT(7)
+#define   IMX3102_PORT_CONF_S_EN	BIT(6)
+#define IMX3102_PORT_SEL		0x41
+#define   IMX3102_PORT_SEL_M1		BIT(7)
+#define   IMX3102_PORT_SEL_S_EN		BIT(6)
+
+struct imx3102 {
+	struct regmap *regmap;
+
+	struct bin_attribute ownership;
+	struct bin_attribute reinit;
+	struct kernfs_node *kn;
+
+	struct i3c_device *i3cdev;
+};
+
+static ssize_t i3c_mux_imx3102_query(struct file *filp, struct kobject *kobj,
+				     struct bin_attribute *attr, char *buf,
+				     loff_t pos, size_t count)
+{
+	struct imx3102 *imx3102;
+	struct device *dev;
+	int ret;
+	u8 data[2];
+
+	imx3102 = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	if (!imx3102)
+		return -1;
+
+	dev = &imx3102->i3cdev->dev;
+
+	ret = regmap_raw_read(imx3102->regmap, IMX3102_DEVICE_TYPE_HI, data, 2);
+	if (ret)
+		sprintf(buf, "N\n");
+	else
+		sprintf(buf, "Y\n");
+
+	return 2;
+}
+
+/* write whatever value to imx3102-mux to release the ownership */
+static ssize_t i3c_mux_imx3102_release_chan(struct file *filp,
+					    struct kobject *kobj,
+					    struct bin_attribute *attr,
+					    char *buf, loff_t pos, size_t count)
+{
+	struct imx3102 *imx3102;
+	struct device *dev;
+	struct regmap *regmap;
+	int ret;
+	u8 select;
+
+	imx3102 = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	if (!imx3102) {
+		count = -1;
+		goto out;
+	}
+
+	dev = &imx3102->i3cdev->dev;
+	regmap = imx3102->regmap;
+	ret = regmap_raw_read(regmap, IMX3102_PORT_SEL, &select, 1);
+	if (ret)
+		goto out;
+
+	/* invert the bit to change the ownership */
+	select ^= IMX3102_PORT_SEL_M1;
+	regmap_raw_write(regmap, IMX3102_PORT_SEL, &select, 1);
+
+out:
+	return count;
+}
+
+static ssize_t i3c_mux_imx3102_bus_reinit(struct file *filp,
+					  struct kobject *kobj,
+					  struct bin_attribute *attr, char *buf,
+					  loff_t pos, size_t count)
+{
+	struct imx3102 *imx3102;
+	int ret;
+
+	imx3102 = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	if (!imx3102) {
+		count = -1;
+		return count;
+	}
+
+	ret = i3c_device_send_ccc_cmd(imx3102->i3cdev, I3C_CCC_SETHID);
+	ret = i3c_device_send_ccc_cmd(imx3102->i3cdev, I3C_CCC_SETAASA);
+
+	return count;
+}
+
+static int i3c_mux_imx3102_probe(struct i3c_device *i3cdev)
+{
+	struct device *dev = &i3cdev->dev;
+	struct imx3102 *imx3102;
+	struct regmap *regmap;
+	struct regmap_config imx3102_i3c_regmap_config = {
+		.reg_bits = 8,
+		.pad_bits = 8,
+		.val_bits = 8,
+	};
+	int ret;
+	u8 data[2];
+
+	if (dev->type == &i3c_masterdev_type)
+		return -ENOTSUPP;
+
+	imx3102 = devm_kzalloc(dev, sizeof(*imx3102), GFP_KERNEL);
+	if (!imx3102)
+		return -ENOMEM;
+
+	imx3102->i3cdev = i3cdev;
+
+	/* register regmap */
+	regmap = devm_regmap_init_i3c(i3cdev, &imx3102_i3c_regmap_config);
+	if (IS_ERR(regmap)) {
+		dev_err(dev, "Failed to register i3c regmap %d\n",
+			(int)PTR_ERR(regmap));
+		return PTR_ERR(regmap);
+	}
+	imx3102->regmap = regmap;
+
+	sysfs_bin_attr_init(&imx3102->ownership);
+	imx3102->ownership.attr.name = "imx3102.ownership";
+	imx3102->ownership.attr.mode = 0600;
+	imx3102->ownership.read = i3c_mux_imx3102_query;
+	imx3102->ownership.write = i3c_mux_imx3102_release_chan;
+	imx3102->ownership.size = 2;
+	ret = sysfs_create_bin_file(&dev->kobj, &imx3102->ownership);
+
+	sysfs_bin_attr_init(&imx3102->reinit);
+	imx3102->reinit.attr.name = "imx3102.reinit";
+	imx3102->reinit.attr.mode = 0200;
+	imx3102->reinit.write = i3c_mux_imx3102_bus_reinit;
+	imx3102->reinit.size = 2;
+	ret = sysfs_create_bin_file(&dev->kobj, &imx3102->reinit);
+
+	imx3102->kn = kernfs_find_and_get(dev->kobj.sd, imx3102->ownership.attr.name);
+	dev_set_drvdata(dev, imx3102);
+
+	ret = regmap_raw_read(regmap, IMX3102_DEVICE_TYPE_HI, data, 2);
+	if (ret) {
+		dev_info(dev, "No ownership\n");
+		return 0;
+	}
+	dev_dbg(dev, "device ID %02x %02x\n", data[0], data[1]);
+
+	/* enable the slave port */
+	regmap_raw_read(regmap, IMX3102_PORT_CONF, &data[0], 2);
+	data[0] |= IMX3102_PORT_CONF_S_EN | IMX3102_PORT_CONF_M1_EN;
+	data[1] |= IMX3102_PORT_SEL_S_EN;
+	regmap_raw_write(regmap, IMX3102_PORT_CONF, data, 2);
+
+	/* send SETAASA to bring the devices behind the mux to I3C mode */
+	i3c_device_send_ccc_cmd(i3cdev, I3C_CCC_SETAASA);
+
+	return 0;
+}
+
+static void i3c_mux_imx3102_remove(struct i3c_device *i3cdev)
+{
+	struct device *dev = &i3cdev->dev;
+	struct imx3102 *imx3102;
+
+	imx3102 = dev_get_drvdata(dev);
+
+	kernfs_put(imx3102->kn);
+	sysfs_remove_bin_file(&dev->kobj, &imx3102->ownership);
+	devm_kfree(dev, imx3102);
+}
+
+static const struct i3c_device_id i3c_mux_imx3102_ids[] = {
+	I3C_DEVICE(0x266, 0x3102, (void *)0),
+	{ /* sentinel */ },
+};
+MODULE_DEVICE_TABLE(i3c, i3c_mux_imx3102_ids);
+
+static struct i3c_driver imx3102_driver = {
+	.driver = {
+		.name = "i3c-mux-imx3102",
+	},
+	.probe = i3c_mux_imx3102_probe,
+	.remove = i3c_mux_imx3102_remove,
+	.id_table = i3c_mux_imx3102_ids,
+};
+module_i3c_driver(imx3102_driver);
+
+MODULE_AUTHOR("Dylan Hung <dylan_hung@aspeedtech.com>");
+MODULE_DESCRIPTION("I3C IMX3102 multiplexer driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/i3c/i3c-slave-mqueue.c b/drivers/i3c/i3c-slave-mqueue.c
new file mode 100644
index 000000000000..ff05ee4e7de8
--- /dev/null
+++ b/drivers/i3c/i3c-slave-mqueue.c
@@ -0,0 +1,206 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (c) 2021 Aspeed Technology Inc.
+ */
+
+#include <linux/i3c/master.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/sysfs.h>
+
+#define MQ_MSGBUF_SIZE		256
+#define MQ_QUEUE_SIZE		4
+#define MQ_QUEUE_NEXT(x)	(((x) + 1) & (MQ_QUEUE_SIZE - 1))
+
+#define IBI_STATUS_LAST_FRAG	BIT(24)
+#define MQ_MDB			IBI_MDB_ID(0b101, 0x1f)
+
+struct mq_msg {
+	int len;
+	u8 *buf;
+};
+
+struct mq_queue {
+	struct bin_attribute bin;
+	struct kernfs_node *kn;
+
+	spinlock_t lock;
+	int in;
+	int out;
+
+	struct mq_msg *curr;
+	int truncated;
+	struct mq_msg queue[MQ_QUEUE_SIZE];
+
+	struct i3c_master_controller *i3c_controller;
+	u8 mdb;
+};
+
+static void i3c_slave_mqueue_callback(struct i3c_master_controller *master,
+				      const struct i3c_slave_payload *payload)
+{
+	struct mq_queue *mq = dev_get_drvdata(&master->dev);
+	struct mq_msg *msg = mq->curr;
+
+	memcpy(msg->buf, (u8 *)payload->data, payload->len);
+	msg->len = payload->len;
+
+	spin_lock(&mq->lock);
+	mq->in = MQ_QUEUE_NEXT(mq->in);
+	mq->curr = &mq->queue[mq->in];
+	mq->curr->len = 0;
+
+	if (mq->out == mq->in)
+		mq->out = MQ_QUEUE_NEXT(mq->out);
+	spin_unlock(&mq->lock);
+	kernfs_notify(mq->kn);
+}
+
+static ssize_t i3c_slave_mqueue_bin_read(struct file *filp, struct kobject *kobj,
+				       struct bin_attribute *attr, char *buf,
+				       loff_t pos, size_t count)
+{
+	struct mq_queue *mq;
+	struct mq_msg *msg;
+	unsigned long flags;
+	bool more = false;
+	ssize_t ret = 0;
+
+	mq = dev_get_drvdata(container_of(kobj, struct device, kobj));
+
+	spin_lock_irqsave(&mq->lock, flags);
+	if (mq->out != mq->in) {
+		msg = &mq->queue[mq->out];
+
+		if (msg->len <= count) {
+			ret = msg->len;
+			memcpy(buf, msg->buf, ret);
+		} else {
+			ret = -EOVERFLOW; /* Drop this HUGE one. */
+		}
+
+		mq->out = MQ_QUEUE_NEXT(mq->out);
+		if (mq->out != mq->in)
+			more = true;
+	}
+	spin_unlock_irqrestore(&mq->lock, flags);
+
+	if (more)
+		kernfs_notify(mq->kn);
+
+	return ret;
+}
+
+static ssize_t i3c_slave_mqueue_bin_write(struct file *filp,
+					  struct kobject *kobj,
+					  struct bin_attribute *attr, char *buf,
+					  loff_t pos, size_t count)
+{
+	struct mq_queue *mq;
+	struct i3c_slave_payload payload, ibi;
+	u8 *data;
+
+	mq = dev_get_drvdata(container_of(kobj, struct device, kobj));
+
+	if (IS_MDB_PENDING_READ_NOTIFY(mq->mdb)) {
+		ibi.data = &mq->mdb;
+		ibi.len = 1;
+		payload.data = buf;
+		payload.len = count;
+		i3c_master_put_read_data(mq->i3c_controller, &payload, &ibi);
+	} else {
+		data = kmalloc(count + 1, GFP_KERNEL);
+		if (!data)
+			return -ENOMEM;
+
+		data[0] = mq->mdb;
+		memcpy(&data[1], buf, count);
+
+		payload.data = data;
+		payload.len = count + 1;
+		i3c_master_send_sir(mq->i3c_controller, &payload);
+		kfree(data);
+	}
+
+	return count;
+}
+
+int i3c_slave_mqueue_probe(struct i3c_master_controller *master)
+{
+	struct mq_queue *mq;
+	int ret, i;
+	void *buf;
+	struct i3c_slave_setup req = {};
+	struct device *dev = &master->dev;
+
+	mq = devm_kzalloc(dev, sizeof(*mq), GFP_KERNEL);
+	if (!mq)
+		return -ENOMEM;
+
+	BUILD_BUG_ON(!is_power_of_2(MQ_QUEUE_SIZE));
+
+	buf = devm_kmalloc_array(dev, MQ_QUEUE_SIZE, MQ_MSGBUF_SIZE,
+				 GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	for (i = 0; i < MQ_QUEUE_SIZE; i++) {
+		mq->queue[i].buf = buf + i * MQ_MSGBUF_SIZE;
+		mq->queue[i].len = 0;
+	}
+
+	dev_set_drvdata(dev, mq);
+
+	spin_lock_init(&mq->lock);
+	mq->curr = &mq->queue[0];
+
+	sysfs_bin_attr_init(&mq->bin);
+	mq->bin.attr.name = "slave-mqueue";
+	mq->bin.attr.mode = 0600;
+	mq->bin.read = i3c_slave_mqueue_bin_read;
+	mq->bin.write = i3c_slave_mqueue_bin_write;
+	mq->bin.size = MQ_MSGBUF_SIZE * MQ_QUEUE_SIZE;
+
+	mq->i3c_controller = master;
+	mq->mdb = MQ_MDB;
+
+	ret = sysfs_create_bin_file(&dev->kobj, &mq->bin);
+	if (ret)
+		return ret;
+
+	mq->kn = kernfs_find_and_get(dev->kobj.sd, mq->bin.attr.name);
+	if (!mq->kn) {
+		sysfs_remove_bin_file(&dev->kobj, &mq->bin);
+		return -EFAULT;
+	}
+
+	req.handler = i3c_slave_mqueue_callback;
+	req.max_payload_len = MQ_MSGBUF_SIZE;
+	req.num_slots = MQ_QUEUE_SIZE;
+
+	ret = i3c_master_register_slave(master, &req);
+
+	if (ret) {
+		kernfs_put(mq->kn);
+		sysfs_remove_bin_file(&dev->kobj, &mq->bin);
+		return ret;
+	}
+
+	return 0;
+}
+
+int i3c_slave_mqueue_remove(struct i3c_master_controller *master)
+{
+	struct device *dev = &master->dev;
+	struct mq_queue *mq = dev_get_drvdata(dev);
+
+	i3c_master_unregister_slave(master);
+
+	kernfs_put(mq->kn);
+	sysfs_remove_bin_file(&dev->kobj, &mq->bin);
+
+	return 0;
+}
diff --git a/drivers/i3c/i3cdev.c b/drivers/i3c/i3cdev.c
new file mode 100644
index 000000000000..ef08b6056165
--- /dev/null
+++ b/drivers/i3c/i3cdev.c
@@ -0,0 +1,509 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (c) 2020 Synopsys, Inc. and/or its affiliates.
+ *
+ * Author: Vitor Soares <soares@synopsys.com>
+ */
+
+#include <linux/cdev.h>
+#include <linux/compat.h>
+#include <linux/device.h>
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/jiffies.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/notifier.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+
+#include <linux/i3c/i3cdev.h>
+
+#include "internals.h"
+
+struct i3cdev_data {
+	struct i3c_device *i3c;
+	struct device *dev;
+	struct mutex xfer_lock; /* prevent detach while transferring */
+	struct cdev cdev;
+	int id;
+};
+
+static DEFINE_IDA(i3cdev_ida);
+static dev_t i3cdev_number;
+#define I3C_MINORS (MINORMASK + 1)
+
+static struct i3cdev_data *get_free_i3cdev(struct i3c_device *i3c)
+{
+	struct i3cdev_data *i3cdev;
+	int id;
+
+	id = ida_simple_get(&i3cdev_ida, 0, I3C_MINORS, GFP_KERNEL);
+	if (id < 0) {
+		pr_err("i3cdev: no minor number available!\n");
+		return ERR_PTR(id);
+	}
+
+	i3cdev = kzalloc(sizeof(*i3cdev), GFP_KERNEL);
+	if (!i3cdev) {
+		ida_simple_remove(&i3cdev_ida, id);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	i3cdev->i3c = i3c;
+	i3cdev->id = id;
+	i3cdev_set_drvdata(i3c, i3cdev);
+
+	return i3cdev;
+}
+
+static void put_i3cdev(struct i3cdev_data *i3cdev)
+{
+	i3cdev_set_drvdata(i3cdev->i3c, NULL);
+	kfree(i3cdev);
+}
+
+static ssize_t
+i3cdev_read(struct file *file, char __user *buf, size_t count, loff_t *f_pos)
+{
+	struct i3cdev_data *i3cdev = file->private_data;
+	struct i3c_device *i3c = i3cdev->i3c;
+	struct i3c_priv_xfer xfers = {
+		.rnw = true,
+		.len = count,
+	};
+	int ret = -EACCES;
+	char *tmp;
+
+	mutex_lock(&i3cdev->xfer_lock);
+	if (!IS_ENABLED(CONFIG_I3CDEV_FORCE_CREATE) && i3c->dev.driver)
+		goto err_out;
+
+	tmp = kzalloc(count, GFP_KERNEL);
+	if (!tmp)
+		return -ENOMEM;
+
+	xfers.data.in = tmp;
+
+	dev_dbg(&i3c->dev, "Reading %zu bytes.\n", count);
+
+	ret = i3c_device_do_priv_xfers(i3c, &xfers, 1);
+	if (!ret)
+		ret = copy_to_user(buf, tmp, xfers.len) ? -EFAULT : xfers.len;
+
+	kfree(tmp);
+
+err_out:
+	mutex_unlock(&i3cdev->xfer_lock);
+	return ret;
+}
+
+static ssize_t
+i3cdev_write(struct file *file, const char __user *buf, size_t count,
+	     loff_t *f_pos)
+{
+	struct i3cdev_data *i3cdev = file->private_data;
+	struct i3c_device *i3c = i3cdev->i3c;
+	struct i3c_priv_xfer xfers = {
+		.rnw = false,
+		.len = count,
+	};
+	int ret = -EACCES;
+	char *tmp;
+
+	mutex_lock(&i3cdev->xfer_lock);
+	if (!IS_ENABLED(CONFIG_I3CDEV_FORCE_CREATE) && i3c->dev.driver)
+		goto err_out;
+
+	tmp = memdup_user(buf, count);
+	if (IS_ERR(tmp))
+		return PTR_ERR(tmp);
+
+	xfers.data.out = tmp;
+
+	dev_dbg(&i3c->dev, "Writing %zu bytes.\n", count);
+
+	ret = i3c_device_do_priv_xfers(i3c, &xfers, 1);
+	kfree(tmp);
+
+err_out:
+	mutex_unlock(&i3cdev->xfer_lock);
+	return (!ret) ? count : ret;
+}
+
+static int
+i3cdev_do_priv_xfer(struct i3c_device *dev, struct i3c_ioc_priv_xfer *xfers,
+		    unsigned int nxfers)
+{
+	struct i3c_priv_xfer *k_xfers;
+	u8 **data_ptrs;
+	int i, ret = 0;
+
+	/* Since we have nxfers we may allocate k_xfer + *data_ptrs together */
+	k_xfers = kcalloc(nxfers, sizeof(*k_xfers) + sizeof(*data_ptrs),
+			  GFP_KERNEL);
+	if (!k_xfers)
+		return -ENOMEM;
+
+	/* set data_ptrs to be after nxfers * i3c_priv_xfer */
+	data_ptrs = (void *)k_xfers + (nxfers * sizeof(*k_xfers));
+
+	for (i = 0; i < nxfers; i++) {
+		data_ptrs[i] = memdup_user((const u8 __user *)
+					   (uintptr_t)xfers[i].data,
+					   xfers[i].len);
+		if (IS_ERR(data_ptrs[i])) {
+			ret = PTR_ERR(data_ptrs[i]);
+			break;
+		}
+
+		k_xfers[i].len = xfers[i].len;
+		if (xfers[i].rnw) {
+			k_xfers[i].rnw = true;
+			k_xfers[i].data.in = data_ptrs[i];
+		} else {
+			k_xfers[i].rnw = false;
+			k_xfers[i].data.out = data_ptrs[i];
+		}
+	}
+
+	if (ret < 0) {
+		i--;
+		goto err_free_mem;
+	}
+
+	ret = i3c_device_do_priv_xfers(dev, k_xfers, nxfers);
+	if (ret)
+		goto err_free_mem;
+
+	for (i = 0; i < nxfers; i++) {
+		if (xfers[i].rnw) {
+			if (copy_to_user(u64_to_user_ptr(xfers[i].data),
+					 data_ptrs[i], xfers[i].len))
+				ret = -EFAULT;
+		}
+	}
+
+err_free_mem:
+	for (; i >= 0; i--)
+		kfree(data_ptrs[i]);
+	kfree(k_xfers);
+	return ret;
+}
+
+static int
+i3cdev_send_hdr_xfer(struct i3c_device *dev, struct i3c_ioc_priv_xfer *xfers,
+		    unsigned int nxfers)
+{
+	struct i3c_hdr_cmd *k_xfers;
+	u8 **data_ptrs;
+	u16 xfer_len;
+	int i, ret = 0;
+
+	/* Since we have nxfers we may allocate k_xfer + *data_ptrs together */
+	k_xfers = kcalloc(nxfers, sizeof(*k_xfers) + sizeof(*data_ptrs),
+			  GFP_KERNEL);
+	if (!k_xfers)
+		return -ENOMEM;
+
+	/* set data_ptrs to be after nxfers * i3c_priv_xfer */
+	data_ptrs = (void *)k_xfers + (nxfers * sizeof(*k_xfers));
+
+	for (i = 0; i < nxfers; i++) {
+		xfer_len = roundup(xfers[i].len, 2);
+		data_ptrs[i] = kzalloc(xfer_len, GFP_KERNEL);
+		if (!data_ptrs[i])
+			return -ENOMEM;
+		if (copy_from_user(data_ptrs[i],
+				   (const u8 __user *)(uintptr_t)xfers[i].data,
+				   xfers[i].len)) {
+			kfree(data_ptrs[i]);
+			return -EFAULT;
+		}
+		if (IS_ERR(data_ptrs[i])) {
+			ret = PTR_ERR(data_ptrs[i]);
+			break;
+		}
+		k_xfers[i].mode = I3C_HDR_DDR;
+		k_xfers[i].ndatawords = DIV_ROUND_UP(xfers[i].len, 2);
+		if (xfers[i].rnw) {
+			k_xfers[i].code = 0x80;
+			k_xfers[i].data.in = data_ptrs[i];
+		} else {
+			k_xfers[i].code = 0;
+			k_xfers[i].data.out = data_ptrs[i];
+		}
+	}
+
+	if (ret < 0) {
+		i--;
+		goto err_free_mem;
+	}
+
+	ret = i3c_device_send_hdr_cmds(dev, k_xfers, nxfers);
+	if (ret)
+		goto err_free_mem;
+
+	for (i = 0; i < nxfers; i++) {
+		if (xfers[i].rnw) {
+			if (copy_to_user(u64_to_user_ptr(xfers[i].data),
+					 data_ptrs[i], xfers[i].len))
+				ret = -EFAULT;
+		}
+	}
+
+err_free_mem:
+	for (; i >= 0; i--)
+		kfree(data_ptrs[i]);
+	kfree(k_xfers);
+	return ret;
+}
+
+static struct i3c_ioc_priv_xfer *
+i3cdev_get_ioc_priv_xfer(unsigned int cmd, struct i3c_ioc_priv_xfer *u_xfers,
+			 unsigned int *nxfers)
+{
+	u32 tmp = _IOC_SIZE(cmd);
+
+	if ((tmp % sizeof(struct i3c_ioc_priv_xfer)) != 0)
+		return ERR_PTR(-EINVAL);
+
+	*nxfers = tmp / sizeof(struct i3c_ioc_priv_xfer);
+	if (*nxfers == 0)
+		return ERR_PTR(-EINVAL);
+
+	return memdup_user(u_xfers, tmp);
+}
+
+static int
+i3cdev_ioc_priv_xfer(struct i3c_device *i3c, unsigned int cmd,
+		     struct i3c_ioc_priv_xfer *u_xfers)
+{
+	struct i3c_ioc_priv_xfer *k_xfers;
+	unsigned int nxfers;
+	int ret;
+
+	k_xfers = i3cdev_get_ioc_priv_xfer(cmd, u_xfers, &nxfers);
+	if (IS_ERR(k_xfers))
+		return PTR_ERR(k_xfers);
+
+	if (i3c->desc->info.hdr_cap & BIT(I3C_HDR_DDR) &&
+	    IS_ENABLED(CONFIG_I3CDEV_XFER_HDR_DDR))
+		ret = i3cdev_send_hdr_xfer(i3c, k_xfers, nxfers);
+	else
+		ret = i3cdev_do_priv_xfer(i3c, k_xfers, nxfers);
+
+	kfree(k_xfers);
+
+	return ret;
+}
+
+static long
+i3cdev_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	struct i3cdev_data *i3cdev = file->private_data;
+	struct i3c_device *i3c = i3cdev->i3c;
+	int ret = -EACCES;
+
+	dev_dbg(&i3c->dev, "ioctl, cmd=0x%02x, arg=0x%02lx\n", cmd, arg);
+
+	if (_IOC_TYPE(cmd) != I3C_DEV_IOC_MAGIC)
+		return -ENOTTY;
+
+	/* Use the xfer_lock to prevent device detach during ioctl call */
+	mutex_lock(&i3cdev->xfer_lock);
+	if (!IS_ENABLED(CONFIG_I3CDEV_FORCE_CREATE) && i3c->dev.driver)
+		goto err_no_dev;
+
+	/* Check command number and direction */
+	if (_IOC_NR(cmd) == _IOC_NR(I3C_IOC_PRIV_XFER(0)) &&
+	    _IOC_DIR(cmd) == (_IOC_READ | _IOC_WRITE))
+		ret = i3cdev_ioc_priv_xfer(i3c, cmd,
+					(struct i3c_ioc_priv_xfer __user *)arg);
+
+err_no_dev:
+	mutex_unlock(&i3cdev->xfer_lock);
+	return ret;
+}
+
+static int i3cdev_open(struct inode *inode, struct file *file)
+{
+	struct i3cdev_data *i3cdev = container_of(inode->i_cdev,
+						  struct i3cdev_data,
+						  cdev);
+	file->private_data = i3cdev;
+
+	return 0;
+}
+
+static int i3cdev_release(struct inode *inode, struct file *file)
+{
+	file->private_data = NULL;
+
+	return 0;
+}
+
+static const struct file_operations i3cdev_fops = {
+	.owner		= THIS_MODULE,
+	.read		= i3cdev_read,
+	.write		= i3cdev_write,
+	.unlocked_ioctl	= i3cdev_ioctl,
+	.compat_ioctl	= compat_ptr_ioctl,
+	.open		= i3cdev_open,
+	.release	= i3cdev_release,
+};
+
+/* ------------------------------------------------------------------------- */
+
+static struct class *i3cdev_class;
+
+static int i3cdev_attach(struct device *dev, void *dummy)
+{
+	struct i3cdev_data *i3cdev;
+	struct i3c_device *i3c;
+	int res;
+
+	if (dev->type == &i3c_masterdev_type)
+		return 0;
+
+	if (!IS_ENABLED(CONFIG_I3CDEV_FORCE_CREATE) && dev->driver)
+		return 0;
+
+	i3c = dev_to_i3cdev(dev);
+
+	/* Get a device */
+	i3cdev = get_free_i3cdev(i3c);
+	if (IS_ERR(i3cdev))
+		return PTR_ERR(i3cdev);
+
+	mutex_init(&i3cdev->xfer_lock);
+	cdev_init(&i3cdev->cdev, &i3cdev_fops);
+	i3cdev->cdev.owner = THIS_MODULE;
+	res = cdev_add(&i3cdev->cdev,
+		       MKDEV(MAJOR(i3cdev_number), i3cdev->id), 1);
+	if (res)
+		goto error_cdev;
+
+	/* register this i3c device with the driver core */
+	i3cdev->dev = device_create(i3cdev_class, &i3c->dev,
+				    MKDEV(MAJOR(i3cdev_number), i3cdev->id),
+				    NULL, "bus!i3c!%s", dev_name(&i3c->dev));
+	if (IS_ERR(i3cdev->dev)) {
+		res = PTR_ERR(i3cdev->dev);
+		goto error;
+	}
+	pr_debug("i3cdev: I3C device [%s] registered as minor %d\n",
+		 dev_name(&i3c->dev), i3cdev->id);
+	return 0;
+
+error:
+	cdev_del(&i3cdev->cdev);
+error_cdev:
+	put_i3cdev(i3cdev);
+	return res;
+}
+
+static int i3cdev_detach(struct device *dev, void *dummy)
+{
+	struct i3cdev_data *i3cdev;
+	struct i3c_device *i3c;
+
+	if (dev->type == &i3c_masterdev_type)
+		return 0;
+
+	i3c = dev_to_i3cdev(dev);
+
+	i3cdev = i3cdev_get_drvdata(i3c);
+	if (!i3cdev)
+		return 0;
+
+	/* Prevent transfers while cdev removal */
+	mutex_lock(&i3cdev->xfer_lock);
+	cdev_del(&i3cdev->cdev);
+	device_destroy(i3cdev_class, MKDEV(MAJOR(i3cdev_number), i3cdev->id));
+	mutex_unlock(&i3cdev->xfer_lock);
+
+	ida_simple_remove(&i3cdev_ida, i3cdev->id);
+	put_i3cdev(i3cdev);
+
+	pr_debug("i3cdev: device [%s] unregistered\n", dev_name(&i3c->dev));
+
+	return 0;
+}
+
+static int i3cdev_notifier_call(struct notifier_block *nb,
+				unsigned long action,
+				void *data)
+{
+	struct device *dev = data;
+
+	switch (action) {
+	case BUS_NOTIFY_ADD_DEVICE:
+	case BUS_NOTIFY_UNBOUND_DRIVER:
+		return i3cdev_attach(dev, NULL);
+	case BUS_NOTIFY_BIND_DRIVER:
+		if (IS_ENABLED(CONFIG_I3CDEV_FORCE_CREATE))
+			break;
+
+		fallthrough;
+	case BUS_NOTIFY_DEL_DEVICE:
+	case BUS_NOTIFY_REMOVED_DEVICE:
+		return i3cdev_detach(dev, NULL);
+	}
+
+	return 0;
+}
+
+static struct notifier_block i3cdev_notifier = {
+	.notifier_call = i3cdev_notifier_call,
+};
+
+static int __init i3cdev_init(void)
+{
+	int res;
+
+	/* Dynamically request unused major number */
+	res = alloc_chrdev_region(&i3cdev_number, 0, I3C_MINORS, "i3c");
+	if (res)
+		goto out;
+
+	/* Create a classe to populate sysfs entries*/
+	i3cdev_class = class_create(THIS_MODULE, "i3cdev");
+	if (IS_ERR(i3cdev_class)) {
+		res = PTR_ERR(i3cdev_class);
+		goto out_unreg_chrdev;
+	}
+
+	/* Keep track of busses which have devices to add or remove later */
+	res = bus_register_notifier(&i3c_bus_type, &i3cdev_notifier);
+	if (res)
+		goto out_unreg_class;
+
+	/* Bind to already existing device without driver right away */
+	i3c_for_each_dev(NULL, i3cdev_attach);
+
+	return 0;
+
+out_unreg_class:
+	class_destroy(i3cdev_class);
+out_unreg_chrdev:
+	unregister_chrdev_region(i3cdev_number, I3C_MINORS);
+out:
+	pr_err("%s: Driver Initialisation failed\n", __FILE__);
+	return res;
+}
+
+static void __exit i3cdev_exit(void)
+{
+	bus_unregister_notifier(&i3c_bus_type, &i3cdev_notifier);
+	i3c_for_each_dev(NULL, i3cdev_detach);
+	class_destroy(i3cdev_class);
+	unregister_chrdev_region(i3cdev_number, I3C_MINORS);
+}
+
+MODULE_AUTHOR("Vitor Soares <soares@synopsys.com>");
+MODULE_DESCRIPTION("I3C /dev entries driver");
+MODULE_LICENSE("GPL");
+
+module_init(i3cdev_init);
+module_exit(i3cdev_exit);
diff --git a/drivers/i3c/internals.h b/drivers/i3c/internals.h
index 86b7b44cfca2..a9c6979b31ca 100644
--- a/drivers/i3c/internals.h
+++ b/drivers/i3c/internals.h
@@ -9,8 +9,10 @@
 #define I3C_INTERNALS_H
 
 #include <linux/i3c/master.h>
+#include <linux/i3c/target.h>
 
 extern struct bus_type i3c_bus_type;
+extern const struct device_type i3c_masterdev_type;
 
 void i3c_bus_normaluse_lock(struct i3c_bus *bus);
 void i3c_bus_normaluse_unlock(struct i3c_bus *bus);
@@ -18,9 +20,22 @@ void i3c_bus_normaluse_unlock(struct i3c_bus *bus);
 int i3c_dev_do_priv_xfers_locked(struct i3c_dev_desc *dev,
 				 struct i3c_priv_xfer *xfers,
 				 int nxfers);
+int i3c_master_send_hdr_cmds_locked(struct i3c_master_controller *master,
+				    struct i3c_hdr_cmd *cmds, int ncmds);
 int i3c_dev_disable_ibi_locked(struct i3c_dev_desc *dev);
 int i3c_dev_enable_ibi_locked(struct i3c_dev_desc *dev);
 int i3c_dev_request_ibi_locked(struct i3c_dev_desc *dev,
 			       const struct i3c_ibi_setup *req);
 void i3c_dev_free_ibi_locked(struct i3c_dev_desc *dev);
+int i3c_dev_send_ccc_cmd_locked(struct i3c_dev_desc *dev, u8 ccc_id);
+int i3c_dev_getstatus_locked(struct i3c_dev_desc *dev, struct i3c_device_info *info);
+int i3c_master_getmrl_locked(struct i3c_master_controller *master, struct i3c_device_info *info);
+int i3c_master_getmwl_locked(struct i3c_master_controller *master, struct i3c_device_info *info);
+int i3c_master_setmrl_locked(struct i3c_master_controller *master,
+			     struct i3c_device_info *info, u16 read_len, u8 ibi_len);
+int i3c_master_setmwl_locked(struct i3c_master_controller *master,
+			     struct i3c_device_info *info, u16 write_len);
+int i3c_for_each_dev(void *data, int (*fn)(struct device *, void *));
+int i3c_dev_generate_ibi_locked(struct i3c_dev_desc *dev, const u8 *data, int len);
+int i3c_dev_control_pec(struct i3c_dev_desc *dev, bool pec);
 #endif /* I3C_INTERNAL_H */
diff --git a/drivers/i3c/master.c b/drivers/i3c/master.c
index 351c81a929a6..f45569816493 100644
--- a/drivers/i3c/master.c
+++ b/drivers/i3c/master.c
@@ -262,6 +262,24 @@ static ssize_t modalias_show(struct device *dev,
 }
 static DEVICE_ATTR_RO(modalias);
 
+static ssize_t bus_reset_store(struct device *dev, struct device_attribute *da,
+			       const char *buf, size_t count)
+{
+	struct i3c_master_controller *master;
+	ssize_t ret = count;
+
+	master = dev_to_i3cmaster(dev);
+	dev_dbg(&master->dev, "Reset bus to return to i2c_mode...\n");
+	i3c_bus_maintenance_lock(&master->bus);
+	if (master->ops->bus_reset)
+		master->ops->bus_reset(master);
+
+	i3c_bus_maintenance_unlock(&master->bus);
+
+	return ret;
+}
+static DEVICE_ATTR_WO(bus_reset);
+
 static struct attribute *i3c_device_attrs[] = {
 	&dev_attr_bcr.attr,
 	&dev_attr_dcr.attr,
@@ -298,19 +316,24 @@ static const struct device_type i3c_device_type = {
 	.uevent = i3c_device_uevent,
 };
 
+const struct device_type i3c_target_device_type = {
+};
+
 static int i3c_device_match(struct device *dev, struct device_driver *drv)
 {
 	struct i3c_device *i3cdev;
 	struct i3c_driver *i3cdrv;
 
-	if (dev->type != &i3c_device_type)
+	if (dev->type != &i3c_device_type && dev->type != &i3c_target_device_type)
 		return 0;
 
 	i3cdev = dev_to_i3cdev(dev);
 	i3cdrv = drv_to_i3cdrv(drv);
-	if (i3c_device_match_id(i3cdev, i3cdrv->id_table))
-		return 1;
 
+	if ((dev->type == &i3c_device_type && !i3cdrv->target) ||
+	    (dev->type == &i3c_target_device_type && i3cdrv->target))
+		if (i3c_device_match_id(i3cdev, i3cdrv->id_table))
+			return 1;
 	return 0;
 }
 
@@ -330,7 +353,8 @@ static void i3c_device_remove(struct device *dev)
 	if (driver->remove)
 		driver->remove(i3cdev);
 
-	i3c_device_free_ibi(i3cdev);
+	if (!driver->target)
+		i3c_device_free_ibi(i3cdev);
 }
 
 struct bus_type i3c_bus_type = {
@@ -339,6 +363,7 @@ struct bus_type i3c_bus_type = {
 	.probe = i3c_device_probe,
 	.remove = i3c_device_remove,
 };
+EXPORT_SYMBOL_GPL(i3c_bus_type);
 
 static enum i3c_addr_slot_status
 i3c_bus_get_addr_slot_status(struct i3c_bus *bus, u16 addr)
@@ -524,6 +549,7 @@ static struct attribute *i3c_masterdev_attrs[] = {
 	&dev_attr_pid.attr,
 	&dev_attr_dynamic_address.attr,
 	&dev_attr_hdrcap.attr,
+	&dev_attr_bus_reset.attr,
 	NULL,
 };
 ATTRIBUTE_GROUPS(i3c_masterdev);
@@ -542,9 +568,10 @@ static void i3c_masterdev_release(struct device *dev)
 	of_node_put(dev->of_node);
 }
 
-static const struct device_type i3c_masterdev_type = {
+const struct device_type i3c_masterdev_type = {
 	.groups	= i3c_masterdev_groups,
 };
+EXPORT_SYMBOL_GPL(i3c_masterdev_type);
 
 static int i3c_bus_set_mode(struct i3c_bus *i3cbus, enum i3c_bus_mode mode,
 			    unsigned long max_i2c_scl_rate)
@@ -644,13 +671,15 @@ static void i3c_ccc_cmd_dest_cleanup(struct i3c_ccc_cmd_dest *dest)
 
 static void i3c_ccc_cmd_init(struct i3c_ccc_cmd *cmd, bool rnw, u8 id,
 			     struct i3c_ccc_cmd_dest *dests,
-			     unsigned int ndests)
+			     unsigned int ndests, bool dbp, u8 db)
 {
 	cmd->rnw = rnw ? 1 : 0;
 	cmd->id = id;
 	cmd->dests = dests;
 	cmd->ndests = ndests;
 	cmd->err = I3C_ERROR_UNKNOWN;
+	cmd->dbp = dbp;
+	cmd->db = db;
 }
 
 static int i3c_master_send_ccc_cmd_locked(struct i3c_master_controller *master,
@@ -749,7 +778,7 @@ i3c_master_alloc_i3c_dev(struct i3c_master_controller *master,
 	return dev;
 }
 
-static int i3c_master_rstdaa_locked(struct i3c_master_controller *master,
+int i3c_master_rstdaa_locked(struct i3c_master_controller *master,
 				    u8 addr)
 {
 	enum i3c_addr_slot_status addrstat;
@@ -767,12 +796,13 @@ static int i3c_master_rstdaa_locked(struct i3c_master_controller *master,
 	i3c_ccc_cmd_dest_init(&dest, addr, 0);
 	i3c_ccc_cmd_init(&cmd, false,
 			 I3C_CCC_RSTDAA(addr == I3C_BROADCAST_ADDR),
-			 &dest, 1);
+			 &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	i3c_ccc_cmd_dest_cleanup(&dest);
 
 	return ret;
 }
+EXPORT_SYMBOL_GPL(i3c_master_rstdaa_locked);
 
 /**
  * i3c_master_entdaa_locked() - start a DAA (Dynamic Address Assignment)
@@ -797,7 +827,7 @@ int i3c_master_entdaa_locked(struct i3c_master_controller *master)
 	int ret;
 
 	i3c_ccc_cmd_dest_init(&dest, I3C_BROADCAST_ADDR, 0);
-	i3c_ccc_cmd_init(&cmd, false, I3C_CCC_ENTDAA, &dest, 1);
+	i3c_ccc_cmd_init(&cmd, false, I3C_CCC_ENTDAA, &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	i3c_ccc_cmd_dest_cleanup(&dest);
 
@@ -822,7 +852,7 @@ static int i3c_master_enec_disec_locked(struct i3c_master_controller *master,
 			 enable ?
 			 I3C_CCC_ENEC(addr == I3C_BROADCAST_ADDR) :
 			 I3C_CCC_DISEC(addr == I3C_BROADCAST_ADDR),
-			 &dest, 1);
+			 &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	i3c_ccc_cmd_dest_cleanup(&dest);
 
@@ -955,7 +985,7 @@ int i3c_master_defslvs_locked(struct i3c_master_controller *master)
 		desc++;
 	}
 
-	i3c_ccc_cmd_init(&cmd, false, I3C_CCC_DEFSLVS, &dest, 1);
+	i3c_ccc_cmd_init(&cmd, false, I3C_CCC_DEFSLVS, &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	i3c_ccc_cmd_dest_cleanup(&dest);
 
@@ -981,7 +1011,22 @@ static int i3c_master_setda_locked(struct i3c_master_controller *master,
 	setda->addr = newaddr << 1;
 	i3c_ccc_cmd_init(&cmd, false,
 			 setdasa ? I3C_CCC_SETDASA : I3C_CCC_SETNEWDA,
-			 &dest, 1);
+			 &dest, 1, false, 0);
+	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
+	i3c_ccc_cmd_dest_cleanup(&dest);
+
+	return ret;
+}
+
+static int i3c_master_setaasa_locked(struct i3c_master_controller *master)
+{
+	struct i3c_ccc_cmd_dest dest;
+	struct i3c_ccc_cmd cmd;
+	int ret;
+
+	i3c_ccc_cmd_dest_init(&dest, I3C_BROADCAST_ADDR, 0);
+	i3c_ccc_cmd_init(&cmd, false, I3C_CCC_SETAASA, &dest, 1, false, 0);
+
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	i3c_ccc_cmd_dest_cleanup(&dest);
 
@@ -1000,8 +1045,27 @@ static int i3c_master_setnewda_locked(struct i3c_master_controller *master,
 	return i3c_master_setda_locked(master, oldaddr, newaddr, false);
 }
 
-static int i3c_master_getmrl_locked(struct i3c_master_controller *master,
-				    struct i3c_device_info *info)
+static int i3c_master_sethid_locked(struct i3c_master_controller *master)
+{
+	struct i3c_ccc_cmd_dest dest;
+	struct i3c_ccc_cmd cmd;
+	struct i3c_ccc_sethid *sethid;
+	int ret;
+
+	sethid = i3c_ccc_cmd_dest_init(&dest, I3C_BROADCAST_ADDR, 1);
+	if (!sethid)
+		return -ENOMEM;
+
+	sethid->hid = 0;
+	i3c_ccc_cmd_init(&cmd, false, I3C_CCC_SETHID, &dest, 1, false, 0);
+
+	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
+	i3c_ccc_cmd_dest_cleanup(&dest);
+
+	return ret;
+}
+
+int i3c_master_getmrl_locked(struct i3c_master_controller *master, struct i3c_device_info *info)
 {
 	struct i3c_ccc_cmd_dest dest;
 	struct i3c_ccc_mrl *mrl;
@@ -1019,7 +1083,7 @@ static int i3c_master_getmrl_locked(struct i3c_master_controller *master,
 	if (!(info->bcr & I3C_BCR_IBI_PAYLOAD))
 		dest.payload.len -= 1;
 
-	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETMRL, &dest, 1);
+	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETMRL, &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	if (ret)
 		goto out;
@@ -1042,8 +1106,7 @@ static int i3c_master_getmrl_locked(struct i3c_master_controller *master,
 	return ret;
 }
 
-static int i3c_master_getmwl_locked(struct i3c_master_controller *master,
-				    struct i3c_device_info *info)
+int i3c_master_getmwl_locked(struct i3c_master_controller *master, struct i3c_device_info *info)
 {
 	struct i3c_ccc_cmd_dest dest;
 	struct i3c_ccc_mwl *mwl;
@@ -1054,7 +1117,7 @@ static int i3c_master_getmwl_locked(struct i3c_master_controller *master,
 	if (!mwl)
 		return -ENOMEM;
 
-	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETMWL, &dest, 1);
+	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETMWL, &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	if (ret)
 		goto out;
@@ -1072,6 +1135,61 @@ static int i3c_master_getmwl_locked(struct i3c_master_controller *master,
 	return ret;
 }
 
+int i3c_master_setmrl_locked(struct i3c_master_controller *master,
+			     struct i3c_device_info *info, u16 read_len, u8 ibi_len)
+{
+	struct i3c_ccc_cmd_dest dest;
+	struct i3c_ccc_cmd cmd;
+	struct i3c_ccc_mrl *mrl;
+	int ret;
+
+	mrl = i3c_ccc_cmd_dest_init(&dest, info->dyn_addr, sizeof(*mrl));
+	if (!mrl)
+		return -ENOMEM;
+
+	/*
+	 * When the device does not have IBI payload SETMRL only sends 2
+	 * bytes of data.
+	 */
+	if (!(info->bcr & I3C_BCR_IBI_PAYLOAD))
+		dest.payload.len -= 1;
+
+	mrl->read_len = cpu_to_be16(read_len);
+	mrl->ibi_len = ibi_len;
+	info->max_read_len = read_len;
+	info->max_ibi_len = mrl->ibi_len;
+	i3c_ccc_cmd_init(&cmd, false, I3C_CCC_SETMRL(false), &dest, 1, false,
+			 0);
+
+	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
+	i3c_ccc_cmd_dest_cleanup(&dest);
+
+	return ret;
+}
+
+int i3c_master_setmwl_locked(struct i3c_master_controller *master,
+			     struct i3c_device_info *info, u16 write_len)
+{
+	struct i3c_ccc_cmd_dest dest;
+	struct i3c_ccc_cmd cmd;
+	struct i3c_ccc_mwl *mwl;
+	int ret;
+
+	mwl = i3c_ccc_cmd_dest_init(&dest, info->dyn_addr, sizeof(*mwl));
+	if (!mwl)
+		return -ENOMEM;
+
+	mwl->len = cpu_to_be16(write_len);
+	info->max_write_len = write_len;
+	i3c_ccc_cmd_init(&cmd, false, I3C_CCC_SETMWL(false), &dest, 1, false,
+			 0);
+
+	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
+	i3c_ccc_cmd_dest_cleanup(&dest);
+
+	return ret;
+}
+
 static int i3c_master_getmxds_locked(struct i3c_master_controller *master,
 				     struct i3c_device_info *info)
 {
@@ -1085,7 +1203,7 @@ static int i3c_master_getmxds_locked(struct i3c_master_controller *master,
 	if (!getmaxds)
 		return -ENOMEM;
 
-	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETMXDS, &dest, 1);
+	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETMXDS, &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	if (ret)
 		goto out;
@@ -1121,7 +1239,7 @@ static int i3c_master_gethdrcap_locked(struct i3c_master_controller *master,
 	if (!gethdrcap)
 		return -ENOMEM;
 
-	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETHDRCAP, &dest, 1);
+	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETHDRCAP, &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	if (ret)
 		goto out;
@@ -1151,7 +1269,7 @@ static int i3c_master_getpid_locked(struct i3c_master_controller *master,
 	if (!getpid)
 		return -ENOMEM;
 
-	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETPID, &dest, 1);
+	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETPID, &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	if (ret)
 		goto out;
@@ -1181,7 +1299,7 @@ static int i3c_master_getbcr_locked(struct i3c_master_controller *master,
 	if (!getbcr)
 		return -ENOMEM;
 
-	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETBCR, &dest, 1);
+	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETBCR, &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	if (ret)
 		goto out;
@@ -1206,7 +1324,7 @@ static int i3c_master_getdcr_locked(struct i3c_master_controller *master,
 	if (!getdcr)
 		return -ENOMEM;
 
-	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETDCR, &dest, 1);
+	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETDCR, &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	if (ret)
 		goto out;
@@ -1219,6 +1337,32 @@ static int i3c_master_getdcr_locked(struct i3c_master_controller *master,
 	return ret;
 }
 
+int i3c_dev_getstatus_locked(struct i3c_dev_desc *dev,
+			     struct i3c_device_info *info)
+{
+	struct i3c_master_controller *master = i3c_dev_get_master(dev);
+	struct i3c_ccc_getstatus *getsts;
+	struct i3c_ccc_cmd_dest dest;
+	struct i3c_ccc_cmd cmd;
+	int ret;
+
+	getsts = i3c_ccc_cmd_dest_init(&dest, info->dyn_addr, sizeof(*getsts));
+	if (!getsts)
+		return -ENOMEM;
+
+	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETSTATUS, &dest, 1, false, 0);
+	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
+	if (ret)
+		goto out;
+
+	info->status = getsts->status;
+
+out:
+	i3c_ccc_cmd_dest_cleanup(&dest);
+
+	return ret;
+}
+
 static int i3c_master_retrieve_dev_info(struct i3c_dev_desc *dev)
 {
 	struct i3c_master_controller *master = i3c_dev_get_master(dev);
@@ -1234,6 +1378,13 @@ static int i3c_master_retrieve_dev_info(struct i3c_dev_desc *dev)
 	    slot_status == I3C_ADDR_SLOT_I2C_DEV)
 		return -EINVAL;
 
+	if (master->jdec_spd && dev->boardinfo) {
+		dev->info.pid = dev->boardinfo->pid;
+		dev->info.dcr = dev->boardinfo->dcr;
+		dev->info.bcr = dev->boardinfo->bcr;
+		return 0;
+	}
+
 	ret = i3c_master_getpid_locked(master, &dev->info);
 	if (ret)
 		return ret;
@@ -1452,10 +1603,17 @@ static int i3c_master_early_i3c_dev_add(struct i3c_master_controller *master,
 	if (ret)
 		goto err_free_dev;
 
-	ret = i3c_master_setdasa_locked(master, i3cdev->info.static_addr,
-					i3cdev->boardinfo->init_dyn_addr);
-	if (ret)
-		goto err_detach_dev;
+	/*
+	 * JESD403-1 devices only support SETAASA (will be called in do_daa)
+	 * Here we use SETDASA for non-JESD403-1 devices
+	 */
+	if (!I3C_DCR_IS_JESD403_COMPLIANT(i3cdev->boardinfo->dcr)) {
+		ret = i3c_master_setdasa_locked(
+			master, i3cdev->info.static_addr,
+			i3cdev->boardinfo->init_dyn_addr);
+		if (ret)
+			goto err_detach_dev;
+	}
 
 	i3cdev->info.dyn_addr = i3cdev->boardinfo->init_dyn_addr;
 	ret = i3c_master_reattach_i3c_dev(i3cdev, 0);
@@ -1534,7 +1692,11 @@ int i3c_master_do_daa(struct i3c_master_controller *master)
 	int ret;
 
 	i3c_bus_maintenance_lock(&master->bus);
-	ret = master->ops->do_daa(master);
+	if (master->jdec_spd) {
+		i3c_master_sethid_locked(master);
+		i3c_master_setaasa_locked(master);
+	} else
+		ret = master->ops->do_daa(master);
 	i3c_bus_maintenance_unlock(&master->bus);
 
 	if (ret)
@@ -1548,6 +1710,29 @@ int i3c_master_do_daa(struct i3c_master_controller *master)
 }
 EXPORT_SYMBOL_GPL(i3c_master_do_daa);
 
+/**
+ * i3c_master_enable_hj() - enable hot-join
+ * @master: master broadcast the enec ccc to enable hot-join.
+ *
+ * This function must be called after the master init done to satisfy
+ * the description "Hot-Join does not allow Targets to join the I3C
+ * Bus before the I3C Bus has been configured." in i3c specification.
+ *
+ * Return: a 0 in case of success, an negative error code otherwise.
+ */
+int i3c_master_enable_hj(struct i3c_master_controller *master)
+{
+	if (!master->init_done)
+		return -ENOPROTOOPT;
+
+	i3c_bus_maintenance_lock(&master->bus);
+	i3c_master_enec_locked(master, I3C_BROADCAST_ADDR, I3C_CCC_EVENT_HJ);
+	i3c_bus_maintenance_unlock(&master->bus);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(i3c_master_enable_hj);
+
 /**
  * i3c_master_set_info() - set master device information
  * @master: master used to send frames on the bus
@@ -1581,10 +1766,6 @@ int i3c_master_set_info(struct i3c_master_controller *master,
 	if (!i3c_bus_dev_addr_is_avail(&master->bus, info->dyn_addr))
 		return -EINVAL;
 
-	if (I3C_BCR_DEVICE_ROLE(info->bcr) == I3C_BCR_I3C_MASTER &&
-	    master->secondary)
-		return -EINVAL;
-
 	if (master->this)
 		return -EINVAL;
 
@@ -1593,7 +1774,10 @@ int i3c_master_set_info(struct i3c_master_controller *master,
 		return PTR_ERR(i3cdev);
 
 	master->this = i3cdev;
-	master->bus.cur_master = master->this;
+	if (master->secondary)
+		master->bus.cur_master = NULL;
+	else
+		master->bus.cur_master = master->this;
 
 	ret = i3c_master_attach_i3c_dev(master, i3cdev);
 	if (ret)
@@ -1725,6 +1909,9 @@ static int i3c_master_bus_init(struct i3c_master_controller *master)
 		goto err_bus_cleanup;
 	}
 
+	if (master->secondary)
+		return 0;
+
 	/*
 	 * Reset all dynamic address that may have been assigned before
 	 * (assigned by the bootloader for example).
@@ -1763,9 +1950,15 @@ static int i3c_master_bus_init(struct i3c_master_controller *master)
 			goto err_rstdaa;
 		}
 
-		i3c_bus_set_addr_slot_status(&master->bus,
-					     i3cboardinfo->init_dyn_addr,
-					     I3C_ADDR_SLOT_I3C_DEV);
+		/*
+		 * If the static address equals to the assigned dynamic address,
+		 * don't reserve the address slot here, it will be set after the
+		 * DA has been assigned.
+		 */
+		if (i3cboardinfo->static_addr != i3cboardinfo->init_dyn_addr)
+			i3c_bus_set_addr_slot_status(
+				&master->bus, i3cboardinfo->init_dyn_addr,
+				I3C_ADDR_SLOT_I3C_DEV);
 
 		/*
 		 * Only try to create/attach devices that have a static
@@ -1779,9 +1972,18 @@ static int i3c_master_bus_init(struct i3c_master_controller *master)
 			i3c_master_early_i3c_dev_add(master, i3cboardinfo);
 	}
 
+	/*
+	 * Not support mix mode on JEDEC bus context. Here We only handle
+	 * the I2C part so simply return with success code.
+	 */
+	if (master->jdec_spd && master->bus.mode != I3C_BUS_MODE_PURE)
+		return 0;
+
 	ret = i3c_master_do_daa(master);
 	if (ret)
-		goto err_rstdaa;
+		dev_dbg(&master->dev,
+			"Failed to do DAA: %d. However, devices with static address can still be accessed\n",
+			ret);
 
 	return 0;
 
@@ -1904,6 +2106,16 @@ int i3c_master_add_i3c_dev_locked(struct i3c_master_controller *master,
 			i3c_dev_free_ibi_locked(olddev);
 		}
 		mutex_unlock(&olddev->ibi_lock);
+		if (olddev->info.max_ibi_len != newdev->info.max_ibi_len ||
+		    olddev->info.max_read_len != newdev->info.max_read_len)
+			i3c_master_setmrl_locked(master, &newdev->info,
+					      olddev->info.max_read_len,
+					      olddev->info.max_ibi_len);
+		if (olddev->info.max_write_len != newdev->info.max_write_len)
+			i3c_master_setmwl_locked(master, &newdev->info,
+					      olddev->info.max_write_len);
+		if (olddev->info.pec != newdev->info.pec)
+			i3c_device_control_pec(newdev->dev, olddev->info.pec);
 
 		old_dyn_addr = olddev->info.dyn_addr;
 
@@ -2031,6 +2243,8 @@ of_i3c_master_add_i3c_boardinfo(struct i3c_master_controller *master,
 	struct device *dev = &master->dev;
 	enum i3c_addr_slot_status addrstatus;
 	u32 init_dyn_addr = 0;
+	u32 bcr = 0;
+	u32 dcr = 0;
 
 	boardinfo = devm_kzalloc(dev, sizeof(*boardinfo), GFP_KERNEL);
 	if (!boardinfo)
@@ -2064,6 +2278,16 @@ of_i3c_master_add_i3c_boardinfo(struct i3c_master_controller *master,
 	    I3C_PID_RND_LOWER_32BITS(boardinfo->pid))
 		return -EINVAL;
 
+	if (!of_property_read_u32(node, "dcr", &dcr)) {
+		if (dcr > I3C_DCR_MAX)
+			return -EINVAL;
+
+		boardinfo->dcr = dcr;
+	}
+
+	if (!of_property_read_u32(node, "bcr", &bcr))
+		boardinfo->bcr = bcr;
+
 	boardinfo->init_dyn_addr = init_dyn_addr;
 	boardinfo->of_node = of_node_get(node);
 	list_add_tail(&boardinfo->node, &master->boardinfo.i3c);
@@ -2101,17 +2325,30 @@ static int of_populate_i3c_bus(struct i3c_master_controller *master)
 	struct device *dev = &master->dev;
 	struct device_node *i3cbus_np = dev->of_node;
 	struct device_node *node;
-	int ret;
+	int ret, i;
 	u32 val;
 
 	if (!i3cbus_np)
 		return 0;
 
+	if (of_get_property(i3cbus_np, "jdec-spd", NULL)) {
+		master->jdec_spd = 1;
+	}
+
+	/* For SPD bus, undo unnecessary address reservations. */
+	if (master->jdec_spd) {
+		for (i = 0; i < 7; i++)
+			i3c_bus_set_addr_slot_status(&master->bus, I3C_BROADCAST_ADDR ^ BIT(i),
+						     I3C_ADDR_SLOT_FREE);
+	}
+
 	for_each_available_child_of_node(i3cbus_np, node) {
-		ret = of_i3c_master_add_dev(master, node);
-		if (ret) {
-			of_node_put(node);
-			return ret;
+		if (node->name && of_node_cmp(node->name, "hub")) {
+			ret = of_i3c_master_add_dev(master, node);
+			if (ret) {
+				of_node_put(node);
+				return ret;
+			}
 		}
 	}
 
@@ -2587,9 +2824,6 @@ int i3c_master_register(struct i3c_master_controller *master,
 	struct i2c_dev_boardinfo *i2cbi;
 	int ret;
 
-	/* We do not support secondary masters yet. */
-	if (secondary)
-		return -ENOTSUPP;
 
 	ret = i3c_master_check_ops(ops);
 	if (ret)
@@ -2672,6 +2906,15 @@ int i3c_master_register(struct i3c_master_controller *master,
 	master->init_done = true;
 	i3c_bus_normaluse_lock(&master->bus);
 	i3c_master_register_new_i3c_devs(master);
+#ifdef CONFIG_I3C_SLAVE_MQUEUE
+	if (master->secondary)
+		i3c_slave_mqueue_probe(master);
+#endif
+
+#ifdef CONFIG_I3C_SLAVE_EEPROM
+	if (master->secondary)
+		i3c_slave_eeprom_probe(master);
+#endif
 	i3c_bus_normaluse_unlock(&master->bus);
 
 	return 0;
@@ -2708,6 +2951,255 @@ int i3c_master_unregister(struct i3c_master_controller *master)
 }
 EXPORT_SYMBOL_GPL(i3c_master_unregister);
 
+static int i3c_target_bus_init(struct i3c_master_controller *master)
+{
+	return master->target_ops->bus_init(master);
+}
+
+static void i3c_target_bus_cleanup(struct i3c_master_controller *master)
+{
+	if (master->target_ops->bus_cleanup)
+		master->target_ops->bus_cleanup(master);
+}
+
+static void i3c_targetdev_release(struct device *dev)
+{
+	struct i3c_master_controller *master = container_of(dev, struct i3c_master_controller, dev);
+	struct i3c_bus *bus = &master->bus;
+
+	mutex_lock(&i3c_core_lock);
+	idr_remove(&i3c_bus_idr, bus->id);
+	mutex_unlock(&i3c_core_lock);
+
+	of_node_put(dev->of_node);
+}
+
+static void i3c_target_device_release(struct device *dev)
+{
+	struct i3c_device *i3cdev = dev_to_i3cdev(dev);
+	struct i3c_dev_desc *desc = i3cdev->desc;
+
+	kfree(i3cdev);
+	kfree(desc);
+}
+
+static void
+i3c_target_register_new_i3c_dev(struct i3c_master_controller *master, struct i3c_device_info info)
+{
+	struct i3c_dev_desc *desc;
+	int ret;
+
+	desc = kzalloc(sizeof(*desc), GFP_KERNEL);
+	if (!desc)
+		return;
+
+	desc->dev = kzalloc(sizeof(*desc->dev), GFP_KERNEL);
+	if (!desc->dev) {
+		kfree(desc);
+		return;
+	}
+
+	desc->dev->bus = &master->bus;
+	desc->dev->desc = desc;
+	desc->dev->dev.parent = &master->dev;
+	desc->dev->dev.type = &i3c_target_device_type;
+	desc->dev->dev.bus = &i3c_bus_type;
+	desc->dev->dev.release = i3c_target_device_release;
+	desc->info = info;
+	desc->common.master = master;
+	dev_set_name(&desc->dev->dev, "%d-target", master->bus.id);
+
+	ret = device_register(&desc->dev->dev);
+	if (ret)
+		dev_err(&master->dev, "Failed to add I3C target device (err = %d)\n", ret);
+
+	master->this = desc;
+}
+
+static void i3c_target_unregister_i3c_dev(struct i3c_master_controller *master)
+{
+	struct i3c_dev_desc *i3cdev = master->this;
+
+	if (device_is_registered(&i3cdev->dev->dev))
+		device_unregister(&i3cdev->dev->dev);
+	else
+		put_device(&i3cdev->dev->dev);
+}
+
+static void i3c_target_read_device_info(struct device_node *np, struct i3c_device_info *info)
+{
+	u64 pid;
+	u32 dcr;
+	int ret;
+
+	ret = of_property_read_u64(np, "pid", &pid);
+	if (ret)
+		info->pid = 0;
+	else
+		info->pid = pid;
+
+	ret = of_property_read_u32(np, "dcr", &dcr);
+	if (ret)
+		info->pid = 0;
+	else
+		info->dcr = dcr;
+}
+
+static int i3c_target_check_ops(const struct i3c_target_ops *ops)
+{
+	if (!ops || !ops->bus_init)
+		return -EINVAL;
+
+	return 0;
+}
+
+int i3c_target_register(struct i3c_master_controller *master, struct device *parent,
+			const struct i3c_target_ops *ops)
+{
+	struct i3c_bus *i3cbus = i3c_master_get_bus(master);
+	struct i3c_device_info info;
+	int ret;
+
+	ret = i3c_target_check_ops(ops);
+	if (ret)
+		return ret;
+
+	master->dev.parent = parent;
+	master->dev.of_node = of_node_get(parent->of_node);
+	master->dev.bus = &i3c_bus_type;
+	master->dev.release = i3c_targetdev_release;
+	master->target_ops = ops;
+	i3cbus->mode = I3C_BUS_MODE_PURE;
+
+	init_rwsem(&i3cbus->lock);
+	mutex_lock(&i3c_core_lock);
+	ret = idr_alloc(&i3c_bus_idr, i3cbus, 0, 0, GFP_KERNEL);
+	mutex_unlock(&i3c_core_lock);
+	if (ret < 0)
+		return ret;
+	i3cbus->id = ret;
+
+	device_initialize(&master->dev);
+	dev_set_name(&master->dev, "i3c-%d", i3cbus->id);
+
+	ret = device_add(&master->dev);
+	if (ret)
+		goto err_put_device;
+
+	i3c_target_read_device_info(master->dev.of_node, &info);
+
+	i3c_target_register_new_i3c_dev(master, info);
+
+	ret = i3c_target_bus_init(master);
+	if (ret)
+		goto err_cleanup_bus;
+
+	return 0;
+
+err_cleanup_bus:
+	i3c_target_bus_cleanup(master);
+
+err_put_device:
+	put_device(&master->dev);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_target_register);
+
+int i3c_target_unregister(struct i3c_master_controller *master)
+{
+	i3c_target_unregister_i3c_dev(master);
+	i3c_target_bus_cleanup(master);
+	device_unregister(&master->dev);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(i3c_target_unregister);
+
+int i3c_target_read_register(struct i3c_device *dev, const struct i3c_target_read_setup *setup)
+{
+	dev->desc->target_info.read_handler = setup->handler;
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(i3c_target_read_register);
+
+int i3c_register(struct i3c_master_controller *master,
+		 struct device *parent,
+		 const struct i3c_master_controller_ops *master_ops,
+		 const struct i3c_target_ops *target_ops,
+		 bool secondary)
+{
+	const char *role;
+	int ret;
+
+	ret = of_property_read_string(parent->of_node, "initial-role", &role);
+	if (ret || !strcmp("primary", role)) {
+		return i3c_master_register(master, parent, master_ops, secondary);
+	} else if (!strcmp("target", role)) {
+		master->target = true;
+		return i3c_target_register(master, parent, target_ops);
+	} else {
+		return -EOPNOTSUPP;
+	}
+}
+EXPORT_SYMBOL_GPL(i3c_register);
+
+int i3c_unregister(struct i3c_master_controller *master)
+{
+	if (master->target)
+		i3c_target_unregister(master);
+	else
+		i3c_master_unregister(master);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(i3c_unregister);
+
+int i3c_master_send_hdr_cmds_locked(struct i3c_master_controller *master,
+				    struct i3c_hdr_cmd *cmds, int ncmds)
+{
+	int i;
+
+	if (!cmds || !master || ncmds <= 0)
+		return -EINVAL;
+
+	if (!master->ops->send_hdr_cmds)
+		return -ENOTSUPP;
+
+	for (i = 0; i < ncmds; i++) {
+		if (!(master->this->info.hdr_cap & BIT(cmds[i].mode)))
+			return -ENOTSUPP;
+	}
+
+	return master->ops->send_hdr_cmds(master, cmds, ncmds);
+}
+
+/**
+ * i3c_master_send_hdr_cmds() - send HDR commands on the I3C bus
+ * @master: master used to send frames on the bus
+ * @cmds: array of HDR commands
+ * @ncmds: number of commands to send
+ *
+ * Send one or several HDR commands.
+ *
+ * This function can sleep and thus cannot be called in atomic context.
+ *
+ * Return: 0 in case of success, a negative error code otherwise.
+ */
+int i3c_master_send_hdr_cmds(struct i3c_master_controller *master,
+			     struct i3c_hdr_cmd *cmds, int ncmds)
+{
+	int ret;
+
+	i3c_bus_normaluse_lock(&master->bus);
+	ret = i3c_master_send_hdr_cmds_locked(master, cmds, ncmds);
+	i3c_bus_normaluse_unlock(&master->bus);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_master_send_hdr_cmds);
+
 int i3c_dev_do_priv_xfers_locked(struct i3c_dev_desc *dev,
 				 struct i3c_priv_xfer *xfers,
 				 int nxfers)
@@ -2721,10 +3213,38 @@ int i3c_dev_do_priv_xfers_locked(struct i3c_dev_desc *dev,
 	if (!master || !xfers)
 		return -EINVAL;
 
-	if (!master->ops->priv_xfers)
-		return -ENOTSUPP;
+	if (!master->target) {
+		if (!master->ops->priv_xfers)
+			return -EOPNOTSUPP;
+
+		return master->ops->priv_xfers(dev, xfers, nxfers);
+	}
+
+	if (!master->target_ops->priv_xfers)
+		return -EOPNOTSUPP;
+
+	return master->target_ops->priv_xfers(dev, xfers, nxfers);
+}
+
+int i3c_dev_generate_ibi_locked(struct i3c_dev_desc *dev, const u8 *data, int len)
+
+{
+	struct i3c_master_controller *master;
+
+	if (!dev)
+		return -ENOENT;
+
+	master = i3c_dev_get_master(dev);
+	if (!master)
+		return -EINVAL;
+
+	if (!master->target)
+		return -EINVAL;
+
+	if (!master->target_ops->generate_ibi)
+		return -EOPNOTSUPP;
 
-	return master->ops->priv_xfers(dev, xfers, nxfers);
+	return master->target_ops->generate_ibi(dev, data, len);
 }
 
 int i3c_dev_disable_ibi_locked(struct i3c_dev_desc *dev)
@@ -2812,6 +3332,113 @@ void i3c_dev_free_ibi_locked(struct i3c_dev_desc *dev)
 	dev->ibi = NULL;
 }
 
+int i3c_dev_send_ccc_cmd_locked(struct i3c_dev_desc *dev, u8 ccc_id)
+{
+	struct i3c_master_controller *master = i3c_dev_get_master(dev);
+	int ret;
+
+	switch (ccc_id) {
+	case I3C_CCC_SETAASA:
+		ret = i3c_master_setaasa_locked(master);
+		break;
+	case I3C_CCC_SETHID:
+		ret = i3c_master_sethid_locked(master);
+		break;
+	case I3C_CCC_RSTDAA(false):
+		ret = i3c_master_rstdaa_locked(master, dev->info.dyn_addr);
+		break;
+	case I3C_CCC_RSTDAA(true):
+		ret = i3c_master_rstdaa_locked(master, I3C_BROADCAST_ADDR);
+		break;
+	default:
+		dev_err(&master->dev, "Unpermitted ccc: %x\n", ccc_id);
+		return -ENOTSUPP;
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_dev_send_ccc_cmd_locked);
+
+int i3c_master_register_slave(struct i3c_master_controller *master,
+			      const struct i3c_slave_setup *req)
+{
+	if (!master->ops->register_slave)
+		return -ENOTSUPP;
+
+	return master->ops->register_slave(master, req);
+}
+
+int i3c_master_unregister_slave(struct i3c_master_controller *master)
+{
+	if (!master->ops->unregister_slave)
+		return -ENOTSUPP;
+
+	return master->ops->unregister_slave(master);
+}
+
+int i3c_master_send_sir(struct i3c_master_controller *master,
+			struct i3c_slave_payload *payload)
+{
+	int ret;
+
+	if (!master->ops->send_sir)
+		return -ENOTSUPP;
+
+	i3c_bus_normaluse_lock(&master->bus);
+	ret = master->ops->send_sir(master, payload);
+	i3c_bus_normaluse_unlock(&master->bus);
+
+	return ret;
+}
+
+int i3c_master_put_read_data(struct i3c_master_controller *master,
+			     struct i3c_slave_payload *data,
+			     struct i3c_slave_payload *ibi_notify)
+{
+	int ret;
+
+	if (!master->ops->put_read_data)
+		return -ENOTSUPP;
+
+	i3c_bus_normaluse_lock(&master->bus);
+	ret = master->ops->put_read_data(master, data, ibi_notify);
+	i3c_bus_normaluse_unlock(&master->bus);
+
+	return ret;
+}
+
+int i3c_for_each_dev(void *data, int (*fn)(struct device *, void *))
+{
+	int res;
+
+	mutex_lock(&i3c_core_lock);
+	res = bus_for_each_dev(&i3c_bus_type, NULL, data, fn);
+	mutex_unlock(&i3c_core_lock);
+
+	return res;
+}
+EXPORT_SYMBOL_GPL(i3c_for_each_dev);
+
+int i3c_dev_control_pec(struct i3c_dev_desc *dev, bool pec)
+{
+	struct i3c_master_controller *master = i3c_dev_get_master(dev);
+
+	if (!master->pec_supported)
+		return -EOPNOTSUPP;
+
+	dev->info.pec = pec;
+
+	/*
+	 * TODO: There are two cases which shall be covered
+	 * 1. Controller doesn't support PEC.
+	 *    In this case we could just fallback to SW implementation.
+	 * 2. Device doesn't support PEC.
+	 *    Then we really can't use PEC - and should error-out.
+	 */
+
+	return 0;
+}
+
 static int __init i3c_init(void)
 {
 	int res = bus_register_notifier(&i2c_bus_type, &i2cdev_notifier);
diff --git a/drivers/i3c/master/Kconfig b/drivers/i3c/master/Kconfig
index 3b8f95916f46..4cb8ea236eb6 100644
--- a/drivers/i3c/master/Kconfig
+++ b/drivers/i3c/master/Kconfig
@@ -22,6 +22,34 @@ config DW_I3C_MASTER
 	  This driver can also be built as a module.  If so, the module
 	  will be called dw-i3c-master.
 
+config AST2600_I3C_MASTER
+	tristate "Aspeed AST2600 I3C master driver"
+	depends on I3C
+	depends on HAS_IOMEM
+	depends on MACH_ASPEED_G6
+	help
+	  Support for Aspeed AST2600 MIPI I3C Controller.
+
+	  This driver can also be built as a module.  If so, the module
+	  will be called ast2600-i3c-master.
+
+if AST2600_I3C_MASTER
+config AST2600_I3C_IBI_MAX_PAYLOAD
+	int "Max IBI payload size"
+	default 255
+
+config AST2600_I3C_MRL
+	int "Max read length"
+	default 256
+
+config AST2600_I3C_CCC_WORKAROUND
+	bool "Workaround for AST2600A1 errata item#30"
+	default n
+	help
+	  Say y to enable the workaround for AST2600A1 errata item#30.  No need
+	  to enable this option if you are using AST2600A2 or later versions.
+endif
+
 config SVC_I3C_MASTER
 	tristate "Silvaco I3C Dual-Role Master driver"
 	depends on I3C
diff --git a/drivers/i3c/master/Makefile b/drivers/i3c/master/Makefile
index b3fee0f690b2..77e9c5b4bdaf 100644
--- a/drivers/i3c/master/Makefile
+++ b/drivers/i3c/master/Makefile
@@ -1,5 +1,6 @@
 # SPDX-License-Identifier: GPL-2.0-only
 obj-$(CONFIG_CDNS_I3C_MASTER)		+= i3c-master-cdns.o
 obj-$(CONFIG_DW_I3C_MASTER)		+= dw-i3c-master.o
+obj-$(CONFIG_AST2600_I3C_MASTER)	+= ast2600-i3c-global.o ast2600-i3c-master.o
 obj-$(CONFIG_SVC_I3C_MASTER)		+= svc-i3c-master.o
 obj-$(CONFIG_MIPI_I3C_HCI)		+= mipi-i3c-hci/
diff --git a/drivers/i3c/master/ast2600-i3c-global.c b/drivers/i3c/master/ast2600-i3c-global.c
new file mode 100644
index 000000000000..3536849dbe5f
--- /dev/null
+++ b/drivers/i3c/master/ast2600-i3c-global.c
@@ -0,0 +1,141 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2021 ASPEED Technology Inc.
+ *
+ * Author: Dylan Hung <dylan_hung@aspeedtech.com>
+ * Based on a work from: Ryan Chen <ryan_chen@aspeedtech.com>
+ */
+#include <linux/clk.h>
+#include <linux/module.h>
+#include <linux/of_platform.h>
+#include <linux/platform_device.h>
+#include <linux/of_address.h>
+#include <linux/io.h>
+#include <linux/reset.h>
+#include <linux/delay.h>
+#include <linux/slab.h>
+
+#define I3CG_REG0(x)			((x * 0x10) + 0x10)
+#define I3CG_REG0_SDA_PULLUP_EN_MASK	GENMASK(29, 28)
+#define I3CG_REG0_SDA_PULLUP_EN_2K	(0x1 << 28)
+#define I3CG_REG0_SDA_PULLUP_EN_750	(0x2 << 28)
+#define I3CG_REG0_SDA_PULLUP_EN_545	(0x3 << 28)
+
+#define I3CG_REG1(x)			((x * 0x10) + 0x14)
+#define I3CG_REG1_I2C_MODE		BIT(0)
+#define I3CG_REG1_TEST_MODE		BIT(1)
+#define I3CG_REG1_ACT_MODE_MASK		GENMASK(3, 2)
+#define I3CG_REG1_ACT_MODE(x)		(((x) << 2) & I3CG_REG1_ACT_MODE_MASK)
+#define I3CG_REG1_PENDING_INT_MASK	GENMASK(7, 4)
+#define I3CG_REG1_PENDING_INT(x)	(((x) << 4) & I3CG_REG1_PENDING_INT_MASK)
+#define I3CG_REG1_SA_MASK		GENMASK(14, 8)
+#define I3CG_REG1_SA(x)			(((x) << 8) & I3CG_REG1_SA_MASK)
+#define I3CG_REG1_SA_EN			BIT(15)
+#define I3CG_REG1_INST_ID_MASK		GENMASK(19, 16)
+#define I3CG_REG1_INST_ID(x)		(((x) << 16) & I3CG_REG1_INST_ID_MASK)
+
+struct aspeed_i3c_global {
+	void __iomem *regs;
+	struct reset_control *rst;
+};
+
+static const struct of_device_id aspeed_i3c_of_match[] = {
+	{ .compatible = "aspeed,ast2600-i3c-global", },
+	{},
+};
+MODULE_DEVICE_TABLE(of, aspeed_i3c_of_match);
+
+static u32 pullup_resistor_ohm_to_reg(u32 ohm)
+{
+	switch (ohm) {
+	case 545:
+		return I3CG_REG0_SDA_PULLUP_EN_545;
+	case 750:
+		return I3CG_REG0_SDA_PULLUP_EN_750;
+	case 2000:
+	default:
+		return I3CG_REG0_SDA_PULLUP_EN_2K;
+	}
+}
+
+static int aspeed_i3c_global_probe(struct platform_device *pdev)
+{
+	struct aspeed_i3c_global *i3cg;
+	u32 reg0, reg1, num_i3cs;
+	u32 *pullup_resistors;
+	int i, ret;
+
+	i3cg = devm_kzalloc(&pdev->dev, sizeof(*i3cg), GFP_KERNEL);
+	if (!i3cg)
+		return -ENOMEM;
+
+	i3cg->regs = devm_platform_ioremap_resource(pdev, 0);
+	if (IS_ERR(i3cg->regs))
+		return -ENOMEM;
+
+	i3cg->rst = devm_reset_control_get(&pdev->dev, NULL);
+	if (IS_ERR(i3cg->rst)) {
+		dev_err(&pdev->dev,
+			"missing or invalid reset controller device tree entry");
+		return PTR_ERR(i3cg->rst);
+	}
+
+	reset_control_assert(i3cg->rst);
+	udelay(3);
+	reset_control_deassert(i3cg->rst);
+
+	ret = of_property_read_u32(pdev->dev.of_node, "num-i3cs", &num_i3cs);
+	if (ret < 0) {
+		dev_err(&pdev->dev, "unable to get number of i3c controllers");
+		return -ENOMEM;
+	}
+
+	pullup_resistors = kcalloc(num_i3cs, sizeof(u32), GFP_KERNEL);
+	if (!pullup_resistors)
+		return -ENOMEM;
+
+	ret = of_property_read_u32_array(pdev->dev.of_node, "pull-up-resistors",
+					 pullup_resistors, num_i3cs);
+	if (ret < 0) {
+		dev_warn(&pdev->dev,
+			 "use 2K Ohm SDA pull up resistor by default");
+	}
+
+	reg1 = I3CG_REG1_ACT_MODE(1) | I3CG_REG1_PENDING_INT(0xc) |
+	       I3CG_REG1_SA(0x74);
+
+	for (i = 0; i < num_i3cs; i++) {
+		reg0 = readl(i3cg->regs + I3CG_REG0(i));
+		reg0 &= ~I3CG_REG0_SDA_PULLUP_EN_MASK;
+		reg0 |= pullup_resistor_ohm_to_reg(pullup_resistors[i]);
+		writel(reg0, i3cg->regs + I3CG_REG0(i));
+
+		reg1 &= ~I3CG_REG1_INST_ID_MASK;
+		reg1 |= I3CG_REG1_INST_ID(i);
+		writel(reg1, i3cg->regs + I3CG_REG1(i));
+	}
+
+	kfree(pullup_resistors);
+
+	return 0;
+}
+
+static struct platform_driver aspeed_i3c_driver = {
+	.probe  = aspeed_i3c_global_probe,
+	.driver = {
+		.name = KBUILD_MODNAME,
+		.of_match_table = of_match_ptr(aspeed_i3c_of_match),
+	},
+};
+
+//static int __init aspeed_i3c_global_init(void)
+//{
+//	return platform_driver_register(&aspeed_i3c_driver);
+//}
+//postcore_initcall(aspeed_i3c_global_init);
+module_platform_driver(aspeed_i3c_driver);
+
+MODULE_AUTHOR("Ryan Chen <ryan_chen@aspeedtech.com>");
+MODULE_AUTHOR("Dylan Hung <dylan_hung@aspeedtech.com>");
+MODULE_DESCRIPTION("ASPEED I3C Global Driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/i3c/master/ast2600-i3c-master.c b/drivers/i3c/master/ast2600-i3c-master.c
new file mode 100644
index 000000000000..eaab50a5816b
--- /dev/null
+++ b/drivers/i3c/master/ast2600-i3c-master.c
@@ -0,0 +1,2882 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright (c) 2021 ASPEED Technology Inc.
+ *
+ * Derived from dw-i3c-master.c by Vitor Soares <vitor.soares@synopsys.com>
+ */
+
+#include <linux/bitops.h>
+#include <linux/bitfield.h>
+#include <linux/clk.h>
+#include <linux/completion.h>
+#include <linux/err.h>
+#include <linux/errno.h>
+#include <linux/i3c/master.h>
+#include <linux/interrupt.h>
+#include <linux/ioport.h>
+#include <linux/iopoll.h>
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/platform_device.h>
+#include <linux/reset.h>
+#include <linux/slab.h>
+#include <linux/mfd/syscon.h>
+#include <linux/regmap.h>
+
+#define I3C_CHANNEL_MAX 5
+
+#define DEVICE_CTRL			0x0
+#define DEV_CTRL_ENABLE			BIT(31)
+#define DEV_CTRL_RESUME			BIT(30)
+#define DEV_CTRL_AUTO_HJ_DISABLE	BIT(27)
+#define DEV_CTRL_SLAVE_MDB		GENMASK(23, 16)
+#define DEV_CTRL_SLAVE_PEC_EN		BIT(10)
+#define DEV_CRTL_IBI_PAYLOAD_EN		BIT(9)
+#define DEV_CTRL_HOT_JOIN_NACK		BIT(8)
+#define DEV_CTRL_I2C_SLAVE_PRESENT	BIT(7)
+#define DEV_CTRL_IBA_INCLUDE		BIT(0)
+
+#define DEVICE_ADDR			0x4
+#define DEV_ADDR_DYNAMIC_ADDR_VALID	BIT(31)
+#define DEV_ADDR_DYNAMIC(x)		(((x) << 16) & GENMASK(22, 16))
+#define DEV_ADDR_STATIC_ADDR_VALID	BIT(15)
+#define DEV_ADDR_STATIC(x)		(((x) << 0) & GENMASK(6, 0))
+
+#define HW_CAPABILITY			0x8
+#define COMMAND_QUEUE_PORT		0xc
+#define COMMAND_PORT_PEC		BIT(31)
+#define COMMAND_PORT_TOC		BIT(30)
+#define COMMAND_PORT_READ_TRANSFER	BIT(28)
+#define COMMAND_PORT_SDAP		BIT(27)
+#define COMMAND_PORT_ROC		BIT(26)
+#define COMMAND_PORT_DBP(x)		((x) << 25)
+#define COMMAND_PORT_SPEED(x)		(((x) << 21) & GENMASK(23, 21))
+#define   SPEED_I3C_SDR0		0x0
+#define   SPEED_I3C_SDR1		0x1
+#define   SPEED_I3C_SDR2		0x2
+#define   SPEED_I3C_SDR3		0x3
+#define   SPEED_I3C_SDR4		0x4
+#define   SPEED_I3C_HDR_TS		0x5
+#define   SPEED_I3C_HDR_DDR		0x6
+#define   SPEED_I3C_I2C_FM		0x7
+#define   SPEED_I2C_FM			0x0
+#define   SPEED_I2C_FMP			0x1
+#define COMMAND_PORT_DEV_INDEX(x)	(((x) << 16) & GENMASK(20, 16))
+#define COMMAND_PORT_CP			BIT(15)
+#define COMMAND_PORT_CMD(x)		(((x) << 7) & GENMASK(14, 7))
+#define COMMAND_PORT_TID(x)		(((x) << 3) & GENMASK(6, 3))
+
+#define COMMAND_PORT_ARG_DBP(x)		(((x) << 8) & GENMASK(15, 8))
+#define COMMAND_PORT_ARG_DATA_LEN(x)	(((x) << 16) & GENMASK(31, 16))
+#define COMMAND_PORT_ARG_DATA_LEN_MAX	65536
+#define COMMAND_PORT_TRANSFER_ARG	0x01
+
+#define COMMAND_ATTR_SLAVE_DATA		0x0
+#define COMMAND_PORT_SLAVE_TID(x)      (((x) << 3) & GENMASK(5, 3))
+#define COMMAND_PORT_SLAVE_DATA_LEN	GENMASK(31, 16)
+
+#define COMMAND_PORT_SDA_DATA_BYTE_3(x)	(((x) << 24) & GENMASK(31, 24))
+#define COMMAND_PORT_SDA_DATA_BYTE_2(x)	(((x) << 16) & GENMASK(23, 16))
+#define COMMAND_PORT_SDA_DATA_BYTE_1(x)	(((x) << 8) & GENMASK(15, 8))
+#define COMMAND_PORT_SDA_BYTE_STRB_3	BIT(5)
+#define COMMAND_PORT_SDA_BYTE_STRB_2	BIT(4)
+#define COMMAND_PORT_SDA_BYTE_STRB_1	BIT(3)
+#define COMMAND_PORT_SHORT_DATA_ARG	0x02
+
+#define COMMAND_PORT_DEV_COUNT(x)	(((x) << 21) & GENMASK(25, 21))
+#define COMMAND_PORT_ADDR_ASSGN_CMD	0x03
+
+#define RESPONSE_QUEUE_PORT		0x10
+#define RESPONSE_PORT_ERR_STATUS(x)	(((x) & GENMASK(31, 28)) >> 28)
+#define RESPONSE_NO_ERROR		0
+#define RESPONSE_ERROR_CRC		1
+#define RESPONSE_ERROR_PARITY		2
+#define RESPONSE_ERROR_FRAME		3
+#define RESPONSE_ERROR_IBA_NACK		4
+#define RESPONSE_ERROR_ADDRESS_NACK	5
+#define RESPONSE_ERROR_OVER_UNDER_FLOW	6
+#define RESPONSE_ERROR_TRANSF_ABORT	8
+#define RESPONSE_ERROR_I2C_W_NACK_ERR	9
+#define RESPONSE_ERROR_EARLY_TERMINATE	10
+#define RESPONSE_ERROR_PEC_ERR		12
+#define RESPONSE_PORT_TID(x)		(((x) & GENMASK(27, 24)) >> 24)
+#define   TID_SLAVE_IBI_DONE		0b0001
+#define   TID_MASTER_READ_DATA		0b0010
+#define   TID_MASTER_WRITE_DATA		0b1000
+#define   TID_CCC_WRITE_DATA		0b1111
+#define RESPONSE_PORT_DATA_LEN(x)	((x) & GENMASK(15, 0))
+
+#define RX_TX_DATA_PORT			0x14
+#define IBI_QUEUE_STATUS		0x18
+#define IBI_QUEUE_STATUS_RSP_NACK	BIT(31)
+#define IBI_QUEUE_STATUS_PEC_ERR	BIT(30)
+#define IBI_QUEUE_STATUS_LAST_FRAG	BIT(24)
+#define IBI_QUEUE_STATUS_IBI_ID(x)	(((x) & GENMASK(15, 8)) >> 8)
+#define IBI_QUEUE_STATUS_DATA_LEN(x)	((x) & GENMASK(7, 0))
+
+#define IBI_QUEUE_IBI_ADDR(x)		(IBI_QUEUE_STATUS_IBI_ID(x) >> 1)
+#define IBI_QUEUE_IBI_RNW(x)		(IBI_QUEUE_STATUS_IBI_ID(x) & BIT(0))
+#define IBI_TYPE_MR(x)                                                         \
+	((IBI_QUEUE_IBI_ADDR(x) != I3C_HOT_JOIN_ADDR) && !IBI_QUEUE_IBI_RNW(x))
+#define IBI_TYPE_HJ(x)                                                         \
+	((IBI_QUEUE_IBI_ADDR(x) == I3C_HOT_JOIN_ADDR) && !IBI_QUEUE_IBI_RNW(x))
+#define IBI_TYPE_SIR(x)                                                        \
+	((IBI_QUEUE_IBI_ADDR(x) != I3C_HOT_JOIN_ADDR) && IBI_QUEUE_IBI_RNW(x))
+
+#define IBI_QUEUE_DATA			0x18
+#define QUEUE_THLD_CTRL			0x1c
+#define QUEUE_THLD_CTRL_IBI_STA_MASK	GENMASK(31, 24)
+#define QUEUE_THLD_CTRL_IBI_STA(x)	(((x) - 1) << 24)
+#define QUEUE_THLD_CTRL_IBI_DAT_MASK	GENMASK(23, 16)
+#define QUEUE_THLD_CTRL_IBI_DAT(x)	((x) << 16)
+#define QUEUE_THLD_CTRL_RESP_BUF_MASK	GENMASK(15, 8)
+#define QUEUE_THLD_CTRL_RESP_BUF(x)	(((x) - 1) << 8)
+
+#define DATA_BUFFER_THLD_CTRL		0x20
+#define DATA_BUFFER_THLD_CTRL_RX_BUF	GENMASK(11, 8)
+
+#define IBI_QUEUE_CTRL			0x24
+#define IBI_MR_REQ_REJECT		0x2C
+#define IBI_SIR_REQ_REJECT		0x30
+#define IBI_REQ_REJECT_ALL		GENMASK(31, 0)
+
+#define RESET_CTRL			0x34
+#define RESET_CTRL_BUS			BIT(31)
+#define RESET_CTRL_BUS_RESET_TYPE	GENMASK(30, 29)
+#define   BUS_RESET_TYPE_EXIT		0b00
+#define   BUS_RESET_TYPE_SCL_LOW	0b11
+#define RESET_CTRL_IBI_QUEUE		BIT(5)
+#define RESET_CTRL_RX_FIFO		BIT(4)
+#define RESET_CTRL_TX_FIFO		BIT(3)
+#define RESET_CTRL_RESP_QUEUE		BIT(2)
+#define RESET_CTRL_CMD_QUEUE		BIT(1)
+#define RESET_CTRL_SOFT			BIT(0)
+#define RESET_CTRL_ALL                  (RESET_CTRL_IBI_QUEUE	              |\
+					 RESET_CTRL_RX_FIFO	              |\
+					 RESET_CTRL_TX_FIFO	              |\
+					 RESET_CTRL_RESP_QUEUE	              |\
+					 RESET_CTRL_CMD_QUEUE	              |\
+					 RESET_CTRL_SOFT)
+#define RESET_CTRL_QUEUES		(RESET_CTRL_IBI_QUEUE	              |\
+					 RESET_CTRL_RX_FIFO	              |\
+					 RESET_CTRL_TX_FIFO	              |\
+					 RESET_CTRL_RESP_QUEUE	              |\
+					 RESET_CTRL_CMD_QUEUE)
+
+#define SLV_EVENT_CTRL			0x38
+#define SLV_EVENT_CTRL_MWL_UPD		BIT(7)
+#define SLV_EVENT_CTRL_MRL_UPD		BIT(6)
+#define SLV_EVENT_CTRL_SIR_EN		BIT(0)
+#define SLV_EVETN_CTRL_W1C_MASK		(SLV_EVENT_CTRL_MWL_UPD |\
+					 SLV_EVENT_CTRL_MRL_UPD)
+
+#define INTR_STATUS			0x3c
+#define INTR_STATUS_EN			0x40
+#define INTR_SIGNAL_EN			0x44
+#define INTR_FORCE			0x48
+#define INTR_BUSOWNER_UPDATE_STAT	BIT(13)
+#define INTR_IBI_UPDATED_STAT		BIT(12)
+#define INTR_READ_REQ_RECV_STAT		BIT(11)
+#define INTR_DEFSLV_STAT		BIT(10)
+#define INTR_TRANSFER_ERR_STAT		BIT(9)
+#define INTR_DYN_ADDR_ASSGN_STAT	BIT(8)
+#define INTR_CCC_UPDATED_STAT		BIT(6)
+#define INTR_TRANSFER_ABORT_STAT	BIT(5)
+#define INTR_RESP_READY_STAT		BIT(4)
+#define INTR_CMD_QUEUE_READY_STAT	BIT(3)
+#define INTR_IBI_THLD_STAT		BIT(2)
+#define INTR_RX_THLD_STAT		BIT(1)
+#define INTR_TX_THLD_STAT		BIT(0)
+#define INTR_ALL			(INTR_BUSOWNER_UPDATE_STAT |	\
+					INTR_IBI_UPDATED_STAT |		\
+					INTR_READ_REQ_RECV_STAT |	\
+					INTR_DEFSLV_STAT |		\
+					INTR_TRANSFER_ERR_STAT |	\
+					INTR_DYN_ADDR_ASSGN_STAT |	\
+					INTR_CCC_UPDATED_STAT |		\
+					INTR_TRANSFER_ABORT_STAT |	\
+					INTR_RESP_READY_STAT |		\
+					INTR_CMD_QUEUE_READY_STAT |	\
+					INTR_IBI_THLD_STAT |		\
+					INTR_TX_THLD_STAT |		\
+					INTR_RX_THLD_STAT)
+#define INTR_MASTER_MASK		(INTR_TRANSFER_ERR_STAT |	\
+					 INTR_RESP_READY_STAT)
+#define INTR_2ND_MASTER_MASK		(INTR_TRANSFER_ERR_STAT |	\
+					 INTR_RESP_READY_STAT	|	\
+					 INTR_IBI_UPDATED_STAT  |	\
+					 INTR_CCC_UPDATED_STAT)
+#define QUEUE_STATUS_LEVEL		0x4c
+#define QUEUE_STATUS_IBI_STATUS_CNT(x)	(((x) & GENMASK(28, 24)) >> 24)
+#define QUEUE_STATUS_IBI_BUF_BLR(x)	(((x) & GENMASK(23, 16)) >> 16)
+#define QUEUE_STATUS_LEVEL_RESP(x)	(((x) & GENMASK(15, 8)) >> 8)
+#define QUEUE_STATUS_LEVEL_CMD(x)	((x) & GENMASK(7, 0))
+
+#define DATA_BUFFER_STATUS_LEVEL	0x50
+#define DATA_BUFFER_STATUS_LEVEL_TX(x)	((x) & GENMASK(7, 0))
+
+#define PRESENT_STATE			0x54
+#define   CM_TFR_ST_STS			GENMASK(21, 16)
+#define     CM_TFR_ST_STS_HALT		0x13
+#define   CM_TFR_STS			GENMASK(13, 8)
+#define     CM_TFR_STS_MASTER_SERV_IBI	0xe
+#define     CM_TFR_STS_MASTER_HALT	0xf
+#define     CM_TFR_STS_SLAVE_HALT	0x6
+
+#define CCC_DEVICE_STATUS		0x58
+#define DEVICE_ADDR_TABLE_POINTER	0x5c
+#define DEVICE_ADDR_TABLE_DEPTH(x)	(((x) & GENMASK(31, 16)) >> 16)
+#define DEVICE_ADDR_TABLE_ADDR(x)	((x) & GENMASK(7, 0))
+
+#define DEV_CHAR_TABLE_POINTER		0x60
+#define VENDOR_SPECIFIC_REG_POINTER	0x6c
+#define SLV_MIPI_PID_VALUE		0x70
+#define PID_MANUF_ID_ASPEED		0x03f6
+
+#define SLV_PID_VALUE			0x74
+#define SLV_PID_PART_ID(x)		(((x) << 16) & GENMASK(31, 16))
+#define SLV_PID_INST_ID(x)		(((x) << 12) & GENMASK(15, 12))
+#define SLV_PID_DCR(x)			((x) & GENMASK(11, 0))
+
+#define PID_PART_ID_AST2600_SERIES	0x0500
+#define PID_PART_ID_AST1030_A0		0x8000
+
+#define SLV_CHAR_CTRL			0x78
+#define SLV_CHAR_GET_DCR(x)		(((x) & GENMASK(15, 8)) >> 8)
+#define SLV_CHAR_GET_BCR(x)		(((x) & GENMASK(7, 0)) >> 0)
+#define SLV_MAX_LEN			0x7c
+#define MAX_READ_TURNAROUND		0x80
+#define MAX_DATA_SPEED			0x84
+#define SLV_DEBUG_STATUS		0x88
+#define SLV_INTR_REQ			0x8c
+#define SLV_INTR_REQ_IBI_STS(x)		((x) & GENMASK(9, 8) >> 8)
+#define SLV_IBI_STS_OK			0x1
+
+#define DEVICE_CTRL_EXTENDED		0xb0
+#define DEVICE_CTRL_ROLE_MASK		GENMASK(1, 0)
+#define DEVICE_CTRL_ROLE_MASTER		0
+#define DEVICE_CTRL_ROLE_SLAVE		1
+#define SCL_I3C_OD_TIMING		0xb4
+#define SCL_I3C_PP_TIMING		0xb8
+#define SCL_I3C_TIMING_HCNT		GENMASK(23, 16)
+#define SCL_I3C_TIMING_LCNT		GENMASK(7, 0)
+#define SCL_I3C_TIMING_CNT_MIN		5
+
+#define SCL_I2C_FM_TIMING		0xbc
+#define SCL_I2C_FM_TIMING_HCNT		GENMASK(31, 16)
+#define SCL_I2C_FM_TIMING_LCNT		GENMASK(15, 0)
+
+#define SCL_I2C_FMP_TIMING		0xc0
+#define SCL_I2C_FMP_TIMING_HCNT		GENMASK(23, 16)
+#define SCL_I2C_FMP_TIMING_LCNT		GENMASK(15, 0)
+
+#define SCL_EXT_LCNT_TIMING		0xc8
+#define SCL_EXT_LCNT_4(x)		(((x) << 24) & GENMASK(31, 24))
+#define SCL_EXT_LCNT_3(x)		(((x) << 16) & GENMASK(23, 16))
+#define SCL_EXT_LCNT_2(x)		(((x) << 8) & GENMASK(15, 8))
+#define SCL_EXT_LCNT_1(x)		((x) & GENMASK(7, 0))
+
+#define SCL_EXT_TERMN_LCNT_TIMING	0xcc
+#define SDA_HOLD_SWITCH_DLY_TIMING	0xd0
+#define SDA_TX_HOLD			GENMASK(18, 16)
+#define   SDA_TX_HOLD_MIN		0b001
+#define   SDA_TX_HOLD_MAX		0b111
+#define SDA_PP_OD_SWITCH_DLY		GENMASK(10, 8)
+#define SDA_OD_PP_SWITCH_DLY		GENMASK(2, 0)
+#define BUS_FREE_TIMING			0xd4
+#define BUS_I3C_AVAILABLE_TIME(x)	(((x) << 16) & GENMASK(31, 16))
+#define BUS_I3C_MST_FREE(x)		((x) & GENMASK(15, 0))
+
+#define BUS_IDLE_TIMING			0xd8
+#define SCL_LOW_MST_EXT_TIMEOUT		0xdc
+#define I3C_VER_ID			0xe0
+#define I3C_VER_TYPE			0xe4
+#define EXTENDED_CAPABILITY		0xe8
+#define SLAVE_CONFIG			0xec
+
+#define DEV_ADDR_TABLE_LEGACY_I2C_DEV	BIT(31)
+#define DEV_ADDR_TABLE_DEV_NACK_RETRY	GENMASK(30, 29)
+#define DEV_ADDR_TABLE_IBI_ADDR_MASK	GENMASK(25, 24)
+#define   IBI_ADDR_MASK_OFF		0b00
+#define   IBI_ADDR_MASK_LAST_3BITS	0b01
+#define   IBI_ADDR_MASK_LAST_4BITS	0b10
+#define DEV_ADDR_TABLE_DA_PARITY	BIT(23)
+#define DEV_ADDR_TABLE_DYNAMIC_ADDR	GENMASK(22, 16)
+#define DEV_ADDR_TABLE_MR_REJECT	BIT(14)
+#define DEV_ADDR_TABLE_SIR_REJECT	BIT(13)
+#define DEV_ADDR_TABLE_IBI_WITH_DATA	BIT(12)
+#define DEV_ADDR_TABLE_IBI_PEC_EN	BIT(11)
+#define DEV_ADDR_TABLE_STATIC_ADDR	GENMASK(6, 0)
+
+#define DEV_ADDR_TABLE_LOC(start, idx)	((start) + ((idx) << 2))
+#define GET_DAT_FROM_POS(_master, _pos)                                        \
+	(readl(_master->regs + DEV_ADDR_TABLE_LOC(_master->datstartaddr, _pos)))
+
+#define MAX_DEVS			128
+#define MAX_IBI_FRAG_SIZE		124
+
+#define I3C_BUS_SDR1_SCL_RATE		8000000
+#define I3C_BUS_SDR2_SCL_RATE		6000000
+#define I3C_BUS_SDR3_SCL_RATE		4000000
+#define I3C_BUS_SDR4_SCL_RATE		2000000
+#define I3C_BUS_I2C_STD_TLOW_MIN_NS	4700
+#define I3C_BUS_I2C_STD_THIGH_MIN_NS	4000
+#define I3C_BUS_I2C_STD_TR_MAX_NS	1000
+#define I3C_BUS_I2C_STD_TF_MAX_NS	300
+#define I3C_BUS_I2C_FM_TLOW_MIN_NS	1300
+#define I3C_BUS_I2C_FM_THIGH_MIN_NS	600
+#define I3C_BUS_I2C_FM_TR_MAX_NS	300
+#define I3C_BUS_I2C_FM_TF_MAX_NS	300
+#define I3C_BUS_I2C_FMP_TLOW_MIN_NS	500
+#define I3C_BUS_I2C_FMP_THIGH_MIN_NS	260
+#define I3C_BUS_I2C_FMP_TR_MAX_NS	120
+#define I3C_BUS_I2C_FMP_TF_MAX_NS	120
+#define I3C_BUS_JESD403_PP_TLOW_MIN_NS	35
+#define I3C_BUS_JESD403_PP_THIGH_MIN_NS	35
+#define I3C_BUS_JESD403_PP_TR_MAX_NS	5
+#define I3C_BUS_JESD403_PP_TF_MAX_NS	5
+#define I3C_BUS_THIGH_MAX_NS		41
+
+#define I3C_BUS_EXT_TERMN_CNT		4
+#define JESD403_TIMED_RESET_NS_DEF	52428800
+
+#define XFER_TIMEOUT			(msecs_to_jiffies(1000))
+
+#define ast_setbits(x, set)		writel(readl(x) | (set), x)
+#define ast_clrbits(x, clr)		writel(readl(x) & ~(clr), x)
+#define ast_clrsetbits(x, clr, set)	writel((readl(x) & ~(clr)) | (set), x)
+
+#define MAX_GROUPS			(1 << 4)
+#define MAX_DEVS_IN_GROUP		(1 << 3)
+#define ALL_DEVS_IN_GROUP_ARE_FREE	((1 << MAX_DEVS_IN_GROUP) - 1)
+#define ADDR_GRP_SHIFT			3
+#define ADDR_GRP_MASK			GENMASK(6, ADDR_GRP_SHIFT)
+#define ADDR_GRP(x)			(((x) & ADDR_GRP_MASK) >> ADDR_GRP_SHIFT)
+#define ADDR_HID_MASK			GENMASK(ADDR_GRP_SHIFT - 1, 0)
+#define ADDR_HID(x)			((x) & ADDR_HID_MASK)
+
+struct aspeed_i3c_master_caps {
+	u8 cmdfifodepth;
+	u8 datafifodepth;
+};
+
+struct aspeed_i3c_cmd {
+	u32 cmd_lo;
+	u32 cmd_hi;
+	u16 tx_len;
+	const void *tx_buf;
+	u16 rx_len;
+	void *rx_buf;
+	u8 error;
+};
+
+struct aspeed_i3c_xfer {
+	struct list_head node;
+	struct completion comp;
+	int ret;
+	unsigned int ncmds;
+	struct aspeed_i3c_cmd cmds[];
+};
+
+struct aspeed_i3c_dev_group {
+	u32 dat[8];
+	u32 free_pos;
+	int hw_index;
+	struct {
+		u32 set;
+		u32 clr;
+	} mask;
+};
+
+struct aspeed_i3c_master {
+	struct device *dev;
+	struct i3c_master_controller base;
+	struct regmap *i3cg;
+	u16 maxdevs;
+	u16 datstartaddr;
+	u32 free_pos;
+	struct aspeed_i3c_dev_group dev_group[MAX_GROUPS];
+	struct {
+		struct list_head list;
+		struct aspeed_i3c_xfer *cur;
+		spinlock_t lock;
+	} xferqueue;
+	struct {
+		struct i3c_dev_desc *slots[MAX_DEVS];
+		u32 received_ibi_len[MAX_DEVS];
+		spinlock_t lock;
+	} ibi;
+	struct aspeed_i3c_master_caps caps;
+	void __iomem *regs;
+	struct reset_control *core_rst;
+	struct clk *core_clk;
+	char version[5];
+	char type[5];
+	u8 addrs[MAX_DEVS];
+	u32 channel;
+	bool secondary;
+	struct {
+		u32 *buf;
+		void (*callback)(struct i3c_master_controller *m,
+				 const struct i3c_slave_payload *payload);
+	} slave_data;
+	struct completion sir_complete;
+	struct completion data_read_complete;
+
+	struct {
+		unsigned long core_rate;
+		unsigned long core_period;
+		u32 i3c_od_scl_freq;
+		u32 i3c_od_scl_low;
+		u32 i3c_od_scl_high;
+		u32 i3c_pp_scl_freq;
+		u32 i3c_pp_scl_low;
+		u32 i3c_pp_scl_high;
+	} timing;
+	struct work_struct hj_work;
+};
+
+struct aspeed_i3c_i2c_dev_data {
+	struct i3c_generic_ibi_pool *ibi_pool;
+	u8 index;
+	s8 ibi;
+};
+
+static u8 even_parity(u8 p)
+{
+	p ^= p >> 4;
+	p &= 0xf;
+
+	return (0x9669 >> p) & 1;
+}
+
+#define I3CG_REG1(x)			((x * 0x10) + 0x14)
+#define SDA_OUT_SW_MODE_EN		BIT(31)
+#define SCL_OUT_SW_MODE_EN		BIT(30)
+#define SDA_IN_SW_MODE_EN		BIT(29)
+#define SCL_IN_SW_MODE_EN		BIT(28)
+#define SDA_IN_SW_MODE_VAL		BIT(27)
+#define SDA_OUT_SW_MODE_VAL		BIT(25)
+#define SDA_SW_MODE_OE			BIT(24)
+#define SCL_IN_SW_MODE_VAL		BIT(23)
+#define SCL_OUT_SW_MODE_VAL		BIT(21)
+#define SCL_SW_MODE_OE			BIT(20)
+
+static void aspeed_i3c_isolate_scl_sda(struct aspeed_i3c_master *master, bool iso)
+{
+	if (iso) {
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SCL_IN_SW_MODE_VAL | SDA_IN_SW_MODE_VAL,
+				  SCL_IN_SW_MODE_VAL | SDA_IN_SW_MODE_VAL);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SCL_IN_SW_MODE_EN | SDA_IN_SW_MODE_EN,
+				  SCL_IN_SW_MODE_EN | SDA_IN_SW_MODE_EN);
+	} else {
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SCL_IN_SW_MODE_EN | SDA_IN_SW_MODE_EN, 0);
+	}
+}
+
+static void aspeed_i3c_toggle_scl_in(struct aspeed_i3c_master *master, u32 times)
+{
+	for (; times; times--) {
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SCL_IN_SW_MODE_VAL, 0);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SCL_IN_SW_MODE_VAL, SCL_IN_SW_MODE_VAL);
+	}
+}
+
+static void aspeed_i3c_gen_stop_to_internal(struct aspeed_i3c_master *master)
+{
+	regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+			  SCL_IN_SW_MODE_VAL, SCL_IN_SW_MODE_VAL);
+	regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+			  SDA_IN_SW_MODE_VAL, 0);
+	regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+			  SDA_IN_SW_MODE_VAL, SDA_IN_SW_MODE_VAL);
+}
+
+static bool aspeed_i3c_fsm_is_idle(struct aspeed_i3c_master *master)
+{
+	/*
+	 * Clear the IBI queue to enable the hardware to generate SCL and
+	 * begin detecting the T-bit low to stop reading IBI data.
+	 */
+	readl(master->regs + IBI_QUEUE_DATA);
+	if (FIELD_GET(CM_TFR_STS, readl(master->regs + PRESENT_STATE)))
+		return false;
+	return true;
+}
+
+static void aspeed_i3c_gen_tbits_in(struct aspeed_i3c_master *master)
+{
+	bool is_idle;
+	int ret;
+
+	regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+			  SDA_IN_SW_MODE_VAL, SDA_IN_SW_MODE_VAL);
+	regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+			  SDA_IN_SW_MODE_EN, SDA_IN_SW_MODE_EN);
+
+	regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+			  SDA_IN_SW_MODE_VAL, 0);
+	ret = readx_poll_timeout_atomic(aspeed_i3c_fsm_is_idle, master, is_idle,
+					is_idle, 0, 2000000);
+	regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+			  SDA_IN_SW_MODE_EN, 0);
+	if (ret)
+		dev_err(master->dev,
+			"Failed to recovery the i3c fsm from %lx to idle: %d",
+			FIELD_GET(CM_TFR_STS,
+				  readl(master->regs + PRESENT_STATE)),
+			ret);
+}
+
+static bool aspeed_i3c_master_supports_ccc_cmd(struct i3c_master_controller *m,
+					   const struct i3c_ccc_cmd *cmd)
+{
+	if (cmd->ndests > 1)
+		return false;
+
+	switch (cmd->id) {
+	case I3C_CCC_ENEC(true):
+	case I3C_CCC_ENEC(false):
+	case I3C_CCC_DISEC(true):
+	case I3C_CCC_DISEC(false):
+	case I3C_CCC_ENTAS(0, true):
+	case I3C_CCC_ENTAS(0, false):
+	case I3C_CCC_RSTDAA(true):
+	case I3C_CCC_RSTDAA(false):
+	case I3C_CCC_ENTDAA:
+	case I3C_CCC_SETMWL(true):
+	case I3C_CCC_SETMWL(false):
+	case I3C_CCC_SETMRL(true):
+	case I3C_CCC_SETMRL(false):
+	case I3C_CCC_ENTHDR(0):
+	case I3C_CCC_SETDASA:
+	case I3C_CCC_SETNEWDA:
+	case I3C_CCC_GETMWL:
+	case I3C_CCC_GETMRL:
+	case I3C_CCC_GETPID:
+	case I3C_CCC_GETBCR:
+	case I3C_CCC_GETDCR:
+	case I3C_CCC_GETSTATUS:
+	case I3C_CCC_GETMXDS:
+	case I3C_CCC_GETHDRCAP:
+	case I3C_CCC_SETAASA:
+	case I3C_CCC_SETHID:
+		return true;
+	default:
+		return false;
+	}
+}
+
+static inline struct aspeed_i3c_master *
+to_aspeed_i3c_master(struct i3c_master_controller *master)
+{
+	return container_of(master, struct aspeed_i3c_master, base);
+}
+
+static void aspeed_i3c_master_iba_ctrl(struct aspeed_i3c_master *master, bool ctrl)
+{
+	ctrl ? writel(readl(master->regs + DEVICE_CTRL) | DEV_CTRL_IBA_INCLUDE,
+		      master->regs + DEVICE_CTRL) :
+	       writel(readl(master->regs + DEVICE_CTRL) & ~DEV_CTRL_IBA_INCLUDE,
+		      master->regs + DEVICE_CTRL);
+}
+
+static int aspeed_i3c_master_disable(struct aspeed_i3c_master *master)
+{
+	if (master->secondary)
+		aspeed_i3c_isolate_scl_sda(master, true);
+	writel(readl(master->regs + DEVICE_CTRL) & ~DEV_CTRL_ENABLE,
+	       master->regs + DEVICE_CTRL);
+	if (master->secondary) {
+		aspeed_i3c_toggle_scl_in(master, 8);
+		if (readl(master->regs + DEVICE_CTRL) & DEV_CTRL_ENABLE) {
+			dev_warn(master->dev,
+					"Failed to disable controller");
+			aspeed_i3c_isolate_scl_sda(master, false);
+			return -EACCES;
+		}
+		aspeed_i3c_isolate_scl_sda(master, false);
+	}
+	return 0;
+}
+
+static int aspeed_i3c_master_enable(struct aspeed_i3c_master *master)
+{
+	u32 wait_enable_us;
+
+	if (master->secondary)
+		aspeed_i3c_isolate_scl_sda(master, true);
+	writel(readl(master->regs + DEVICE_CTRL) | DEV_CTRL_ENABLE,
+	       master->regs + DEVICE_CTRL);
+	if (master->secondary) {
+		wait_enable_us =
+			DIV_ROUND_UP(master->timing.core_period *
+					     FIELD_GET(GENMASK(31, 16),
+						       readl(master->regs +
+							     BUS_FREE_TIMING)),
+				     NSEC_PER_USEC);
+		udelay(wait_enable_us);
+		aspeed_i3c_toggle_scl_in(master, 8);
+		if (!(readl(master->regs + DEVICE_CTRL) & DEV_CTRL_ENABLE)) {
+			dev_warn(master->dev, "Failed to enable controller");
+			aspeed_i3c_isolate_scl_sda(master, false);
+			return -EACCES;
+		}
+		aspeed_i3c_gen_stop_to_internal(master);
+		aspeed_i3c_isolate_scl_sda(master, false);
+	}
+
+	return 0;
+}
+
+static void aspeed_i3c_master_resume(struct aspeed_i3c_master *master)
+{
+	writel(readl(master->regs + DEVICE_CTRL) | DEV_CTRL_RESUME,
+	       master->regs + DEVICE_CTRL);
+}
+
+static void aspeed_i3c_master_set_role(struct aspeed_i3c_master *master)
+{
+	u32 reg;
+	u32 role = DEVICE_CTRL_ROLE_MASTER;
+
+	if (master->secondary)
+		role = DEVICE_CTRL_ROLE_SLAVE;
+
+	reg = readl(master->regs + DEVICE_CTRL_EXTENDED);
+	reg = (reg & ~DEVICE_CTRL_ROLE_MASK) | role;
+	writel(reg, master->regs + DEVICE_CTRL_EXTENDED);
+}
+
+static int aspeed_i3c_master_get_free_pos(struct aspeed_i3c_master *master)
+{
+	if (!(master->free_pos & GENMASK(master->maxdevs - 1, 0)))
+		return -ENOSPC;
+
+	return ffs(master->free_pos) - 1;
+}
+
+static void aspeed_i3c_master_init_group_dat(struct aspeed_i3c_master *master)
+{
+	struct aspeed_i3c_dev_group *dev_grp;
+	int i, j;
+	u32 def_set, def_clr;
+
+	def_clr = DEV_ADDR_TABLE_IBI_ADDR_MASK;
+
+	/* For now don't support Hot-Join */
+	def_set = DEV_ADDR_TABLE_MR_REJECT | DEV_ADDR_TABLE_SIR_REJECT;
+
+	for (i = 0; i < MAX_GROUPS; i++) {
+		dev_grp = &master->dev_group[i];
+		dev_grp->hw_index = -1;
+		dev_grp->free_pos = ALL_DEVS_IN_GROUP_ARE_FREE;
+		dev_grp->mask.set = def_set;
+		dev_grp->mask.clr = def_clr;
+		for (j = 0; j < MAX_DEVS_IN_GROUP; j++)
+			dev_grp->dat[j] = 0;
+	}
+
+	for (i = 0; i < master->maxdevs; i++)
+		writel(def_set,
+		       master->regs +
+			       DEV_ADDR_TABLE_LOC(master->datstartaddr, i));
+}
+
+static int aspeed_i3c_master_set_group_dat(struct aspeed_i3c_master *master, u8 addr,
+				       u32 val)
+{
+	struct aspeed_i3c_dev_group *dev_grp = &master->dev_group[ADDR_GRP(addr)];
+	u8 idx = ADDR_HID(addr);
+
+	val &= ~DEV_ADDR_TABLE_DA_PARITY;
+	val |= FIELD_PREP(DEV_ADDR_TABLE_DA_PARITY, even_parity(addr));
+	dev_grp->dat[idx] = val;
+
+	if (val) {
+		dev_grp->free_pos &= ~BIT(idx);
+
+		/*
+		 * reserve the hw dat resource for the first member of the
+		 * group. all the members in the group share the same hw dat.
+		 */
+		if (dev_grp->hw_index == -1) {
+			dev_grp->hw_index = aspeed_i3c_master_get_free_pos(master);
+			if (dev_grp->hw_index < 0)
+				goto out;
+
+			master->free_pos &= ~BIT(dev_grp->hw_index);
+			val &= dev_grp->mask.clr;
+			val |= dev_grp->mask.set;
+			writel(val, master->regs + DEV_ADDR_TABLE_LOC(
+							   master->datstartaddr,
+							   dev_grp->hw_index));
+		}
+	} else {
+		dev_grp->free_pos |= BIT(idx);
+
+		/*
+		 * release the hw dat resource if all the members in the group
+		 * are free.
+		 */
+		if (dev_grp->free_pos == ALL_DEVS_IN_GROUP_ARE_FREE) {
+			writel(dev_grp->mask.set,
+			       master->regs +
+				       DEV_ADDR_TABLE_LOC(master->datstartaddr,
+							  dev_grp->hw_index));
+			master->free_pos |= BIT(dev_grp->hw_index);
+			dev_grp->hw_index = -1;
+		}
+	}
+out:
+	return dev_grp->hw_index;
+}
+
+static u32 aspeed_i3c_master_get_group_dat(struct aspeed_i3c_master *master, u8 addr)
+{
+	struct aspeed_i3c_dev_group *dev_grp = &master->dev_group[ADDR_GRP(addr)];
+
+	return dev_grp->dat[ADDR_HID(addr)];
+}
+
+static int aspeed_i3c_master_get_group_hw_index(struct aspeed_i3c_master *master,
+					    u8 addr)
+{
+	struct aspeed_i3c_dev_group *dev_grp = &master->dev_group[ADDR_GRP(addr)];
+
+	return dev_grp->hw_index;
+}
+
+static struct aspeed_i3c_dev_group *
+aspeed_i3c_master_get_group(struct aspeed_i3c_master *master, u8 addr)
+{
+	return &master->dev_group[ADDR_GRP(addr)];
+}
+
+static int aspeed_i3c_master_sync_hw_dat(struct aspeed_i3c_master *master, u8 addr)
+{
+	struct aspeed_i3c_dev_group *dev_grp = &master->dev_group[ADDR_GRP(addr)];
+	u32 dat = dev_grp->dat[ADDR_HID(addr)];
+	int hw_index = dev_grp->hw_index;
+
+	if (!dat || hw_index < 0)
+		return -1;
+
+	dat &= ~dev_grp->mask.clr;
+	dat |= dev_grp->mask.set;
+	writel(dat, master->regs +
+			    DEV_ADDR_TABLE_LOC(master->datstartaddr, hw_index));
+	return hw_index;
+}
+
+static void aspeed_i3c_master_wr_tx_fifo(struct aspeed_i3c_master *master,
+				     const u8 *bytes, int nbytes)
+{
+	/*
+	 * ensure all memory accesses are done before we move the data from
+	 * memory to the hardware FIFO
+	 */
+	wmb();
+
+	writesl(master->regs + RX_TX_DATA_PORT, bytes, nbytes / 4);
+	if (nbytes & 3) {
+		u32 tmp = 0;
+
+		memcpy(&tmp, bytes + (nbytes & ~3), nbytes & 3);
+		writesl(master->regs + RX_TX_DATA_PORT, &tmp, 1);
+		dev_dbg(master->dev, "TX data = %08x\n", tmp);
+	}
+}
+
+static void aspeed_i3c_master_read_fifo(struct aspeed_i3c_master *master, u32 fifo_reg,
+				    u8 *bytes, int nbytes)
+{
+	readsl(master->regs + fifo_reg, bytes, nbytes / 4);
+	if (nbytes & 3) {
+		u32 tmp;
+
+		readsl(master->regs + fifo_reg, &tmp, 1);
+		memcpy(bytes + (nbytes & ~3), &tmp, nbytes & 3);
+	}
+}
+
+static void aspeed_i3c_master_read_rx_fifo(struct aspeed_i3c_master *master,
+					      u8 *bytes, int nbytes)
+{
+	aspeed_i3c_master_read_fifo(master, RX_TX_DATA_PORT, bytes, nbytes);
+}
+
+static void aspeed_i3c_master_read_ibi_fifo(struct aspeed_i3c_master *master,
+					       u8 *bytes, int nbytes)
+{
+	aspeed_i3c_master_read_fifo(master, IBI_QUEUE_DATA, bytes, nbytes);
+}
+
+static struct aspeed_i3c_xfer *
+aspeed_i3c_master_alloc_xfer(struct aspeed_i3c_master *master, unsigned int ncmds)
+{
+	struct aspeed_i3c_xfer *xfer;
+
+	xfer = kzalloc(struct_size(xfer, cmds, ncmds), GFP_KERNEL);
+	if (!xfer)
+		return NULL;
+
+	INIT_LIST_HEAD(&xfer->node);
+	xfer->ncmds = ncmds;
+	xfer->ret = -ETIMEDOUT;
+
+	return xfer;
+}
+
+static void aspeed_i3c_master_free_xfer(struct aspeed_i3c_xfer *xfer)
+{
+	kfree(xfer);
+}
+
+static void aspeed_i3c_master_start_xfer_locked(struct aspeed_i3c_master *master)
+{
+	struct aspeed_i3c_xfer *xfer = master->xferqueue.cur;
+	unsigned int i;
+	u32 thld_ctrl;
+
+	if (!xfer)
+		return;
+
+	for (i = 0; i < xfer->ncmds; i++) {
+		struct aspeed_i3c_cmd *cmd = &xfer->cmds[i];
+
+		aspeed_i3c_master_wr_tx_fifo(master, cmd->tx_buf, cmd->tx_len);
+	}
+
+	thld_ctrl = readl(master->regs + QUEUE_THLD_CTRL);
+	thld_ctrl &= ~QUEUE_THLD_CTRL_RESP_BUF_MASK;
+	thld_ctrl |= QUEUE_THLD_CTRL_RESP_BUF(xfer->ncmds);
+	writel(thld_ctrl, master->regs + QUEUE_THLD_CTRL);
+
+	for (i = 0; i < xfer->ncmds; i++) {
+		struct aspeed_i3c_cmd *cmd = &xfer->cmds[i];
+
+		writel(cmd->cmd_hi, master->regs + COMMAND_QUEUE_PORT);
+		writel(cmd->cmd_lo, master->regs + COMMAND_QUEUE_PORT);
+	}
+}
+
+static void aspeed_i3c_master_enqueue_xfer(struct aspeed_i3c_master *master,
+				       struct aspeed_i3c_xfer *xfer)
+{
+	unsigned long flags;
+
+	init_completion(&xfer->comp);
+	spin_lock_irqsave(&master->xferqueue.lock, flags);
+	if (master->xferqueue.cur) {
+		list_add_tail(&xfer->node, &master->xferqueue.list);
+	} else {
+		master->xferqueue.cur = xfer;
+		aspeed_i3c_master_start_xfer_locked(master);
+	}
+	spin_unlock_irqrestore(&master->xferqueue.lock, flags);
+}
+
+static void aspeed_i3c_master_dequeue_xfer_locked(struct aspeed_i3c_master *master,
+					      struct aspeed_i3c_xfer *xfer)
+{
+	if (master->xferqueue.cur == xfer) {
+		u32 status;
+
+		master->xferqueue.cur = NULL;
+
+		writel(RESET_CTRL_RX_FIFO | RESET_CTRL_TX_FIFO |
+		       RESET_CTRL_RESP_QUEUE | RESET_CTRL_CMD_QUEUE,
+		       master->regs + RESET_CTRL);
+
+		readl_poll_timeout_atomic(master->regs + RESET_CTRL, status,
+					  !status, 10, 1000000);
+	} else {
+		list_del_init(&xfer->node);
+	}
+}
+
+static void aspeed_i3c_master_dequeue_xfer(struct aspeed_i3c_master *master,
+				       struct aspeed_i3c_xfer *xfer)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&master->xferqueue.lock, flags);
+	aspeed_i3c_master_dequeue_xfer_locked(master, xfer);
+	spin_unlock_irqrestore(&master->xferqueue.lock, flags);
+}
+
+static void aspeed_i3c_master_sir_handler(struct aspeed_i3c_master *master,
+				      u32 ibi_status)
+{
+	struct aspeed_i3c_i2c_dev_data *data;
+	struct i3c_dev_desc *dev;
+	struct i3c_ibi_slot *slot;
+	u8 addr = IBI_QUEUE_IBI_ADDR(ibi_status);
+	u8 length = IBI_QUEUE_STATUS_DATA_LEN(ibi_status);
+	u8 *buf;
+	bool data_consumed = false;
+
+	dev = master->ibi.slots[addr];
+	if (!dev) {
+		pr_warn("no matching dev\n");
+		goto out;
+	}
+
+	spin_lock(&master->ibi.lock);
+	data = i3c_dev_get_master_data(dev);
+	slot = i3c_generic_ibi_get_free_slot(data->ibi_pool);
+	if (!slot) {
+		pr_err("no free ibi slot\n");
+		goto out_unlock;
+	}
+	master->ibi.received_ibi_len[addr] += length;
+	if (master->ibi.received_ibi_len[addr] >
+	    slot->dev->ibi->max_payload_len) {
+		pr_err("received ibi payload %d > device requested buffer %d",
+		       master->ibi.received_ibi_len[addr],
+		       slot->dev->ibi->max_payload_len);
+		goto out_unlock;
+	}
+	if (ibi_status & IBI_QUEUE_STATUS_LAST_FRAG)
+		master->ibi.received_ibi_len[addr] = 0;
+	buf = slot->data;
+	/* prepend ibi status */
+	memcpy(buf, &ibi_status, sizeof(ibi_status));
+	buf += sizeof(ibi_status);
+
+	aspeed_i3c_master_read_ibi_fifo(master, buf, length);
+	slot->len = length + sizeof(ibi_status);
+	i3c_master_queue_ibi(dev, slot);
+	data_consumed = true;
+out_unlock:
+	spin_unlock(&master->ibi.lock);
+
+out:
+	/* Consume data from the FIFO if it's not been done already. */
+	if (!data_consumed) {
+		int nwords = (length + 3) >> 2;
+		int i;
+
+		for (i = 0; i < nwords; i++)
+			readl(master->regs + IBI_QUEUE_DATA);
+		if (FIELD_GET(CM_TFR_STS,
+			      readl(master->regs + PRESENT_STATE)) ==
+		    CM_TFR_STS_MASTER_SERV_IBI)
+			aspeed_i3c_gen_tbits_in(master);
+		master->ibi.received_ibi_len[addr] = 0;
+	}
+}
+
+static void aspeed_i3c_master_demux_ibis(struct aspeed_i3c_master *master)
+{
+	u32 nibi, status;
+	int i;
+	u8 addr;
+
+	nibi = readl(master->regs + QUEUE_STATUS_LEVEL);
+	nibi = QUEUE_STATUS_IBI_STATUS_CNT(nibi);
+	if (!nibi)
+		return;
+
+	for (i = 0; i < nibi; i++) {
+		status = readl(master->regs + IBI_QUEUE_STATUS);
+		addr = IBI_QUEUE_IBI_ADDR(status);
+
+		/* FIXME: how to handle the unrecognized slave? */
+		if (status & IBI_QUEUE_STATUS_RSP_NACK)
+			pr_warn_once("ibi from unrecognized slave %02x\n",
+				     addr);
+
+		if (IBI_TYPE_SIR(status))
+			aspeed_i3c_master_sir_handler(master, status);
+
+		if (IBI_TYPE_HJ(status))
+			queue_work(master->base.wq, &master->hj_work);
+
+		if (IBI_TYPE_MR(status))
+			pr_info("get mr from %02x\n", addr);
+	}
+}
+
+static void aspeed_i3c_master_end_xfer_locked(struct aspeed_i3c_master *master, u32 isr)
+{
+	struct aspeed_i3c_xfer *xfer = master->xferqueue.cur;
+	int i, ret = 0;
+	u32 nresp;
+
+	if (!xfer)
+		return;
+
+	nresp = readl(master->regs + QUEUE_STATUS_LEVEL);
+	nresp = QUEUE_STATUS_LEVEL_RESP(nresp);
+
+	for (i = 0; i < nresp; i++) {
+		struct aspeed_i3c_cmd *cmd;
+		u32 resp;
+
+		resp = readl(master->regs + RESPONSE_QUEUE_PORT);
+
+		cmd = &xfer->cmds[RESPONSE_PORT_TID(resp)];
+		cmd->rx_len = RESPONSE_PORT_DATA_LEN(resp);
+		cmd->error = RESPONSE_PORT_ERR_STATUS(resp);
+		if (cmd->rx_len && !cmd->error)
+			aspeed_i3c_master_read_rx_fifo(master, cmd->rx_buf,
+						   cmd->rx_len);
+	}
+
+	for (i = 0; i < nresp; i++) {
+		switch (xfer->cmds[i].error) {
+		case RESPONSE_NO_ERROR:
+			break;
+		case RESPONSE_ERROR_PARITY:
+		case RESPONSE_ERROR_IBA_NACK:
+		case RESPONSE_ERROR_TRANSF_ABORT:
+		case RESPONSE_ERROR_CRC:
+		case RESPONSE_ERROR_FRAME:
+		case RESPONSE_ERROR_PEC_ERR:
+			ret = -EIO;
+			break;
+		case RESPONSE_ERROR_OVER_UNDER_FLOW:
+			ret = -ENOSPC;
+			break;
+		case RESPONSE_ERROR_I2C_W_NACK_ERR:
+		case RESPONSE_ERROR_ADDRESS_NACK:
+		default:
+			ret = -EINVAL;
+			break;
+		}
+	}
+
+	xfer->ret = ret;
+	complete(&xfer->comp);
+
+	if (ret < 0) {
+		aspeed_i3c_master_dequeue_xfer_locked(master, xfer);
+		aspeed_i3c_master_resume(master);
+	}
+
+	xfer = list_first_entry_or_null(&master->xferqueue.list,
+					struct aspeed_i3c_xfer,
+					node);
+	if (xfer)
+		list_del_init(&xfer->node);
+
+	master->xferqueue.cur = xfer;
+	aspeed_i3c_master_start_xfer_locked(master);
+}
+
+struct i3c_scl_timing_cfg {
+	unsigned long fscl;
+	u16 period_hi;
+	u16 period_lo;
+};
+
+static struct i3c_scl_timing_cfg jesd403_timing_cfg[5] = {
+	{ .fscl = I3C_BUS_TYP_I3C_SCL_RATE, .period_hi = 40, .period_lo = 40 },
+	{ .fscl = I3C_BUS_SDR1_SCL_RATE, .period_hi = 50, .period_lo = 75 },
+	{ .fscl = I3C_BUS_SDR2_SCL_RATE, .period_hi = 65, .period_lo = 100 },
+	{ .fscl = I3C_BUS_SDR3_SCL_RATE, .period_hi = 100, .period_lo = 150 },
+	{ .fscl = I3C_BUS_SDR4_SCL_RATE, .period_hi = 200, .period_lo = 300 }
+};
+
+struct i3c_scl_timing_cfg *ast2600_i3c_jesd403_scl_search(unsigned long fscl)
+{
+	int i;
+
+	for (i = 0; i < 5; i++) {
+		if (fscl == jesd403_timing_cfg[i].fscl)
+			return &jesd403_timing_cfg[i];
+	}
+
+	/* use typical 12.5M SCL if not found */
+	return &jesd403_timing_cfg[0];
+}
+
+static int calc_i2c_clk(struct aspeed_i3c_master *master, unsigned long fscl,
+			u16 *hcnt, u16 *lcnt)
+{
+	unsigned long core_rate, core_period;
+	u32 period_cnt, margin;
+	u32 hcnt_min, lcnt_min;
+
+	core_rate = master->timing.core_rate;
+	core_period = master->timing.core_period;
+
+	if (fscl <= 100000) {
+		lcnt_min = DIV_ROUND_UP(I3C_BUS_I2C_STD_TLOW_MIN_NS +
+						I3C_BUS_I2C_STD_TF_MAX_NS,
+					core_period);
+		hcnt_min = DIV_ROUND_UP(I3C_BUS_I2C_STD_THIGH_MIN_NS +
+						I3C_BUS_I2C_STD_TR_MAX_NS,
+					core_period);
+	} else if (fscl <= 400000) {
+		lcnt_min = DIV_ROUND_UP(I3C_BUS_I2C_FM_TLOW_MIN_NS +
+						I3C_BUS_I2C_FM_TF_MAX_NS,
+					core_period);
+		hcnt_min = DIV_ROUND_UP(I3C_BUS_I2C_FM_THIGH_MIN_NS +
+						I3C_BUS_I2C_FM_TR_MAX_NS,
+					core_period);
+	} else {
+		lcnt_min = DIV_ROUND_UP(I3C_BUS_I2C_FMP_TLOW_MIN_NS +
+						I3C_BUS_I2C_FMP_TF_MAX_NS,
+					core_period);
+		hcnt_min = DIV_ROUND_UP(I3C_BUS_I2C_FMP_THIGH_MIN_NS +
+						I3C_BUS_I2C_FMP_TR_MAX_NS,
+					core_period);
+	}
+
+	period_cnt = DIV_ROUND_UP(core_rate, fscl);
+	margin = (period_cnt - hcnt_min - lcnt_min) >> 1;
+	*lcnt = lcnt_min + margin;
+	*hcnt = max(period_cnt - *lcnt, hcnt_min);
+
+	return 0;
+}
+
+static int aspeed_i3c_clk_cfg(struct aspeed_i3c_master *master)
+{
+	unsigned long core_rate, core_period;
+	u32 scl_timing;
+	u16 hcnt, lcnt;
+
+	core_rate = master->timing.core_rate;
+	core_period = master->timing.core_period;
+
+	/* I3C PP mode */
+	if (master->timing.i3c_pp_scl_high && master->timing.i3c_pp_scl_low) {
+		hcnt = DIV_ROUND_CLOSEST(master->timing.i3c_pp_scl_high,
+					 core_period);
+		lcnt = DIV_ROUND_CLOSEST(master->timing.i3c_pp_scl_low,
+					 core_period);
+	} else if (master->base.jdec_spd) {
+		struct i3c_scl_timing_cfg *pp_timing;
+
+		pp_timing = ast2600_i3c_jesd403_scl_search(
+			master->base.bus.scl_rate.i3c);
+		hcnt = DIV_ROUND_UP(pp_timing->period_hi, core_period);
+		lcnt = DIV_ROUND_UP(pp_timing->period_lo, core_period);
+	} else {
+		hcnt = DIV_ROUND_UP(I3C_BUS_THIGH_MAX_NS, core_period) - 1;
+		if (hcnt < SCL_I3C_TIMING_CNT_MIN)
+			hcnt = SCL_I3C_TIMING_CNT_MIN;
+
+		lcnt = DIV_ROUND_UP(core_rate, I3C_BUS_TYP_I3C_SCL_RATE) - hcnt;
+		if (lcnt < SCL_I3C_TIMING_CNT_MIN)
+			lcnt = SCL_I3C_TIMING_CNT_MIN;
+	}
+	hcnt = min_t(u16, hcnt, FIELD_MAX(SCL_I3C_TIMING_HCNT));
+	lcnt = min_t(u16, lcnt, FIELD_MAX(SCL_I3C_TIMING_LCNT));
+	scl_timing = FIELD_PREP(SCL_I3C_TIMING_HCNT, hcnt) |
+		     FIELD_PREP(SCL_I3C_TIMING_LCNT, lcnt);
+	writel(scl_timing, master->regs + SCL_I3C_PP_TIMING);
+
+	/* I3C OD mode:
+	 * User defined
+	 *     check if hcnt/lcnt exceed the max value of the register
+	 *
+	 * JESD403 timing constrain for I2C/I3C OP mode
+	 *     tHIGH > 260, tLOW > 500 (same with MIPI 1.1 FMP constrain)
+	 *
+	 * MIPI 1.1 timing constrain for I3C OP mode
+	 *     tHIGH < 41, tLOW > 200
+	 */
+	if (master->timing.i3c_od_scl_high && master->timing.i3c_od_scl_low) {
+		hcnt = DIV_ROUND_CLOSEST(master->timing.i3c_od_scl_high,
+					 core_period);
+		lcnt = DIV_ROUND_CLOSEST(master->timing.i3c_od_scl_low,
+					 core_period);
+	} else if (master->base.jdec_spd) {
+		calc_i2c_clk(master, I3C_BUS_I2C_FM_PLUS_SCL_RATE, &hcnt, &lcnt);
+	} else {
+		lcnt = DIV_ROUND_UP(I3C_BUS_TLOW_OD_MIN_NS, core_period);
+		scl_timing = readl(master->regs + SCL_I3C_PP_TIMING);
+		hcnt = FIELD_GET(SCL_I3C_TIMING_HCNT, scl_timing);
+	}
+	hcnt = min_t(u16, hcnt, FIELD_MAX(SCL_I3C_TIMING_HCNT));
+	lcnt = min_t(u16, lcnt, FIELD_MAX(SCL_I3C_TIMING_LCNT));
+	scl_timing = FIELD_PREP(SCL_I3C_TIMING_HCNT, hcnt) |
+		     FIELD_PREP(SCL_I3C_TIMING_LCNT, lcnt);
+	writel(scl_timing, master->regs + SCL_I3C_OD_TIMING);
+
+	/* I2C FM mode */
+	calc_i2c_clk(master, master->base.bus.scl_rate.i2c, &hcnt, &lcnt);
+	scl_timing = FIELD_PREP(SCL_I2C_FM_TIMING_HCNT, hcnt) |
+		     FIELD_PREP(SCL_I2C_FM_TIMING_LCNT, lcnt);
+	writel(scl_timing, master->regs + SCL_I2C_FM_TIMING);
+
+	/*
+	 * I3C register 0xd4[15:0] BUS_FREE_TIMING used to control several parameters:
+	 * - tCAS & tCASr (tHD_STA in JESD403)
+	 * - tCBP & tCBPr (tSU_STO in JESD403)
+	 * - bus free time between a STOP condition and a START condition
+	 *
+	 * The constraints of these two parameters are different in different bus contexts
+	 * - MIPI I3C, mixed bus: 0xd4[15:0] = I2C SCL low period (handled in aspeed_i2c_clk_cfg)
+	 * - MIPI I3C, pure bus : 0xd4[15:0] = I3C SCL PP low period
+	 * - JESD403            : 0xd4[15:0] = I3C SCL OD low period
+	 */
+	if (!(readl(master->regs + DEVICE_CTRL) & DEV_CTRL_I2C_SLAVE_PRESENT)) {
+		if (master->base.jdec_spd)
+			scl_timing = readl(master->regs + SCL_I3C_OD_TIMING);
+		else
+			scl_timing = readl(master->regs + SCL_I3C_PP_TIMING);
+
+		lcnt = FIELD_GET(SCL_I3C_TIMING_LCNT, scl_timing);
+		scl_timing = BUS_I3C_AVAILABLE_TIME(0xffff);
+		scl_timing |= BUS_I3C_MST_FREE(lcnt);
+		writel(scl_timing, master->regs + BUS_FREE_TIMING);
+	}
+
+	/* Extend SDR: use PP mode hcnt */
+	scl_timing = readl(master->regs + SCL_I3C_PP_TIMING);
+	hcnt = scl_timing >> 16;
+	lcnt = DIV_ROUND_UP(core_rate, I3C_BUS_SDR1_SCL_RATE) - hcnt;
+	scl_timing = SCL_EXT_LCNT_1(lcnt);
+	lcnt = DIV_ROUND_UP(core_rate, I3C_BUS_SDR2_SCL_RATE) - hcnt;
+	scl_timing |= SCL_EXT_LCNT_2(lcnt);
+	lcnt = DIV_ROUND_UP(core_rate, I3C_BUS_SDR3_SCL_RATE) - hcnt;
+	scl_timing |= SCL_EXT_LCNT_3(lcnt);
+	lcnt = DIV_ROUND_UP(core_rate, I3C_BUS_SDR4_SCL_RATE) - hcnt;
+	scl_timing |= SCL_EXT_LCNT_4(lcnt);
+	writel(scl_timing, master->regs + SCL_EXT_LCNT_TIMING);
+
+	ast_clrsetbits(master->regs + SCL_EXT_TERMN_LCNT_TIMING, GENMASK(3, 0),
+		      I3C_BUS_EXT_TERMN_CNT);
+
+	return 0;
+}
+
+static int aspeed_i2c_clk_cfg(struct aspeed_i3c_master *master)
+{
+	unsigned long core_rate, core_period;
+	u16 hcnt, lcnt;
+	u32 scl_timing;
+
+	core_rate = master->timing.core_rate;
+	core_period = master->timing.core_period;
+
+	calc_i2c_clk(master, I3C_BUS_I2C_FM_PLUS_SCL_RATE, &hcnt, &lcnt);
+	hcnt = min_t(u16, hcnt, FIELD_MAX(SCL_I2C_FMP_TIMING_HCNT));
+	lcnt = min_t(u16, lcnt, FIELD_MAX(SCL_I2C_FMP_TIMING_LCNT));
+	scl_timing = FIELD_PREP(SCL_I2C_FMP_TIMING_HCNT, hcnt) |
+		     FIELD_PREP(SCL_I2C_FMP_TIMING_LCNT, lcnt);
+	writel(scl_timing, master->regs + SCL_I2C_FMP_TIMING);
+
+	calc_i2c_clk(master, master->base.bus.scl_rate.i2c, &hcnt, &lcnt);
+	scl_timing = FIELD_PREP(SCL_I2C_FM_TIMING_HCNT, hcnt) |
+		     FIELD_PREP(SCL_I2C_FM_TIMING_LCNT, lcnt);
+	writel(scl_timing, master->regs + SCL_I2C_FM_TIMING);
+
+	scl_timing = BUS_I3C_AVAILABLE_TIME(0xffff);
+	scl_timing |= BUS_I3C_MST_FREE(lcnt);
+	writel(scl_timing, master->regs + BUS_FREE_TIMING);
+	writel(readl(master->regs + DEVICE_CTRL) | DEV_CTRL_I2C_SLAVE_PRESENT,
+	       master->regs + DEVICE_CTRL);
+
+	return 0;
+}
+
+static int aspeed_i3c_master_set_info(struct aspeed_i3c_master *master,
+				       struct i3c_device_info *info)
+{
+#define ASPEED_SCU_REV_ID_REG 0x14
+#define ASPEED_HW_REV(x) (((x)&GENMASK(31, 16)) >> 16)
+
+	struct regmap *scu;
+	unsigned int reg;
+	u32 part_id, inst_id;
+
+	writel(PID_MANUF_ID_ASPEED << 1, master->regs + SLV_MIPI_PID_VALUE);
+
+	scu = syscon_regmap_lookup_by_phandle(master->dev->of_node, "aspeed,scu");
+	if (IS_ERR(scu)) {
+		dev_err(master->dev, "cannot to find SCU regmap\n");
+		return -ENODEV;
+	}
+	regmap_read(scu, ASPEED_SCU_REV_ID_REG, &reg);
+	part_id = ASPEED_HW_REV(reg);
+	inst_id = master->base.bus.id;
+
+	reg = SLV_PID_PART_ID(part_id) | SLV_PID_INST_ID(inst_id) |
+	      SLV_PID_DCR(0);
+	writel(reg, master->regs + SLV_PID_VALUE);
+
+	reg = readl(master->regs + SLV_CHAR_CTRL);
+	info->dcr = SLV_CHAR_GET_DCR(reg);
+	info->bcr = SLV_CHAR_GET_BCR(reg);
+	info->pid = (u64)readl(master->regs + SLV_MIPI_PID_VALUE) << 32;
+	info->pid |= readl(master->regs + SLV_PID_VALUE);
+	info->hdr_cap = I3C_CCC_HDR_MODE(I3C_HDR_DDR);
+
+	return 0;
+};
+
+static int aspeed_i3c_master_bus_init(struct i3c_master_controller *m)
+{
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	struct i3c_bus *bus = i3c_master_get_bus(m);
+	struct i3c_device_info info = { };
+	u32 thld_ctrl;
+	int ret;
+
+	aspeed_i3c_master_set_role(master);
+
+	switch (bus->mode) {
+	case I3C_BUS_MODE_MIXED_FAST:
+	case I3C_BUS_MODE_MIXED_LIMITED:
+		ret = aspeed_i2c_clk_cfg(master);
+		if (ret)
+			return ret;
+		fallthrough;
+	case I3C_BUS_MODE_PURE:
+		ret = aspeed_i3c_clk_cfg(master);
+		if (ret)
+			return ret;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	thld_ctrl = readl(master->regs + QUEUE_THLD_CTRL);
+	thld_ctrl &= ~QUEUE_THLD_CTRL_RESP_BUF_MASK;
+	writel(thld_ctrl, master->regs + QUEUE_THLD_CTRL);
+
+	thld_ctrl = readl(master->regs + DATA_BUFFER_THLD_CTRL);
+	thld_ctrl &= ~DATA_BUFFER_THLD_CTRL_RX_BUF;
+	writel(thld_ctrl, master->regs + DATA_BUFFER_THLD_CTRL);
+
+	writel(INTR_ALL, master->regs + INTR_STATUS);
+	if (master->secondary) {
+		writel(INTR_2ND_MASTER_MASK, master->regs + INTR_STATUS_EN);
+		/*
+		 * No need for INTR_IBI_UPDATED_STAT signal, check this bit
+		 * when INTR_RESP_READY_STAT signal is up.  This can guarantee
+		 * the SIR payload is ACKed by the master.
+		 */
+		writel(INTR_2ND_MASTER_MASK & ~INTR_IBI_UPDATED_STAT,
+		       master->regs + INTR_SIGNAL_EN);
+	} else {
+		writel(INTR_MASTER_MASK, master->regs + INTR_STATUS_EN);
+		writel(INTR_MASTER_MASK, master->regs + INTR_SIGNAL_EN);
+	}
+
+	memset(&info, 0, sizeof(info));
+	ret = aspeed_i3c_master_set_info(master, &info);
+	if (ret < 0)
+		return ret;
+
+	ret = i3c_master_get_free_addr(m, 0);
+	if (ret < 0)
+		return ret;
+
+	if (master->secondary)
+		writel(DEV_ADDR_STATIC_ADDR_VALID | DEV_ADDR_STATIC(ret),
+		       master->regs + DEVICE_ADDR);
+	else
+		writel(DEV_ADDR_DYNAMIC_ADDR_VALID | DEV_ADDR_DYNAMIC(ret),
+		       master->regs + DEVICE_ADDR);
+
+	info.dyn_addr = ret;
+
+	ret = i3c_master_set_info(&master->base, &info);
+	if (ret)
+		return ret;
+
+	thld_ctrl = readl(master->regs + QUEUE_THLD_CTRL);
+	thld_ctrl &=
+		~(QUEUE_THLD_CTRL_IBI_STA_MASK | QUEUE_THLD_CTRL_IBI_DAT_MASK);
+	thld_ctrl |= QUEUE_THLD_CTRL_IBI_STA(1);
+	thld_ctrl |= QUEUE_THLD_CTRL_IBI_DAT(MAX_IBI_FRAG_SIZE >> 2);
+	writel(thld_ctrl, master->regs + QUEUE_THLD_CTRL);
+
+	writel(IBI_REQ_REJECT_ALL, master->regs + IBI_SIR_REQ_REJECT);
+	writel(IBI_REQ_REJECT_ALL, master->regs + IBI_MR_REQ_REJECT);
+
+	/* For now don't support Hot-Join */
+	ast_setbits(master->regs + DEVICE_CTRL,
+		   DEV_CTRL_AUTO_HJ_DISABLE |
+		   DEV_CTRL_HOT_JOIN_NACK |
+		   DEV_CRTL_IBI_PAYLOAD_EN);
+
+	ret = aspeed_i3c_master_enable(master);
+	if (ret)
+		return ret;
+	/* workaround for aspeed slave devices.  The aspeed slave devices need
+	 * for a dummy ccc and resume before accessing. Hide this workarond here
+	 * and later the i3c subsystem code will do the rstdaa again.
+	 */
+	if (!master->secondary)
+		i3c_master_rstdaa_locked(m, I3C_BROADCAST_ADDR);
+
+	return 0;
+}
+
+static void aspeed_i3c_master_bus_cleanup(struct i3c_master_controller *m)
+{
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+
+	aspeed_i3c_master_disable(master);
+}
+
+static void aspeed_i3c_master_bus_reset(struct i3c_master_controller *m)
+{
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	u32 reset;
+	int i;
+
+	if (master->base.jdec_spd) {
+		reset = RESET_CTRL_BUS |
+			FIELD_PREP(RESET_CTRL_BUS_RESET_TYPE, BUS_RESET_TYPE_SCL_LOW);
+
+		writel(reset, master->regs + RESET_CTRL);
+	} else {
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SDA_OUT_SW_MODE_VAL | SCL_OUT_SW_MODE_VAL,
+				  SDA_OUT_SW_MODE_VAL | SCL_OUT_SW_MODE_VAL);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SDA_SW_MODE_OE | SCL_SW_MODE_OE,
+				  SDA_SW_MODE_OE | SCL_SW_MODE_OE);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SDA_OUT_SW_MODE_EN | SCL_OUT_SW_MODE_EN,
+				  SDA_OUT_SW_MODE_EN | SCL_OUT_SW_MODE_EN);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SDA_IN_SW_MODE_VAL | SCL_IN_SW_MODE_VAL,
+				  SDA_IN_SW_MODE_VAL | SCL_IN_SW_MODE_VAL);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SDA_IN_SW_MODE_EN | SCL_IN_SW_MODE_EN,
+				  SDA_IN_SW_MODE_EN | SCL_IN_SW_MODE_EN);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SCL_OUT_SW_MODE_VAL, 0);
+		for (i = 0; i < 7; i++) {
+			regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+					  SDA_OUT_SW_MODE_VAL, 0);
+			regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+					  SDA_OUT_SW_MODE_VAL,
+					  SDA_OUT_SW_MODE_VAL);
+		}
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SCL_OUT_SW_MODE_VAL, SCL_OUT_SW_MODE_VAL);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SDA_OUT_SW_MODE_VAL, 0);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SDA_OUT_SW_MODE_VAL, SDA_OUT_SW_MODE_VAL);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SDA_OUT_SW_MODE_EN | SCL_OUT_SW_MODE_EN, 0);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SDA_IN_SW_MODE_EN | SCL_IN_SW_MODE_EN, 0);
+	}
+}
+
+static int aspeed_i3c_ccc_set(struct aspeed_i3c_master *master,
+			  struct i3c_ccc_cmd *ccc)
+{
+	struct aspeed_i3c_xfer *xfer;
+	struct aspeed_i3c_cmd *cmd;
+	int ret, pos = 0;
+
+	if (ccc->id & I3C_CCC_DIRECT) {
+		pos = aspeed_i3c_master_sync_hw_dat(master, ccc->dests[0].addr);
+		if (pos < 0)
+			return pos;
+	}
+
+	xfer = aspeed_i3c_master_alloc_xfer(master, 1);
+	if (!xfer)
+		return -ENOMEM;
+
+	cmd = xfer->cmds;
+	cmd->tx_buf = ccc->dests[0].payload.data;
+	cmd->tx_len = ccc->dests[0].payload.len;
+
+	cmd->cmd_hi = COMMAND_PORT_ARG_DATA_LEN(ccc->dests[0].payload.len) |
+		      COMMAND_PORT_TRANSFER_ARG | COMMAND_PORT_ARG_DBP(ccc->db);
+
+	cmd->cmd_lo = COMMAND_PORT_CP |
+		      COMMAND_PORT_DEV_INDEX(pos) |
+		      COMMAND_PORT_CMD(ccc->id) |
+		      COMMAND_PORT_TOC |
+		      COMMAND_PORT_ROC |
+		      COMMAND_PORT_DBP(ccc->dbp);
+
+	if (ccc->id == I3C_CCC_SETHID || ccc->id == I3C_CCC_DEVCTRL)
+		cmd->cmd_lo |= COMMAND_PORT_SPEED(SPEED_I3C_I2C_FM);
+
+	dev_dbg(master->dev, "%s:cmd_hi=0x%08x cmd_lo=0x%08x tx_len=%d id=%x\n",
+		__func__, cmd->cmd_hi, cmd->cmd_lo, cmd->tx_len, ccc->id);
+
+	aspeed_i3c_master_enqueue_xfer(master, xfer);
+	if (!wait_for_completion_timeout(&xfer->comp, XFER_TIMEOUT))
+		aspeed_i3c_master_dequeue_xfer(master, xfer);
+
+	ret = xfer->ret;
+	if (ret)
+		dev_err(master->dev, "xfer error: %x\n", xfer->cmds[0].error);
+	if (xfer->cmds[0].error == RESPONSE_ERROR_IBA_NACK)
+		ccc->err = I3C_ERROR_M2;
+
+	aspeed_i3c_master_free_xfer(xfer);
+
+	return ret;
+}
+
+static int aspeed_i3c_ccc_get(struct aspeed_i3c_master *master, struct i3c_ccc_cmd *ccc)
+{
+	struct aspeed_i3c_xfer *xfer;
+	struct aspeed_i3c_cmd *cmd;
+	int ret, pos;
+
+	pos = aspeed_i3c_master_sync_hw_dat(master, ccc->dests[0].addr);
+	if (pos < 0)
+		return pos;
+
+	xfer = aspeed_i3c_master_alloc_xfer(master, 1);
+	if (!xfer)
+		return -ENOMEM;
+
+	cmd = xfer->cmds;
+	cmd->rx_buf = ccc->dests[0].payload.data;
+	cmd->rx_len = ccc->dests[0].payload.len;
+
+	cmd->cmd_hi = COMMAND_PORT_ARG_DATA_LEN(ccc->dests[0].payload.len) |
+		      COMMAND_PORT_TRANSFER_ARG | COMMAND_PORT_ARG_DBP(ccc->db);
+
+	cmd->cmd_lo = COMMAND_PORT_READ_TRANSFER |
+		      COMMAND_PORT_CP |
+		      COMMAND_PORT_DEV_INDEX(pos) |
+		      COMMAND_PORT_CMD(ccc->id) |
+		      COMMAND_PORT_TOC |
+		      COMMAND_PORT_ROC |
+		      COMMAND_PORT_DBP(ccc->dbp);
+
+	dev_dbg(master->dev, "%s:cmd_hi=0x%08x cmd_lo=0x%08x rx_len=%d id=%x\n",
+		__func__, cmd->cmd_hi, cmd->cmd_lo, cmd->rx_len, ccc->id);
+
+	aspeed_i3c_master_enqueue_xfer(master, xfer);
+	if (!wait_for_completion_timeout(&xfer->comp, XFER_TIMEOUT))
+		aspeed_i3c_master_dequeue_xfer(master, xfer);
+
+	ret = xfer->ret;
+	if (ret)
+		dev_err(master->dev, "xfer error: %x\n", xfer->cmds[0].error);
+	if (xfer->cmds[0].error == RESPONSE_ERROR_IBA_NACK)
+		ccc->err = I3C_ERROR_M2;
+	aspeed_i3c_master_free_xfer(xfer);
+
+	return ret;
+}
+
+static int aspeed_i3c_master_send_ccc_cmd(struct i3c_master_controller *m,
+				      struct i3c_ccc_cmd *ccc)
+{
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	int ret = 0;
+	u32 i3c_pp_timing, i3c_od_timing;
+
+	if (ccc->id == I3C_CCC_ENTDAA)
+		return -EINVAL;
+
+	i3c_od_timing = readl(master->regs + SCL_I3C_OD_TIMING);
+	i3c_pp_timing = readl(master->regs + SCL_I3C_PP_TIMING);
+
+	dev_dbg(master->dev, "ccc-id %02x rnw=%d\n", ccc->id, ccc->rnw);
+
+	if (ccc->rnw)
+		ret = aspeed_i3c_ccc_get(master, ccc);
+	else
+		ret = aspeed_i3c_ccc_set(master, ccc);
+
+	return ret;
+}
+
+#define IS_MANUF_ID_ASPEED(x) (I3C_PID_MANUF_ID(x) == PID_MANUF_ID_ASPEED)
+#define IS_PART_ID_AST2600_SERIES(x)                                           \
+	((I3C_PID_PART_ID(x) & PID_PART_ID_AST2600_SERIES) ==                  \
+	 PID_PART_ID_AST2600_SERIES)
+#define IS_PART_ID_AST1030_A0(x)                                               \
+	((I3C_PID_PART_ID(x) & PID_PART_ID_AST1030_A0) ==                      \
+	 PID_PART_ID_AST1030_A0)
+
+static int aspeed_i3c_master_daa(struct i3c_master_controller *m)
+{
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	struct aspeed_i3c_xfer *xfer;
+	struct aspeed_i3c_cmd *cmd;
+	u32 olddevs, newdevs, dat;
+	u8 p, last_addr = 0, last_grp = 0;
+	int ret, pos, ndevs;
+
+	olddevs = ~(master->free_pos);
+	ndevs = 0;
+
+	/* Prepare DAT before launching DAA. */
+	for (pos = 0; pos < master->maxdevs; pos++) {
+		if (olddevs & BIT(pos))
+			continue;
+
+		ret = i3c_master_get_free_addr(m, (last_grp + 1) << ADDR_GRP_SHIFT);
+		if (ret < 0)
+			break;
+
+		ndevs++;
+
+		master->addrs[pos] = ret;
+		p = even_parity(ret);
+		last_addr = ret;
+		last_grp = ADDR_GRP(last_addr);
+
+		dat = readl(master->regs +
+			    DEV_ADDR_TABLE_LOC(master->datstartaddr, pos));
+		dat &= ~(DEV_ADDR_TABLE_DYNAMIC_ADDR |
+			 DEV_ADDR_TABLE_DA_PARITY);
+		dat |= FIELD_PREP(DEV_ADDR_TABLE_DYNAMIC_ADDR, ret) |
+		       FIELD_PREP(DEV_ADDR_TABLE_DA_PARITY, p);
+		writel(dat, master->regs + DEV_ADDR_TABLE_LOC(
+						   master->datstartaddr, pos));
+	}
+
+	if (!ndevs)
+		return -ENOSPC;
+
+	xfer = aspeed_i3c_master_alloc_xfer(master, 1);
+	if (!xfer)
+		return -ENOMEM;
+
+	pos = aspeed_i3c_master_get_free_pos(master);
+	if (pos < 0) {
+		aspeed_i3c_master_free_xfer(xfer);
+		return pos;
+	}
+	cmd = &xfer->cmds[0];
+	cmd->cmd_hi = 0x1;
+	cmd->cmd_lo = COMMAND_PORT_DEV_COUNT(ndevs) |
+		      COMMAND_PORT_DEV_INDEX(pos) |
+		      COMMAND_PORT_CMD(I3C_CCC_ENTDAA) |
+		      COMMAND_PORT_ADDR_ASSGN_CMD |
+		      COMMAND_PORT_TOC |
+		      COMMAND_PORT_ROC;
+
+	aspeed_i3c_master_enqueue_xfer(master, xfer);
+	if (!wait_for_completion_timeout(&xfer->comp, XFER_TIMEOUT))
+		aspeed_i3c_master_dequeue_xfer(master, xfer);
+
+	newdevs = GENMASK(ndevs - cmd->rx_len - 1, 0) << pos;
+	for (pos = 0; pos < master->maxdevs; pos++) {
+		if (newdevs & BIT(pos)) {
+			u32 addr;
+
+			dat = GET_DAT_FROM_POS(master, pos);
+			addr = FIELD_GET(DEV_ADDR_TABLE_DYNAMIC_ADDR, dat);
+
+			aspeed_i3c_master_set_group_dat(master, addr, dat);
+			i3c_master_add_i3c_dev_locked(m, addr);
+		}
+
+		/* cleanup the free HW DATs */
+		if (master->free_pos & BIT(pos)) {
+			dat = readl(
+				master->regs +
+				DEV_ADDR_TABLE_LOC(master->datstartaddr, pos));
+			dat &= ~(DEV_ADDR_TABLE_DYNAMIC_ADDR |
+				 DEV_ADDR_TABLE_DA_PARITY);
+			dat |= FIELD_PREP(DEV_ADDR_TABLE_DA_PARITY,
+					  even_parity(0));
+			writel(dat, master->regs +
+					    DEV_ADDR_TABLE_LOC(
+						    master->datstartaddr, pos));
+		}
+	}
+
+	aspeed_i3c_master_free_xfer(xfer);
+
+	return 0;
+}
+#ifdef CONFIG_AST2600_I3C_CCC_WORKAROUND
+/*
+ * Provide an interface for sending CCC from userspace.  Especially for the
+ * transfers with PEC and direct CCC.
+ */
+static int aspeed_i3c_master_ccc_xfers(struct i3c_dev_desc *dev,
+				    struct i3c_priv_xfer *i3c_xfers,
+				    int i3c_nxfers)
+{
+	struct aspeed_i3c_i2c_dev_data *data = i3c_dev_get_master_data(dev);
+	struct i3c_master_controller *m = i3c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	struct aspeed_i3c_xfer *xfer;
+	int i, ret = 0;
+	struct aspeed_i3c_cmd *cmd_ccc;
+
+	xfer = aspeed_i3c_master_alloc_xfer(master, i3c_nxfers);
+	if (!xfer)
+		return -ENOMEM;
+
+	/* i3c_xfers[0] handles the CCC data */
+	cmd_ccc = &xfer->cmds[0];
+	cmd_ccc->cmd_hi = COMMAND_PORT_ARG_DATA_LEN(i3c_xfers[0].len - 1) |
+			  COMMAND_PORT_TRANSFER_ARG;
+	cmd_ccc->tx_buf = i3c_xfers[0].data.out + 1;
+	cmd_ccc->tx_len = i3c_xfers[0].len - 1;
+	cmd_ccc->cmd_lo = COMMAND_PORT_SPEED(dev->info.max_write_ds);
+	cmd_ccc->cmd_lo |= COMMAND_PORT_TID(0) |
+			   COMMAND_PORT_DEV_INDEX(master->maxdevs - 1) |
+			   COMMAND_PORT_ROC;
+	if (i3c_nxfers == 1)
+		cmd_ccc->cmd_lo |= COMMAND_PORT_TOC;
+
+	dev_dbg(master->dev,
+		"%s:cmd_ccc_hi=0x%08x cmd_ccc_lo=0x%08x tx_len=%d\n", __func__,
+		cmd_ccc->cmd_hi, cmd_ccc->cmd_lo, cmd_ccc->tx_len);
+
+	for (i = 1; i < i3c_nxfers; i++) {
+		struct aspeed_i3c_cmd *cmd = &xfer->cmds[i];
+
+		cmd->cmd_hi = COMMAND_PORT_ARG_DATA_LEN(i3c_xfers[i].len) |
+			COMMAND_PORT_TRANSFER_ARG;
+
+		if (i3c_xfers[i].rnw) {
+			cmd->rx_buf = i3c_xfers[i].data.in;
+			cmd->rx_len = i3c_xfers[i].len;
+			cmd->cmd_lo = COMMAND_PORT_READ_TRANSFER |
+				      COMMAND_PORT_SPEED(dev->info.max_read_ds);
+
+		} else {
+			cmd->tx_buf = i3c_xfers[i].data.out;
+			cmd->tx_len = i3c_xfers[i].len;
+			cmd->cmd_lo =
+				COMMAND_PORT_SPEED(dev->info.max_write_ds);
+		}
+
+		cmd->cmd_lo |= COMMAND_PORT_TID(i) |
+			       COMMAND_PORT_DEV_INDEX(data->index) |
+			       COMMAND_PORT_ROC;
+
+		if (i == (i3c_nxfers - 1))
+			cmd->cmd_lo |= COMMAND_PORT_TOC;
+
+		dev_dbg(master->dev,
+			"%s:cmd_hi=0x%08x cmd_lo=0x%08x tx_len=%d rx_len=%d\n",
+			__func__, cmd->cmd_hi, cmd->cmd_lo, cmd->tx_len,
+			cmd->rx_len);
+	}
+
+	aspeed_i3c_master_enqueue_xfer(master, xfer);
+	if (!wait_for_completion_timeout(&xfer->comp, XFER_TIMEOUT))
+		aspeed_i3c_master_dequeue_xfer(master, xfer);
+
+	ret = xfer->ret;
+	aspeed_i3c_master_free_xfer(xfer);
+
+	return ret;
+}
+#endif
+static int aspeed_i3c_master_priv_xfers(struct i3c_dev_desc *dev,
+				    struct i3c_priv_xfer *i3c_xfers,
+				    int i3c_nxfers)
+{
+	struct aspeed_i3c_i2c_dev_data *data = i3c_dev_get_master_data(dev);
+	struct i3c_master_controller *m = i3c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	unsigned int nrxwords = 0, ntxwords = 0;
+	struct aspeed_i3c_xfer *xfer;
+	int i, ret = 0;
+
+	if (!i3c_nxfers)
+		return 0;
+
+	if (i3c_nxfers > master->caps.cmdfifodepth)
+		return -ENOTSUPP;
+
+	for (i = 0; i < i3c_nxfers; i++) {
+		if (i3c_xfers[i].rnw)
+			nrxwords += DIV_ROUND_UP(i3c_xfers[i].len, 4);
+		else
+			ntxwords += DIV_ROUND_UP(i3c_xfers[i].len, 4);
+	}
+
+	if (ntxwords > master->caps.datafifodepth ||
+	    nrxwords > master->caps.datafifodepth)
+		return -ENOTSUPP;
+
+#ifdef CONFIG_AST2600_I3C_CCC_WORKAROUND
+	if (i3c_xfers[0].rnw == 0) {
+		/* write command: check if hit special address */
+		u8 tmp;
+
+		memcpy(&tmp, i3c_xfers[0].data.out, 1);
+		if (tmp == 0xff)
+			return aspeed_i3c_master_ccc_xfers(dev, i3c_xfers, i3c_nxfers);
+	}
+#endif
+
+	xfer = aspeed_i3c_master_alloc_xfer(master, i3c_nxfers);
+	if (!xfer)
+		return -ENOMEM;
+
+	data->index = aspeed_i3c_master_sync_hw_dat(master, dev->info.dyn_addr);
+
+	for (i = 0; i < i3c_nxfers; i++) {
+		struct aspeed_i3c_cmd *cmd = &xfer->cmds[i];
+
+		cmd->cmd_hi = COMMAND_PORT_ARG_DATA_LEN(i3c_xfers[i].len) |
+			COMMAND_PORT_TRANSFER_ARG;
+
+		if (i3c_xfers[i].rnw) {
+			cmd->rx_buf = i3c_xfers[i].data.in;
+			cmd->rx_len = i3c_xfers[i].len;
+			cmd->cmd_lo = COMMAND_PORT_READ_TRANSFER |
+				      COMMAND_PORT_SPEED(dev->info.max_read_ds);
+
+		} else {
+			cmd->tx_buf = i3c_xfers[i].data.out;
+			cmd->tx_len = i3c_xfers[i].len;
+			cmd->cmd_lo =
+				COMMAND_PORT_SPEED(dev->info.max_write_ds);
+		}
+
+		cmd->cmd_lo |= COMMAND_PORT_TID(i) |
+			       COMMAND_PORT_DEV_INDEX(data->index) |
+			       COMMAND_PORT_ROC;
+
+		if (i == (i3c_nxfers - 1))
+			cmd->cmd_lo |= COMMAND_PORT_TOC;
+
+		if (dev->info.pec)
+			cmd->cmd_lo |= COMMAND_PORT_PEC;
+
+		dev_dbg(master->dev,
+			"%s:cmd_hi=0x%08x cmd_lo=0x%08x tx_len=%d rx_len=%d\n",
+			__func__, cmd->cmd_hi, cmd->cmd_lo, cmd->tx_len,
+			cmd->rx_len);
+	}
+
+	aspeed_i3c_master_enqueue_xfer(master, xfer);
+	if (!wait_for_completion_timeout(&xfer->comp, XFER_TIMEOUT))
+		aspeed_i3c_master_dequeue_xfer(master, xfer);
+
+	for (i = 0; i < i3c_nxfers; i++) {
+		struct aspeed_i3c_cmd *cmd = &xfer->cmds[i];
+
+		if (i3c_xfers[i].rnw)
+			i3c_xfers[i].len = cmd->rx_len;
+	}
+
+	ret = xfer->ret;
+	if (ret)
+		dev_err(master->dev, "xfer error: %x\n", xfer->cmds[0].error);
+	aspeed_i3c_master_free_xfer(xfer);
+
+	return ret;
+}
+
+static int aspeed_i3c_master_send_hdr_cmd(struct i3c_master_controller *m,
+					  struct i3c_hdr_cmd *cmds,
+					  int ncmds)
+{
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	u8 dat_index;
+	int ret, i, ntxwords = 0, nrxwords = 0;
+	struct aspeed_i3c_xfer *xfer;
+
+	if (ncmds < 1)
+		return 0;
+
+	dev_dbg(master->dev, "ncmds = %x", ncmds);
+
+	if (ncmds > master->caps.cmdfifodepth)
+		return -ENOTSUPP;
+
+	for (i = 0; i < ncmds; i++) {
+		dev_dbg(master->dev, "cmds[%d] mode = %x", i, cmds[i].mode);
+		if (cmds[i].mode != I3C_HDR_DDR)
+			return -ENOTSUPP;
+		if (cmds[i].code & 0x80)
+			nrxwords += DIV_ROUND_UP(cmds[i].ndatawords, 2);
+		else
+			ntxwords += DIV_ROUND_UP(cmds[i].ndatawords, 2);
+	}
+	dev_dbg(master->dev, "ntxwords = %x, nrxwords = %x", ntxwords,
+		 nrxwords);
+	if (ntxwords > master->caps.datafifodepth ||
+	    nrxwords > master->caps.datafifodepth)
+		return -ENOTSUPP;
+
+	xfer = aspeed_i3c_master_alloc_xfer(master, ncmds);
+	if (!xfer)
+		return -ENOMEM;
+
+	for (i = 0; i < ncmds; i++) {
+		struct aspeed_i3c_cmd *cmd = &xfer->cmds[i];
+
+		dat_index = aspeed_i3c_master_sync_hw_dat(master, cmds[i].addr);
+
+		cmd->cmd_hi = COMMAND_PORT_ARG_DATA_LEN(cmds[i].ndatawords << 1) |
+			      COMMAND_PORT_TRANSFER_ARG;
+
+		if (cmds[i].code & 0x80) {
+			cmd->rx_buf = cmds[i].data.in;
+			cmd->rx_len = cmds[i].ndatawords << 1;
+			cmd->cmd_lo = COMMAND_PORT_READ_TRANSFER |
+				      COMMAND_PORT_CP |
+				      COMMAND_PORT_CMD(cmds[i].code) |
+				      COMMAND_PORT_SPEED(SPEED_I3C_HDR_DDR);
+
+		} else {
+			cmd->tx_buf = cmds[i].data.out;
+			cmd->tx_len = cmds[i].ndatawords << 1;
+			cmd->cmd_lo = COMMAND_PORT_CP |
+				      COMMAND_PORT_CMD(cmds[i].code) |
+				      COMMAND_PORT_SPEED(SPEED_I3C_HDR_DDR);
+		}
+
+		cmd->cmd_lo |= COMMAND_PORT_TID(i) |
+			       COMMAND_PORT_DEV_INDEX(dat_index) |
+			       COMMAND_PORT_ROC;
+
+		if (i == (ncmds - 1))
+			cmd->cmd_lo |= COMMAND_PORT_TOC;
+
+		dev_dbg(master->dev,
+			 "%s:cmd_hi=0x%08x cmd_lo=0x%08x tx_len=%d rx_len=%d\n",
+			 __func__, cmd->cmd_hi, cmd->cmd_lo, cmd->tx_len,
+			 cmd->rx_len);
+	}
+
+	aspeed_i3c_master_enqueue_xfer(master, xfer);
+	if (!wait_for_completion_timeout(&xfer->comp, XFER_TIMEOUT))
+		aspeed_i3c_master_dequeue_xfer(master, xfer);
+
+	for (i = 0; i < ncmds; i++) {
+		struct aspeed_i3c_cmd *cmd = &xfer->cmds[i];
+
+		if (cmds[i].code & 0x80)
+			cmds[i].ndatawords = DIV_ROUND_UP(cmd->rx_len, 2);
+	}
+
+	ret = xfer->ret;
+	if (ret)
+		dev_err(master->dev, "xfer error: %x\n", xfer->cmds[0].error);
+	aspeed_i3c_master_free_xfer(xfer);
+
+	return ret;
+}
+
+static int aspeed_i3c_master_reattach_i3c_dev(struct i3c_dev_desc *dev,
+					  u8 old_dyn_addr)
+{
+	struct aspeed_i3c_i2c_dev_data *data = i3c_dev_get_master_data(dev);
+	struct i3c_master_controller *m = i3c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+
+	aspeed_i3c_master_set_group_dat(
+		master, dev->info.dyn_addr,
+		FIELD_PREP(DEV_ADDR_TABLE_DYNAMIC_ADDR, dev->info.dyn_addr));
+
+	master->addrs[data->index] = dev->info.dyn_addr;
+
+	return 0;
+}
+
+static int aspeed_i3c_master_attach_i3c_dev(struct i3c_dev_desc *dev)
+{
+	struct i3c_master_controller *m = i3c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	struct aspeed_i3c_i2c_dev_data *data;
+	int pos;
+	u8 addr = dev->info.dyn_addr ? : dev->info.static_addr;
+
+	pos = aspeed_i3c_master_set_group_dat(
+		master, addr, FIELD_PREP(DEV_ADDR_TABLE_DYNAMIC_ADDR, addr));
+	if (pos < 0)
+		return pos;
+
+	data = kzalloc(sizeof(*data), GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	data->index = aspeed_i3c_master_get_group_hw_index(master, addr);
+	master->addrs[pos] = addr;
+	i3c_dev_set_master_data(dev, data);
+
+	if (master->base.jdec_spd)
+		dev->info.max_write_ds = dev->info.max_read_ds = 0;
+
+	return 0;
+}
+
+static void aspeed_i3c_master_detach_i3c_dev(struct i3c_dev_desc *dev)
+{
+	struct aspeed_i3c_i2c_dev_data *data = i3c_dev_get_master_data(dev);
+	struct i3c_master_controller *m = i3c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+
+	aspeed_i3c_master_set_group_dat(master, dev->info.dyn_addr, 0);
+
+	i3c_dev_set_master_data(dev, NULL);
+	master->addrs[data->index] = 0;
+	kfree(data);
+}
+
+static int aspeed_i3c_master_i2c_xfers(struct i2c_dev_desc *dev,
+				   const struct i2c_msg *i2c_xfers,
+				   int i2c_nxfers)
+{
+	struct aspeed_i3c_i2c_dev_data *data = i2c_dev_get_master_data(dev);
+	struct i3c_master_controller *m = i2c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	unsigned int nrxwords = 0, ntxwords = 0;
+	struct aspeed_i3c_xfer *xfer;
+	int i, ret = 0;
+
+	if (!i2c_nxfers)
+		return 0;
+
+	if (i2c_nxfers > master->caps.cmdfifodepth)
+		return -ENOTSUPP;
+
+	for (i = 0; i < i2c_nxfers; i++) {
+		if (i2c_xfers[i].flags & I2C_M_RD)
+			nrxwords += DIV_ROUND_UP(i2c_xfers[i].len, 4);
+		else
+			ntxwords += DIV_ROUND_UP(i2c_xfers[i].len, 4);
+	}
+
+	if (ntxwords > master->caps.datafifodepth ||
+	    nrxwords > master->caps.datafifodepth)
+		return -ENOTSUPP;
+
+	xfer = aspeed_i3c_master_alloc_xfer(master, i2c_nxfers);
+	if (!xfer)
+		return -ENOMEM;
+
+	data->index = aspeed_i3c_master_sync_hw_dat(master, dev->addr);
+
+	for (i = 0; i < i2c_nxfers; i++) {
+		struct aspeed_i3c_cmd *cmd = &xfer->cmds[i];
+
+		cmd->cmd_hi = COMMAND_PORT_ARG_DATA_LEN(i2c_xfers[i].len) |
+			COMMAND_PORT_TRANSFER_ARG;
+
+		cmd->cmd_lo = COMMAND_PORT_TID(i) |
+			      COMMAND_PORT_DEV_INDEX(data->index) |
+			      COMMAND_PORT_ROC;
+
+		if (i2c_xfers[i].flags & I2C_M_RD) {
+			cmd->cmd_lo |= COMMAND_PORT_READ_TRANSFER;
+			cmd->rx_buf = i2c_xfers[i].buf;
+			cmd->rx_len = i2c_xfers[i].len;
+		} else {
+			cmd->tx_buf = i2c_xfers[i].buf;
+			cmd->tx_len = i2c_xfers[i].len;
+		}
+
+		if (i == (i2c_nxfers - 1))
+			cmd->cmd_lo |= COMMAND_PORT_TOC;
+
+		dev_dbg(master->dev,
+			"%s:cmd_hi=0x%08x cmd_lo=0x%08x tx_len=%d rx_len=%d\n",
+			__func__, cmd->cmd_hi, cmd->cmd_lo, cmd->tx_len,
+			cmd->rx_len);
+	}
+
+	aspeed_i3c_master_enqueue_xfer(master, xfer);
+	if (!wait_for_completion_timeout(&xfer->comp, XFER_TIMEOUT))
+		aspeed_i3c_master_dequeue_xfer(master, xfer);
+
+	ret = xfer->ret;
+	if (ret)
+		dev_err(master->dev, "xfer error: %x\n", xfer->cmds[0].error);
+	aspeed_i3c_master_free_xfer(xfer);
+
+	return ret;
+}
+
+static int aspeed_i3c_master_attach_i2c_dev(struct i2c_dev_desc *dev)
+{
+	struct i3c_master_controller *m = i2c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	struct aspeed_i3c_i2c_dev_data *data;
+	int pos;
+
+	pos = aspeed_i3c_master_set_group_dat(
+		master, dev->addr,
+		DEV_ADDR_TABLE_LEGACY_I2C_DEV |
+			FIELD_PREP(DEV_ADDR_TABLE_STATIC_ADDR, dev->addr));
+
+	if (pos < 0)
+		return pos;
+
+	data = kzalloc(sizeof(*data), GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	data->index = aspeed_i3c_master_get_group_hw_index(master, dev->addr);
+	master->addrs[data->index] = dev->addr;
+	i2c_dev_set_master_data(dev, data);
+
+
+	return 0;
+}
+
+static void aspeed_i3c_master_detach_i2c_dev(struct i2c_dev_desc *dev)
+{
+	struct aspeed_i3c_i2c_dev_data *data = i2c_dev_get_master_data(dev);
+	struct i3c_master_controller *m = i2c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+
+	aspeed_i3c_master_set_group_dat(master, dev->addr, 0);
+
+	i2c_dev_set_master_data(dev, NULL);
+	master->addrs[data->index] = 0;
+	kfree(data);
+}
+
+static void aspeed_i3c_slave_event_handler(struct aspeed_i3c_master *master)
+{
+	u32 event = readl(master->regs + SLV_EVENT_CTRL);
+	u32 reg = readl(master->regs + PRESENT_STATE);
+	u32 cm_state = FIELD_GET(CM_TFR_STS, reg);
+
+	if (cm_state == CM_TFR_STS_SLAVE_HALT) {
+		dev_dbg(master->dev, "slave in halt state\n");
+		aspeed_i3c_master_resume(master);
+	}
+
+	dev_dbg(master->dev, "slave event=%08x\n", event);
+	if (event & SLV_EVENT_CTRL_MRL_UPD)
+		dev_dbg(master->dev, "isr: master set mrl=%d\n",
+			readl(master->regs + SLV_MAX_LEN) >> 16);
+
+	if (event & SLV_EVENT_CTRL_MWL_UPD)
+		dev_dbg(master->dev, "isr: master set mwl=%ld\n",
+			readl(master->regs + SLV_MAX_LEN) & GENMASK(15, 0));
+
+	writel(event, master->regs + SLV_EVENT_CTRL);
+}
+
+static void aspeed_i3c_slave_resp_handler(struct aspeed_i3c_master *master,
+					  u32 status)
+{
+	int i, has_error = 0;
+	u32 resp, nbytes, nresp;
+	u8 error, tid;
+	u32 *buf = master->slave_data.buf;
+	struct i3c_slave_payload payload;
+
+	nresp = readl(master->regs + QUEUE_STATUS_LEVEL);
+	nresp = QUEUE_STATUS_LEVEL_RESP(nresp);
+
+	for (i = 0; i < nresp; i++) {
+		resp = readl(master->regs + RESPONSE_QUEUE_PORT);
+		error = RESPONSE_PORT_ERR_STATUS(resp);
+		nbytes = RESPONSE_PORT_DATA_LEN(resp);
+		tid = RESPONSE_PORT_TID(resp);
+
+		if (error) {
+			has_error = 1;
+			if (error == RESPONSE_ERROR_EARLY_TERMINATE)
+				dev_dbg(master->dev,
+					"early termination, remain length %d\n",
+					nbytes);
+		}
+
+		if (!error && nbytes) {
+			aspeed_i3c_master_read_rx_fifo(master, (u8 *)buf, nbytes);
+
+			payload.len = nbytes;
+			payload.data = buf;
+			/* currently only support master write transfer */
+			if (master->slave_data.callback && (tid == TID_MASTER_WRITE_DATA))
+				master->slave_data.callback(&master->base, &payload);
+		}
+
+		if (!error && !nbytes) {
+			if (status & INTR_IBI_UPDATED_STAT && tid == TID_SLAVE_IBI_DONE)
+				complete(&master->sir_complete);
+			else if (tid == TID_MASTER_READ_DATA)
+				complete(&master->data_read_complete);
+			else
+				dev_warn(master->dev, "Unreogized response %x",
+					 resp);
+		}
+	}
+
+	if (has_error) {
+		writel(RESET_CTRL_QUEUES, master->regs + RESET_CTRL);
+		aspeed_i3c_master_resume(master);
+	}
+}
+
+static irqreturn_t aspeed_i3c_master_irq_handler(int irq, void *dev_id)
+{
+	struct aspeed_i3c_master *master = dev_id;
+	u32 status;
+
+	status = readl(master->regs + INTR_STATUS);
+
+	if (!(status & readl(master->regs + INTR_STATUS_EN))) {
+		writel(INTR_ALL, master->regs + INTR_STATUS);
+		return IRQ_NONE;
+	}
+
+	if (master->secondary) {
+		if (status & INTR_READ_REQ_RECV_STAT)
+			dev_dbg(master->dev, "read queue received\n");
+
+		if (status & INTR_RESP_READY_STAT)
+			aspeed_i3c_slave_resp_handler(master, status);
+
+		if (status & INTR_CCC_UPDATED_STAT)
+			aspeed_i3c_slave_event_handler(master);
+	} else {
+		u32 reg, cm_state, xfr_state;
+
+		if (status & INTR_RESP_READY_STAT ||
+		    status & INTR_TRANSFER_ERR_STAT) {
+			spin_lock(&master->xferqueue.lock);
+			aspeed_i3c_master_end_xfer_locked(master, status);
+			if (status & INTR_TRANSFER_ERR_STAT)
+				writel(INTR_TRANSFER_ERR_STAT, master->regs + INTR_STATUS);
+			spin_unlock(&master->xferqueue.lock);
+		}
+
+		if (status & INTR_IBI_THLD_STAT)
+			aspeed_i3c_master_demux_ibis(master);
+
+		/*
+		 * check whether the controller is in halt state and resume the
+		 * controller if it is in halt state
+		 */
+		reg = readl(master->regs + PRESENT_STATE);
+		cm_state = FIELD_GET(CM_TFR_ST_STS, reg);
+		xfr_state = FIELD_GET(CM_TFR_STS, reg);
+
+		if (cm_state == CM_TFR_ST_STS_HALT ||
+		    xfr_state == CM_TFR_STS_MASTER_HALT) {
+			dev_dbg(master->dev, "master in halt state, resume\n");
+			writel(RESET_CTRL_QUEUES, master->regs + RESET_CTRL);
+			aspeed_i3c_master_resume(master);
+		}
+	}
+
+	writel(status, master->regs + INTR_STATUS);
+
+	return IRQ_HANDLED;
+}
+
+static void aspeed_i3c_master_enable_ibi_irq(struct aspeed_i3c_master *master)
+{
+	u32 reg;
+
+	reg = readl(master->regs + INTR_STATUS_EN);
+	reg |= INTR_IBI_THLD_STAT;
+	writel(reg, master->regs + INTR_STATUS_EN);
+
+	reg = readl(master->regs + INTR_SIGNAL_EN);
+	reg |= INTR_IBI_THLD_STAT;
+	writel(reg, master->regs + INTR_SIGNAL_EN);
+}
+
+static void aspeed_i3c_master_disable_ibi_irq(struct aspeed_i3c_master *master)
+{
+	u32 reg;
+
+	reg = readl(master->regs + INTR_STATUS_EN);
+	reg &= ~INTR_IBI_THLD_STAT;
+	writel(reg, master->regs + INTR_STATUS_EN);
+
+	reg = readl(master->regs + INTR_SIGNAL_EN);
+	reg &= ~INTR_IBI_THLD_STAT;
+	writel(reg, master->regs + INTR_SIGNAL_EN);
+}
+
+static int aspeed_i3c_master_disable_ibi(struct i3c_dev_desc *dev)
+{
+	struct i3c_master_controller *m = i3c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	struct aspeed_i3c_i2c_dev_data *data = i3c_dev_get_master_data(dev);
+	struct aspeed_i3c_dev_group *dev_grp =
+		aspeed_i3c_master_get_group(master, dev->info.dyn_addr);
+	unsigned long flags;
+	u32 sirmap, dat, hj_nack;
+	int ret, i;
+	bool ibi_enable = false;
+
+	ret = i3c_master_disec_locked(m, dev->info.dyn_addr,
+				      I3C_CCC_EVENT_SIR);
+	if (ret)
+		return ret;
+
+	spin_lock_irqsave(&master->ibi.lock, flags);
+	dat = aspeed_i3c_master_get_group_dat(master, dev->info.dyn_addr);
+	dat |= DEV_ADDR_TABLE_SIR_REJECT;
+	dat &= ~DEV_ADDR_TABLE_IBI_WITH_DATA;
+	aspeed_i3c_master_set_group_dat(master, dev->info.dyn_addr, dat);
+
+	/*
+	 * if any available device in this group still needs to enable ibi, then
+	 * just keep the hw setting until all of the devices agree to disable ibi
+	 */
+	for (i = 0; i < MAX_DEVS_IN_GROUP; i++) {
+		if ((!(dev_grp->free_pos & BIT(i))) &&
+		    (!(dev_grp->dat[i] & DEV_ADDR_TABLE_SIR_REJECT))) {
+			ibi_enable = true;
+			break;
+		}
+	}
+
+	if (!ibi_enable) {
+		sirmap = readl(master->regs + IBI_SIR_REQ_REJECT);
+		sirmap |= BIT(data->ibi);
+		writel(sirmap, master->regs + IBI_SIR_REQ_REJECT);
+
+		dev_grp->mask.clr |= DEV_ADDR_TABLE_IBI_WITH_DATA |
+				     DEV_ADDR_TABLE_IBI_ADDR_MASK;
+		dev_grp->mask.set &= ~DEV_ADDR_TABLE_IBI_WITH_DATA;
+		dev_grp->mask.set |= DEV_ADDR_TABLE_SIR_REJECT;
+	}
+
+	sirmap = readl(master->regs + IBI_SIR_REQ_REJECT);
+	hj_nack = readl(master->regs + DEVICE_CTRL) & DEV_CTRL_HOT_JOIN_NACK;
+	if (sirmap == IBI_REQ_REJECT_ALL && hj_nack)
+		aspeed_i3c_master_disable_ibi_irq(master);
+	else
+		aspeed_i3c_master_enable_ibi_irq(master);
+
+	spin_unlock_irqrestore(&master->ibi.lock, flags);
+
+	return ret;
+}
+
+static int aspeed_i3c_master_enable_ibi(struct i3c_dev_desc *dev)
+{
+	struct i3c_master_controller *m = i3c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	struct aspeed_i3c_i2c_dev_data *data = i3c_dev_get_master_data(dev);
+	struct aspeed_i3c_dev_group *dev_grp =
+		aspeed_i3c_master_get_group(master, dev->info.dyn_addr);
+	unsigned long flags;
+	u32 sirmap, hj_nack;
+	u32 sirmap_backup, mask_clr_backup, mask_set_backup;
+	int ret;
+
+	spin_lock_irqsave(&master->ibi.lock, flags);
+	sirmap_backup = readl(master->regs + IBI_SIR_REQ_REJECT);
+	sirmap = sirmap_backup & ~BIT(data->ibi);
+	writel(sirmap, master->regs + IBI_SIR_REQ_REJECT);
+
+	mask_clr_backup = dev_grp->mask.clr;
+	mask_set_backup = dev_grp->mask.set;
+	dev_grp->mask.clr |= DEV_ADDR_TABLE_SIR_REJECT | DEV_ADDR_TABLE_IBI_ADDR_MASK;
+	dev_grp->mask.set &= ~DEV_ADDR_TABLE_SIR_REJECT;
+	dev_grp->mask.set |= FIELD_PREP(DEV_ADDR_TABLE_IBI_ADDR_MASK,
+					IBI_ADDR_MASK_LAST_3BITS);
+	if (IS_MANUF_ID_ASPEED(dev->info.pid))
+		dev_grp->mask.set |= DEV_ADDR_TABLE_IBI_PEC_EN;
+	if (dev->info.bcr & I3C_BCR_IBI_PAYLOAD)
+		dev_grp->mask.set |= DEV_ADDR_TABLE_IBI_WITH_DATA;
+
+	spin_unlock_irqrestore(&master->ibi.lock, flags);
+
+	dev_dbg(master->dev, "addr:%x, hw_index:%d, data->ibi:%d, mask: %08x %08x\n",
+		dev->info.dyn_addr, dev_grp->hw_index, data->ibi, dev_grp->mask.set,
+		dev_grp->mask.clr);
+
+	/* Dat will be synchronized before sending the CCC */
+	ret = i3c_master_enec_locked(m, dev->info.dyn_addr,
+				     I3C_CCC_EVENT_SIR);
+
+	if (ret) {
+		spin_lock_irqsave(&master->ibi.lock, flags);
+		writel(sirmap_backup, master->regs + IBI_SIR_REQ_REJECT);
+
+		dev_grp->mask.clr = mask_clr_backup;
+		dev_grp->mask.set = mask_set_backup;
+		aspeed_i3c_master_sync_hw_dat(master, dev->info.dyn_addr);
+		spin_unlock_irqrestore(&master->ibi.lock, flags);
+	}
+
+	sirmap = readl(master->regs + IBI_SIR_REQ_REJECT);
+	hj_nack = readl(master->regs + DEVICE_CTRL) & DEV_CTRL_HOT_JOIN_NACK;
+	if (sirmap == IBI_REQ_REJECT_ALL && hj_nack)
+		aspeed_i3c_master_disable_ibi_irq(master);
+	else
+		aspeed_i3c_master_enable_ibi_irq(master);
+
+	return ret;
+}
+
+static int aspeed_i3c_master_request_ibi(struct i3c_dev_desc *dev,
+				       const struct i3c_ibi_setup *req)
+{
+	struct i3c_master_controller *m = i3c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	struct aspeed_i3c_i2c_dev_data *data = i3c_dev_get_master_data(dev);
+	unsigned long flags;
+	unsigned int i;
+
+	data->ibi_pool = i3c_generic_ibi_alloc_pool(dev, req);
+	if (IS_ERR(data->ibi_pool))
+		return PTR_ERR(data->ibi_pool);
+
+	spin_lock_irqsave(&master->ibi.lock, flags);
+	master->ibi.slots[dev->info.dyn_addr & 0x7f] = dev;
+	master->ibi.received_ibi_len[dev->info.dyn_addr & 0x7f] = 0;
+	data->ibi = aspeed_i3c_master_get_group_hw_index(master,
+							 dev->info.dyn_addr);
+	spin_unlock_irqrestore(&master->ibi.lock, flags);
+
+	if (i < MAX_DEVS)
+		return 0;
+
+	i3c_generic_ibi_free_pool(data->ibi_pool);
+	data->ibi_pool = NULL;
+
+	return -ENOSPC;
+}
+
+static void aspeed_i3c_master_free_ibi(struct i3c_dev_desc *dev)
+{
+	struct i3c_master_controller *m = i3c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	struct aspeed_i3c_i2c_dev_data *data = i3c_dev_get_master_data(dev);
+	unsigned long flags;
+
+	spin_lock_irqsave(&master->ibi.lock, flags);
+	master->ibi.slots[dev->info.dyn_addr] = NULL;
+	data->ibi = -1;
+	spin_unlock_irqrestore(&master->ibi.lock, flags);
+
+	i3c_generic_ibi_free_pool(data->ibi_pool);
+}
+
+static void aspeed_i3c_master_recycle_ibi_slot(struct i3c_dev_desc *dev,
+					     struct i3c_ibi_slot *slot)
+{
+	struct aspeed_i3c_i2c_dev_data *data = i3c_dev_get_master_data(dev);
+
+	i3c_generic_ibi_recycle_slot(data->ibi_pool, slot);
+}
+
+static int aspeed_i3c_master_register_slave(struct i3c_master_controller *m,
+			      const struct i3c_slave_setup *req)
+{
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	u32 *buf;
+
+	buf = kzalloc(req->max_payload_len, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	master->slave_data.callback = req->handler;
+	master->slave_data.buf = buf;
+
+	return 0;
+}
+
+static int aspeed_i3c_master_unregister_slave(struct i3c_master_controller *m)
+{
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+
+	master->slave_data.callback = NULL;
+	kfree(master->slave_data.buf);
+
+	return 0;
+}
+
+static int aspeed_i3c_master_send_sir(struct i3c_master_controller *m,
+				      struct i3c_slave_payload *payload)
+{
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	uint32_t slv_event, intr_req, reg, thld_ctrl;
+	uint8_t *data = (uint8_t *)payload->data;
+
+	slv_event = readl(master->regs + SLV_EVENT_CTRL);
+	if ((slv_event & SLV_EVENT_CTRL_SIR_EN) == 0)
+		return -EPERM;
+
+	if (!payload)
+		return -ENXIO;
+
+	if (payload->len > (CONFIG_AST2600_I3C_IBI_MAX_PAYLOAD + 1)) {
+		dev_err(master->dev,
+			"input length %d exceeds max ibi payload size %d\n",
+			payload->len, CONFIG_AST2600_I3C_IBI_MAX_PAYLOAD + 1);
+		return -E2BIG;
+	}
+
+	init_completion(&master->sir_complete);
+
+	reg = readl(master->regs + DEVICE_CTRL);
+	reg &= ~DEV_CTRL_SLAVE_MDB;
+	reg |= FIELD_PREP(DEV_CTRL_SLAVE_MDB, data[0]);
+	writel(reg, master->regs + DEVICE_CTRL);
+
+	aspeed_i3c_master_wr_tx_fifo(master, data, payload->len);
+
+	reg = FIELD_PREP(COMMAND_PORT_SLAVE_DATA_LEN, payload->len);
+	writel(reg, master->regs + COMMAND_QUEUE_PORT);
+
+	thld_ctrl = readl(master->regs + QUEUE_THLD_CTRL);
+	thld_ctrl &= ~QUEUE_THLD_CTRL_RESP_BUF_MASK;
+	thld_ctrl |= QUEUE_THLD_CTRL_RESP_BUF(1);
+	writel(thld_ctrl, master->regs + QUEUE_THLD_CTRL);
+
+	writel(1, master->regs + SLV_INTR_REQ);
+	if (!wait_for_completion_timeout(&master->sir_complete, XFER_TIMEOUT)) {
+		dev_err(master->dev, "send sir timeout\n");
+		writel(RESET_CTRL_RX_FIFO | RESET_CTRL_TX_FIFO |
+			       RESET_CTRL_RESP_QUEUE | RESET_CTRL_CMD_QUEUE,
+		       master->regs + RESET_CTRL);
+	}
+
+	intr_req = readl(master->regs + SLV_INTR_REQ);
+	if (SLV_INTR_REQ_IBI_STS(intr_req) != SLV_IBI_STS_OK) {
+		slv_event = readl(master->regs + SLV_EVENT_CTRL);
+		if ((slv_event & SLV_EVENT_CTRL_SIR_EN) == 0)
+			pr_warn("sir is disabled by master\n");
+		return -EACCES;
+	}
+
+	return 0;
+}
+
+static int aspeed_i3c_slave_reset_queue(struct aspeed_i3c_master *master)
+{
+	int ret = 0;
+
+	ret = aspeed_i3c_master_disable(master);
+	if (ret)
+		return ret;
+	writel(RESET_CTRL_QUEUES, master->regs + RESET_CTRL);
+	ret = aspeed_i3c_master_enable(master);
+	if (ret)
+		return ret;
+	return ret;
+}
+
+static int aspeed_i3c_master_put_read_data(struct i3c_master_controller *m,
+					   struct i3c_slave_payload *data,
+					   struct i3c_slave_payload *ibi_notify)
+{
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	u32 reg, thld_ctrl;
+	u8 *buf;
+	int ret;
+
+	if (!data)
+		return -ENXIO;
+
+	if (ibi_notify) {
+		buf = (u8 *)ibi_notify->data;
+		init_completion(&master->sir_complete);
+
+		reg = readl(master->regs + DEVICE_CTRL);
+		reg &= ~DEV_CTRL_SLAVE_MDB;
+		reg |= FIELD_PREP(DEV_CTRL_SLAVE_MDB, buf[0]);
+		writel(reg, master->regs + DEVICE_CTRL);
+
+		aspeed_i3c_master_wr_tx_fifo(master, buf, ibi_notify->len);
+
+		reg = FIELD_PREP(COMMAND_PORT_SLAVE_DATA_LEN, ibi_notify->len) |
+		      COMMAND_PORT_SLAVE_TID(TID_SLAVE_IBI_DONE);
+		writel(reg, master->regs + COMMAND_QUEUE_PORT);
+
+		thld_ctrl = readl(master->regs + QUEUE_THLD_CTRL);
+		thld_ctrl &= ~QUEUE_THLD_CTRL_RESP_BUF_MASK;
+		thld_ctrl |= QUEUE_THLD_CTRL_RESP_BUF(1);
+		writel(thld_ctrl, master->regs + QUEUE_THLD_CTRL);
+	}
+
+	buf = (u8 *)data->data;
+	init_completion(&master->data_read_complete);
+	aspeed_i3c_master_wr_tx_fifo(master, buf, data->len);
+
+	reg = FIELD_PREP(COMMAND_PORT_SLAVE_DATA_LEN, data->len) |
+	      COMMAND_PORT_SLAVE_TID(TID_MASTER_READ_DATA);
+	writel(reg, master->regs + COMMAND_QUEUE_PORT);
+
+	if (ibi_notify) {
+		writel(1, master->regs + SLV_INTR_REQ);
+		if (!wait_for_completion_timeout(&master->sir_complete,
+						 XFER_TIMEOUT)) {
+			dev_err(master->dev, "send sir timeout\n");
+			writel(RESET_CTRL_QUEUES, master->regs + RESET_CTRL);
+		}
+	}
+
+	/* Wait data to be read */
+	if (!wait_for_completion_timeout(&master->data_read_complete,
+						 XFER_TIMEOUT)) {
+		dev_err(master->dev, "wait master read timeout\n");
+		ret = aspeed_i3c_slave_reset_queue(master);
+		if (ret) {
+			dev_err(master->dev, "i3c queue reset failed");
+			return ret;
+		}
+	}
+
+	return 0;
+}
+
+static int aspeed_i3c_master_timing_config(struct aspeed_i3c_master *master,
+					   struct device_node *np)
+{
+	u32 val, reg;
+	u32 timed_reset_scl_low_ns;
+	u32 sda_tx_hold_ns;
+
+	master->timing.core_rate = clk_get_rate(master->core_clk);
+	if (!master->timing.core_rate) {
+		dev_err(master->dev, "core clock rate not found\n");
+		return -EINVAL;
+	}
+
+	/* core_period is in nanosecond */
+	master->timing.core_period =
+		DIV_ROUND_UP(1000000000, master->timing.core_rate);
+
+	/* setup default timing configuration */
+	sda_tx_hold_ns = SDA_TX_HOLD_MIN * master->timing.core_period;
+	timed_reset_scl_low_ns = JESD403_TIMED_RESET_NS_DEF;
+
+	/* parse configurations from DT */
+	if (!of_property_read_u32(np, "i3c-pp-scl-hi-period-ns", &val))
+		master->timing.i3c_pp_scl_high = val;
+
+	if (!of_property_read_u32(np, "i3c-pp-scl-lo-period-ns", &val))
+		master->timing.i3c_pp_scl_low = val;
+
+	if (!of_property_read_u32(np, "i3c-od-scl-hi-period-ns", &val))
+		master->timing.i3c_od_scl_high = val;
+
+	if (!of_property_read_u32(np, "i3c-od-scl-lo-period-ns", &val))
+		master->timing.i3c_od_scl_low = val;
+
+	if (!of_property_read_u32(np, "sda-tx-hold-ns", &val))
+		sda_tx_hold_ns = val;
+
+	if (!of_property_read_u32(np, "timed-reset-scl-low-ns", &val))
+		timed_reset_scl_low_ns = val;
+
+	val = clamp((u32)DIV_ROUND_CLOSEST(sda_tx_hold_ns,
+					   master->timing.core_period),
+		    (u32)SDA_TX_HOLD_MIN, (u32)SDA_TX_HOLD_MAX);
+	reg = readl(master->regs + SDA_HOLD_SWITCH_DLY_TIMING);
+	reg &= ~SDA_TX_HOLD;
+	reg |= FIELD_PREP(SDA_TX_HOLD, val);
+	writel(reg, master->regs + SDA_HOLD_SWITCH_DLY_TIMING);
+
+	val = DIV_ROUND_CLOSEST(timed_reset_scl_low_ns,
+				master->timing.core_period);
+	writel(val, master->regs + SCL_LOW_MST_EXT_TIMEOUT);
+
+	return 0;
+}
+
+static const struct i3c_master_controller_ops aspeed_i3c_ops = {
+	.bus_init = aspeed_i3c_master_bus_init,
+	.bus_cleanup = aspeed_i3c_master_bus_cleanup,
+	.bus_reset = aspeed_i3c_master_bus_reset,
+	.attach_i3c_dev = aspeed_i3c_master_attach_i3c_dev,
+	.reattach_i3c_dev = aspeed_i3c_master_reattach_i3c_dev,
+	.detach_i3c_dev = aspeed_i3c_master_detach_i3c_dev,
+	.do_daa = aspeed_i3c_master_daa,
+	.supports_ccc_cmd = aspeed_i3c_master_supports_ccc_cmd,
+	.send_ccc_cmd = aspeed_i3c_master_send_ccc_cmd,
+	.send_hdr_cmds = aspeed_i3c_master_send_hdr_cmd,
+	.priv_xfers = aspeed_i3c_master_priv_xfers,
+	.attach_i2c_dev = aspeed_i3c_master_attach_i2c_dev,
+	.detach_i2c_dev = aspeed_i3c_master_detach_i2c_dev,
+	.i2c_xfers = aspeed_i3c_master_i2c_xfers,
+	.enable_ibi = aspeed_i3c_master_enable_ibi,
+	.disable_ibi = aspeed_i3c_master_disable_ibi,
+	.request_ibi = aspeed_i3c_master_request_ibi,
+	.free_ibi = aspeed_i3c_master_free_ibi,
+	.recycle_ibi_slot = aspeed_i3c_master_recycle_ibi_slot,
+	.register_slave = aspeed_i3c_master_register_slave,
+	.unregister_slave = aspeed_i3c_master_unregister_slave,
+	.send_sir = aspeed_i3c_master_send_sir,
+	.put_read_data = aspeed_i3c_master_put_read_data,
+};
+
+static void aspeed_i3c_master_hj(struct work_struct *work)
+{
+	struct aspeed_i3c_master *master =
+		container_of(work, typeof(*master), hj_work);
+
+	i3c_master_do_daa(&master->base);
+}
+
+static int aspeed_i3c_master_enable_hj(struct aspeed_i3c_master *master)
+{
+	int ret;
+
+	aspeed_i3c_master_enable_ibi_irq(master);
+	ast_clrbits(master->regs + DEVICE_CTRL, DEV_CTRL_HOT_JOIN_NACK);
+	ret = i3c_master_enable_hj(&master->base);
+
+	return ret;
+}
+
+static int aspeed_i3c_probe(struct platform_device *pdev)
+{
+	struct aspeed_i3c_master *master;
+	struct device_node *np;
+	int ret, irq;
+
+	master = devm_kzalloc(&pdev->dev, sizeof(*master), GFP_KERNEL);
+	if (!master)
+		return -ENOMEM;
+
+	master->i3cg = syscon_regmap_lookup_by_phandle(pdev->dev.of_node,
+						       "aspeed,i3cg");
+	if (IS_ERR(master->i3cg)) {
+		dev_err(master->dev,
+			"i3c controller missing 'aspeed,i3cg' property\n");
+		return PTR_ERR(master->i3cg);
+	}
+
+	master->regs = devm_platform_ioremap_resource(pdev, 0);
+	if (IS_ERR(master->regs))
+		return PTR_ERR(master->regs);
+
+	master->core_clk = devm_clk_get(&pdev->dev, NULL);
+	if (IS_ERR(master->core_clk))
+		return PTR_ERR(master->core_clk);
+
+	master->core_rst = devm_reset_control_get_optional_exclusive(&pdev->dev,
+								    "core_rst");
+	if (IS_ERR(master->core_rst))
+		return PTR_ERR(master->core_rst);
+
+	ret = clk_prepare_enable(master->core_clk);
+	if (ret)
+		goto err_disable_core_clk;
+
+	reset_control_deassert(master->core_rst);
+
+	spin_lock_init(&master->ibi.lock);
+	spin_lock_init(&master->xferqueue.lock);
+	INIT_LIST_HEAD(&master->xferqueue.list);
+
+	writel(RESET_CTRL_ALL, master->regs + RESET_CTRL);
+	while (readl(master->regs + RESET_CTRL))
+		;
+
+	writel(INTR_ALL, master->regs + INTR_STATUS);
+	irq = platform_get_irq(pdev, 0);
+	ret = devm_request_irq(&pdev->dev, irq,
+			       aspeed_i3c_master_irq_handler, 0,
+			       dev_name(&pdev->dev), master);
+	if (ret)
+		goto err_assert_rst;
+
+	platform_set_drvdata(pdev, master);
+
+	np = pdev->dev.of_node;
+	if (of_get_property(np, "secondary", NULL))
+		master->secondary = true;
+	else
+		master->secondary = false;
+
+	ret = of_property_read_u32(np, "i3c_chan", &master->channel);
+	if ((ret != 0) || (master->channel > I3C_CHANNEL_MAX)) {
+		dev_err(&pdev->dev, "no valid 'i3c_chan' %d %d configured\n", ret, master->channel);
+		return -EINVAL;
+	}
+
+	ret = aspeed_i3c_master_timing_config(master, np);
+	if (ret)
+		goto err_assert_rst;
+
+	/* Information regarding the FIFOs/QUEUEs depth */
+	ret = readl(master->regs + QUEUE_STATUS_LEVEL);
+	master->caps.cmdfifodepth = QUEUE_STATUS_LEVEL_CMD(ret);
+
+	ret = readl(master->regs + DATA_BUFFER_STATUS_LEVEL);
+	master->caps.datafifodepth = DATA_BUFFER_STATUS_LEVEL_TX(ret);
+
+	ret = readl(master->regs + DEVICE_ADDR_TABLE_POINTER);
+	master->datstartaddr = ret;
+	master->maxdevs = ret >> 16;
+	master->free_pos = GENMASK(master->maxdevs - 1, 0);
+	aspeed_i3c_master_init_group_dat(master);
+#ifdef CONFIG_AST2600_I3C_CCC_WORKAROUND
+	master->free_pos &= ~BIT(master->maxdevs - 1);
+	ret = (even_parity(I3C_BROADCAST_ADDR) << 7) | I3C_BROADCAST_ADDR;
+	master->addrs[master->maxdevs - 1] = ret;
+	writel(FIELD_PREP(DEV_ADDR_TABLE_DYNAMIC_ADDR, ret),
+	       master->regs + DEV_ADDR_TABLE_LOC(master->datstartaddr,
+						 master->maxdevs - 1));
+#endif
+	master->dev = &pdev->dev;
+	master->base.pec_supported = true;
+	INIT_WORK(&master->hj_work, aspeed_i3c_master_hj);
+	ret = i3c_master_register(&master->base, &pdev->dev,
+				  &aspeed_i3c_ops, master->secondary);
+	if (ret)
+		goto err_assert_rst;
+
+	if (!master->secondary && !master->base.jdec_spd) {
+		aspeed_i3c_master_iba_ctrl(master, true);
+		ret = aspeed_i3c_master_enable_hj(master);
+		if (ret)
+			goto err_master_register;
+	}
+
+	return 0;
+
+err_master_register:
+	i3c_master_unregister(&master->base);
+
+err_assert_rst:
+	reset_control_assert(master->core_rst);
+
+err_disable_core_clk:
+	clk_disable_unprepare(master->core_clk);
+
+	return ret;
+}
+
+static int aspeed_i3c_remove(struct platform_device *pdev)
+{
+	struct aspeed_i3c_master *master = platform_get_drvdata(pdev);
+	int ret;
+
+	ret = i3c_master_unregister(&master->base);
+	if (ret)
+		return ret;
+
+	reset_control_assert(master->core_rst);
+
+	clk_disable_unprepare(master->core_clk);
+
+	return 0;
+}
+
+static const struct of_device_id aspeed_i3c_master_of_match[] = {
+	{ .compatible = "aspeed,ast2600-i3c", },
+	{},
+};
+MODULE_DEVICE_TABLE(of, aspeed_i3c_master_of_match);
+
+static struct platform_driver aspeed_i3c_driver = {
+	.probe = aspeed_i3c_probe,
+	.remove = aspeed_i3c_remove,
+	.driver = {
+		.name = "ast2600-i3c-master",
+		.of_match_table = of_match_ptr(aspeed_i3c_master_of_match),
+	},
+};
+module_platform_driver(aspeed_i3c_driver);
+
+MODULE_AUTHOR("Dylan Hung <dylan_hung@aspeedtech.com>");
+MODULE_DESCRIPTION("Aspeed MIPI I3C driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/i3c/master/svc-i3c-master.c b/drivers/i3c/master/svc-i3c-master.c
index d6e9ed74cdcf..879e5a64acaf 100644
--- a/drivers/i3c/master/svc-i3c-master.c
+++ b/drivers/i3c/master/svc-i3c-master.c
@@ -17,9 +17,7 @@
 #include <linux/list.h>
 #include <linux/module.h>
 #include <linux/of.h>
-#include <linux/pinctrl/consumer.h>
 #include <linux/platform_device.h>
-#include <linux/pm_runtime.h>
 
 /* Master Mode Registers */
 #define SVC_I3C_MCONFIG      0x000
@@ -121,7 +119,6 @@
 #define   SVC_MDYNADDR_ADDR(x) FIELD_PREP(GENMASK(7, 1), (x))
 
 #define SVC_I3C_MAX_DEVS 32
-#define SVC_I3C_PM_TIMEOUT_MS 1000
 
 /* This parameter depends on the implementation and may be tuned */
 #define SVC_I3C_FIFO_SIZE 16
@@ -239,40 +236,6 @@ static void svc_i3c_master_disable_interrupts(struct svc_i3c_master *master)
 	writel(mask, master->regs + SVC_I3C_MINTCLR);
 }
 
-static void svc_i3c_master_clear_merrwarn(struct svc_i3c_master *master)
-{
-	/* Clear pending warnings */
-	writel(readl(master->regs + SVC_I3C_MERRWARN),
-	       master->regs + SVC_I3C_MERRWARN);
-}
-
-static void svc_i3c_master_flush_fifo(struct svc_i3c_master *master)
-{
-	/* Flush FIFOs */
-	writel(SVC_I3C_MDATACTRL_FLUSHTB | SVC_I3C_MDATACTRL_FLUSHRB,
-	       master->regs + SVC_I3C_MDATACTRL);
-}
-
-static void svc_i3c_master_reset_fifo_trigger(struct svc_i3c_master *master)
-{
-	u32 reg;
-
-	/* Set RX and TX tigger levels, flush FIFOs */
-	reg = SVC_I3C_MDATACTRL_FLUSHTB |
-	      SVC_I3C_MDATACTRL_FLUSHRB |
-	      SVC_I3C_MDATACTRL_UNLOCK_TRIG |
-	      SVC_I3C_MDATACTRL_TXTRIG_FIFO_NOT_FULL |
-	      SVC_I3C_MDATACTRL_RXTRIG_FIFO_NOT_EMPTY;
-	writel(reg, master->regs + SVC_I3C_MDATACTRL);
-}
-
-static void svc_i3c_master_reset(struct svc_i3c_master *master)
-{
-	svc_i3c_master_clear_merrwarn(master);
-	svc_i3c_master_reset_fifo_trigger(master);
-	svc_i3c_master_disable_interrupts(master);
-}
-
 static inline struct svc_i3c_master *
 to_svc_i3c_master(struct i3c_master_controller *master)
 {
@@ -316,6 +279,12 @@ static void svc_i3c_master_emit_stop(struct svc_i3c_master *master)
 	udelay(1);
 }
 
+static void svc_i3c_master_clear_merrwarn(struct svc_i3c_master *master)
+{
+	writel(readl(master->regs + SVC_I3C_MERRWARN),
+	       master->regs + SVC_I3C_MERRWARN);
+}
+
 static int svc_i3c_master_handle_ibi(struct svc_i3c_master *master,
 				     struct i3c_dev_desc *dev)
 {
@@ -480,23 +449,13 @@ static int svc_i3c_master_bus_init(struct i3c_master_controller *m)
 	struct i3c_device_info info = {};
 	unsigned long fclk_rate, fclk_period_ns;
 	unsigned int high_period_ns, od_low_period_ns;
-	u32 ppbaud, pplow, odhpp, odbaud, odstop, i2cbaud, reg;
+	u32 ppbaud, pplow, odhpp, odbaud, i2cbaud, reg;
 	int ret;
 
-	ret = pm_runtime_resume_and_get(master->dev);
-	if (ret < 0) {
-		dev_err(master->dev,
-			"<%s> cannot resume i3c bus master, err: %d\n",
-			__func__, ret);
-		return ret;
-	}
-
 	/* Timings derivation */
 	fclk_rate = clk_get_rate(master->fclk);
-	if (!fclk_rate) {
-		ret = -EINVAL;
-		goto rpm_out;
-	}
+	if (!fclk_rate)
+		return -EINVAL;
 
 	fclk_period_ns = DIV_ROUND_UP(1000000000, fclk_rate);
 
@@ -520,7 +479,6 @@ static int svc_i3c_master_bus_init(struct i3c_master_controller *m)
 	switch (bus->mode) {
 	case I3C_BUS_MODE_PURE:
 		i2cbaud = 0;
-		odstop = 0;
 		break;
 	case I3C_BUS_MODE_MIXED_FAST:
 	case I3C_BUS_MODE_MIXED_LIMITED:
@@ -529,7 +487,6 @@ static int svc_i3c_master_bus_init(struct i3c_master_controller *m)
 		 * between the high and low period does not really matter.
 		 */
 		i2cbaud = DIV_ROUND_UP(1000, od_low_period_ns) - 2;
-		odstop = 1;
 		break;
 	case I3C_BUS_MODE_MIXED_SLOW:
 		/*
@@ -537,16 +494,15 @@ static int svc_i3c_master_bus_init(struct i3c_master_controller *m)
 		 * constraints as the FM+ mode.
 		 */
 		i2cbaud = DIV_ROUND_UP(2500, od_low_period_ns) - 2;
-		odstop = 1;
 		break;
 	default:
-		goto rpm_out;
+		return -EINVAL;
 	}
 
 	reg = SVC_I3C_MCONFIG_MASTER_EN |
 	      SVC_I3C_MCONFIG_DISTO(0) |
 	      SVC_I3C_MCONFIG_HKEEP(0) |
-	      SVC_I3C_MCONFIG_ODSTOP(odstop) |
+	      SVC_I3C_MCONFIG_ODSTOP(0) |
 	      SVC_I3C_MCONFIG_PPBAUD(ppbaud) |
 	      SVC_I3C_MCONFIG_PPLOW(pplow) |
 	      SVC_I3C_MCONFIG_ODBAUD(odbaud) |
@@ -558,7 +514,7 @@ static int svc_i3c_master_bus_init(struct i3c_master_controller *m)
 	/* Master core's registration */
 	ret = i3c_master_get_free_addr(m, 0);
 	if (ret < 0)
-		goto rpm_out;
+		return ret;
 
 	info.dyn_addr = ret;
 
@@ -567,33 +523,21 @@ static int svc_i3c_master_bus_init(struct i3c_master_controller *m)
 
 	ret = i3c_master_set_info(&master->base, &info);
 	if (ret)
-		goto rpm_out;
+		return ret;
 
-rpm_out:
-	pm_runtime_mark_last_busy(master->dev);
-	pm_runtime_put_autosuspend(master->dev);
+	svc_i3c_master_enable_interrupts(master, SVC_I3C_MINT_SLVSTART);
 
-	return ret;
+	return 0;
 }
 
 static void svc_i3c_master_bus_cleanup(struct i3c_master_controller *m)
 {
 	struct svc_i3c_master *master = to_svc_i3c_master(m);
-	int ret;
-
-	ret = pm_runtime_resume_and_get(master->dev);
-	if (ret < 0) {
-		dev_err(master->dev, "<%s> Cannot get runtime PM.\n", __func__);
-		return;
-	}
 
 	svc_i3c_master_disable_interrupts(master);
 
 	/* Disable master */
 	writel(0, master->regs + SVC_I3C_MCONFIG);
-
-	pm_runtime_mark_last_busy(master->dev);
-	pm_runtime_put_autosuspend(master->dev);
 }
 
 static int svc_i3c_master_reserve_slot(struct svc_i3c_master *master)
@@ -712,10 +656,8 @@ static int svc_i3c_master_readb(struct svc_i3c_master *master, u8 *dst,
 	u32 reg;
 
 	for (i = 0; i < len; i++) {
-		ret = readl_poll_timeout_atomic(master->regs + SVC_I3C_MSTATUS,
-						reg,
-						SVC_I3C_MSTATUS_RXPEND(reg),
-						0, 1000);
+		ret = readl_poll_timeout(master->regs + SVC_I3C_MSTATUS, reg,
+					 SVC_I3C_MSTATUS_RXPEND(reg), 0, 1000);
 		if (ret)
 			return ret;
 
@@ -745,11 +687,10 @@ static int svc_i3c_master_do_daa_locked(struct svc_i3c_master *master,
 		 * Either one slave will send its ID, or the assignment process
 		 * is done.
 		 */
-		ret = readl_poll_timeout_atomic(master->regs + SVC_I3C_MSTATUS,
-						reg,
-						SVC_I3C_MSTATUS_RXPEND(reg) |
-						SVC_I3C_MSTATUS_MCTRLDONE(reg),
-						1, 1000);
+		ret = readl_poll_timeout(master->regs + SVC_I3C_MSTATUS, reg,
+					 SVC_I3C_MSTATUS_RXPEND(reg) |
+					 SVC_I3C_MSTATUS_MCTRLDONE(reg),
+					 1, 1000);
 		if (ret)
 			return ret;
 
@@ -803,12 +744,11 @@ static int svc_i3c_master_do_daa_locked(struct svc_i3c_master *master,
 		}
 
 		/* Wait for the slave to be ready to receive its address */
-		ret = readl_poll_timeout_atomic(master->regs + SVC_I3C_MSTATUS,
-						reg,
-						SVC_I3C_MSTATUS_MCTRLDONE(reg) &&
-						SVC_I3C_MSTATUS_STATE_DAA(reg) &&
-						SVC_I3C_MSTATUS_BETWEEN(reg),
-						0, 1000);
+		ret = readl_poll_timeout(master->regs + SVC_I3C_MSTATUS, reg,
+					 SVC_I3C_MSTATUS_MCTRLDONE(reg) &&
+					 SVC_I3C_MSTATUS_STATE_DAA(reg) &&
+					 SVC_I3C_MSTATUS_BETWEEN(reg),
+					 0, 1000);
 		if (ret)
 			return ret;
 
@@ -892,36 +832,31 @@ static int svc_i3c_master_do_daa(struct i3c_master_controller *m)
 	unsigned int dev_nb;
 	int ret, i;
 
-	ret = pm_runtime_resume_and_get(master->dev);
-	if (ret < 0) {
-		dev_err(master->dev, "<%s> Cannot get runtime PM.\n", __func__);
-		return ret;
-	}
-
 	spin_lock_irqsave(&master->xferqueue.lock, flags);
 	ret = svc_i3c_master_do_daa_locked(master, addrs, &dev_nb);
 	spin_unlock_irqrestore(&master->xferqueue.lock, flags);
-	if (ret) {
-		svc_i3c_master_emit_stop(master);
-		svc_i3c_master_clear_merrwarn(master);
-		goto rpm_out;
-	}
+	if (ret)
+		goto emit_stop;
 
 	/* Register all devices who participated to the core */
 	for (i = 0; i < dev_nb; i++) {
 		ret = i3c_master_add_i3c_dev_locked(m, addrs[i]);
 		if (ret)
-			goto rpm_out;
+			return ret;
 	}
 
 	/* Configure IBI auto-rules */
 	ret = svc_i3c_update_ibirules(master);
-	if (ret)
+	if (ret) {
 		dev_err(master->dev, "Cannot handle such a list of devices");
+		return ret;
+	}
 
-rpm_out:
-	pm_runtime_mark_last_busy(master->dev);
-	pm_runtime_put_autosuspend(master->dev);
+	return 0;
+
+emit_stop:
+	svc_i3c_master_emit_stop(master);
+	svc_i3c_master_clear_merrwarn(master);
 
 	return ret;
 }
@@ -929,35 +864,27 @@ static int svc_i3c_master_do_daa(struct i3c_master_controller *m)
 static int svc_i3c_master_read(struct svc_i3c_master *master,
 			       u8 *in, unsigned int len)
 {
-	int offset = 0, i;
-	u32 mdctrl, mstatus;
-	bool completed = false;
-	unsigned int count;
-	unsigned long start = jiffies;
+	int offset = 0, i, ret;
+	u32 mdctrl;
 
-	while (!completed) {
-		mstatus = readl(master->regs + SVC_I3C_MSTATUS);
-		if (SVC_I3C_MSTATUS_COMPLETE(mstatus) != 0)
-			completed = true;
+	while (offset < len) {
+		unsigned int count;
 
-		if (time_after(jiffies, start + msecs_to_jiffies(1000))) {
-			dev_dbg(master->dev, "I3C read timeout\n");
-			return -ETIMEDOUT;
-		}
+		ret = readl_poll_timeout(master->regs + SVC_I3C_MDATACTRL,
+					 mdctrl,
+					 !(mdctrl & SVC_I3C_MDATACTRL_RXEMPTY),
+					 0, 1000);
+		if (ret)
+			return ret;
 
-		mdctrl = readl(master->regs + SVC_I3C_MDATACTRL);
 		count = SVC_I3C_MDATACTRL_RXCOUNT(mdctrl);
-		if (offset + count > len) {
-			dev_err(master->dev, "I3C receive length too long!\n");
-			return -EINVAL;
-		}
 		for (i = 0; i < count; i++)
 			in[offset + i] = readl(master->regs + SVC_I3C_MRDATAB);
 
 		offset += count;
 	}
 
-	return offset;
+	return 0;
 }
 
 static int svc_i3c_master_write(struct svc_i3c_master *master,
@@ -990,7 +917,7 @@ static int svc_i3c_master_write(struct svc_i3c_master *master,
 static int svc_i3c_master_xfer(struct svc_i3c_master *master,
 			       bool rnw, unsigned int xfer_type, u8 addr,
 			       u8 *in, const u8 *out, unsigned int xfer_len,
-			       unsigned int *read_len, bool continued)
+			       unsigned int read_len, bool continued)
 {
 	u32 reg;
 	int ret;
@@ -1000,7 +927,7 @@ static int svc_i3c_master_xfer(struct svc_i3c_master *master,
 	       SVC_I3C_MCTRL_IBIRESP_NACK |
 	       SVC_I3C_MCTRL_DIR(rnw) |
 	       SVC_I3C_MCTRL_ADDR(addr) |
-	       SVC_I3C_MCTRL_RDTERM(*read_len),
+	       SVC_I3C_MCTRL_RDTERM(read_len),
 	       master->regs + SVC_I3C_MCTRL);
 
 	ret = readl_poll_timeout(master->regs + SVC_I3C_MSTATUS, reg,
@@ -1012,27 +939,17 @@ static int svc_i3c_master_xfer(struct svc_i3c_master *master,
 		ret = svc_i3c_master_read(master, in, xfer_len);
 	else
 		ret = svc_i3c_master_write(master, out, xfer_len);
-	if (ret < 0)
+	if (ret)
 		goto emit_stop;
 
-	if (rnw)
-		*read_len = ret;
-
 	ret = readl_poll_timeout(master->regs + SVC_I3C_MSTATUS, reg,
 				 SVC_I3C_MSTATUS_COMPLETE(reg), 0, 1000);
 	if (ret)
 		goto emit_stop;
 
-	writel(SVC_I3C_MINT_COMPLETE, master->regs + SVC_I3C_MSTATUS);
-
-	if (!continued) {
+	if (!continued)
 		svc_i3c_master_emit_stop(master);
 
-		/* Wait idle if stop is sent. */
-		readl_poll_timeout(master->regs + SVC_I3C_MSTATUS, reg,
-				   SVC_I3C_MSTATUS_STATE_IDLE(reg), 0, 1000);
-	}
-
 	return 0;
 
 emit_stop:
@@ -1090,29 +1007,17 @@ static void svc_i3c_master_start_xfer_locked(struct svc_i3c_master *master)
 	if (!xfer)
 		return;
 
-	ret = pm_runtime_resume_and_get(master->dev);
-	if (ret < 0) {
-		dev_err(master->dev, "<%s> Cannot get runtime PM.\n", __func__);
-		return;
-	}
-
-	svc_i3c_master_clear_merrwarn(master);
-	svc_i3c_master_flush_fifo(master);
-
 	for (i = 0; i < xfer->ncmds; i++) {
 		struct svc_i3c_cmd *cmd = &xfer->cmds[i];
 
 		ret = svc_i3c_master_xfer(master, cmd->rnw, xfer->type,
 					  cmd->addr, cmd->in, cmd->out,
-					  cmd->len, &cmd->read_len,
+					  cmd->len, cmd->read_len,
 					  cmd->continued);
 		if (ret)
 			break;
 	}
 
-	pm_runtime_mark_last_busy(master->dev);
-	pm_runtime_put_autosuspend(master->dev);
-
 	xfer->ret = ret;
 	complete(&xfer->comp);
 
@@ -1236,9 +1141,6 @@ static int svc_i3c_master_send_direct_ccc_cmd(struct svc_i3c_master *master,
 	if (!wait_for_completion_timeout(&xfer->comp, msecs_to_jiffies(1000)))
 		svc_i3c_master_dequeue_xfer(master, xfer);
 
-	if (cmd->read_len != xfer_len)
-		ccc->dests[0].payload.len = cmd->read_len;
-
 	ret = xfer->ret;
 	svc_i3c_master_free_xfer(xfer);
 
@@ -1389,16 +1291,6 @@ static void svc_i3c_master_free_ibi(struct i3c_dev_desc *dev)
 static int svc_i3c_master_enable_ibi(struct i3c_dev_desc *dev)
 {
 	struct i3c_master_controller *m = i3c_dev_get_master(dev);
-	struct svc_i3c_master *master = to_svc_i3c_master(m);
-	int ret;
-
-	ret = pm_runtime_resume_and_get(master->dev);
-	if (ret < 0) {
-		dev_err(master->dev, "<%s> Cannot get runtime PM.\n", __func__);
-		return ret;
-	}
-
-	svc_i3c_master_enable_interrupts(master, SVC_I3C_MINT_SLVSTART);
 
 	return i3c_master_enec_locked(m, dev->info.dyn_addr, I3C_CCC_EVENT_SIR);
 }
@@ -1406,17 +1298,8 @@ static int svc_i3c_master_enable_ibi(struct i3c_dev_desc *dev)
 static int svc_i3c_master_disable_ibi(struct i3c_dev_desc *dev)
 {
 	struct i3c_master_controller *m = i3c_dev_get_master(dev);
-	struct svc_i3c_master *master = to_svc_i3c_master(m);
-	int ret;
-
-	svc_i3c_master_disable_interrupts(master);
 
-	ret = i3c_master_disec_locked(m, dev->info.dyn_addr, I3C_CCC_EVENT_SIR);
-
-	pm_runtime_mark_last_busy(master->dev);
-	pm_runtime_put_autosuspend(master->dev);
-
-	return ret;
+	return i3c_master_disec_locked(m, dev->info.dyn_addr, I3C_CCC_EVENT_SIR);
 }
 
 static void svc_i3c_master_recycle_ibi_slot(struct i3c_dev_desc *dev,
@@ -1447,35 +1330,23 @@ static const struct i3c_master_controller_ops svc_i3c_master_ops = {
 	.disable_ibi = svc_i3c_master_disable_ibi,
 };
 
-static int svc_i3c_master_prepare_clks(struct svc_i3c_master *master)
+static void svc_i3c_master_reset(struct svc_i3c_master *master)
 {
-	int ret = 0;
-
-	ret = clk_prepare_enable(master->pclk);
-	if (ret)
-		return ret;
-
-	ret = clk_prepare_enable(master->fclk);
-	if (ret) {
-		clk_disable_unprepare(master->pclk);
-		return ret;
-	}
+	u32 reg;
 
-	ret = clk_prepare_enable(master->sclk);
-	if (ret) {
-		clk_disable_unprepare(master->pclk);
-		clk_disable_unprepare(master->fclk);
-		return ret;
-	}
+	/* Clear pending warnings */
+	writel(readl(master->regs + SVC_I3C_MERRWARN),
+	       master->regs + SVC_I3C_MERRWARN);
 
-	return 0;
-}
+	/* Set RX and TX tigger levels, flush FIFOs */
+	reg = SVC_I3C_MDATACTRL_FLUSHTB |
+	      SVC_I3C_MDATACTRL_FLUSHRB |
+	      SVC_I3C_MDATACTRL_UNLOCK_TRIG |
+	      SVC_I3C_MDATACTRL_TXTRIG_FIFO_NOT_FULL |
+	      SVC_I3C_MDATACTRL_RXTRIG_FIFO_NOT_EMPTY;
+	writel(reg, master->regs + SVC_I3C_MDATACTRL);
 
-static void svc_i3c_master_unprepare_clks(struct svc_i3c_master *master)
-{
-	clk_disable_unprepare(master->pclk);
-	clk_disable_unprepare(master->fclk);
-	clk_disable_unprepare(master->sclk);
+	svc_i3c_master_disable_interrupts(master);
 }
 
 static int svc_i3c_master_probe(struct platform_device *pdev)
@@ -1510,16 +1381,26 @@ static int svc_i3c_master_probe(struct platform_device *pdev)
 
 	master->dev = dev;
 
-	ret = svc_i3c_master_prepare_clks(master);
+	svc_i3c_master_reset(master);
+
+	ret = clk_prepare_enable(master->pclk);
 	if (ret)
 		return ret;
 
+	ret = clk_prepare_enable(master->fclk);
+	if (ret)
+		goto err_disable_pclk;
+
+	ret = clk_prepare_enable(master->sclk);
+	if (ret)
+		goto err_disable_fclk;
+
 	INIT_WORK(&master->hj_work, svc_i3c_master_hj_work);
 	INIT_WORK(&master->ibi_work, svc_i3c_master_ibi_work);
 	ret = devm_request_irq(dev, master->irq, svc_i3c_master_irq_handler,
 			       IRQF_NO_SUSPEND, "svc-i3c-irq", master);
 	if (ret)
-		goto err_disable_clks;
+		goto err_disable_sclk;
 
 	master->free_slots = GENMASK(SVC_I3C_MAX_DEVS - 1, 0);
 
@@ -1533,38 +1414,27 @@ static int svc_i3c_master_probe(struct platform_device *pdev)
 					 GFP_KERNEL);
 	if (!master->ibi.slots) {
 		ret = -ENOMEM;
-		goto err_disable_clks;
+		goto err_disable_sclk;
 	}
 
 	platform_set_drvdata(pdev, master);
 
-	pm_runtime_set_autosuspend_delay(&pdev->dev, SVC_I3C_PM_TIMEOUT_MS);
-	pm_runtime_use_autosuspend(&pdev->dev);
-	pm_runtime_get_noresume(&pdev->dev);
-	pm_runtime_set_active(&pdev->dev);
-	pm_runtime_enable(&pdev->dev);
-
-	svc_i3c_master_reset(master);
-
 	/* Register the master */
 	ret = i3c_master_register(&master->base, &pdev->dev,
 				  &svc_i3c_master_ops, false);
 	if (ret)
-		goto rpm_disable;
-
-	pm_runtime_mark_last_busy(&pdev->dev);
-	pm_runtime_put_autosuspend(&pdev->dev);
+		goto err_disable_sclk;
 
 	return 0;
 
-rpm_disable:
-	pm_runtime_dont_use_autosuspend(&pdev->dev);
-	pm_runtime_put_noidle(&pdev->dev);
-	pm_runtime_set_suspended(&pdev->dev);
-	pm_runtime_disable(&pdev->dev);
+err_disable_sclk:
+	clk_disable_unprepare(master->sclk);
+
+err_disable_fclk:
+	clk_disable_unprepare(master->fclk);
 
-err_disable_clks:
-	svc_i3c_master_unprepare_clks(master);
+err_disable_pclk:
+	clk_disable_unprepare(master->pclk);
 
 	return ret;
 }
@@ -1578,44 +1448,17 @@ static int svc_i3c_master_remove(struct platform_device *pdev)
 	if (ret)
 		return ret;
 
-	pm_runtime_dont_use_autosuspend(&pdev->dev);
-	pm_runtime_disable(&pdev->dev);
-
-	return 0;
-}
-
-static int __maybe_unused svc_i3c_runtime_suspend(struct device *dev)
-{
-	struct svc_i3c_master *master = dev_get_drvdata(dev);
-
-	svc_i3c_master_unprepare_clks(master);
-	pinctrl_pm_select_sleep_state(dev);
-
-	return 0;
-}
-
-static int __maybe_unused svc_i3c_runtime_resume(struct device *dev)
-{
-	struct svc_i3c_master *master = dev_get_drvdata(dev);
-
-	pinctrl_pm_select_default_state(dev);
-	svc_i3c_master_prepare_clks(master);
+	clk_disable_unprepare(master->pclk);
+	clk_disable_unprepare(master->fclk);
+	clk_disable_unprepare(master->sclk);
 
 	return 0;
 }
 
-static const struct dev_pm_ops svc_i3c_pm_ops = {
-	SET_NOIRQ_SYSTEM_SLEEP_PM_OPS(pm_runtime_force_suspend,
-				      pm_runtime_force_resume)
-	SET_RUNTIME_PM_OPS(svc_i3c_runtime_suspend,
-			   svc_i3c_runtime_resume, NULL)
-};
-
 static const struct of_device_id svc_i3c_master_of_match_tbl[] = {
 	{ .compatible = "silvaco,i3c-master" },
 	{ /* sentinel */ },
 };
-MODULE_DEVICE_TABLE(of, svc_i3c_master_of_match_tbl);
 
 static struct platform_driver svc_i3c_master = {
 	.probe = svc_i3c_master_probe,
@@ -1623,7 +1466,6 @@ static struct platform_driver svc_i3c_master = {
 	.driver = {
 		.name = "silvaco-i3c-master",
 		.of_match_table = svc_i3c_master_of_match_tbl,
-		.pm = &svc_i3c_pm_ops,
 	},
 };
 module_platform_driver(svc_i3c_master);
diff --git a/drivers/jtag/Kconfig b/drivers/jtag/Kconfig
new file mode 100644
index 000000000000..ae956dbc410f
--- /dev/null
+++ b/drivers/jtag/Kconfig
@@ -0,0 +1,45 @@
+menuconfig JTAG
+	tristate "JTAG support"
+	help
+	  This provides basic core functionality support for JTAG class devices.
+	  Hardware that is equipped with a JTAG microcontroller can be
+	  supported by using this driver's interfaces.
+	  This driver exposes a set of IOCTLs to the user space for
+	  the following commands:
+	    SDR: Performs an IEEE 1149.1 Data Register scan
+	    SIR: Performs an IEEE 1149.1 Instruction Register scan.
+	    RUNTEST: Forces the IEEE 1149.1 bus to a run state for a specified
+	    number of clocks or a specified time period.
+
+	  If you want this support, you should say Y here.
+
+	  To compile this driver as a module, choose M here: the module will
+	  be called jtag.
+
+menuconfig JTAG_ASPEED
+	tristate "Aspeed SoC JTAG controller support"
+	depends on JTAG && HAS_IOMEM
+	depends on ARCH_ASPEED || COMPILE_TEST
+	help
+	  This provides a support for Aspeed JTAG device, equipped on
+	  Aspeed SoC 24xx and 25xx families. Drivers allows programming
+	  of hardware devices, connected to SoC through the JTAG interface.
+
+	  If you want this support, you should say Y here.
+
+	  To compile this driver as a module, choose M here: the module will
+	  be called jtag-aspeed.
+
+menuconfig JTAG_ASPEED_INTERNAL
+	tristate "Aspeed SoC JTAG controller support internal"
+	depends on JTAG && HAS_IOMEM
+	depends on ARCH_ASPEED || COMPILE_TEST
+	help
+	  This provides a support for Aspeed JTAG device, equipped on
+	  Aspeed SoC 24xx, 25xx and 26xx families. Drivers allows programming
+	  of hardware devices, connected to SoC through the JTAG interface.
+
+	  If you want this support, you should say Y here.
+
+	  To compile this driver as a module, choose M here: the module will
+	  be called jtag-aspeed.
diff --git a/drivers/jtag/Makefile b/drivers/jtag/Makefile
new file mode 100644
index 000000000000..149ce3144461
--- /dev/null
+++ b/drivers/jtag/Makefile
@@ -0,0 +1,3 @@
+obj-$(CONFIG_JTAG)		+= jtag.o
+obj-$(CONFIG_JTAG_ASPEED)	+= jtag-aspeed.o
+obj-$(CONFIG_JTAG_ASPEED_INTERNAL) += jtag-aspeed-internal.o
\ No newline at end of file
diff --git a/drivers/jtag/jtag-aspeed-internal.c b/drivers/jtag/jtag-aspeed-internal.c
new file mode 100644
index 000000000000..679afabbd0b4
--- /dev/null
+++ b/drivers/jtag/jtag-aspeed-internal.c
@@ -0,0 +1,1083 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * JTAG driver for the Aspeed SoC
+ *
+ * Copyright (C) 2021 ASPEED Technology Inc.
+ * Ryan Chen <ryan_chen@aspeedtech.com>
+ *
+ */
+#include <linux/poll.h>
+#include <linux/sysfs.h>
+#include <linux/clk.h>
+#include <linux/fs.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/miscdevice.h>
+#include <linux/slab.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/jtag.h>
+#include <linux/platform_device.h>
+#include <linux/reset.h>
+#include <linux/io.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/uaccess.h>
+#include <uapi/linux/jtag.h>
+/******************************************************************************/
+#define ASPEED_JTAG_DATA		0x00
+#define ASPEED_JTAG_INST		0x04
+#define ASPEED_JTAG_CTRL		0x08
+#define ASPEED_JTAG_ISR			0x0C
+#define ASPEED_JTAG_SW			0x10
+#define ASPEED_JTAG_TCK			0x14
+#define ASPEED_JTAG_IDLE		0x18
+
+/* ASPEED_JTAG_CTRL - 0x08 : Engine Control */
+#define JTAG_ENG_EN			BIT(31)
+#define JTAG_ENG_OUT_EN			BIT(30)
+#define JTAG_FORCE_TMS			BIT(29)
+
+#define JTAG_IR_UPDATE			BIT(26)		//AST2500 only
+
+#define JTAG_G6_RESET_FIFO		BIT(21)		//AST2600 only
+#define JTAG_G6_CTRL_MODE		BIT(20)		//AST2600 only
+#define JTAG_G6_XFER_LEN_MASK		(0x3ff << 8)	//AST2600 only
+#define JTAG_G6_SET_XFER_LEN(x)		(x << 8)
+#define JTAG_G6_MSB_FIRST		BIT(6)		//AST2600 only
+#define JTAG_G6_TERMINATE_XFER		BIT(5)		//AST2600 only
+#define JTAG_G6_LAST_XFER		BIT(4)		//AST2600 only
+#define JTAG_G6_INST_EN			BIT(1)
+
+#define JTAG_INST_LEN_MASK		(0x3f << 20)
+#define JTAG_SET_INST_LEN(x)		(x << 20)
+#define JTAG_SET_INST_MSB		BIT(19)
+#define JTAG_TERMINATE_INST		BIT(18)
+#define JTAG_LAST_INST			BIT(17)
+#define JTAG_INST_EN			BIT(16)
+#define JTAG_DATA_LEN_MASK		(0x3f << 4)
+
+#define JTAG_DR_UPDATE			BIT(10)		//AST2500 only
+#define JTAG_DATA_LEN(x)		(x << 4)
+#define JTAG_MSB_FIRST			BIT(3)
+#define JTAG_TERMINATE_DATA		BIT(2)
+#define JTAG_LAST_DATA			BIT(1)
+#define JTAG_DATA_EN			BIT(0)
+
+/* ASPEED_JTAG_ISR	- 0x0C : INterrupt status and enable */
+#define JTAG_INST_PAUSE			BIT(19)
+#define JTAG_INST_COMPLETE		BIT(18)
+#define JTAG_DATA_PAUSE			BIT(17)
+#define JTAG_DATA_COMPLETE		BIT(16)
+
+#define JTAG_INST_PAUSE_EN		BIT(3)
+#define JTAG_INST_COMPLETE_EN		BIT(2)
+#define JTAG_DATA_PAUSE_EN		BIT(1)
+#define JTAG_DATA_COMPLETE_EN		BIT(0)
+
+/* ASPEED_JTAG_SW	- 0x10 : Software Mode and Status */
+#define JTAG_SW_MODE_EN			BIT(19)
+#define JTAG_SW_MODE_TCK		BIT(18)
+#define JTAG_SW_MODE_TMS		BIT(17)
+#define JTAG_SW_MODE_TDIO		BIT(16)
+//
+#define JTAG_STS_INST_PAUSE		BIT(2)
+#define JTAG_STS_DATA_PAUSE		BIT(1)
+#define JTAG_STS_ENG_IDLE		(0x1)
+
+/* ASPEED_JTAG_TCK	- 0x14 : TCK Control */
+#define JTAG_TCK_INVERSE		BIT(31)
+#define JTAG_TCK_DIVISOR_MASK		(0x7ff)
+#define JTAG_GET_TCK_DIVISOR(x)		(x & 0x7ff)
+
+/*  ASPEED_JTAG_IDLE - 0x18 : Ctroller set for go to IDLE */
+#define JTAG_CTRL_TRSTn_HIGH		BIT(31)
+#define JTAG_GO_IDLE			BIT(0)
+
+#define TCK_FREQ			1000000
+#define ASPEED_JTAG_MAX_PAD_SIZE	1024
+/******************************************************************************/
+#define ASPEED_JTAG_DEBUG
+
+#ifdef ASPEED_JTAG_DEBUG
+#define JTAG_DBUG(fmt, args...)                                                \
+	pr_debug("%s() " fmt, __func__, ##args)
+#else
+#define JTAG_DBUG(fmt, args...)
+#endif
+
+static char *end_status_str[] = { "tlr",   "idle",  "selDR", "capDR",
+				  "sDR",   "ex1DR", "pDR",   "ex2DR",
+				  "updDR", "selIR", "capIR", "sIR",
+				  "ex1IR", "pIR",   "ex2IR", "updIR" };
+
+struct aspeed_jtag_config {
+	u8	jtag_version;
+	u32	jtag_buff_len;
+};
+
+struct aspeed_jtag_info {
+	void __iomem			*reg_base;
+	struct device			*dev;
+	struct aspeed_jtag_config	*config;
+	enum jtag_tapstate		sts;
+	int				irq;
+	struct reset_control		*reset;
+	struct clk			*clk;
+	u32				clkin;
+	u32				tck_period;
+	u32				sw_delay;
+	u32				flag;
+	wait_queue_head_t		jtag_wq;
+	u32				mode;
+	u8 pad_data_one[ASPEED_JTAG_MAX_PAD_SIZE];
+	u8 pad_data_zero[ASPEED_JTAG_MAX_PAD_SIZE];
+};
+/******************************************************************************/
+static inline u32
+aspeed_jtag_read(struct aspeed_jtag_info *aspeed_jtag, u32 reg)
+{
+	int val;
+
+	val = readl(aspeed_jtag->reg_base + reg);
+	return val;
+}
+
+static inline void
+aspeed_jtag_write(struct aspeed_jtag_info *aspeed_jtag, u32 val, u32 reg)
+{
+	writel(val, aspeed_jtag->reg_base + reg);
+}
+
+/******************************************************************************/
+static int aspeed_jtag_set_freq(struct jtag *jtag, u32 freq)
+{
+	struct aspeed_jtag_info *aspeed_jtag = jtag_priv(jtag);
+	u32 div;
+
+	/* SW mode frequency setting */
+	aspeed_jtag->sw_delay = DIV_ROUND_UP(NSEC_PER_SEC, freq);
+	/*
+	 * HW mode frequency setting
+	 * AST2600: TCK period = Period of HCLK * (JTAG14[10:0] + 1)
+	 * AST2500: TCK period = Period of PCLK * (JTAG14[10:0] + 1) * 2
+	 */
+	if (aspeed_jtag->config->jtag_version == 6)
+		div = DIV_ROUND_UP(aspeed_jtag->clkin, freq) - 1;
+	else
+		div = DIV_ROUND_UP(aspeed_jtag->clkin, freq * 2) - 1;
+	if (div > JTAG_TCK_DIVISOR_MASK) {
+		pr_warn("The actual frequency will faster than required\n");
+		div = JTAG_TCK_DIVISOR_MASK;
+	}
+	/*
+	 * HW constraint:
+	 * AST2600 minimal TCK divisor = 7
+	 * AST2500 minimal TCK divisor = 1
+	 */
+	if (aspeed_jtag->config->jtag_version == 6) {
+		if (div < 7)
+			div = 7;
+		aspeed_jtag->tck_period = DIV_ROUND_UP_ULL(
+			(u64)NSEC_PER_SEC * (div + 1), aspeed_jtag->clkin);
+	} else if (aspeed_jtag->config->jtag_version == 0) {
+		if (div < 1)
+			div = 1;
+		aspeed_jtag->tck_period = DIV_ROUND_UP_ULL(
+			(u64)NSEC_PER_SEC * (div + 1) << 2, aspeed_jtag->clkin);
+	}
+	/*
+	 * At ast2500: Change clock divider may cause hardware logic confusion.
+	 * Enable software mode to assert the jtag hw logical before change
+	 * clock divider.
+	 */
+	if (aspeed_jtag->config->jtag_version == 0)
+		aspeed_jtag_write(aspeed_jtag,
+				  JTAG_SW_MODE_EN |
+					  aspeed_jtag_read(aspeed_jtag,
+							   ASPEED_JTAG_SW),
+				  ASPEED_JTAG_SW);
+	aspeed_jtag_write(aspeed_jtag,
+			  ((aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_TCK) &
+			    ~JTAG_TCK_DIVISOR_MASK) |
+			   div),
+			  ASPEED_JTAG_TCK);
+	if (aspeed_jtag->config->jtag_version == 0) {
+		aspeed_jtag_write(aspeed_jtag, 0, ASPEED_JTAG_SW);
+		aspeed_jtag->sts = JTAG_STATE_IDLE;
+	}
+	JTAG_DBUG("Operation freq = %d / %d\n", aspeed_jtag->clkin, div + 1);
+	return 0;
+}
+
+static int aspeed_jtag_get_freq(struct jtag *jtag, u32 *freq)
+{
+	struct aspeed_jtag_info *aspeed_jtag = jtag_priv(jtag);
+
+	if (aspeed_jtag->config->jtag_version == 6) {
+		/* TCK period = Period of HCLK * (JTAG14[10:0] + 1) */
+		*freq = aspeed_jtag->clkin /
+		       (JTAG_GET_TCK_DIVISOR(aspeed_jtag_read(
+				aspeed_jtag, ASPEED_JTAG_TCK)) + 1);
+	} else if (aspeed_jtag->config->jtag_version == 0) {
+		/* TCK period = Period of PCLK * (JTAG14[10:0] + 1) * 2 */
+		*freq = (aspeed_jtag->clkin /
+			(JTAG_GET_TCK_DIVISOR(aspeed_jtag_read(
+				 aspeed_jtag, ASPEED_JTAG_TCK)) + 1)) >> 1;
+	} else {
+		/* unknown jtag version */
+		*freq = 0;
+	}
+	return 0;
+}
+/******************************************************************************/
+static u8 TCK_Cycle(struct aspeed_jtag_info *aspeed_jtag, u8 TMS, u8 TDI)
+{
+	u8 tdo;
+
+	/* IEEE 1149.1
+	 * TMS & TDI shall be sampled by the test logic on the rising edge
+	 * test logic shall change TDO on the falling edge
+	 */
+	// TCK = 0
+	aspeed_jtag_write(aspeed_jtag,
+			  JTAG_SW_MODE_EN | (TMS * JTAG_SW_MODE_TMS) |
+				  (TDI * JTAG_SW_MODE_TDIO),
+			  ASPEED_JTAG_SW);
+	aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_SW);
+
+	/* Target device have their operating frequency*/
+	ndelay(aspeed_jtag->sw_delay >> 1);
+
+	// TCK = 1
+	aspeed_jtag_write(aspeed_jtag,
+			  JTAG_SW_MODE_EN | JTAG_SW_MODE_TCK |
+				  (TMS * JTAG_SW_MODE_TMS) |
+				  (TDI * JTAG_SW_MODE_TDIO),
+			  ASPEED_JTAG_SW);
+	aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_SW);
+
+	ndelay(aspeed_jtag->sw_delay >> 1);
+	/* Sampled TDI(slave, master's TDO) on the rising edge */
+	if (aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_SW) & JTAG_SW_MODE_TDIO)
+		tdo = 1;
+	else
+		tdo = 0;
+
+	return tdo;
+}
+
+static int aspeed_jtag_sw_set_tap_state(struct aspeed_jtag_info *aspeed_jtag,
+				      enum jtag_tapstate endstate)
+{
+	int i = 0;
+	enum jtag_tapstate from, to;
+
+	from = aspeed_jtag->sts;
+	to = endstate;
+	/* Send 8 TMS high to ensure jtag tap state go to TLRESET */
+	if (endstate == JTAG_STATE_TLRESET)
+		for (i = 0; i < 8 ; i++)
+			TCK_Cycle(aspeed_jtag, ((0xff >> i) & 0x1), 0);
+	else
+		for (i = 0; i < _tms_cycle_lookup[from][to].count; i++)
+			TCK_Cycle(aspeed_jtag,
+				  ((_tms_cycle_lookup[from][to].tmsbits >> i) &
+				   0x1),
+				  0);
+	aspeed_jtag->sts = endstate;
+	return 0;
+}
+
+/******************************************************************************/
+static void aspeed_jtag_wait_instruction_pause_complete(
+	struct aspeed_jtag_info *aspeed_jtag)
+{
+	wait_event_interruptible(aspeed_jtag->jtag_wq,
+				 (aspeed_jtag->flag & JTAG_INST_PAUSE));
+	aspeed_jtag->flag &= ~JTAG_INST_PAUSE;
+}
+static void
+aspeed_jtag_wait_instruction_complete(struct aspeed_jtag_info *aspeed_jtag)
+{
+	wait_event_interruptible(aspeed_jtag->jtag_wq,
+				 (aspeed_jtag->flag & JTAG_INST_COMPLETE));
+	aspeed_jtag->flag &= ~JTAG_INST_COMPLETE;
+}
+static void
+aspeed_jtag_wait_data_pause_complete(struct aspeed_jtag_info *aspeed_jtag)
+{
+	wait_event_interruptible(aspeed_jtag->jtag_wq,
+				 (aspeed_jtag->flag & JTAG_DATA_PAUSE));
+	aspeed_jtag->flag &= ~JTAG_DATA_PAUSE;
+}
+static void aspeed_jtag_wait_data_complete(struct aspeed_jtag_info *aspeed_jtag)
+{
+	wait_event_interruptible(aspeed_jtag->jtag_wq,
+				 (aspeed_jtag->flag & JTAG_DATA_COMPLETE));
+	aspeed_jtag->flag &= ~JTAG_DATA_COMPLETE;
+}
+static int aspeed_jtag_run_to_tlr(struct aspeed_jtag_info *aspeed_jtag)
+{
+	if (aspeed_jtag->sts == JTAG_STATE_PAUSEIR)
+		aspeed_jtag_write(aspeed_jtag, JTAG_INST_COMPLETE_EN,
+				ASPEED_JTAG_ISR);
+	else if (aspeed_jtag->sts == JTAG_STATE_PAUSEDR)
+		aspeed_jtag_write(aspeed_jtag, JTAG_DATA_COMPLETE_EN,
+				  ASPEED_JTAG_ISR);
+	aspeed_jtag_write(aspeed_jtag,
+			  JTAG_ENG_EN | JTAG_ENG_OUT_EN | JTAG_FORCE_TMS,
+			  ASPEED_JTAG_CTRL); // x TMS high + 1 TMS low
+	if (aspeed_jtag->sts == JTAG_STATE_PAUSEIR)
+		aspeed_jtag_wait_instruction_complete(aspeed_jtag);
+	else if (aspeed_jtag->sts == JTAG_STATE_PAUSEDR)
+		aspeed_jtag_wait_data_complete(aspeed_jtag);
+	/* After that the fsm will go to idle state: hw constraint */
+	aspeed_jtag->sts = JTAG_STATE_IDLE;
+	return 0;
+}
+
+static int aspeed_jtag_run_to_idle(struct aspeed_jtag_info *aspeed_jtag)
+{
+	if (aspeed_jtag->sts == JTAG_STATE_IDLE) {
+		/* nothing to do */
+	} else if (aspeed_jtag->sts == JTAG_STATE_PAUSEDR) {
+		aspeed_jtag_write(aspeed_jtag, JTAG_DATA_COMPLETE_EN,
+					  ASPEED_JTAG_ISR);
+		if (aspeed_jtag->config->jtag_version == 6) {
+			aspeed_jtag_write(aspeed_jtag,
+					JTAG_ENG_EN | JTAG_ENG_OUT_EN |
+						JTAG_G6_TERMINATE_XFER |
+						JTAG_DATA_EN,
+					ASPEED_JTAG_CTRL);
+		} else {
+			aspeed_jtag_write(aspeed_jtag,
+					  JTAG_ENG_EN | JTAG_ENG_OUT_EN |
+						  JTAG_TERMINATE_DATA |
+						  JTAG_DATA_EN,
+					  ASPEED_JTAG_CTRL);
+		}
+		aspeed_jtag_wait_data_complete(aspeed_jtag);
+	} else if (aspeed_jtag->sts == JTAG_STATE_PAUSEIR) {
+		aspeed_jtag_write(aspeed_jtag, JTAG_INST_COMPLETE_EN,
+					  ASPEED_JTAG_ISR);
+		if (aspeed_jtag->config->jtag_version == 6) {
+			aspeed_jtag_write(aspeed_jtag,
+					JTAG_ENG_EN | JTAG_ENG_OUT_EN |
+						JTAG_G6_TERMINATE_XFER |
+						JTAG_G6_INST_EN,
+					ASPEED_JTAG_CTRL);
+		} else {
+			aspeed_jtag_write(aspeed_jtag,
+					JTAG_ENG_EN | JTAG_ENG_OUT_EN |
+						JTAG_TERMINATE_INST |
+						JTAG_INST_EN,
+					ASPEED_JTAG_CTRL);
+		}
+		aspeed_jtag_wait_instruction_complete(aspeed_jtag);
+	} else {
+		pr_err("Should not get here unless aspeed_jtag->sts error!");
+		return -EFAULT;
+	}
+	aspeed_jtag->sts = JTAG_STATE_IDLE;
+	return 0;
+}
+
+static int aspeed_jtag_hw_set_tap_state(struct aspeed_jtag_info *aspeed_jtag,
+				      enum jtag_tapstate endstate)
+{
+	int ret;
+
+	if (endstate == JTAG_STATE_TLRESET) {
+		ret = aspeed_jtag_run_to_tlr(aspeed_jtag);
+	} else if (endstate == JTAG_STATE_IDLE) {
+		ret = aspeed_jtag_run_to_idle(aspeed_jtag);
+	} else {
+		/* other stable state will auto handle by hardware */
+		return 0;
+	}
+	return ret;
+}
+
+/******************************************************************************/
+/* JTAG_reset() is to generate at leaspeed 9 TMS high and
+ * 1 TMS low to force devices into Run-Test/Idle State
+ */
+static int aspeed_jtag_status_set(struct jtag *jtag,
+				  struct jtag_tap_state *tapstate)
+{
+	struct aspeed_jtag_info *aspeed_jtag = jtag_priv(jtag);
+	int ret;
+	uint32_t i;
+
+	if (tapstate->from == JTAG_STATE_CURRENT)
+		tapstate->from = aspeed_jtag->sts;
+	if (tapstate->endstate == JTAG_STATE_CURRENT)
+		tapstate->endstate = aspeed_jtag->sts;
+	JTAG_DBUG("reset:%d from:%s end:%s tck:%d", tapstate->reset,
+		  end_status_str[tapstate->from],
+		  end_status_str[tapstate->endstate], tapstate->tck);
+	if (aspeed_jtag->mode == JTAG_XFER_HW_MODE) {
+		if (tapstate->reset == JTAG_FORCE_RESET)
+			aspeed_jtag_hw_set_tap_state(aspeed_jtag,
+						     JTAG_STATE_TLRESET);
+		ret = aspeed_jtag_hw_set_tap_state(aspeed_jtag,
+						   tapstate->endstate);
+		for (i = 0; i < tapstate->tck; i++)
+			ndelay(aspeed_jtag->tck_period);
+	} else {
+		if (tapstate->reset == JTAG_FORCE_RESET)
+			aspeed_jtag_sw_set_tap_state(aspeed_jtag,
+						     JTAG_STATE_TLRESET);
+		ret = aspeed_jtag_sw_set_tap_state(aspeed_jtag,
+						   tapstate->endstate);
+		if (tapstate->endstate == JTAG_STATE_TLRESET ||
+		    tapstate->endstate == JTAG_STATE_IDLE ||
+		    tapstate->endstate == JTAG_STATE_PAUSEDR ||
+		    tapstate->endstate == JTAG_STATE_PAUSEIR)
+			for (i = 0; i < tapstate->tck; i++)
+				TCK_Cycle(aspeed_jtag, 0, 0);
+	}
+	if (ret)
+		return ret;
+	return 0;
+}
+
+static int aspeed_jtag_status_get(struct jtag *jtag, u32 *status)
+{
+	struct aspeed_jtag_info *aspeed_jtag = jtag_priv(jtag);
+
+	*status = aspeed_jtag->sts;
+	return 0;
+}
+static void aspeed_sw_jtag_xfer(struct aspeed_jtag_info *aspeed_jtag,
+				struct jtag_xfer *xfer, u8 *xfer_data)
+{
+	unsigned int index = 0;
+	u32 shift_bits = 0;
+	u8 tdi = 0, tdo = 0, tdo_buff = 0;
+	u32 remain_xfer = xfer->length;
+
+	if (xfer->type == JTAG_SIR_XFER)
+		aspeed_jtag_sw_set_tap_state(aspeed_jtag, JTAG_STATE_SHIFTIR);
+	else
+		aspeed_jtag_sw_set_tap_state(aspeed_jtag, JTAG_STATE_SHIFTDR);
+
+	while (remain_xfer) {
+		tdi = (xfer_data[index]) >> (shift_bits % 8) & (0x1);
+		if (remain_xfer == 1 &&
+		    xfer->endstate != (xfer->type == JTAG_SIR_XFER ?
+						     JTAG_STATE_SHIFTIR :
+						     JTAG_STATE_SHIFTDR)) {
+			tdo = TCK_Cycle(aspeed_jtag, 1, tdi); // go to Exit1-XR
+			aspeed_jtag->sts = xfer->type == JTAG_SIR_XFER ?
+							 JTAG_STATE_EXIT1IR :
+							 JTAG_STATE_EXIT1DR;
+		} else
+			tdo = TCK_Cycle(aspeed_jtag, 0, tdi); // go to XRShift
+		tdo_buff |= (tdo << (shift_bits % 8));
+		shift_bits++;
+		remain_xfer--;
+		if ((shift_bits % 8) == 0) {
+			if (xfer->direction & JTAG_READ_XFER)
+				xfer_data[index] = tdo_buff;
+			tdo_buff = 0;
+			index++;
+		}
+	}
+	if (xfer->direction & JTAG_READ_XFER && (shift_bits % 8))
+		xfer_data[index] = tdo_buff;
+	aspeed_jtag_sw_set_tap_state(aspeed_jtag, xfer->endstate);
+}
+static int aspeed_hw_ir_scan(struct aspeed_jtag_info *aspeed_jtag,
+			     enum jtag_tapstate endstate, u32 shift_bits)
+{
+	if (endstate == JTAG_STATE_PAUSEIR) {
+		aspeed_jtag_write(aspeed_jtag, JTAG_INST_PAUSE_EN,
+					  ASPEED_JTAG_ISR);
+		if (aspeed_jtag->config->jtag_version == 6) {
+			aspeed_jtag_write(
+				aspeed_jtag,
+				JTAG_ENG_EN | JTAG_ENG_OUT_EN |
+					JTAG_G6_SET_XFER_LEN(shift_bits),
+				ASPEED_JTAG_CTRL);
+			aspeed_jtag_write(
+				aspeed_jtag,
+				JTAG_ENG_EN | JTAG_ENG_OUT_EN |
+					JTAG_G6_SET_XFER_LEN(shift_bits) |
+					JTAG_G6_INST_EN,
+				ASPEED_JTAG_CTRL);
+		} else {
+			if (aspeed_jtag->sts == JTAG_STATE_PAUSEDR)
+				aspeed_jtag_write(aspeed_jtag,
+						  JTAG_INST_PAUSE_EN |
+							  JTAG_DATA_COMPLETE_EN,
+						  ASPEED_JTAG_ISR);
+			aspeed_jtag_write(aspeed_jtag,
+					  JTAG_ENG_EN | JTAG_ENG_OUT_EN |
+						  JTAG_SET_INST_LEN(shift_bits),
+					  ASPEED_JTAG_CTRL);
+			aspeed_jtag_write(
+				aspeed_jtag,
+				JTAG_ENG_EN | JTAG_ENG_OUT_EN |
+					JTAG_SET_INST_LEN(shift_bits) |
+					JTAG_INST_EN,
+				ASPEED_JTAG_CTRL);
+			if (aspeed_jtag->sts == JTAG_STATE_PAUSEDR)
+				aspeed_jtag_wait_data_complete(aspeed_jtag);
+		}
+		aspeed_jtag_wait_instruction_pause_complete(aspeed_jtag);
+		aspeed_jtag->sts = JTAG_STATE_PAUSEIR;
+	} else if (endstate == JTAG_STATE_IDLE) {
+		aspeed_jtag_write(aspeed_jtag, JTAG_INST_COMPLETE_EN,
+					  ASPEED_JTAG_ISR);
+		if (aspeed_jtag->config->jtag_version == 6) {
+			aspeed_jtag_write(
+				aspeed_jtag,
+				JTAG_ENG_EN | JTAG_ENG_OUT_EN |
+					JTAG_G6_LAST_XFER |
+					JTAG_G6_SET_XFER_LEN(shift_bits),
+				ASPEED_JTAG_CTRL);
+			aspeed_jtag_write(
+				aspeed_jtag,
+				JTAG_ENG_EN | JTAG_ENG_OUT_EN |
+					JTAG_G6_LAST_XFER |
+					JTAG_G6_SET_XFER_LEN(shift_bits) |
+					JTAG_G6_INST_EN,
+				ASPEED_JTAG_CTRL);
+		} else {
+			aspeed_jtag_write(aspeed_jtag,
+					  JTAG_ENG_EN | JTAG_ENG_OUT_EN |
+						  JTAG_LAST_INST |
+						  JTAG_SET_INST_LEN(shift_bits),
+					  ASPEED_JTAG_CTRL);
+			aspeed_jtag_write(
+				aspeed_jtag,
+				JTAG_ENG_EN | JTAG_ENG_OUT_EN | JTAG_LAST_INST |
+					JTAG_SET_INST_LEN(shift_bits) |
+					JTAG_INST_EN,
+				ASPEED_JTAG_CTRL);
+		}
+		aspeed_jtag_wait_instruction_complete(aspeed_jtag);
+		aspeed_jtag->sts = JTAG_STATE_IDLE;
+	} else {
+		pr_err("End state %d not support", endstate);
+		return -EFAULT;
+	}
+	return 0;
+}
+static int aspeed_hw_dr_scan(struct aspeed_jtag_info *aspeed_jtag,
+			     enum jtag_tapstate endstate, u32 shift_bits)
+{
+	if (endstate == JTAG_STATE_PAUSEDR) {
+		aspeed_jtag_write(aspeed_jtag, JTAG_DATA_PAUSE_EN,
+					  ASPEED_JTAG_ISR);
+		if (aspeed_jtag->config->jtag_version == 6) {
+			aspeed_jtag_write(
+				aspeed_jtag,
+				JTAG_ENG_EN | JTAG_ENG_OUT_EN |
+					JTAG_G6_SET_XFER_LEN(shift_bits),
+				ASPEED_JTAG_CTRL);
+			aspeed_jtag_write(
+				aspeed_jtag,
+				JTAG_ENG_EN | JTAG_ENG_OUT_EN |
+					JTAG_G6_SET_XFER_LEN(shift_bits) |
+					JTAG_DATA_EN,
+				ASPEED_JTAG_CTRL);
+		} else {
+			if (aspeed_jtag->sts == JTAG_STATE_PAUSEIR)
+				aspeed_jtag_write(aspeed_jtag,
+						  JTAG_DATA_PAUSE_EN |
+							  JTAG_INST_COMPLETE_EN,
+						  ASPEED_JTAG_ISR);
+			aspeed_jtag_write(aspeed_jtag,
+					  JTAG_ENG_EN | JTAG_ENG_OUT_EN |
+						  JTAG_DATA_LEN(shift_bits),
+					  ASPEED_JTAG_CTRL);
+			aspeed_jtag_write(aspeed_jtag,
+					  JTAG_ENG_EN | JTAG_ENG_OUT_EN |
+						  JTAG_DATA_LEN(shift_bits) |
+						  JTAG_DATA_EN,
+					  ASPEED_JTAG_CTRL);
+			if (aspeed_jtag->sts == JTAG_STATE_PAUSEIR)
+				aspeed_jtag_wait_instruction_complete(
+					aspeed_jtag);
+		}
+		aspeed_jtag_wait_data_pause_complete(aspeed_jtag);
+		aspeed_jtag->sts = JTAG_STATE_PAUSEDR;
+	} else if (endstate == JTAG_STATE_IDLE) {
+		aspeed_jtag_write(aspeed_jtag, JTAG_DATA_COMPLETE_EN,
+					  ASPEED_JTAG_ISR);
+		if (aspeed_jtag->config->jtag_version == 6) {
+			aspeed_jtag_write(
+				aspeed_jtag,
+				JTAG_ENG_EN | JTAG_ENG_OUT_EN |
+					JTAG_G6_LAST_XFER |
+					JTAG_G6_SET_XFER_LEN(shift_bits),
+				ASPEED_JTAG_CTRL);
+			aspeed_jtag_write(
+				aspeed_jtag,
+				JTAG_ENG_EN | JTAG_ENG_OUT_EN |
+					JTAG_G6_LAST_XFER |
+					JTAG_G6_SET_XFER_LEN(shift_bits) |
+					JTAG_DATA_EN,
+				ASPEED_JTAG_CTRL);
+		} else {
+			aspeed_jtag_write(aspeed_jtag,
+					  JTAG_ENG_EN | JTAG_ENG_OUT_EN |
+						  JTAG_LAST_DATA |
+						  JTAG_DATA_LEN(shift_bits),
+					  ASPEED_JTAG_CTRL);
+			aspeed_jtag_write(aspeed_jtag,
+					  JTAG_ENG_EN | JTAG_ENG_OUT_EN |
+						  JTAG_LAST_DATA |
+						  JTAG_DATA_LEN(shift_bits) |
+						  JTAG_DATA_EN,
+					  ASPEED_JTAG_CTRL);
+		}
+		aspeed_jtag_wait_data_complete(aspeed_jtag);
+		aspeed_jtag->sts = JTAG_STATE_IDLE;
+	} else {
+		pr_err("End state %d not support", endstate);
+		return -EFAULT;
+	}
+	return 0;
+}
+static void aspeed_hw_jtag_xfer(struct aspeed_jtag_info *aspeed_jtag,
+				struct jtag_xfer *xfer, u8 *xfer_data)
+{
+	unsigned int index = 0;
+	u32 shift_bits = 0;
+	u32 remain_xfer = xfer->length;
+	int i, tmp_idx = 0;
+	u32 fifo_reg = xfer->type ? ASPEED_JTAG_DATA : ASPEED_JTAG_INST;
+	u32 *xfer_data_32 = (u32 *)xfer_data;
+	enum jtag_tapstate endstate;
+
+	/* Translate the end tap status to the stable tap status for hw mode */
+	if (xfer->endstate == JTAG_STATE_PAUSEDR ||
+	    xfer->endstate == JTAG_STATE_SHIFTDR)
+		endstate = JTAG_STATE_PAUSEDR;
+	else if (xfer->endstate == JTAG_STATE_PAUSEIR ||
+		 xfer->endstate == JTAG_STATE_SHIFTIR)
+		endstate = JTAG_STATE_PAUSEIR;
+	else
+		endstate = JTAG_STATE_IDLE;
+
+	while (remain_xfer) {
+		if (remain_xfer > aspeed_jtag->config->jtag_buff_len) {
+			shift_bits = aspeed_jtag->config->jtag_buff_len;
+			tmp_idx = shift_bits / 32;
+			for (i = 0; i < tmp_idx; i++)
+				aspeed_jtag_write(aspeed_jtag,
+						  xfer_data_32[index + i],
+						  fifo_reg);
+			/*
+			 * Add 1 tck period delay to avoid jtag hardware
+			 * transfer will get wrong fifo pointer issue.
+			 */
+			ndelay(aspeed_jtag->tck_period);
+			if (xfer->type == JTAG_SIR_XFER)
+				aspeed_hw_ir_scan(aspeed_jtag,
+						  JTAG_STATE_PAUSEIR,
+						  shift_bits);
+			else
+				aspeed_hw_dr_scan(aspeed_jtag,
+						  JTAG_STATE_PAUSEDR,
+						  shift_bits);
+		} else {
+			shift_bits = remain_xfer;
+			tmp_idx = shift_bits / 32;
+			if (shift_bits % 32)
+				tmp_idx += 1;
+			for (i = 0; i < tmp_idx; i++)
+				aspeed_jtag_write(aspeed_jtag,
+						  xfer_data_32[index + i],
+						  fifo_reg);
+			ndelay(aspeed_jtag->tck_period);
+			if (xfer->type == JTAG_SIR_XFER)
+				aspeed_hw_ir_scan(aspeed_jtag, endstate,
+						  shift_bits);
+			else
+				aspeed_hw_dr_scan(aspeed_jtag, endstate,
+						  shift_bits);
+		}
+
+		remain_xfer = remain_xfer - shift_bits;
+
+		//handle tdo data
+		if (xfer->direction & JTAG_READ_XFER) {
+			tmp_idx = shift_bits / 32;
+			if (shift_bits % 32)
+				tmp_idx += 1;
+			for (i = 0; i < tmp_idx; i++) {
+				if (shift_bits < 32)
+					xfer_data_32[index + i] =
+						aspeed_jtag_read(aspeed_jtag,
+								 fifo_reg) >>
+						(32 - shift_bits);
+				else
+					xfer_data_32[index + i] =
+						aspeed_jtag_read(aspeed_jtag,
+								 fifo_reg);
+				shift_bits -= 32;
+			}
+		}
+		index += tmp_idx;
+	}
+}
+
+static int aspeed_jtag_xfer(struct jtag *jtag, struct jtag_xfer *xfer,
+			    u8 *xfer_data)
+{
+	struct aspeed_jtag_info *aspeed_jtag = jtag_priv(jtag);
+	union pad_config padding;
+	struct jtag_xfer pre_xfer, post_xfer;
+	struct jtag_xfer peri_xfer = {
+		.type = xfer->type,
+		.direction = xfer->direction,
+		.from = xfer->from,
+		.endstate = xfer->endstate,
+		.padding = 0,
+		.length = xfer->length,
+	};
+
+	padding.int_value = xfer->padding;
+	JTAG_DBUG(
+		"%s mode, type: %s direction: %d, END : %s, padding: (value: %d) pre_pad: %d post_pad: %d, len: %d\n",
+		aspeed_jtag->mode ? "HW" : "SW", xfer->type ? "DR" : "IR",
+		xfer->direction, end_status_str[xfer->endstate],
+		padding.pad_data, padding.pre_pad_number,
+		padding.post_pad_number, xfer->length);
+	if (padding.pre_pad_number) {
+		pre_xfer.type = xfer->type;
+		pre_xfer.direction = JTAG_WRITE_XFER;
+		pre_xfer.from = xfer->from;
+		pre_xfer.endstate =
+			xfer->type ? JTAG_STATE_PAUSEDR : JTAG_STATE_PAUSEIR;
+		pre_xfer.padding = xfer->padding;
+		pre_xfer.length = padding.pre_pad_number;
+
+		peri_xfer.from = pre_xfer.endstate;
+	}
+
+	if (padding.post_pad_number) {
+		peri_xfer.endstate =
+			xfer->type ? JTAG_STATE_PAUSEDR : JTAG_STATE_PAUSEIR;
+
+		post_xfer.type = xfer->type;
+		post_xfer.direction = JTAG_WRITE_XFER;
+		post_xfer.from = peri_xfer.endstate;
+		post_xfer.endstate = xfer->endstate;
+		post_xfer.padding = xfer->padding;
+		post_xfer.length = padding.post_pad_number;
+	}
+	if (padding.pre_pad_number) {
+		if (aspeed_jtag->mode == JTAG_XFER_HW_MODE)
+			aspeed_hw_jtag_xfer(aspeed_jtag, &pre_xfer,
+					    padding.pad_data ?
+							  aspeed_jtag->pad_data_one :
+							  aspeed_jtag->pad_data_zero);
+		else
+			aspeed_sw_jtag_xfer(aspeed_jtag, &pre_xfer,
+					    padding.pad_data ?
+							  aspeed_jtag->pad_data_one :
+							  aspeed_jtag->pad_data_zero);
+	}
+
+	if (aspeed_jtag->mode == JTAG_XFER_HW_MODE)
+		aspeed_hw_jtag_xfer(aspeed_jtag, &peri_xfer, xfer_data);
+	else
+		aspeed_sw_jtag_xfer(aspeed_jtag, &peri_xfer, xfer_data);
+
+	if (padding.post_pad_number) {
+		if (aspeed_jtag->mode == JTAG_XFER_HW_MODE)
+			aspeed_hw_jtag_xfer(aspeed_jtag, &post_xfer,
+					    padding.pad_data ?
+							  aspeed_jtag->pad_data_one :
+							  aspeed_jtag->pad_data_zero);
+		else
+			aspeed_sw_jtag_xfer(aspeed_jtag, &post_xfer,
+					    padding.pad_data ?
+							  aspeed_jtag->pad_data_one :
+							  aspeed_jtag->pad_data_zero);
+	}
+
+	return 0;
+}
+
+static irqreturn_t aspeed_jtag_isr(int this_irq, void *dev_id)
+{
+	u32 status;
+	struct aspeed_jtag_info *aspeed_jtag = dev_id;
+
+	status = aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_ISR);
+
+	if (status & JTAG_INST_PAUSE) {
+		aspeed_jtag_write(aspeed_jtag, JTAG_INST_PAUSE | (status & 0xf),
+				  ASPEED_JTAG_ISR);
+		aspeed_jtag->flag |= JTAG_INST_PAUSE;
+	}
+
+	if (status & JTAG_INST_COMPLETE) {
+		aspeed_jtag_write(aspeed_jtag,
+				  JTAG_INST_COMPLETE | (status & 0xf),
+				  ASPEED_JTAG_ISR);
+		aspeed_jtag->flag |= JTAG_INST_COMPLETE;
+	}
+
+	if (status & JTAG_DATA_PAUSE) {
+		aspeed_jtag_write(aspeed_jtag, JTAG_DATA_PAUSE | (status & 0xf),
+				  ASPEED_JTAG_ISR);
+		aspeed_jtag->flag |= JTAG_DATA_PAUSE;
+	}
+
+	if (status & JTAG_DATA_COMPLETE) {
+		aspeed_jtag_write(aspeed_jtag,
+				  JTAG_DATA_COMPLETE | (status & 0xf),
+				  ASPEED_JTAG_ISR);
+		aspeed_jtag->flag |= JTAG_DATA_COMPLETE;
+	}
+
+	if (aspeed_jtag->flag) {
+		wake_up_interruptible(&aspeed_jtag->jtag_wq);
+		return IRQ_HANDLED;
+	}
+	pr_err("TODO Check JTAG's interrupt %x\n",
+		aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_ISR));
+	return IRQ_NONE;
+}
+
+
+static struct aspeed_jtag_config jtag_config = {
+	.jtag_version = 0,
+	.jtag_buff_len = 32,
+};
+
+static struct aspeed_jtag_config jtag_g6_config = {
+	.jtag_version = 6,
+	.jtag_buff_len = 32,
+};
+
+static const struct of_device_id aspeed_jtag_of_matches[] = {
+	{
+		.compatible = "aspeed,ast2400-jtag",
+		.data = &jtag_config,
+	},
+	{
+		.compatible = "aspeed,ast2500-jtag",
+		.data = &jtag_config,
+	},
+	{
+		.compatible = "aspeed,ast2600-jtag",
+		.data = &jtag_g6_config,
+	},
+	{},
+};
+MODULE_DEVICE_TABLE(of, aspeed_jtag_of_matches);
+
+static int aspeed_jtag_bitbang(struct jtag *jtag,
+			       struct bitbang_packet *bitbang,
+			       struct tck_bitbang *bitbang_data)
+{
+	struct aspeed_jtag_info *aspeed_jtag = jtag_priv(jtag);
+	int i = 0;
+
+	for (i = 0; i < bitbang->length; i++) {
+		bitbang_data[i].tdo =
+			TCK_Cycle(aspeed_jtag, bitbang_data[i].tms,
+					      bitbang_data[i].tdi);
+	}
+	if (aspeed_jtag->mode == JTAG_XFER_HW_MODE)
+		aspeed_jtag_write(aspeed_jtag, 0, ASPEED_JTAG_SW);
+
+	return 0;
+}
+
+static inline void aspeed_jtag_xfer_mode_set(struct aspeed_jtag_info *aspeed_jtag, u32 mode)
+{
+	if (mode == JTAG_XFER_HW_MODE)
+		aspeed_jtag_write(aspeed_jtag, 0, ASPEED_JTAG_SW);
+	aspeed_jtag->mode = mode;
+}
+
+static int aspeed_jtag_mode_set(struct jtag *jtag, struct jtag_mode *jtag_mode)
+{
+	struct aspeed_jtag_info *aspeed_jtag = jtag_priv(jtag);
+
+	switch (jtag_mode->feature) {
+	case JTAG_XFER_MODE:
+		aspeed_jtag_xfer_mode_set(aspeed_jtag, jtag_mode->mode);
+		break;
+	case JTAG_CONTROL_MODE:
+		return -ENOTSUPP;
+	default:
+		return -EINVAL;
+	}
+	return 0;
+}
+
+static int aspeed_jtag_trst_set(struct jtag *jtag, u32 active)
+{
+	struct aspeed_jtag_info *aspeed_jtag = jtag_priv(jtag);
+
+	aspeed_jtag_write(aspeed_jtag, active ? 0 : JTAG_CTRL_TRSTn_HIGH,
+			  ASPEED_JTAG_IDLE);
+	return 0;
+}
+
+static int aspeed_jtag_enable(struct jtag *jtag)
+{
+	return 0;
+}
+
+static int aspeed_jtag_disable(struct jtag *jtag)
+{
+	return 0;
+}
+
+static const struct jtag_ops aspeed_jtag_ops = {
+	.freq_get = aspeed_jtag_get_freq,
+	.freq_set = aspeed_jtag_set_freq,
+	.status_get = aspeed_jtag_status_get,
+	.status_set = aspeed_jtag_status_set,
+	.xfer = aspeed_jtag_xfer,
+	.mode_set = aspeed_jtag_mode_set,
+	.trst_set = aspeed_jtag_trst_set,
+	.bitbang = aspeed_jtag_bitbang,
+	.enable = aspeed_jtag_enable,
+	.disable = aspeed_jtag_disable,
+};
+
+static int aspeed_jtag_probe(struct platform_device *pdev)
+{
+	struct aspeed_jtag_info *aspeed_jtag;
+	struct jtag *jtag;
+	const struct of_device_id *jtag_dev_id;
+	struct resource *res;
+	int ret = 0;
+
+	jtag = jtag_alloc(&pdev->dev, sizeof(*aspeed_jtag),
+			  &aspeed_jtag_ops);
+	if (!jtag)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, jtag);
+	aspeed_jtag = jtag_priv(jtag);
+	aspeed_jtag->dev = &pdev->dev;
+
+	jtag_dev_id = of_match_device(aspeed_jtag_of_matches, &pdev->dev);
+	if (!jtag_dev_id)
+		return -EINVAL;
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (res == NULL) {
+		dev_err(&pdev->dev, "cannot get IORESOURCE_MEM\n");
+		ret = -ENOENT;
+		goto out;
+	}
+
+	aspeed_jtag->reg_base = devm_ioremap_resource(&pdev->dev, res);
+	if (!aspeed_jtag->reg_base) {
+		ret = -EIO;
+		goto out;
+	}
+
+	aspeed_jtag->irq = platform_get_irq(pdev, 0);
+	if (aspeed_jtag->irq < 0) {
+		dev_err(&pdev->dev, "no irq specified\n");
+		ret = -ENOENT;
+		goto out;
+	}
+	aspeed_jtag->reset =
+		devm_reset_control_get_exclusive(&pdev->dev, NULL);
+	if (IS_ERR(aspeed_jtag->reset)) {
+		dev_err(&pdev->dev, "can't get jtag reset\n");
+		return PTR_ERR(aspeed_jtag->reset);
+	}
+
+	aspeed_jtag->clk = devm_clk_get(&pdev->dev, NULL);
+	if (IS_ERR(aspeed_jtag->clk)) {
+		dev_err(&pdev->dev, "no clock defined\n");
+		return -ENODEV;
+	}
+
+	aspeed_jtag->clkin = clk_get_rate(aspeed_jtag->clk);
+	dev_dbg(&pdev->dev, "aspeed_jtag->clkin %d\n", aspeed_jtag->clkin);
+
+	aspeed_jtag->config = (struct aspeed_jtag_config *)jtag_dev_id->data;
+	// SCU init
+	reset_control_assert(aspeed_jtag->reset);
+	udelay(3);
+	reset_control_deassert(aspeed_jtag->reset);
+
+	ret = devm_request_irq(&pdev->dev, aspeed_jtag->irq, aspeed_jtag_isr,
+			       0, dev_name(&pdev->dev), aspeed_jtag);
+	if (ret) {
+		dev_dbg(&pdev->dev, "JTAG Unable to get IRQ");
+		goto out;
+	}
+
+	// clear interrupt
+	aspeed_jtag_write(aspeed_jtag,
+			  JTAG_INST_PAUSE | JTAG_INST_COMPLETE |
+			  JTAG_DATA_PAUSE | JTAG_DATA_COMPLETE,
+			  ASPEED_JTAG_ISR);
+
+	aspeed_jtag_xfer_mode_set(aspeed_jtag, JTAG_XFER_HW_MODE);
+	aspeed_jtag->flag = 0;
+	aspeed_jtag->sts = JTAG_STATE_IDLE;
+	init_waitqueue_head(&aspeed_jtag->jtag_wq);
+
+	aspeed_jtag_set_freq(jtag, TCK_FREQ);
+	/* Enable jtag clock */
+	aspeed_jtag_write(aspeed_jtag, JTAG_ENG_OUT_EN, ASPEED_JTAG_CTRL);
+
+	/* Initialize JTAG core structure*/
+	ret = devm_jtag_register(aspeed_jtag->dev, jtag);
+	if (ret)
+		goto out;
+
+	memset(aspeed_jtag->pad_data_one, ~0,
+	       sizeof(aspeed_jtag->pad_data_one));
+	memset(aspeed_jtag->pad_data_zero, 0,
+	       sizeof(aspeed_jtag->pad_data_zero));
+
+	dev_info(&pdev->dev, "aspeed_jtag: driver successfully loaded.\n");
+
+	return 0;
+
+out:
+	reset_control_assert(aspeed_jtag->reset);
+	jtag_free(jtag);
+	dev_warn(&pdev->dev, "aspeed_jtag: driver init failed (ret=%d)!\n",
+		 ret);
+	return ret;
+}
+
+static int aspeed_jtag_remove(struct platform_device *pdev)
+{
+	struct jtag *jtag = platform_get_drvdata(pdev);
+	struct aspeed_jtag_info *aspeed_jtag;
+
+	aspeed_jtag = jtag_priv(jtag);
+	reset_control_assert(aspeed_jtag->reset);
+	jtag_free(jtag);
+	return 0;
+}
+
+static struct platform_driver aspeed_jtag_driver = {
+	.probe		= aspeed_jtag_probe,
+	.remove		= aspeed_jtag_remove,
+	.driver		= {
+		.name	= "aspeed-jtag",
+		.of_match_table = aspeed_jtag_of_matches,
+	},
+};
+
+module_platform_driver(aspeed_jtag_driver);
+
+MODULE_AUTHOR("Ryan Chen <ryan_chen@aspeedtech.com>");
+MODULE_DESCRIPTION("AST JTAG LIB Driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/jtag/jtag-aspeed.c b/drivers/jtag/jtag-aspeed.c
new file mode 100644
index 000000000000..f1c4fb09f40f
--- /dev/null
+++ b/drivers/jtag/jtag-aspeed.c
@@ -0,0 +1,1626 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (c) 2018 Mellanox Technologies. All rights reserved.
+// Copyright (c) 2018 Oleksandr Shamray <oleksandrs@mellanox.com>
+// Copyright (c) 2019 Intel Corporation
+
+#include <linux/clk.h>
+#include <linux/device.h>
+#include <linux/interrupt.h>
+#include <linux/jtag.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of_address.h>
+#include <linux/platform_device.h>
+#include <linux/reset.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <linux/delay.h>
+#include <uapi/linux/jtag.h>
+
+#define ASPEED_JTAG_DATA		0x00
+#define ASPEED_JTAG_INST		0x04
+#define ASPEED_JTAG_CTRL		0x08
+#define ASPEED_JTAG_ISR		0x0C
+#define ASPEED_JTAG_SW			0x10
+#define ASPEED_JTAG_TCK		0x14
+#define ASPEED_JTAG_EC			0x18
+
+#define ASPEED_JTAG_DATA_MSB			0x01
+#define ASPEED_JTAG_DATA_CHUNK_SIZE		0x20
+#define ASPEED_JTAG_HW2_DATA_CHUNK_SIZE	512
+
+/* ASPEED_JTAG_CTRL: Engine Control 24xx and 25xx series*/
+#define ASPEED_JTAG_CTL_ENG_EN		BIT(31)
+#define ASPEED_JTAG_CTL_ENG_OUT_EN	BIT(30)
+#define ASPEED_JTAG_CTL_FORCE_TMS	BIT(29)
+#define ASPEED_JTAG_CTL_IR_UPDATE	BIT(26)
+#define ASPEED_JTAG_CTL_INST_LEN(x)	((x) << 20)
+#define ASPEED_JTAG_CTL_LASPEED_INST	BIT(17)
+#define ASPEED_JTAG_CTL_INST_EN	BIT(16)
+#define ASPEED_JTAG_CTL_DR_UPDATE	BIT(10)
+#define ASPEED_JTAG_CTL_DATA_LEN(x)	((x) << 4)
+#define ASPEED_JTAG_CTL_LASPEED_DATA	BIT(1)
+#define ASPEED_JTAG_CTL_DATA_EN	BIT(0)
+
+/* ASPEED_JTAG_CTRL: Engine Control 26xx series*/
+#define ASPEED_JTAG_CTL_26XX_RESET_FIFO	BIT(21)
+#define ASPEED_JTAG_CTL_26XX_FIFO_MODE_CTRL	BIT(20)
+#define ASPEED_JTAG_CTL_26XX_TRANS_LEN(x)	((x) << 8)
+#define ASPEED_JTAG_CTL_26XX_TRANS_MASK	GENMASK(17, 8)
+#define ASPEED_JTAG_CTL_26XX_MSB_FIRST		BIT(6)
+#define ASPEED_JTAG_CTL_26XX_TERM_TRANS	BIT(5)
+#define ASPEED_JTAG_CTL_26XX_LASPEED_TRANS	BIT(4)
+#define ASPEED_JTAG_CTL_26XX_INST_EN		BIT(1)
+
+/* ASPEED_JTAG_ISR : Interrupt status and enable */
+#define ASPEED_JTAG_ISR_INST_PAUSE		BIT(19)
+#define ASPEED_JTAG_ISR_INST_COMPLETE		BIT(18)
+#define ASPEED_JTAG_ISR_DATA_PAUSE		BIT(17)
+#define ASPEED_JTAG_ISR_DATA_COMPLETE		BIT(16)
+#define ASPEED_JTAG_ISR_INST_PAUSE_EN		BIT(3)
+#define ASPEED_JTAG_ISR_INST_COMPLETE_EN	BIT(2)
+#define ASPEED_JTAG_ISR_DATA_PAUSE_EN		BIT(1)
+#define ASPEED_JTAG_ISR_DATA_COMPLETE_EN	BIT(0)
+#define ASPEED_JTAG_ISR_INT_EN_MASK		GENMASK(3, 0)
+#define ASPEED_JTAG_ISR_INT_MASK		GENMASK(19, 16)
+
+/* ASPEED_JTAG_SW : Software Mode and Status */
+#define ASPEED_JTAG_SW_MODE_EN			BIT(19)
+#define ASPEED_JTAG_SW_MODE_TCK		BIT(18)
+#define ASPEED_JTAG_SW_MODE_TMS		BIT(17)
+#define ASPEED_JTAG_SW_MODE_TDIO		BIT(16)
+
+/* ASPEED_JTAG_TCK : TCK Control */
+#define ASPEED_JTAG_TCK_DIVISOR_MASK	GENMASK(10, 0)
+#define ASPEED_JTAG_TCK_GET_DIV(x)	((x) & ASPEED_JTAG_TCK_DIVISOR_MASK)
+
+/* ASPEED_JTAG_EC : Controller set for go to IDLE */
+#define ASPEED_JTAG_EC_TRSTn_HIGH	BIT(31)
+#define ASPEED_JTAG_EC_GO_IDLE		BIT(0)
+
+#define ASPEED_JTAG_IOUT_LEN(len) \
+	(ASPEED_JTAG_CTL_ENG_EN | \
+	 ASPEED_JTAG_CTL_ENG_OUT_EN | \
+	 ASPEED_JTAG_CTL_INST_LEN(len))
+
+#define ASPEED_JTAG_DOUT_LEN(len) \
+	(ASPEED_JTAG_CTL_ENG_EN | \
+	 ASPEED_JTAG_CTL_ENG_OUT_EN | \
+	 ASPEED_JTAG_CTL_DATA_LEN(len))
+
+#define ASPEED_JTAG_TRANS_LEN(len) \
+	(ASPEED_JTAG_CTL_ENG_EN | \
+	 ASPEED_JTAG_CTL_ENG_OUT_EN | \
+	 ASPEED_JTAG_CTL_26XX_TRANS_LEN(len))
+
+#define ASPEED_JTAG_SW_TDIO (ASPEED_JTAG_SW_MODE_EN | ASPEED_JTAG_SW_MODE_TDIO)
+
+#define ASPEED_JTAG_GET_TDI(direction, byte) \
+	(((direction) & JTAG_WRITE_XFER) ? byte : UINT_MAX)
+
+#define ASPEED_JTAG_TCK_WAIT		10
+#define ASPEED_JTAG_RESET_CNTR		10
+#define WAIT_ITERATIONS		300
+
+/* Use this macro to switch between HW mode 1(comment out) and 2(defined)  */
+#define ASPEED_JTAG_HW_MODE_2_ENABLE	1
+
+/* ASPEED JTAG HW MODE 2 (Only supported in AST26xx series) */
+#define ASPEED_JTAG_SHDATA		0x20
+#define ASPEED_JTAG_SHINST		0x24
+#define ASPEED_JTAG_PADCTRL0		0x28
+#define ASPEED_JTAG_PADCTRL1		0x2C
+#define ASPEED_JTAG_SHCTRL		0x30
+#define ASPEED_JTAG_GBLCTRL		0x34
+#define ASPEED_JTAG_INTCTRL		0x38
+#define ASPEED_JTAG_STAT		0x3C
+
+/* ASPEED_JTAG_PADCTRLx : Padding control 0 and 1 */
+#define ASPEED_JTAG_PADCTRL_PAD_DATA	BIT(24)
+#define ASPEED_JTAG_PADCTRL_POSTPAD(x)	(((x) & GENMASK(8, 0)) << 12)
+#define ASPEED_JTAG_PADCTRL_PREPAD(x)	(((x) & GENMASK(8, 0)) << 0)
+
+/* ASPEED_JTAG_SHCTRL: Shift Control */
+#define ASPEED_JTAG_SHCTRL_FRUN_TCK_EN	BIT(31)
+#define ASPEED_JTAG_SHCTRL_STSHIFT_EN	BIT(30)
+#define ASPEED_JTAG_SHCTRL_TMS(x)	(((x) & GENMASK(13, 0)) << 16)
+#define ASPEED_JTAG_SHCTRL_POST_TMS(x)	(((x) & GENMASK(2, 0)) << 13)
+#define ASPEED_JTAG_SHCTRL_PRE_TMS(x)	(((x) & GENMASK(2, 0)) << 10)
+#define ASPEED_JTAG_SHCTRL_PAD_SEL0	(0)
+#define ASPEED_JTAG_SHCTRL_PAD_SEL1	BIT(9)
+#define ASPEED_JTAG_SHCTRL_END_SHIFT	BIT(8)
+#define ASPEED_JTAG_SHCTRL_START_SHIFT	BIT(7)
+#define ASPEED_JTAG_SHCTRL_LWRDT_SHIFT(x) ((x) & GENMASK(6, 0))
+
+#define ASPEED_JTAG_END_SHIFT_DISABLED	0
+
+/* ASPEED_JTAG_GBLCTRL : Global Control */
+#define ASPEED_JTAG_GBLCTRL_ENG_MODE_EN	BIT(31)
+#define ASPEED_JTAG_GBLCTRL_ENG_OUT_EN	BIT(30)
+#define ASPEED_JTAG_GBLCTRL_FORCE_TMS	BIT(29)
+#define ASPEED_JTAG_GBLCTRL_SHIFT_COMPLETE  BIT(28)
+#define ASPEED_JTAG_GBLCTRL_RESET_FIFO	BIT(25)
+#define ASPEED_JTAG_GBLCTRL_FIFO_CTRL_MODE	BIT(24)
+#define ASPEED_JTAG_GBLCTRL_UPDT_SHIFT(x)	(((x) & GENMASK(9, 7)) << 13)
+#define ASPEED_JTAG_GBLCTRL_STSHIFT(x)	(((x) & GENMASK(0, 0)) << 16)
+#define ASPEED_JTAG_GBLCTRL_TRST	BIT(15)
+#define ASPEED_JTAG_CLK_DIVISOR_MASK	GENMASK(11, 0)
+#define ASPEED_JTAG_CLK_GET_DIV(x)	((x) & ASPEED_JTAG_CLK_DIVISOR_MASK)
+
+/* ASPEED_JTAG_INTCTRL: Interrupt Control */
+#define ASPEED_JTAG_INTCTRL_SHCPL_IRQ_EN BIT(16)
+#define ASPEED_JTAG_INTCTRL_SHCPL_IRQ_STAT BIT(0)
+
+/* ASPEED_JTAG_STAT: JTAG HW mode 2 status */
+#define ASPEED_JTAG_STAT_ENG_IDLE	BIT(0)
+
+#define ASPEED_JTAG_MAX_PAD_SIZE	512
+
+/* Use this macro to set us delay to WA the intensive R/W FIFO usage issue */
+#define AST26XX_FIFO_UDELAY		2
+
+/* Use this macro to set us delay for JTAG Master Controller to be programmed */
+#define AST26XX_JTAG_CTRL_UDELAY	2
+
+/*#define USE_INTERRUPTS*/
+#define DEBUG_JTAG
+
+static const char * const regnames[] = {
+	[ASPEED_JTAG_DATA] = "ASPEED_JTAG_DATA",
+	[ASPEED_JTAG_INST] = "ASPEED_JTAG_INST",
+	[ASPEED_JTAG_CTRL] = "ASPEED_JTAG_CTRL",
+	[ASPEED_JTAG_ISR]  = "ASPEED_JTAG_ISR",
+	[ASPEED_JTAG_SW]   = "ASPEED_JTAG_SW",
+	[ASPEED_JTAG_TCK]  = "ASPEED_JTAG_TCK",
+	[ASPEED_JTAG_EC]   = "ASPEED_JTAG_EC",
+	[ASPEED_JTAG_SHDATA]  = "ASPEED_JTAG_SHDATA",
+	[ASPEED_JTAG_SHINST]  = "ASPEED_JTAG_SHINST",
+	[ASPEED_JTAG_PADCTRL0] = "ASPEED_JTAG_PADCTRL0",
+	[ASPEED_JTAG_PADCTRL1] = "ASPEED_JTAG_PADCTRL1",
+	[ASPEED_JTAG_SHCTRL]   = "ASPEED_JTAG_SHCTRL",
+	[ASPEED_JTAG_GBLCTRL]  = "ASPEED_JTAG_GBLCTRL",
+	[ASPEED_JTAG_INTCTRL]  = "ASPEED_JTAG_INTCTRL",
+	[ASPEED_JTAG_STAT]     = "ASPEED_JTAG_STAT",
+};
+
+#define ASPEED_JTAG_NAME		"jtag-aspeed"
+
+struct aspeed_jtag {
+	void __iomem			*reg_base;
+	struct device			*dev;
+	struct clk			*pclk;
+	enum jtag_tapstate		status;
+	int				irq;
+	struct reset_control		*rst;
+	u32				flag;
+	wait_queue_head_t		jtag_wq;
+	u32				mode;
+	enum jtag_tapstate		current_state;
+	u32				tck_period;
+	const struct jtag_low_level_functions *llops;
+	u32 pad_data_one[ASPEED_JTAG_MAX_PAD_SIZE / 32];
+	u32 pad_data_zero[ASPEED_JTAG_MAX_PAD_SIZE / 32];
+};
+
+/*
+ * Multi generation support is enabled by fops and low level assped function
+ * mapping using asped_jtag_functions struct as config mechanism.
+ */
+
+struct jtag_low_level_functions {
+	void (*output_disable)(struct aspeed_jtag *aspeed_jtag);
+	void (*master_enable)(struct aspeed_jtag *aspeed_jtag);
+	int (*xfer_push_data)(struct aspeed_jtag *aspeed_jtag,
+			      enum jtag_xfer_type type, u32 bits_len);
+	int (*xfer_push_data_last)(struct aspeed_jtag *aspeed_jtag,
+				   enum jtag_xfer_type type, u32 bits_len);
+	void (*xfer_sw)(struct aspeed_jtag *aspeed_jtag, struct jtag_xfer *xfer,
+			u32 *data);
+	int (*xfer_hw)(struct aspeed_jtag *aspeed_jtag, struct jtag_xfer *xfer,
+		       u32 *data);
+	void (*xfer_hw_fifo_delay)(void);
+	void (*xfer_sw_delay)(struct aspeed_jtag *aspeed_jtag);
+	irqreturn_t (*jtag_interrupt)(s32 this_irq, void *dev_id);
+};
+
+struct aspeed_jtag_functions {
+	const struct jtag_ops *aspeed_jtag_ops;
+	const struct jtag_low_level_functions *aspeed_jtag_llops;
+};
+
+#ifdef DEBUG_JTAG
+static char *end_status_str[] = { "tlr",   "idle",   "selDR", "capDR", "sDR",
+				  "ex1DR", "pDR",    "ex2DR", "updDR", "selIR",
+				  "capIR", "sIR",    "ex1IR", "pIR",   "ex2IR",
+				  "updIR", "current" };
+#endif
+
+static u32 aspeed_jtag_read(struct aspeed_jtag *aspeed_jtag, u32 reg)
+{
+	u32 val = readl(aspeed_jtag->reg_base + reg);
+
+#ifdef DEBUG_JTAG
+	dev_dbg(aspeed_jtag->dev, "read:%s val = 0x%08x\n", regnames[reg], val);
+#endif
+	return val;
+}
+
+static void aspeed_jtag_write(struct aspeed_jtag *aspeed_jtag, u32 val, u32 reg)
+{
+#ifdef DEBUG_JTAG
+	dev_dbg(aspeed_jtag->dev, "write:%s val = 0x%08x\n", regnames[reg],
+		val);
+#endif
+	writel(val, aspeed_jtag->reg_base + reg);
+}
+
+static int aspeed_jtag_freq_set(struct jtag *jtag, u32 freq)
+{
+	struct aspeed_jtag *aspeed_jtag = jtag_priv(jtag);
+	unsigned long apb_frq;
+	u32 tck_val;
+	u16 div;
+
+	if (!freq)
+		return -EINVAL;
+
+	apb_frq = clk_get_rate(aspeed_jtag->pclk);
+	if (!apb_frq)
+		return -EOPNOTSUPP;
+
+	div = (apb_frq - 1) / freq;
+	tck_val = aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_TCK);
+	aspeed_jtag_write(aspeed_jtag,
+			  (tck_val & ~ASPEED_JTAG_TCK_DIVISOR_MASK) | div,
+			  ASPEED_JTAG_TCK);
+	aspeed_jtag->tck_period =
+		DIV_ROUND_UP_ULL((u64)NSEC_PER_SEC * (div + 1), apb_frq);
+	return 0;
+}
+
+static int aspeed_jtag_freq_set_26xx(struct jtag *jtag, u32 freq)
+{
+	struct aspeed_jtag *aspeed_jtag = jtag_priv(jtag);
+	unsigned long apb_frq;
+	u32 tck_val;
+	u16 div;
+
+	if (!freq)
+		return -EINVAL;
+
+	apb_frq = clk_get_rate(aspeed_jtag->pclk);
+	if (!apb_frq)
+		return -EOPNOTSUPP;
+
+	div = (apb_frq - 1) / freq;
+	tck_val = aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_GBLCTRL);
+	aspeed_jtag_write(aspeed_jtag,
+			  (tck_val & ~ASPEED_JTAG_CLK_DIVISOR_MASK) | div,
+			  ASPEED_JTAG_GBLCTRL);
+	return 0;
+}
+
+static int aspeed_jtag_freq_get(struct jtag *jtag, u32 *frq)
+{
+	struct aspeed_jtag *aspeed_jtag = jtag_priv(jtag);
+	u32 pclk;
+	u32 tck;
+
+	pclk = clk_get_rate(aspeed_jtag->pclk);
+	tck = aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_TCK);
+	*frq = pclk / (ASPEED_JTAG_TCK_GET_DIV(tck) + 1);
+
+	return 0;
+}
+
+static int aspeed_jtag_freq_get_26xx(struct jtag *jtag, u32 *frq)
+{
+	struct aspeed_jtag *aspeed_jtag = jtag_priv(jtag);
+	u32 pclk;
+	u32 tck;
+
+	pclk = clk_get_rate(aspeed_jtag->pclk);
+	tck = aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_GBLCTRL);
+	*frq = pclk / (ASPEED_JTAG_CLK_GET_DIV(tck) + 1);
+
+	return 0;
+}
+
+static inline void aspeed_jtag_output_disable(struct aspeed_jtag *aspeed_jtag)
+{
+	aspeed_jtag_write(aspeed_jtag, 0, ASPEED_JTAG_CTRL);
+}
+
+static inline void
+aspeed_jtag_output_disable_26xx(struct aspeed_jtag *aspeed_jtag)
+{
+	u32 reg_val;
+
+	reg_val = aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_GBLCTRL) &
+		  ASPEED_JTAG_CLK_DIVISOR_MASK;
+	aspeed_jtag_write(aspeed_jtag, 0, ASPEED_JTAG_CTRL);
+	aspeed_jtag_write(aspeed_jtag, reg_val, ASPEED_JTAG_GBLCTRL);
+}
+
+static inline void aspeed_jtag_master(struct aspeed_jtag *aspeed_jtag)
+{
+	aspeed_jtag_write(aspeed_jtag,
+			  (ASPEED_JTAG_CTL_ENG_EN | ASPEED_JTAG_CTL_ENG_OUT_EN),
+			  ASPEED_JTAG_CTRL);
+
+	aspeed_jtag_write(aspeed_jtag,
+			  ASPEED_JTAG_SW_MODE_EN | ASPEED_JTAG_SW_MODE_TDIO,
+			  ASPEED_JTAG_SW);
+	aspeed_jtag_write(aspeed_jtag,
+			  ASPEED_JTAG_ISR_INST_PAUSE |
+				  ASPEED_JTAG_ISR_INST_COMPLETE |
+				  ASPEED_JTAG_ISR_DATA_PAUSE |
+				  ASPEED_JTAG_ISR_DATA_COMPLETE |
+				  ASPEED_JTAG_ISR_INST_PAUSE_EN |
+				  ASPEED_JTAG_ISR_INST_COMPLETE_EN |
+				  ASPEED_JTAG_ISR_DATA_PAUSE_EN |
+				  ASPEED_JTAG_ISR_DATA_COMPLETE_EN,
+			  ASPEED_JTAG_ISR); /* Enable Interrupt */
+}
+
+static inline void aspeed_jtag_master_26xx(struct aspeed_jtag *aspeed_jtag)
+{
+	u32 reg_val;
+
+	reg_val = aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_GBLCTRL) &
+		ASPEED_JTAG_CLK_DIVISOR_MASK;
+	if (aspeed_jtag->mode & JTAG_XFER_HW_MODE) {
+		aspeed_jtag_write(aspeed_jtag, 0, ASPEED_JTAG_CTRL);
+		aspeed_jtag_write(aspeed_jtag, 0, ASPEED_JTAG_SW);
+		aspeed_jtag_write(aspeed_jtag,
+				  reg_val | ASPEED_JTAG_GBLCTRL_ENG_MODE_EN |
+					  ASPEED_JTAG_GBLCTRL_ENG_OUT_EN,
+				  ASPEED_JTAG_GBLCTRL);
+	} else {
+		aspeed_jtag_write(aspeed_jtag, reg_val, ASPEED_JTAG_GBLCTRL);
+		aspeed_jtag_write(aspeed_jtag,
+				  ASPEED_JTAG_CTL_ENG_EN |
+					  ASPEED_JTAG_CTL_ENG_OUT_EN,
+				  ASPEED_JTAG_CTRL);
+
+		aspeed_jtag_write(aspeed_jtag,
+				  ASPEED_JTAG_SW_MODE_EN |
+					  ASPEED_JTAG_SW_MODE_TDIO,
+				  ASPEED_JTAG_SW);
+	}
+	aspeed_jtag_write(aspeed_jtag,
+			  ASPEED_JTAG_INTCTRL_SHCPL_IRQ_EN |
+				  ASPEED_JTAG_INTCTRL_SHCPL_IRQ_STAT,
+			  ASPEED_JTAG_INTCTRL); /* Enable HW2 IRQ */
+
+	aspeed_jtag_write(aspeed_jtag,
+			  ASPEED_JTAG_ISR_INST_PAUSE |
+				  ASPEED_JTAG_ISR_INST_COMPLETE |
+				  ASPEED_JTAG_ISR_DATA_PAUSE |
+				  ASPEED_JTAG_ISR_DATA_COMPLETE |
+				  ASPEED_JTAG_ISR_INST_PAUSE_EN |
+				  ASPEED_JTAG_ISR_INST_COMPLETE_EN |
+				  ASPEED_JTAG_ISR_DATA_PAUSE_EN |
+				  ASPEED_JTAG_ISR_DATA_COMPLETE_EN,
+			  ASPEED_JTAG_ISR); /* Enable HW1 Interrupts */
+}
+
+static int aspeed_jtag_mode_set(struct jtag *jtag, struct jtag_mode *jtag_mode)
+{
+	struct aspeed_jtag *aspeed_jtag = jtag_priv(jtag);
+
+	switch (jtag_mode->feature) {
+	case JTAG_XFER_MODE:
+		aspeed_jtag->mode = jtag_mode->mode;
+		aspeed_jtag->llops->master_enable(aspeed_jtag);
+		break;
+	case JTAG_CONTROL_MODE:
+		if (jtag_mode->mode == JTAG_MASTER_OUTPUT_DISABLE)
+			aspeed_jtag->llops->output_disable(aspeed_jtag);
+		else if (jtag_mode->mode == JTAG_MASTER_MODE)
+			aspeed_jtag->llops->master_enable(aspeed_jtag);
+		break;
+	default:
+		return -EINVAL;
+	}
+	return 0;
+}
+
+/*
+ * We read and write from an unused JTAG Master controller register in SW
+ * mode to create a delay in xfers.
+ * We found this mechanism better than any udelay or usleep option.
+ */
+static inline void aspeed_jtag_sw_delay_26xx(struct aspeed_jtag *aspeed_jtag)
+{
+	u32 read_reg = aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_PADCTRL1);
+
+	aspeed_jtag_write(aspeed_jtag, read_reg, ASPEED_JTAG_PADCTRL1);
+}
+
+static char aspeed_jtag_tck_cycle(struct aspeed_jtag *aspeed_jtag, u8 tms,
+				  u8 tdi)
+{
+	char tdo = 0;
+
+	/* TCK = 0 */
+	aspeed_jtag_write(aspeed_jtag,
+			  ASPEED_JTAG_SW_MODE_EN |
+				  (tms * ASPEED_JTAG_SW_MODE_TMS) |
+				  (tdi * ASPEED_JTAG_SW_MODE_TDIO),
+			  ASPEED_JTAG_SW);
+
+	/* Wait until JTAG Master controller finishes the operation */
+	if (aspeed_jtag->llops->xfer_sw_delay)
+		aspeed_jtag->llops->xfer_sw_delay(aspeed_jtag);
+	else
+		aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_SW);
+
+	ndelay(aspeed_jtag->tck_period >> 1);
+
+	/* TCK = 1 */
+	aspeed_jtag_write(aspeed_jtag,
+			  ASPEED_JTAG_SW_MODE_EN | ASPEED_JTAG_SW_MODE_TCK |
+				  (tms * ASPEED_JTAG_SW_MODE_TMS) |
+				  (tdi * ASPEED_JTAG_SW_MODE_TDIO),
+			  ASPEED_JTAG_SW);
+
+	/* Wait until JTAG Master controller finishes the operation */
+	if (aspeed_jtag->llops->xfer_sw_delay)
+		aspeed_jtag->llops->xfer_sw_delay(aspeed_jtag);
+
+	ndelay(aspeed_jtag->tck_period >> 1);
+
+	if (aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_SW) &
+	    ASPEED_JTAG_SW_MODE_TDIO)
+		tdo = 1;
+
+	return tdo;
+}
+
+static int aspeed_jtag_bitbang(struct jtag *jtag,
+			       struct bitbang_packet *bitbang,
+			       struct tck_bitbang *bitbang_data)
+{
+	struct aspeed_jtag *aspeed_jtag = jtag_priv(jtag);
+	int i = 0;
+
+	for (i = 0; i < bitbang->length; i++) {
+		bitbang_data[i].tdo =
+			aspeed_jtag_tck_cycle(aspeed_jtag, bitbang_data[i].tms,
+					      bitbang_data[i].tdi);
+	}
+	return 0;
+}
+
+static inline void aspeed_jtag_xfer_hw_fifo_delay_26xx(void)
+{
+	udelay(AST26XX_FIFO_UDELAY);
+}
+
+static int aspeed_jtag_isr_wait(struct aspeed_jtag *aspeed_jtag, u32 bit)
+{
+	int res = 0;
+#ifdef USE_INTERRUPTS
+	res = wait_event_interruptible(aspeed_jtag->jtag_wq,
+				       aspeed_jtag->flag & bit);
+	aspeed_jtag->flag &= ~bit;
+#else
+	u32 status = 0;
+	u32 iterations = 0;
+
+	while ((status & bit) == 0) {
+		status = aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_ISR);
+#ifdef DEBUG_JTAG
+		dev_dbg(aspeed_jtag->dev, "%s  = 0x%08x\n", __func__, status);
+#endif
+		iterations++;
+		if (iterations > WAIT_ITERATIONS) {
+			dev_err(aspeed_jtag->dev, "%s %d in ASPEED_JTAG_ISR\n",
+				"aspeed_jtag driver timed out waiting for bit",
+				bit);
+			res = -EFAULT;
+			break;
+		}
+		if ((status & ASPEED_JTAG_ISR_DATA_COMPLETE) == 0) {
+			if (iterations % 25 == 0)
+				usleep_range(1, 5);
+			else
+				udelay(1);
+		}
+	}
+	aspeed_jtag_write(aspeed_jtag, bit | (status & 0xf), ASPEED_JTAG_ISR);
+#endif
+	return res;
+}
+
+static int aspeed_jtag_wait_shift_complete(struct aspeed_jtag *aspeed_jtag)
+{
+	int res = 0;
+#ifdef USE_INTERRUPTS
+	res = wait_event_interruptible(aspeed_jtag->jtag_wq,
+				       aspeed_jtag->flag &
+				       ASPEED_JTAG_INTCTRL_SHCPL_IRQ_STAT);
+	aspeed_jtag->flag &= ~ASPEED_JTAG_INTCTRL_SHCPL_IRQ_STAT;
+#else
+	u32 status = 0;
+	u32 iterations = 0;
+
+	while ((status & ASPEED_JTAG_INTCTRL_SHCPL_IRQ_STAT) == 0) {
+		status = aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_INTCTRL);
+#ifdef DEBUG_JTAG
+		dev_dbg(aspeed_jtag->dev, "%s  = 0x%08x\n", __func__, status);
+#endif
+		iterations++;
+		if (iterations > WAIT_ITERATIONS) {
+			dev_err(aspeed_jtag->dev,
+				"aspeed_jtag driver timed out waiting for shift completed\n");
+			res = -EFAULT;
+			break;
+		}
+		if (iterations % 25 == 0)
+			usleep_range(1, 5);
+		else
+			udelay(1);
+	}
+	aspeed_jtag_write(aspeed_jtag,
+			  ASPEED_JTAG_INTCTRL_SHCPL_IRQ_STAT |
+				  ASPEED_JTAG_INTCTRL_SHCPL_IRQ_EN,
+			  ASPEED_JTAG_INTCTRL);
+#endif
+	return res;
+}
+
+static void aspeed_jtag_set_tap_state(struct aspeed_jtag *aspeed_jtag,
+				      enum jtag_tapstate from_state,
+				      enum jtag_tapstate end_state)
+{
+	int i = 0;
+	enum jtag_tapstate from, to;
+
+	from = from_state;
+	to = end_state;
+
+	if (from == JTAG_STATE_CURRENT)
+		from = aspeed_jtag->current_state;
+
+	for (i = 0; i < _tms_cycle_lookup[from][to].count; i++)
+		aspeed_jtag_tck_cycle(aspeed_jtag,
+				      ((_tms_cycle_lookup[from][to].tmsbits
+				      >> i) & 0x1), 0);
+	aspeed_jtag->current_state = end_state;
+}
+
+static void aspeed_jtag_set_tap_state_sw(struct aspeed_jtag *aspeed_jtag,
+					 struct jtag_tap_state *tapstate)
+{
+	int i;
+
+	/* SW mode from curent tap state -> to end_state */
+	if (tapstate->reset || tapstate->endstate == JTAG_STATE_TLRESET) {
+		for (i = 0; i < ASPEED_JTAG_RESET_CNTR; i++)
+			aspeed_jtag_tck_cycle(aspeed_jtag, 1, 0);
+		aspeed_jtag->current_state = JTAG_STATE_TLRESET;
+	}
+
+	aspeed_jtag_set_tap_state(aspeed_jtag, tapstate->from,
+				  tapstate->endstate);
+	if (tapstate->endstate == JTAG_STATE_TLRESET ||
+	    tapstate->endstate == JTAG_STATE_IDLE ||
+	    tapstate->endstate == JTAG_STATE_PAUSEDR ||
+	    tapstate->endstate == JTAG_STATE_PAUSEIR)
+		for (i = 0; i < tapstate->tck; i++)
+			aspeed_jtag_tck_cycle(aspeed_jtag, 0, 0);
+}
+
+static int aspeed_jtag_status_set(struct jtag *jtag,
+				  struct jtag_tap_state *tapstate)
+{
+	struct aspeed_jtag *aspeed_jtag = jtag_priv(jtag);
+	int i;
+
+#ifdef DEBUG_JTAG
+	dev_dbg(aspeed_jtag->dev, "Set TAP state: %s\n",
+		end_status_str[tapstate->endstate]);
+#endif
+
+	if (!(aspeed_jtag->mode & JTAG_XFER_HW_MODE)) {
+		aspeed_jtag_set_tap_state_sw(aspeed_jtag, tapstate);
+		return 0;
+	}
+
+	/* x TMS high + 1 TMS low */
+	if (tapstate->reset) {
+		/* Disable sw mode */
+		aspeed_jtag_write(aspeed_jtag, 0, ASPEED_JTAG_SW);
+		mdelay(1);
+		aspeed_jtag_write(aspeed_jtag,
+				  ASPEED_JTAG_CTL_ENG_EN |
+					  ASPEED_JTAG_CTL_ENG_OUT_EN |
+					  ASPEED_JTAG_CTL_FORCE_TMS,
+				  ASPEED_JTAG_CTRL);
+		mdelay(1);
+		aspeed_jtag_write(aspeed_jtag, ASPEED_JTAG_SW_TDIO,
+				  ASPEED_JTAG_SW);
+		aspeed_jtag->current_state = JTAG_STATE_TLRESET;
+	}
+	for (i = 0; i < tapstate->tck; i++)
+		ndelay(aspeed_jtag->tck_period);
+
+	return 0;
+}
+
+static int aspeed_jtag_shctrl_tms_mask(enum jtag_tapstate from,
+				       enum jtag_tapstate to,
+				       enum jtag_tapstate there,
+				       enum jtag_tapstate endstate,
+				       u32 start_shift, u32 end_shift,
+				       u32 *tms_mask)
+{
+	u32 pre_tms = start_shift ? _tms_cycle_lookup[from][to].count : 0;
+	u32 post_tms = end_shift ? _tms_cycle_lookup[there][endstate].count : 0;
+	u32 tms_value = start_shift ? _tms_cycle_lookup[from][to].tmsbits : 0;
+
+	tms_value |= end_shift ? _tms_cycle_lookup[there][endstate].tmsbits
+					 << pre_tms :
+				 0;
+	if (pre_tms > GENMASK(2, 0) || post_tms > GENMASK(2, 0)) {
+		pr_err("pre/port tms count is greater than hw limit");
+		return -EINVAL;
+	}
+	*tms_mask = start_shift | ASPEED_JTAG_SHCTRL_PRE_TMS(pre_tms) |
+		    end_shift | ASPEED_JTAG_SHCTRL_POST_TMS(post_tms) |
+		    ASPEED_JTAG_SHCTRL_TMS(tms_value);
+	return 0;
+}
+
+static void aspeed_jtag_set_tap_state_hw2(struct aspeed_jtag *aspeed_jtag,
+					  struct jtag_tap_state *tapstate)
+{
+	u32 reg_val;
+
+	/* x TMS high + 1 TMS low */
+	if (tapstate->reset || tapstate->endstate == JTAG_STATE_TLRESET) {
+		/* Disable sw mode */
+		aspeed_jtag_write(aspeed_jtag, 0, ASPEED_JTAG_SW);
+		udelay(AST26XX_JTAG_CTRL_UDELAY);
+		reg_val = aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_GBLCTRL);
+		aspeed_jtag_write(aspeed_jtag,
+				  reg_val | ASPEED_JTAG_GBLCTRL_ENG_MODE_EN |
+					  ASPEED_JTAG_GBLCTRL_ENG_OUT_EN |
+					  ASPEED_JTAG_GBLCTRL_RESET_FIFO |
+					  ASPEED_JTAG_GBLCTRL_FORCE_TMS,
+				  ASPEED_JTAG_GBLCTRL);
+		udelay(AST26XX_JTAG_CTRL_UDELAY);
+		while (aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_GBLCTRL) & ASPEED_JTAG_GBLCTRL_FORCE_TMS)
+			;
+		aspeed_jtag->current_state = JTAG_STATE_TLRESET;
+	} else if (tapstate->endstate == JTAG_STATE_IDLE &&
+		   aspeed_jtag->current_state != JTAG_STATE_IDLE) {
+		/* Always go to RTI, do not wait for shift operation */
+		aspeed_jtag_set_tap_state(aspeed_jtag,
+					  aspeed_jtag->current_state,
+					  JTAG_STATE_IDLE);
+		aspeed_jtag->current_state = JTAG_STATE_IDLE;
+	}
+	/* Run TCK */
+	if (tapstate->tck) {
+		/* Disable sw mode */
+		aspeed_jtag_write(aspeed_jtag, 0, ASPEED_JTAG_SW);
+		aspeed_jtag_write(aspeed_jtag, 0, ASPEED_JTAG_PADCTRL0);
+		reg_val = aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_GBLCTRL);
+		reg_val = reg_val & ~(GENMASK(22, 20));
+		aspeed_jtag_write(aspeed_jtag,
+				  reg_val | ASPEED_JTAG_GBLCTRL_FIFO_CTRL_MODE |
+					  ASPEED_JTAG_GBLCTRL_STSHIFT(0) |
+					  ASPEED_JTAG_GBLCTRL_UPDT_SHIFT(tapstate->tck),
+				  ASPEED_JTAG_GBLCTRL);
+
+		aspeed_jtag_write(aspeed_jtag,
+				  ASPEED_JTAG_SHCTRL_STSHIFT_EN |
+					  ASPEED_JTAG_SHCTRL_LWRDT_SHIFT(tapstate->tck),
+				  ASPEED_JTAG_SHCTRL);
+		aspeed_jtag_wait_shift_complete(aspeed_jtag);
+	}
+}
+
+static int aspeed_jtag_status_set_26xx(struct jtag *jtag,
+				       struct jtag_tap_state *tapstate)
+{
+	struct aspeed_jtag *aspeed_jtag = jtag_priv(jtag);
+
+#ifdef DEBUG_JTAG
+	dev_dbg(aspeed_jtag->dev, "Set TAP state: status %s from %s to %s\n",
+		end_status_str[aspeed_jtag->current_state],
+		end_status_str[tapstate->from],
+		end_status_str[tapstate->endstate]);
+#endif
+
+	if (!(aspeed_jtag->mode & JTAG_XFER_HW_MODE)) {
+		aspeed_jtag_set_tap_state_sw(aspeed_jtag, tapstate);
+		return 0;
+	}
+
+	aspeed_jtag_set_tap_state_hw2(aspeed_jtag, tapstate);
+	return 0;
+}
+
+static void aspeed_jtag_xfer_sw(struct aspeed_jtag *aspeed_jtag,
+				struct jtag_xfer *xfer, u32 *data)
+{
+	unsigned long remain_xfer = xfer->length;
+	unsigned long shift_bits = 0;
+	unsigned long index = 0;
+	unsigned long tdi;
+	char tdo;
+
+#ifdef DEBUG_JTAG
+	dev_dbg(aspeed_jtag->dev, "SW JTAG SHIFT %s, length = %d\n",
+		(xfer->type == JTAG_SIR_XFER) ? "IR" : "DR", xfer->length);
+#endif
+
+	if (xfer->type == JTAG_SIR_XFER)
+		aspeed_jtag_set_tap_state(aspeed_jtag, xfer->from,
+					  JTAG_STATE_SHIFTIR);
+	else
+		aspeed_jtag_set_tap_state(aspeed_jtag, xfer->from,
+					  JTAG_STATE_SHIFTDR);
+
+	tdi = ASPEED_JTAG_GET_TDI(xfer->direction, data[index]);
+	data[index] = 0;
+	while (remain_xfer > 1) {
+		tdo = aspeed_jtag_tck_cycle(aspeed_jtag, 0,
+					    tdi & ASPEED_JTAG_DATA_MSB);
+		data[index] |= tdo
+			       << (shift_bits % ASPEED_JTAG_DATA_CHUNK_SIZE);
+		tdi >>= 1;
+		shift_bits++;
+		remain_xfer--;
+
+		if (shift_bits % ASPEED_JTAG_DATA_CHUNK_SIZE == 0) {
+			tdo = 0;
+			index++;
+			tdi = ASPEED_JTAG_GET_TDI(xfer->direction, data[index]);
+			data[index] = 0;
+		}
+	}
+
+	if ((xfer->endstate == (xfer->type == JTAG_SIR_XFER ?
+					JTAG_STATE_SHIFTIR :
+					JTAG_STATE_SHIFTDR))) {
+		/* Stay in Shift IR/DR*/
+		tdo = aspeed_jtag_tck_cycle(aspeed_jtag, 0,
+					    tdi & ASPEED_JTAG_DATA_MSB);
+		data[index] |= tdo
+			       << (shift_bits % ASPEED_JTAG_DATA_CHUNK_SIZE);
+	} else {
+		/* Goto end state */
+		tdo = aspeed_jtag_tck_cycle(aspeed_jtag, 1,
+					    tdi & ASPEED_JTAG_DATA_MSB);
+		data[index] |= tdo
+			       << (shift_bits % ASPEED_JTAG_DATA_CHUNK_SIZE);
+		aspeed_jtag->status = (xfer->type == JTAG_SIR_XFER) ?
+					      JTAG_STATE_EXIT1IR :
+					      JTAG_STATE_EXIT1DR;
+		aspeed_jtag_set_tap_state(aspeed_jtag, aspeed_jtag->status,
+					  xfer->endstate);
+	}
+}
+
+static int aspeed_jtag_xfer_push_data_26xx(struct aspeed_jtag *aspeed_jtag,
+					   enum jtag_xfer_type type,
+					   u32 bits_len)
+{
+	int res = 0;
+
+	aspeed_jtag_write(aspeed_jtag, ASPEED_JTAG_TRANS_LEN(bits_len),
+			  ASPEED_JTAG_CTRL);
+	if (type == JTAG_SIR_XFER) {
+		aspeed_jtag_write(aspeed_jtag,
+				  ASPEED_JTAG_TRANS_LEN(bits_len) |
+					  ASPEED_JTAG_CTL_26XX_INST_EN,
+				  ASPEED_JTAG_CTRL);
+		res = aspeed_jtag_isr_wait(aspeed_jtag,
+					   ASPEED_JTAG_ISR_INST_PAUSE);
+	} else {
+		aspeed_jtag_write(aspeed_jtag,
+				  ASPEED_JTAG_TRANS_LEN(bits_len) |
+					  ASPEED_JTAG_CTL_DATA_EN,
+				  ASPEED_JTAG_CTRL);
+		res = aspeed_jtag_isr_wait(aspeed_jtag,
+					   ASPEED_JTAG_ISR_DATA_PAUSE);
+	}
+	return res;
+}
+
+static int aspeed_jtag_xfer_push_data(struct aspeed_jtag *aspeed_jtag,
+				      enum jtag_xfer_type type, u32 bits_len)
+{
+	int res = 0;
+
+	if (type == JTAG_SIR_XFER) {
+		aspeed_jtag_write(aspeed_jtag, ASPEED_JTAG_IOUT_LEN(bits_len),
+				  ASPEED_JTAG_CTRL);
+		aspeed_jtag_write(aspeed_jtag,
+				  ASPEED_JTAG_IOUT_LEN(bits_len) |
+					  ASPEED_JTAG_CTL_INST_EN,
+				  ASPEED_JTAG_CTRL);
+		res = aspeed_jtag_isr_wait(aspeed_jtag,
+					   ASPEED_JTAG_ISR_INST_PAUSE);
+	} else {
+		aspeed_jtag_write(aspeed_jtag, ASPEED_JTAG_DOUT_LEN(bits_len),
+				  ASPEED_JTAG_CTRL);
+		aspeed_jtag_write(aspeed_jtag,
+				  ASPEED_JTAG_DOUT_LEN(bits_len) |
+					  ASPEED_JTAG_CTL_DATA_EN,
+				  ASPEED_JTAG_CTRL);
+		res = aspeed_jtag_isr_wait(aspeed_jtag,
+					   ASPEED_JTAG_ISR_DATA_PAUSE);
+	}
+	return res;
+}
+
+static int aspeed_jtag_xfer_push_data_last_26xx(struct aspeed_jtag *aspeed_jtag,
+						enum jtag_xfer_type type,
+						u32 shift_bits)
+{
+	int res = 0;
+
+	aspeed_jtag_write(aspeed_jtag,
+			  ASPEED_JTAG_TRANS_LEN(shift_bits) |
+				  ASPEED_JTAG_CTL_26XX_LASPEED_TRANS,
+			  ASPEED_JTAG_CTRL);
+	if (type == JTAG_SIR_XFER) {
+		aspeed_jtag_write(aspeed_jtag,
+				  ASPEED_JTAG_TRANS_LEN(shift_bits) |
+					  ASPEED_JTAG_CTL_26XX_LASPEED_TRANS |
+					  ASPEED_JTAG_CTL_26XX_INST_EN,
+				  ASPEED_JTAG_CTRL);
+		res = aspeed_jtag_isr_wait(aspeed_jtag,
+					   ASPEED_JTAG_ISR_INST_COMPLETE);
+	} else {
+		aspeed_jtag_write(aspeed_jtag,
+				  ASPEED_JTAG_TRANS_LEN(shift_bits) |
+					  ASPEED_JTAG_CTL_26XX_LASPEED_TRANS |
+					  ASPEED_JTAG_CTL_DATA_EN,
+				  ASPEED_JTAG_CTRL);
+		res = aspeed_jtag_isr_wait(aspeed_jtag,
+					   ASPEED_JTAG_ISR_DATA_COMPLETE);
+	}
+	return res;
+}
+
+static int aspeed_jtag_xfer_push_data_last(struct aspeed_jtag *aspeed_jtag,
+					   enum jtag_xfer_type type,
+					   u32 shift_bits)
+{
+	int res = 0;
+
+	if (type == JTAG_SIR_XFER) {
+		aspeed_jtag_write(aspeed_jtag,
+				  ASPEED_JTAG_IOUT_LEN(shift_bits) |
+					  ASPEED_JTAG_CTL_LASPEED_INST,
+				  ASPEED_JTAG_CTRL);
+		aspeed_jtag_write(aspeed_jtag,
+				  ASPEED_JTAG_IOUT_LEN(shift_bits) |
+					  ASPEED_JTAG_CTL_LASPEED_INST |
+					  ASPEED_JTAG_CTL_INST_EN,
+				  ASPEED_JTAG_CTRL);
+		res = aspeed_jtag_isr_wait(aspeed_jtag,
+					   ASPEED_JTAG_ISR_INST_COMPLETE);
+	} else {
+		aspeed_jtag_write(aspeed_jtag,
+				  ASPEED_JTAG_DOUT_LEN(shift_bits) |
+					  ASPEED_JTAG_CTL_LASPEED_DATA,
+				  ASPEED_JTAG_CTRL);
+		aspeed_jtag_write(aspeed_jtag,
+				  ASPEED_JTAG_DOUT_LEN(shift_bits) |
+					  ASPEED_JTAG_CTL_LASPEED_DATA |
+					  ASPEED_JTAG_CTL_DATA_EN,
+				  ASPEED_JTAG_CTRL);
+		res = aspeed_jtag_isr_wait(aspeed_jtag,
+					   ASPEED_JTAG_ISR_DATA_COMPLETE);
+	}
+	return res;
+}
+
+static int aspeed_jtag_xfer_hw(struct aspeed_jtag *aspeed_jtag,
+			       struct jtag_xfer *xfer, u32 *data)
+{
+	unsigned long remain_xfer = xfer->length;
+	unsigned long index = 0;
+	char shift_bits;
+	u32 data_reg;
+	u32 scan_end;
+	union pad_config padding;
+	int retval = 0;
+
+	padding.int_value = xfer->padding;
+
+#ifdef DEBUG_JTAG
+	dev_dbg(aspeed_jtag->dev, "HW JTAG SHIFT %s, length = %d pad = 0x%x\n",
+		(xfer->type == JTAG_SIR_XFER) ? "IR" : "DR", xfer->length,
+		xfer->padding);
+#endif
+	data_reg = xfer->type == JTAG_SIR_XFER ? ASPEED_JTAG_INST :
+						 ASPEED_JTAG_DATA;
+	if (xfer->endstate == JTAG_STATE_SHIFTIR ||
+	    xfer->endstate == JTAG_STATE_SHIFTDR ||
+	    xfer->endstate == JTAG_STATE_PAUSEIR ||
+	    xfer->endstate == JTAG_STATE_PAUSEDR) {
+		scan_end = 0;
+	} else {
+		if (padding.post_pad_number)
+			scan_end = 0;
+		else
+			scan_end = 1;
+	}
+
+	/* Perform pre padding */
+	if (padding.pre_pad_number) {
+		struct jtag_xfer pre_xfer = {
+			.type = xfer->type,
+			.direction = JTAG_WRITE_XFER,
+			.from = xfer->from,
+			.endstate = xfer->type == JTAG_SIR_XFER ?
+				    JTAG_STATE_SHIFTIR : JTAG_STATE_SHIFTDR,
+			.padding = 0,
+			.length = padding.pre_pad_number,
+		};
+		if (padding.pre_pad_number > ASPEED_JTAG_MAX_PAD_SIZE)
+			return -EINVAL;
+		retval = aspeed_jtag_xfer_hw(aspeed_jtag, &pre_xfer,
+					     padding.pad_data ?
+					     aspeed_jtag->pad_data_one :
+					     aspeed_jtag->pad_data_zero);
+		if (retval)
+			return retval;
+	}
+
+	while (remain_xfer) {
+		if (xfer->direction & JTAG_WRITE_XFER)
+			aspeed_jtag_write(aspeed_jtag, data[index], data_reg);
+		else
+			aspeed_jtag_write(aspeed_jtag, 0, data_reg);
+		if (aspeed_jtag->llops->xfer_hw_fifo_delay)
+			aspeed_jtag->llops->xfer_hw_fifo_delay();
+
+		if (remain_xfer > ASPEED_JTAG_DATA_CHUNK_SIZE) {
+#ifdef DEBUG_JTAG
+			dev_dbg(aspeed_jtag->dev,
+				"Chunk len=%d chunk_size=%d remain_xfer=%lu\n",
+				xfer->length, ASPEED_JTAG_DATA_CHUNK_SIZE,
+				remain_xfer);
+#endif
+			shift_bits = ASPEED_JTAG_DATA_CHUNK_SIZE;
+
+			/*
+			 * Transmit bytes that were not equals to column length
+			 * and after the transfer go to Pause IR/DR.
+			 */
+			if (aspeed_jtag->llops->xfer_push_data(aspeed_jtag,
+							       xfer->type,
+							       shift_bits)
+							       != 0) {
+				return -EFAULT;
+			}
+		} else {
+			/*
+			 * Read bytes equals to column length
+			 */
+			shift_bits = remain_xfer;
+			if (scan_end) {
+				/*
+				 * If this data is the end of the transmission
+				 * send remaining bits and go to endstate
+				 */
+#ifdef DEBUG_JTAG
+				dev_dbg(aspeed_jtag->dev,
+					"Last len=%d chunk_size=%d remain_xfer=%lu\n",
+					xfer->length,
+					ASPEED_JTAG_DATA_CHUNK_SIZE,
+					remain_xfer);
+#endif
+				if (aspeed_jtag->llops->xfer_push_data_last(
+					    aspeed_jtag, xfer->type,
+					    shift_bits) != 0) {
+					return -EFAULT;
+				}
+			} else {
+				/*
+				 * If transmission is waiting for additional
+				 * data send remaining bits and then go to
+				 * Pause IR/DR.
+				 */
+#ifdef DEBUG_JTAG
+				dev_dbg(aspeed_jtag->dev,
+					"Tail len=%d chunk_size=%d remain_xfer=%lu\n",
+					xfer->length,
+					ASPEED_JTAG_DATA_CHUNK_SIZE,
+					remain_xfer);
+#endif
+				if (aspeed_jtag->llops->xfer_push_data(
+					    aspeed_jtag, xfer->type,
+					    shift_bits) != 0) {
+					return -EFAULT;
+				}
+			}
+		}
+
+		if (xfer->direction & JTAG_READ_XFER) {
+			if (shift_bits < ASPEED_JTAG_DATA_CHUNK_SIZE) {
+				data[index] =
+					aspeed_jtag_read(aspeed_jtag, data_reg);
+
+				data[index] >>= ASPEED_JTAG_DATA_CHUNK_SIZE -
+						shift_bits;
+			} else {
+				data[index] =
+					aspeed_jtag_read(aspeed_jtag, data_reg);
+			}
+			if (aspeed_jtag->llops->xfer_hw_fifo_delay)
+				aspeed_jtag->llops->xfer_hw_fifo_delay();
+		}
+
+		remain_xfer = remain_xfer - shift_bits;
+		index++;
+	}
+
+	/* Perform post padding */
+	if (padding.post_pad_number) {
+		struct jtag_xfer post_xfer = {
+			.type = xfer->type,
+			.direction = JTAG_WRITE_XFER,
+			.from = xfer->from,
+			.endstate = xfer->endstate,
+			.padding = 0,
+			.length = padding.post_pad_number,
+		};
+		if (padding.post_pad_number > ASPEED_JTAG_MAX_PAD_SIZE)
+			return -EINVAL;
+		retval = aspeed_jtag_xfer_hw(aspeed_jtag, &post_xfer,
+					     padding.pad_data ?
+					     aspeed_jtag->pad_data_one :
+					     aspeed_jtag->pad_data_zero);
+		if (retval)
+			return retval;
+	}
+	return 0;
+}
+
+static int aspeed_jtag_xfer(struct jtag *jtag, struct jtag_xfer *xfer,
+			    u8 *xfer_data)
+{
+	struct aspeed_jtag *aspeed_jtag = jtag_priv(jtag);
+
+	if (!(aspeed_jtag->mode & JTAG_XFER_HW_MODE)) {
+		/* SW mode */
+		aspeed_jtag_write(aspeed_jtag, ASPEED_JTAG_SW_TDIO,
+				  ASPEED_JTAG_SW);
+
+		aspeed_jtag->llops->xfer_sw(aspeed_jtag, xfer,
+					    (u32 *)xfer_data);
+	} else {
+		/* HW mode */
+		aspeed_jtag_write(aspeed_jtag, 0, ASPEED_JTAG_SW);
+		if (aspeed_jtag->llops->xfer_hw(aspeed_jtag, xfer,
+						(u32 *)xfer_data) != 0)
+			return -EFAULT;
+	}
+
+	aspeed_jtag->status = xfer->endstate;
+	return 0;
+}
+
+static int aspeed_jtag_xfer_hw2(struct aspeed_jtag *aspeed_jtag,
+				struct jtag_xfer *xfer, u32 *data)
+{
+	unsigned long remain_xfer = xfer->length;
+	unsigned long partial_xfer_size = 0;
+	unsigned long index = 0;
+	u32 shift_bits;
+	u32 data_reg;
+	u32 reg_val;
+	enum jtag_tapstate shift;
+	enum jtag_tapstate exit;
+	enum jtag_tapstate exitx;
+	enum jtag_tapstate pause;
+	enum jtag_tapstate endstate;
+	u32 start_shift;
+	u32 end_shift;
+	u32 tms_mask;
+	int ret;
+
+	if (xfer->type == JTAG_SIR_XFER) {
+		data_reg = ASPEED_JTAG_SHDATA;
+		shift = JTAG_STATE_SHIFTIR;
+		pause = JTAG_STATE_PAUSEIR;
+		exit = JTAG_STATE_EXIT1IR;
+		exitx = JTAG_STATE_EXIT1DR;
+	} else {
+		data_reg = ASPEED_JTAG_SHDATA;
+		shift = JTAG_STATE_SHIFTDR;
+		pause = JTAG_STATE_PAUSEDR;
+		exit = JTAG_STATE_EXIT1DR;
+		exitx = JTAG_STATE_EXIT1IR;
+	}
+#ifdef DEBUG_JTAG
+	dev_dbg(aspeed_jtag->dev,
+		"HW2 JTAG SHIFT %s, length %d status %s from %s to %s then %s pad 0x%x\n",
+		(xfer->type == JTAG_SIR_XFER) ? "IR" : "DR", xfer->length,
+		end_status_str[aspeed_jtag->current_state],
+		end_status_str[xfer->from],
+		end_status_str[shift],
+		end_status_str[xfer->endstate], xfer->padding);
+#endif
+
+	if (aspeed_jtag->current_state == shift) {
+		start_shift = 0;
+	} else {
+		start_shift = ASPEED_JTAG_SHCTRL_START_SHIFT;
+	}
+
+	if (xfer->endstate == shift) {
+		/*
+		 * In the case of shifting 1 bit of data and attempting to stay
+		 * in the SHIFT state, the AST2600 JTAG Master Controller in
+		 * Hardware mode 2 has been observed to go to EXIT1 IR/DR
+		 * instead of staying in the SHIFT IR/DR state. The following
+		 * code special cases this one bit shift and directs the state
+		 * machine to go to the PAUSE IR/DR state instead.
+		 * Alternatively, the application making driver calls can avoid
+		 * this situation as follows:
+		 *   1.) Bundle all of the shift bits  together into one call
+		 *       AND/OR
+		 *   2.) Direct all partial shifts to move to the PAUSE-IR/DR
+		 *       state.
+		 */
+		if (xfer->length == 1) {
+#ifdef DEBUG_JTAG
+			dev_warn(aspeed_jtag->dev, "JTAG Silicon WA: going to pause instead of shift");
+#endif
+			end_shift = ASPEED_JTAG_SHCTRL_END_SHIFT;
+			endstate = pause;
+		} else {
+			end_shift = 0;
+			endstate = shift;
+		}
+	} else {
+		endstate = xfer->endstate;
+		end_shift = ASPEED_JTAG_SHCTRL_END_SHIFT;
+	}
+
+	aspeed_jtag_write(aspeed_jtag, xfer->padding, ASPEED_JTAG_PADCTRL0);
+
+	while (remain_xfer) {
+		unsigned long partial_xfer;
+		unsigned long partial_index;
+
+		if (remain_xfer > ASPEED_JTAG_HW2_DATA_CHUNK_SIZE)
+			partial_xfer_size = ASPEED_JTAG_HW2_DATA_CHUNK_SIZE;
+		else
+			partial_xfer_size = remain_xfer;
+
+		partial_index = index;
+		partial_xfer = partial_xfer_size;
+
+		reg_val = aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_GBLCTRL);
+		aspeed_jtag_write(aspeed_jtag, reg_val |
+				  ASPEED_JTAG_GBLCTRL_RESET_FIFO,
+				  ASPEED_JTAG_GBLCTRL);
+
+		/* Switch internal FIFO into CPU mode */
+		reg_val = reg_val & ~BIT(24);
+		aspeed_jtag_write(aspeed_jtag, reg_val,
+				  ASPEED_JTAG_GBLCTRL);
+
+		while (partial_xfer) {
+			if (partial_xfer > ASPEED_JTAG_DATA_CHUNK_SIZE)
+				shift_bits = ASPEED_JTAG_DATA_CHUNK_SIZE;
+			else
+				shift_bits = partial_xfer;
+
+			if (xfer->direction & JTAG_WRITE_XFER)
+				aspeed_jtag_write(aspeed_jtag,
+						  data[partial_index++],
+						  data_reg);
+			else
+				aspeed_jtag_write(aspeed_jtag, 0, data_reg);
+			if (aspeed_jtag->llops->xfer_hw_fifo_delay)
+				aspeed_jtag->llops->xfer_hw_fifo_delay();
+			partial_xfer = partial_xfer - shift_bits;
+		}
+		if (remain_xfer > ASPEED_JTAG_HW2_DATA_CHUNK_SIZE) {
+			shift_bits = ASPEED_JTAG_HW2_DATA_CHUNK_SIZE;
+
+			/*
+			 * Transmit bytes that were not equals to column length
+			 * and after the transfer go to Pause IR/DR.
+			 */
+
+			ret = aspeed_jtag_shctrl_tms_mask(aspeed_jtag->current_state, shift, exit,
+							  endstate, start_shift, 0, &tms_mask);
+			if (ret)
+				return ret;
+
+			reg_val = aspeed_jtag_read(aspeed_jtag,
+						   ASPEED_JTAG_GBLCTRL);
+			reg_val = reg_val & ~(GENMASK(22, 20));
+			aspeed_jtag_write(aspeed_jtag, reg_val |
+					  ASPEED_JTAG_GBLCTRL_FIFO_CTRL_MODE |
+					  ASPEED_JTAG_GBLCTRL_UPDT_SHIFT(
+						shift_bits),
+					  ASPEED_JTAG_GBLCTRL);
+
+			aspeed_jtag_write(aspeed_jtag, tms_mask |
+				ASPEED_JTAG_SHCTRL_LWRDT_SHIFT(shift_bits),
+				ASPEED_JTAG_SHCTRL);
+			aspeed_jtag_wait_shift_complete(aspeed_jtag);
+		} else {
+			/*
+			 * Read bytes equals to column length
+			 */
+			shift_bits = remain_xfer;
+			ret = aspeed_jtag_shctrl_tms_mask(aspeed_jtag->current_state, shift, exit,
+							  endstate, start_shift, end_shift,
+							  &tms_mask);
+			if (ret)
+				return ret;
+
+			reg_val = aspeed_jtag_read(aspeed_jtag,
+						   ASPEED_JTAG_GBLCTRL);
+			reg_val = reg_val & ~(GENMASK(22, 20));
+			aspeed_jtag_write(aspeed_jtag, reg_val |
+					  ASPEED_JTAG_GBLCTRL_FIFO_CTRL_MODE |
+					  ASPEED_JTAG_GBLCTRL_UPDT_SHIFT(
+						shift_bits),
+					  ASPEED_JTAG_GBLCTRL);
+
+			aspeed_jtag_write(aspeed_jtag, tms_mask |
+					  ASPEED_JTAG_SHCTRL_LWRDT_SHIFT(
+						  shift_bits),
+					  ASPEED_JTAG_SHCTRL);
+
+			aspeed_jtag_wait_shift_complete(aspeed_jtag);
+		}
+
+		partial_index = index;
+		partial_xfer = partial_xfer_size;
+		while (partial_xfer) {
+			if (partial_xfer >
+			    ASPEED_JTAG_DATA_CHUNK_SIZE) {
+				shift_bits =
+					ASPEED_JTAG_DATA_CHUNK_SIZE;
+				data[partial_index++] =
+					aspeed_jtag_read(aspeed_jtag,
+							 data_reg);
+
+			} else {
+				shift_bits = partial_xfer;
+				data[partial_index++] =
+					aspeed_jtag_read(aspeed_jtag,
+							 data_reg);
+			}
+			if (aspeed_jtag->llops->xfer_hw_fifo_delay)
+				aspeed_jtag->llops->xfer_hw_fifo_delay();
+			partial_xfer = partial_xfer - shift_bits;
+		}
+
+		remain_xfer = remain_xfer - partial_xfer_size;
+		index = partial_index;
+		start_shift = 0;
+	}
+	aspeed_jtag->current_state = endstate;
+	return 0;
+}
+
+static int aspeed_jtag_status_get(struct jtag *jtag, u32 *status)
+{
+	struct aspeed_jtag *aspeed_jtag = jtag_priv(jtag);
+
+	*status = aspeed_jtag->current_state;
+	return 0;
+}
+
+static irqreturn_t aspeed_jtag_interrupt(s32 this_irq, void *dev_id)
+{
+	struct aspeed_jtag *aspeed_jtag = dev_id;
+	irqreturn_t ret;
+	u32 status;
+
+	status = aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_ISR);
+
+	if (status & ASPEED_JTAG_ISR_INT_MASK) {
+		aspeed_jtag_write(aspeed_jtag,
+				  (status & ASPEED_JTAG_ISR_INT_MASK) |
+					  (status &
+					   ASPEED_JTAG_ISR_INT_EN_MASK),
+				  ASPEED_JTAG_ISR);
+		aspeed_jtag->flag |= status & ASPEED_JTAG_ISR_INT_MASK;
+	}
+
+	if (aspeed_jtag->flag) {
+		wake_up_interruptible(&aspeed_jtag->jtag_wq);
+		ret = IRQ_HANDLED;
+	} else {
+		dev_err(aspeed_jtag->dev, "irq status:%x\n", status);
+		ret = IRQ_NONE;
+	}
+	return ret;
+}
+
+static irqreturn_t aspeed_jtag_interrupt_hw2(s32 this_irq, void *dev_id)
+{
+	struct aspeed_jtag *aspeed_jtag = dev_id;
+	irqreturn_t ret;
+	u32 status;
+
+	status = aspeed_jtag_read(aspeed_jtag, ASPEED_JTAG_INTCTRL);
+
+	if (status & ASPEED_JTAG_INTCTRL_SHCPL_IRQ_STAT) {
+		aspeed_jtag_write(aspeed_jtag,
+				  status | ASPEED_JTAG_INTCTRL_SHCPL_IRQ_STAT,
+				  ASPEED_JTAG_INTCTRL);
+		aspeed_jtag->flag |= status & ASPEED_JTAG_INTCTRL_SHCPL_IRQ_STAT;
+	}
+
+	if (aspeed_jtag->flag) {
+		wake_up_interruptible(&aspeed_jtag->jtag_wq);
+		ret = IRQ_HANDLED;
+	} else {
+		dev_err(aspeed_jtag->dev, "irq status:%x\n", status);
+		ret = IRQ_NONE;
+	}
+	return ret;
+}
+
+static int aspeed_jtag_enable(struct jtag *jtag)
+{
+	struct aspeed_jtag *aspeed_jtag = jtag_priv(jtag);
+
+	aspeed_jtag->llops->master_enable(aspeed_jtag);
+	return 0;
+}
+
+static int aspeed_jtag_disable(struct jtag *jtag)
+{
+	struct aspeed_jtag *aspeed_jtag = jtag_priv(jtag);
+
+	aspeed_jtag->llops->output_disable(aspeed_jtag);
+	return 0;
+}
+
+static int aspeed_jtag_init(struct platform_device *pdev,
+			    struct aspeed_jtag *aspeed_jtag)
+{
+	struct resource *res;
+#ifdef USE_INTERRUPTS
+	int err;
+#endif
+	memset(aspeed_jtag->pad_data_one, ~0,
+	       sizeof(aspeed_jtag->pad_data_one));
+	memset(aspeed_jtag->pad_data_zero, 0,
+	       sizeof(aspeed_jtag->pad_data_zero));
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	aspeed_jtag->reg_base = devm_ioremap_resource(aspeed_jtag->dev, res);
+	if (IS_ERR(aspeed_jtag->reg_base))
+		return -ENOMEM;
+
+	aspeed_jtag->pclk = devm_clk_get(aspeed_jtag->dev, NULL);
+	if (IS_ERR(aspeed_jtag->pclk)) {
+		dev_err(aspeed_jtag->dev, "devm_clk_get failed\n");
+		return PTR_ERR(aspeed_jtag->pclk);
+	}
+
+#ifdef USE_INTERRUPTS
+	aspeed_jtag->irq = platform_get_irq(pdev, 0);
+	if (aspeed_jtag->irq < 0) {
+		dev_err(aspeed_jtag->dev, "no irq specified\n");
+		return -ENOENT;
+	}
+#endif
+
+	if (clk_prepare_enable(aspeed_jtag->pclk)) {
+		dev_err(aspeed_jtag->dev, "no irq specified\n");
+		return -ENOENT;
+	}
+
+	aspeed_jtag->rst = devm_reset_control_get_shared(&pdev->dev, NULL);
+	if (IS_ERR(aspeed_jtag->rst)) {
+		dev_err(aspeed_jtag->dev,
+			"missing or invalid reset controller device tree entry");
+		return PTR_ERR(aspeed_jtag->rst);
+	}
+	reset_control_deassert(aspeed_jtag->rst);
+
+#ifdef USE_INTERRUPTS
+	err = devm_request_irq(aspeed_jtag->dev, aspeed_jtag->irq,
+			       aspeed_jtag->llops->jtag_interrupt, 0,
+			       "aspeed-jtag", aspeed_jtag);
+	if (err) {
+		dev_err(aspeed_jtag->dev, "unable to get IRQ");
+		clk_disable_unprepare(aspeed_jtag->pclk);
+		return err;
+	}
+#endif
+
+	aspeed_jtag->llops->output_disable(aspeed_jtag);
+
+	aspeed_jtag->flag = 0;
+	aspeed_jtag->mode = 0;
+	init_waitqueue_head(&aspeed_jtag->jtag_wq);
+	return 0;
+}
+
+static int aspeed_jtag_deinit(struct platform_device *pdev,
+			      struct aspeed_jtag *aspeed_jtag)
+{
+	aspeed_jtag_write(aspeed_jtag, 0, ASPEED_JTAG_ISR);
+	/* Disable clock */
+	aspeed_jtag_write(aspeed_jtag, 0, ASPEED_JTAG_CTRL);
+	reset_control_assert(aspeed_jtag->rst);
+	clk_disable_unprepare(aspeed_jtag->pclk);
+	return 0;
+}
+
+static int aspeed_jtag_trst_set(struct jtag *jtag, u32 active)
+{
+	struct aspeed_jtag *aspeed_jtag = jtag_priv(jtag);
+
+	aspeed_jtag_write(aspeed_jtag, active ? 0 : ASPEED_JTAG_EC_TRSTn_HIGH,
+			  ASPEED_JTAG_EC);
+	return 0;
+}
+
+static const struct jtag_ops aspeed_jtag_ops = {
+	.freq_get = aspeed_jtag_freq_get,
+	.freq_set = aspeed_jtag_freq_set,
+	.status_get = aspeed_jtag_status_get,
+	.status_set = aspeed_jtag_status_set,
+	.xfer = aspeed_jtag_xfer,
+	.mode_set = aspeed_jtag_mode_set,
+	.bitbang = aspeed_jtag_bitbang,
+	.enable = aspeed_jtag_enable,
+	.disable = aspeed_jtag_disable
+};
+
+static const struct jtag_ops aspeed_jtag_ops_26xx = {
+#ifdef ASPEED_JTAG_HW_MODE_2_ENABLE
+	.freq_get = aspeed_jtag_freq_get_26xx,
+	.freq_set = aspeed_jtag_freq_set_26xx,
+	.status_get = aspeed_jtag_status_get,
+	.status_set = aspeed_jtag_status_set_26xx,
+#else
+	.freq_get = aspeed_jtag_freq_get,
+	.freq_set = aspeed_jtag_freq_set,
+	.status_get = aspeed_jtag_status_get,
+	.status_set = aspeed_jtag_status_set,
+#endif
+	.xfer = aspeed_jtag_xfer,
+	.mode_set = aspeed_jtag_mode_set,
+	.trst_set = aspeed_jtag_trst_set,
+	.bitbang = aspeed_jtag_bitbang,
+	.enable = aspeed_jtag_enable,
+	.disable = aspeed_jtag_disable
+};
+
+static const struct jtag_low_level_functions ast25xx_llops = {
+	.master_enable = aspeed_jtag_master,
+	.output_disable = aspeed_jtag_output_disable,
+	.xfer_push_data = aspeed_jtag_xfer_push_data,
+	.xfer_push_data_last = aspeed_jtag_xfer_push_data_last,
+	.xfer_sw = aspeed_jtag_xfer_sw,
+	.xfer_hw = aspeed_jtag_xfer_hw,
+	.xfer_hw_fifo_delay = NULL,
+	.xfer_sw_delay = NULL,
+	.jtag_interrupt = aspeed_jtag_interrupt
+};
+
+static const struct aspeed_jtag_functions ast25xx_functions = {
+	.aspeed_jtag_ops = &aspeed_jtag_ops,
+	.aspeed_jtag_llops = &ast25xx_llops
+};
+
+static const struct jtag_low_level_functions ast26xx_llops = {
+#ifdef ASPEED_JTAG_HW_MODE_2_ENABLE
+	.master_enable = aspeed_jtag_master_26xx,
+	.output_disable = aspeed_jtag_output_disable_26xx,
+	.xfer_push_data = aspeed_jtag_xfer_push_data_26xx,
+	.xfer_push_data_last = aspeed_jtag_xfer_push_data_last_26xx,
+	.xfer_sw = aspeed_jtag_xfer_sw,
+	.xfer_hw = aspeed_jtag_xfer_hw2,
+	.xfer_hw_fifo_delay = aspeed_jtag_xfer_hw_fifo_delay_26xx,
+	.xfer_sw_delay = aspeed_jtag_sw_delay_26xx,
+	.jtag_interrupt = aspeed_jtag_interrupt_hw2
+#else
+	.master_enable = aspeed_jtag_master,
+	.output_disable = aspeed_jtag_output_disable,
+	.xfer_push_data = aspeed_jtag_xfer_push_data_26xx,
+	.xfer_push_data_last = aspeed_jtag_xfer_push_data_last_26xx,
+	.xfer_sw = aspeed_jtag_xfer_sw,
+	.xfer_hw = aspeed_jtag_xfer_hw,
+	.xfer_hw_fifo_delay = aspeed_jtag_xfer_hw_fifo_delay_26xx,
+	.xfer_sw_delay = aspeed_jtag_sw_delay_26xx,
+	.jtag_interrupt = aspeed_jtag_interrupt
+#endif
+};
+
+static const struct aspeed_jtag_functions ast26xx_functions = {
+	.aspeed_jtag_ops = &aspeed_jtag_ops_26xx,
+	.aspeed_jtag_llops = &ast26xx_llops
+};
+
+static const struct of_device_id aspeed_jtag_of_match[] = {
+	{ .compatible = "aspeed,ast2400-jtag", .data = &ast25xx_functions },
+	{ .compatible = "aspeed,ast2500-jtag", .data = &ast25xx_functions },
+	{ .compatible = "aspeed,ast2600-jtag", .data = &ast26xx_functions },
+	{}
+};
+
+static int aspeed_jtag_probe(struct platform_device *pdev)
+{
+	struct aspeed_jtag *aspeed_jtag;
+	struct jtag *jtag;
+	const struct of_device_id *match;
+	const struct aspeed_jtag_functions *jtag_functions;
+	int err;
+
+	match = of_match_node(aspeed_jtag_of_match, pdev->dev.of_node);
+	if (!match)
+		return -ENODEV;
+	jtag_functions = match->data;
+
+	jtag = jtag_alloc(&pdev->dev, sizeof(*aspeed_jtag),
+			  jtag_functions->aspeed_jtag_ops);
+	if (!jtag)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, jtag);
+	aspeed_jtag = jtag_priv(jtag);
+	aspeed_jtag->dev = &pdev->dev;
+
+	aspeed_jtag->llops = jtag_functions->aspeed_jtag_llops;
+
+	/* Initialize device*/
+	err = aspeed_jtag_init(pdev, aspeed_jtag);
+	if (err)
+		goto err_jtag_init;
+
+	/* Initialize JTAG core structure*/
+	err = devm_jtag_register(aspeed_jtag->dev, jtag);
+	if (err)
+		goto err_jtag_register;
+
+	jtag_functions->aspeed_jtag_ops->freq_set(jtag, 1000000);
+
+	return 0;
+
+err_jtag_register:
+	aspeed_jtag_deinit(pdev, aspeed_jtag);
+err_jtag_init:
+	jtag_free(jtag);
+	return err;
+}
+
+static int aspeed_jtag_remove(struct platform_device *pdev)
+{
+	struct jtag *jtag = platform_get_drvdata(pdev);
+
+	aspeed_jtag_deinit(pdev, jtag_priv(jtag));
+	return 0;
+}
+
+static struct platform_driver aspeed_jtag_driver = {
+	.probe = aspeed_jtag_probe,
+	.remove = aspeed_jtag_remove,
+	.driver = {
+		.name = ASPEED_JTAG_NAME,
+		.of_match_table = aspeed_jtag_of_match,
+	},
+};
+module_platform_driver(aspeed_jtag_driver);
+
+MODULE_AUTHOR("Oleksandr Shamray <oleksandrs@mellanox.com>");
+MODULE_DESCRIPTION("ASPEED JTAG driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/jtag/jtag.c b/drivers/jtag/jtag.c
new file mode 100644
index 000000000000..eea2b0d87005
--- /dev/null
+++ b/drivers/jtag/jtag.c
@@ -0,0 +1,384 @@
+// SPDX-License-Identifier: GPL-2.0-only
+// Copyright (c) 2018 Mellanox Technologies. All rights reserved.
+// Copyright (c) 2018 Oleksandr Shamray <oleksandrs@mellanox.com>
+// Copyright (c) 2019 Intel Corporation
+
+#include <linux/cdev.h>
+#include <linux/device.h>
+#include <linux/jtag.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/miscdevice.h>
+#include <linux/module.h>
+#include <linux/rtnetlink.h>
+#include <linux/spinlock.h>
+#include <uapi/linux/jtag.h>
+
+static char *end_status_str[] = { "tlr",   "idle",   "selDR", "capDR", "sDR",
+				  "ex1DR", "pDR",    "ex2DR", "updDR", "selIR",
+				  "capIR", "sIR",    "ex1IR", "pIR",   "ex2IR",
+				  "updIR", "current" };
+
+struct jtag {
+	struct miscdevice miscdev;
+	const struct jtag_ops *ops;
+	int id;
+	unsigned long *priv;
+};
+
+static DEFINE_IDA(jtag_ida);
+
+void *jtag_priv(struct jtag *jtag)
+{
+	return jtag->priv;
+}
+EXPORT_SYMBOL_GPL(jtag_priv);
+
+static long jtag_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	struct jtag *jtag = file->private_data;
+	struct jtag_tap_state tapstate;
+	struct jtag_xfer xfer;
+	struct bitbang_packet bitbang;
+	struct tck_bitbang *bitbang_data;
+	struct jtag_mode mode;
+	u8 *xfer_data;
+	u32 data_size;
+	u32 value;
+	u32 active;
+	int err;
+
+	if (!arg)
+		return -EINVAL;
+
+	switch (cmd) {
+	case JTAG_GIOCFREQ:
+		if (!jtag->ops->freq_get)
+			return -EOPNOTSUPP;
+
+		err = jtag->ops->freq_get(jtag, &value);
+		if (err)
+			break;
+		dev_dbg(jtag->miscdev.parent, "JTAG_GIOCFREQ: freq get = %d",
+			value);
+
+		if (put_user(value, (__u32 __user *)arg))
+			err = -EFAULT;
+		break;
+
+	case JTAG_SIOCFREQ:
+		if (!jtag->ops->freq_set)
+			return -EOPNOTSUPP;
+
+		if (get_user(value, (__u32 __user *)arg))
+			return -EFAULT;
+		if (value == 0)
+			return -EINVAL;
+
+		err = jtag->ops->freq_set(jtag, value);
+		dev_dbg(jtag->miscdev.parent, "JTAG_SIOCFREQ: freq set = %d",
+			value);
+		break;
+
+	case JTAG_SIOCSTATE:
+		if (copy_from_user(&tapstate, (const void __user *)arg,
+				   sizeof(struct jtag_tap_state)))
+			return -EFAULT;
+
+		if (tapstate.from > JTAG_STATE_CURRENT)
+			return -EINVAL;
+
+		if (tapstate.endstate > JTAG_STATE_CURRENT)
+			return -EINVAL;
+
+		if (tapstate.reset > JTAG_FORCE_RESET)
+			return -EINVAL;
+
+		dev_dbg(jtag->miscdev.parent,
+			"JTAG_SIOCSTATE: status set from %s to %s reset %d tck %d",
+			end_status_str[tapstate.from],
+			end_status_str[tapstate.endstate], tapstate.reset,
+			tapstate.tck);
+
+		err = jtag->ops->status_set(jtag, &tapstate);
+		break;
+
+	case JTAG_IOCXFER:
+	{
+		u8 ubit_mask = GENMASK(7, 0);
+		u8 remaining_bits = 0x0;
+		union pad_config padding;
+
+		if (copy_from_user(&xfer, (const void __user *)arg,
+				   sizeof(struct jtag_xfer)))
+			return -EFAULT;
+
+		if (xfer.length >= JTAG_MAX_XFER_DATA_LEN)
+			return -EINVAL;
+
+		if (xfer.type > JTAG_SDR_XFER)
+			return -EINVAL;
+
+		if (xfer.direction > JTAG_READ_WRITE_XFER)
+			return -EINVAL;
+
+		if (xfer.from > JTAG_STATE_CURRENT)
+			return -EINVAL;
+
+		if (xfer.endstate > JTAG_STATE_CURRENT)
+			return -EINVAL;
+
+		data_size = DIV_ROUND_UP(xfer.length, BITS_PER_BYTE);
+		xfer_data = memdup_user(u64_to_user_ptr(xfer.tdio), data_size);
+
+		/* Save unused remaining bits in this transfer */
+		if ((xfer.length % BITS_PER_BYTE)) {
+			ubit_mask = GENMASK((xfer.length % BITS_PER_BYTE) - 1,
+					    0);
+			remaining_bits = xfer_data[data_size - 1] & ~ubit_mask;
+		}
+
+		if (IS_ERR(xfer_data))
+			return -EFAULT;
+		padding.int_value = xfer.padding;
+		dev_dbg(jtag->miscdev.parent,
+			"JTAG_IOCXFER: type: %s direction: %d, END : %s, padding: (value: %d) pre_pad: %d post_pad: %d, len: %d\n",
+			xfer.type ? "DR" : "IR", xfer.direction,
+			end_status_str[xfer.endstate], padding.pad_data,
+			padding.pre_pad_number, padding.post_pad_number,
+			xfer.length);
+
+		print_hex_dump_debug("I:", DUMP_PREFIX_NONE, 16, 1, xfer_data,
+				     data_size, false);
+
+		err = jtag->ops->xfer(jtag, &xfer, xfer_data);
+		if (err) {
+			kfree(xfer_data);
+			return err;
+		}
+
+		print_hex_dump_debug("O:", DUMP_PREFIX_NONE, 16, 1, xfer_data,
+				     data_size, false);
+
+		/* Restore unused remaining bits in this transfer */
+		xfer_data[data_size - 1] = (xfer_data[data_size - 1]
+					    & ubit_mask) | remaining_bits;
+
+		err = copy_to_user(u64_to_user_ptr(xfer.tdio),
+				   (void *)xfer_data, data_size);
+		kfree(xfer_data);
+		if (err)
+			return -EFAULT;
+
+		if (copy_to_user((void __user *)arg, (void *)&xfer,
+				 sizeof(struct jtag_xfer)))
+			return -EFAULT;
+		break;
+	}
+
+	case JTAG_GIOCSTATUS:
+		err = jtag->ops->status_get(jtag, &value);
+		if (err)
+			break;
+		dev_dbg(jtag->miscdev.parent, "JTAG_GIOCSTATUS: status get %s",
+			end_status_str[value]);
+
+		err = put_user(value, (__u32 __user *)arg);
+		break;
+	case JTAG_IOCBITBANG:
+		if (copy_from_user(&bitbang, (const void __user *)arg,
+				   sizeof(struct bitbang_packet)))
+			return -EFAULT;
+
+		if (bitbang.length >= JTAG_MAX_XFER_DATA_LEN)
+			return -EINVAL;
+
+		data_size = bitbang.length * sizeof(struct tck_bitbang);
+		bitbang_data = memdup_user((void __user *)bitbang.data,
+					   data_size);
+		if (IS_ERR(bitbang_data))
+			return -EFAULT;
+
+		err = jtag->ops->bitbang(jtag, &bitbang, bitbang_data);
+		if (err) {
+			kfree(bitbang_data);
+			return err;
+		}
+		err = copy_to_user((void __user *)bitbang.data,
+				   (void *)bitbang_data, data_size);
+		kfree(bitbang_data);
+		if (err)
+			return -EFAULT;
+		break;
+	case JTAG_SIOCMODE:
+		if (!jtag->ops->mode_set)
+			return -EOPNOTSUPP;
+
+		if (copy_from_user(&mode, (const void __user *)arg,
+				   sizeof(struct jtag_mode)))
+			return -EFAULT;
+
+		dev_dbg(jtag->miscdev.parent,
+			"JTAG_SIOCMODE: mode set feature %d mode %d",
+			mode.feature, mode.mode);
+		err = jtag->ops->mode_set(jtag, &mode);
+		break;
+	case JTAG_SIOCTRST:
+		if (!jtag->ops->trst_set)
+			return -EOPNOTSUPP;
+
+		if (get_user(active, (__u32 __user *)arg))
+			return -EFAULT;
+
+		err = jtag->ops->trst_set(jtag, active);
+		break;
+
+	default:
+		return -EINVAL;
+	}
+	return err;
+}
+
+static int jtag_open(struct inode *inode, struct file *file)
+{
+	struct jtag *jtag = container_of(file->private_data,
+					 struct jtag,
+					 miscdev);
+
+	file->private_data = jtag;
+	if (jtag->ops->enable(jtag))
+		return -EBUSY;
+	return nonseekable_open(inode, file);
+}
+
+static int jtag_release(struct inode *inode, struct file *file)
+{
+	struct jtag *jtag = file->private_data;
+
+	if (jtag->ops->disable(jtag))
+		return -EBUSY;
+	return 0;
+}
+
+static const struct file_operations jtag_fops = {
+	.owner		= THIS_MODULE,
+	.open		= jtag_open,
+	.llseek		= noop_llseek,
+	.unlocked_ioctl	= jtag_ioctl,
+	.release	= jtag_release,
+};
+
+struct jtag *jtag_alloc(struct device *host, size_t priv_size,
+			const struct jtag_ops *ops)
+{
+	struct jtag *jtag;
+
+	if (!host)
+		return NULL;
+
+	if (!ops)
+		return NULL;
+
+	if (!ops->status_set || !ops->status_get || !ops->xfer)
+		return NULL;
+
+	jtag = kzalloc(sizeof(*jtag), GFP_KERNEL);
+	if (!jtag)
+		return NULL;
+	jtag->priv = kzalloc(priv_size, GFP_KERNEL);
+	if (!jtag->priv)
+		return NULL;
+
+	jtag->ops = ops;
+	jtag->miscdev.parent = host;
+
+	return jtag;
+}
+EXPORT_SYMBOL_GPL(jtag_alloc);
+
+void jtag_free(struct jtag *jtag)
+{
+	kfree(jtag);
+}
+EXPORT_SYMBOL_GPL(jtag_free);
+
+static int jtag_register(struct jtag *jtag)
+{
+	struct device *dev = jtag->miscdev.parent;
+	int err;
+	int id;
+
+	if (!dev)
+		return -ENODEV;
+
+	id = ida_simple_get(&jtag_ida, 0, 0, GFP_KERNEL);
+	if (id < 0)
+		return id;
+
+	jtag->id = id;
+
+	jtag->miscdev.fops =  &jtag_fops;
+	jtag->miscdev.minor = MISC_DYNAMIC_MINOR;
+	jtag->miscdev.name = kasprintf(GFP_KERNEL, "jtag%d", id);
+	if (!jtag->miscdev.name) {
+		err = -ENOMEM;
+		goto err_jtag_alloc;
+	}
+
+	err = misc_register(&jtag->miscdev);
+	if (err) {
+		dev_err(jtag->miscdev.parent, "Unable to register device\n");
+		goto err_jtag_name;
+	}
+	return 0;
+
+err_jtag_name:
+	kfree(jtag->miscdev.name);
+err_jtag_alloc:
+	ida_simple_remove(&jtag_ida, id);
+	return err;
+}
+
+static void jtag_unregister(struct jtag *jtag)
+{
+	misc_deregister(&jtag->miscdev);
+	kfree(jtag->miscdev.name);
+	ida_simple_remove(&jtag_ida, jtag->id);
+}
+
+static void devm_jtag_unregister(struct device *dev, void *res)
+{
+	jtag_unregister(*(struct jtag **)res);
+}
+
+int devm_jtag_register(struct device *dev, struct jtag *jtag)
+{
+	struct jtag **ptr;
+	int ret;
+
+	ptr = devres_alloc(devm_jtag_unregister, sizeof(struct jtag *),
+			   GFP_KERNEL);
+	if (!ptr)
+		return -ENOMEM;
+
+	ret = jtag_register(jtag);
+	if (!ret) {
+		*ptr = jtag;
+		devres_add(dev, ptr);
+	} else {
+		devres_free(ptr);
+	}
+	return ret;
+}
+EXPORT_SYMBOL_GPL(devm_jtag_register);
+
+static void __exit jtag_exit(void)
+{
+	ida_destroy(&jtag_ida);
+}
+
+module_exit(jtag_exit);
+
+MODULE_AUTHOR("Oleksandr Shamray <oleksandrs@mellanox.com>");
+MODULE_DESCRIPTION("Generic jtag support");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/pwm/Kconfig b/drivers/pwm/Kconfig
index 60d13a949bc5..54915185d918 100644
--- a/drivers/pwm/Kconfig
+++ b/drivers/pwm/Kconfig
@@ -51,6 +51,16 @@ config PWM_AB8500
 	  To compile this driver as a module, choose M here: the module
 	  will be called pwm-ab8500.
 
+config PWM_ASPEED_AST2600
+	tristate "Aspeed ast2600 PWM support"
+	depends on ARCH_ASPEED || COMPILE_TEST
+	depends on HAVE_CLK && HAS_IOMEM
+	help
+	  This driver provides support for Aspeed ast2600 PWM controllers.
+
+	  To compile this driver as a module, choose M here: the module
+	  will be called pwm-aspeed-ast2600.
+
 config PWM_ATMEL
 	tristate "Atmel PWM support"
 	depends on ARCH_AT91 || COMPILE_TEST
diff --git a/drivers/pwm/Makefile b/drivers/pwm/Makefile
index 7bf1a29f02b8..5169c34056e6 100644
--- a/drivers/pwm/Makefile
+++ b/drivers/pwm/Makefile
@@ -2,6 +2,7 @@
 obj-$(CONFIG_PWM)		+= core.o
 obj-$(CONFIG_PWM_SYSFS)		+= sysfs.o
 obj-$(CONFIG_PWM_AB8500)	+= pwm-ab8500.o
+obj-$(CONFIG_PWM_ASPEED_AST2600)	+= pwm-aspeed-ast2600.o
 obj-$(CONFIG_PWM_ATMEL)		+= pwm-atmel.o
 obj-$(CONFIG_PWM_ATMEL_HLCDC_PWM)	+= pwm-atmel-hlcdc.o
 obj-$(CONFIG_PWM_ATMEL_TCB)	+= pwm-atmel-tcb.o
diff --git a/drivers/pwm/pwm-aspeed-ast2600.c b/drivers/pwm/pwm-aspeed-ast2600.c
new file mode 100644
index 000000000000..c277d2c26b4a
--- /dev/null
+++ b/drivers/pwm/pwm-aspeed-ast2600.c
@@ -0,0 +1,359 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright (C) 2021 Aspeed Technology Inc.
+ *
+ * PWM controller driver for Aspeed ast2600 SoCs.
+ * This drivers doesn't support earlier version of the IP.
+ *
+ * The hardware operates in time quantities of length
+ * Q := (DIV_L + 1) << DIV_H / input-clk
+ * The length of a PWM period is (DUTY_CYCLE_PERIOD + 1) * Q.
+ * The maximal value for DUTY_CYCLE_PERIOD is used here to provide
+ * a fine grained selection for the duty cycle.
+ *
+ * This driver uses DUTY_CYCLE_RISING_POINT = 0, so from the start of a
+ * period the output is active until DUTY_CYCLE_FALLING_POINT * Q. Note
+ * that if DUTY_CYCLE_RISING_POINT = DUTY_CYCLE_FALLING_POINT the output is
+ * always active.
+ *
+ * Register usage:
+ * PIN_ENABLE: When it is unset the pwm controller will emit inactive level to the external.
+ * Use to determine whether the PWM channel is enabled or disabled
+ * CLK_ENABLE: When it is unset the pwm controller will assert the duty counter reset and
+ * emit inactive level to the PIN_ENABLE mux after that the driver can still change the pwm period
+ * and duty and the value will apply when CLK_ENABLE be set again.
+ * Use to determine whether duty_cycle bigger than 0.
+ * PWM_ASPEED_CTRL_INVERSE: When it is toggled the output value will inverse immediately.
+ * PWM_ASPEED_DUTY_CYCLE_FALLING_POINT/PWM_ASPEED_DUTY_CYCLE_RISING_POINT: When these two
+ * values are equal it means the duty cycle = 100%.
+ *
+ * The glitch may generate at:
+ * - Enabled changing when the duty_cycle bigger than 0% and less than 100%.
+ * - Polarity changing when the duty_cycle bigger than 0% and less than 100%.
+ *
+ * Limitations:
+ * - When changing both duty cycle and period, we cannot prevent in
+ *   software that the output might produce a period with mixed
+ *   settings.
+ * - Disabling the PWM doesn't complete the current period.
+ *
+ * Improvements:
+ * - When only changing one of duty cycle or period, our pwm controller will not
+ *   generate the glitch, the configure will change at next cycle of pwm.
+ *   This improvement can disable/enable through PWM_ASPEED_CTRL_DUTY_SYNC_DISABLE.
+ */
+
+#include <linux/bitfield.h>
+#include <linux/clk.h>
+#include <linux/errno.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/math64.h>
+#include <linux/mfd/syscon.h>
+#include <linux/module.h>
+#include <linux/of_device.h>
+#include <linux/of_platform.h>
+#include <linux/platform_device.h>
+#include <linux/pwm.h>
+#include <linux/regmap.h>
+#include <linux/reset.h>
+#include <linux/sysfs.h>
+
+/* The channel number of Aspeed pwm controller */
+#define PWM_ASPEED_NR_PWMS			16
+/* PWM Control Register */
+#define PWM_ASPEED_CTRL(ch)			((ch) * 0x10 + 0x00)
+#define PWM_ASPEED_CTRL_LOAD_SEL_RISING_AS_WDT	BIT(19)
+#define PWM_ASPEED_CTRL_DUTY_LOAD_AS_WDT_ENABLE	BIT(18)
+#define PWM_ASPEED_CTRL_DUTY_SYNC_DISABLE	BIT(17)
+#define PWM_ASPEED_CTRL_CLK_ENABLE		BIT(16)
+#define PWM_ASPEED_CTRL_LEVEL_OUTPUT		BIT(15)
+#define PWM_ASPEED_CTRL_INVERSE			BIT(14)
+#define PWM_ASPEED_CTRL_OPEN_DRAIN_ENABLE	BIT(13)
+#define PWM_ASPEED_CTRL_PIN_ENABLE		BIT(12)
+#define PWM_ASPEED_CTRL_CLK_DIV_H		GENMASK(11, 8)
+#define PWM_ASPEED_CTRL_CLK_DIV_L		GENMASK(7, 0)
+
+/* PWM Duty Cycle Register */
+#define PWM_ASPEED_DUTY_CYCLE(ch)		((ch) * 0x10 + 0x04)
+#define PWM_ASPEED_DUTY_CYCLE_PERIOD		GENMASK(31, 24)
+#define PWM_ASPEED_DUTY_CYCLE_POINT_AS_WDT	GENMASK(23, 16)
+#define PWM_ASPEED_DUTY_CYCLE_FALLING_POINT	GENMASK(15, 8)
+#define PWM_ASPEED_DUTY_CYCLE_RISING_POINT	GENMASK(7, 0)
+
+/* PWM fixed value */
+#define PWM_ASPEED_FIXED_PERIOD			FIELD_MAX(PWM_ASPEED_DUTY_CYCLE_PERIOD)
+
+struct aspeed_pwm_data {
+	struct pwm_chip chip;
+	struct clk *clk;
+	struct regmap *regmap;
+	struct reset_control *reset;
+};
+
+static inline struct aspeed_pwm_data *
+aspeed_pwm_chip_to_data(struct pwm_chip *chip)
+{
+	return container_of(chip, struct aspeed_pwm_data, chip);
+}
+
+static void aspeed_pwm_get_state(struct pwm_chip *chip, struct pwm_device *pwm,
+				 struct pwm_state *state)
+{
+	struct device *dev = chip->dev;
+	struct aspeed_pwm_data *priv = aspeed_pwm_chip_to_data(chip);
+	u32 hwpwm = pwm->hwpwm;
+	bool polarity,	pin_en, clk_en;
+	u32 duty_pt, val;
+	unsigned long rate;
+	u64 div_h, div_l, duty_cycle_period, dividend;
+
+	regmap_read(priv->regmap, PWM_ASPEED_CTRL(hwpwm), &val);
+	polarity = FIELD_GET(PWM_ASPEED_CTRL_INVERSE, val);
+	pin_en = FIELD_GET(PWM_ASPEED_CTRL_PIN_ENABLE, val);
+	clk_en = FIELD_GET(PWM_ASPEED_CTRL_CLK_ENABLE, val);
+	div_h = FIELD_GET(PWM_ASPEED_CTRL_CLK_DIV_H, val);
+	div_l = FIELD_GET(PWM_ASPEED_CTRL_CLK_DIV_L, val);
+	regmap_read(priv->regmap, PWM_ASPEED_DUTY_CYCLE(hwpwm), &val);
+	duty_pt = FIELD_GET(PWM_ASPEED_DUTY_CYCLE_FALLING_POINT, val);
+	duty_cycle_period = FIELD_GET(PWM_ASPEED_DUTY_CYCLE_PERIOD, val);
+
+	rate = clk_get_rate(priv->clk);
+
+	/*
+	 * This multiplication doesn't overflow, the upper bound is
+	 * 1000000000 * 256 * 256 << 15 = 0x1dcd650000000000
+	 */
+	dividend = (u64)NSEC_PER_SEC * (div_l + 1) * (duty_cycle_period + 1)
+		       << div_h;
+	state->period = DIV_ROUND_UP_ULL(dividend, rate);
+
+	if (clk_en && duty_pt) {
+		dividend = (u64)NSEC_PER_SEC * (div_l + 1) * duty_pt
+				 << div_h;
+		state->duty_cycle = DIV_ROUND_UP_ULL(dividend, rate);
+	} else {
+		state->duty_cycle = clk_en ? state->period : 0;
+	}
+	state->polarity = polarity ? PWM_POLARITY_INVERSED : PWM_POLARITY_NORMAL;
+	state->enabled = pin_en;
+	dev_dbg(dev, "get period: %lldns, duty_cycle: %lldns", state->period,
+		state->duty_cycle);
+}
+
+static int aspeed_pwm_apply(struct pwm_chip *chip, struct pwm_device *pwm,
+			    const struct pwm_state *state)
+{
+	struct device *dev = chip->dev;
+	struct aspeed_pwm_data *priv = aspeed_pwm_chip_to_data(chip);
+	u32 hwpwm = pwm->hwpwm, duty_pt;
+	unsigned long rate;
+	u64 div_h, div_l, divisor, expect_period;
+	bool clk_en;
+
+	rate = clk_get_rate(priv->clk);
+	expect_period = min(div64_u64(ULLONG_MAX, (u64)rate), state->period);
+	dev_dbg(dev, "expect period: %lldns, duty_cycle: %lldns", expect_period,
+		state->duty_cycle);
+	/*
+	 * Pick the smallest value for div_h so that div_l can be the biggest
+	 * which results in a finer resolution near the target period value.
+	 */
+	divisor = (u64)NSEC_PER_SEC * (PWM_ASPEED_FIXED_PERIOD + 1) *
+		  (FIELD_MAX(PWM_ASPEED_CTRL_CLK_DIV_L) + 1);
+	div_h = order_base_2(DIV64_U64_ROUND_UP(rate * expect_period, divisor));
+	if (div_h > 0xf)
+		div_h = 0xf;
+
+	divisor = ((u64)NSEC_PER_SEC * (PWM_ASPEED_FIXED_PERIOD + 1)) << div_h;
+	div_l = div64_u64(rate * expect_period, divisor);
+
+	if (div_l == 0)
+		return -ERANGE;
+
+	div_l -= 1;
+
+	if (div_l > 255)
+		div_l = 255;
+
+	dev_dbg(dev, "clk source: %ld div_h %lld, div_l : %lld\n", rate, div_h,
+		div_l);
+	/* duty_pt = duty_cycle * (PERIOD + 1) / period */
+	duty_pt = div64_u64(state->duty_cycle * rate,
+			    (u64)NSEC_PER_SEC * (div_l + 1) << div_h);
+	dev_dbg(dev, "duty_cycle = %lld, duty_pt = %d\n", state->duty_cycle,
+		duty_pt);
+
+	/*
+	 * Fixed DUTY_CYCLE_PERIOD to its max value to get a
+	 * fine-grained resolution for duty_cycle at the expense of a
+	 * coarser period resolution.
+	 */
+	regmap_update_bits(priv->regmap, PWM_ASPEED_DUTY_CYCLE(hwpwm),
+			   PWM_ASPEED_DUTY_CYCLE_PERIOD,
+			   FIELD_PREP(PWM_ASPEED_DUTY_CYCLE_PERIOD,
+				      PWM_ASPEED_FIXED_PERIOD));
+	if (duty_pt == 0) {
+		/* emit inactive level and assert the duty counter reset */
+		clk_en = 0;
+	} else {
+		clk_en = 1;
+		if (duty_pt >= (PWM_ASPEED_FIXED_PERIOD + 1))
+			duty_pt = 0;
+		regmap_update_bits(priv->regmap, PWM_ASPEED_DUTY_CYCLE(hwpwm),
+				   PWM_ASPEED_DUTY_CYCLE_RISING_POINT |
+					   PWM_ASPEED_DUTY_CYCLE_FALLING_POINT,
+				   FIELD_PREP(PWM_ASPEED_DUTY_CYCLE_FALLING_POINT, duty_pt));
+	}
+
+	regmap_update_bits(priv->regmap, PWM_ASPEED_CTRL(hwpwm),
+			   PWM_ASPEED_CTRL_CLK_DIV_H | PWM_ASPEED_CTRL_CLK_DIV_L |
+				   PWM_ASPEED_CTRL_PIN_ENABLE | PWM_ASPEED_CTRL_CLK_ENABLE |
+				   PWM_ASPEED_CTRL_INVERSE,
+			   FIELD_PREP(PWM_ASPEED_CTRL_CLK_DIV_H, div_h) |
+				   FIELD_PREP(PWM_ASPEED_CTRL_CLK_DIV_L, div_l) |
+				   FIELD_PREP(PWM_ASPEED_CTRL_PIN_ENABLE, state->enabled) |
+				   FIELD_PREP(PWM_ASPEED_CTRL_CLK_ENABLE, clk_en) |
+				   FIELD_PREP(PWM_ASPEED_CTRL_INVERSE, state->polarity));
+	return 0;
+}
+
+static const struct pwm_ops aspeed_pwm_ops = {
+	.apply = aspeed_pwm_apply,
+	.get_state = aspeed_pwm_get_state,
+	.owner = THIS_MODULE,
+};
+
+static int aspeed_pwm_extend_feature(struct device *dev,
+				     struct device_node *child,
+				     struct aspeed_pwm_data *priv)
+{
+	u32 hwpwm, wdt_reload_duty;
+	bool wdt_reload_en;
+	int ret;
+
+	wdt_reload_en = of_property_read_bool(child, "aspeed,wdt-reload-enable");
+	if (!wdt_reload_en)
+		return wdt_reload_en;
+
+	ret = of_property_read_u32(child, "reg", &hwpwm);
+	if (ret)
+		return ret;
+
+	ret = of_property_read_u32(child, "aspeed,wdt-reload-duty-point",
+				   &wdt_reload_duty);
+	if (ret)
+		return ret;
+
+	regmap_update_bits(
+		priv->regmap, PWM_ASPEED_CTRL(hwpwm),
+		PWM_ASPEED_CTRL_LOAD_SEL_RISING_AS_WDT |
+			PWM_ASPEED_CTRL_DUTY_LOAD_AS_WDT_ENABLE,
+		FIELD_PREP(PWM_ASPEED_CTRL_LOAD_SEL_RISING_AS_WDT, 0) |
+			FIELD_PREP(PWM_ASPEED_CTRL_DUTY_LOAD_AS_WDT_ENABLE,
+				   wdt_reload_en));
+	regmap_update_bits(priv->regmap, PWM_ASPEED_DUTY_CYCLE(hwpwm),
+			   PWM_ASPEED_DUTY_CYCLE_POINT_AS_WDT,
+			   FIELD_PREP(PWM_ASPEED_DUTY_CYCLE_POINT_AS_WDT,
+				      wdt_reload_duty));
+	return 0;
+}
+
+static void aspeed_pwm_reset_assert(void *data)
+{
+	struct reset_control *rst = data;
+
+	reset_control_assert(rst);
+}
+
+static void aspeed_pwm_chip_remove(void *data)
+{
+	struct pwm_chip *chip = data;
+
+	pwmchip_remove(chip);
+}
+
+static int aspeed_pwm_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	int ret;
+	struct aspeed_pwm_data *priv;
+	struct device_node *np, *child;
+	struct platform_device *parent_dev;
+
+	priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	np = pdev->dev.parent->of_node;
+	if (!of_device_is_compatible(np, "aspeed,ast2600-pwm-tach"))
+		return dev_err_probe(dev, -ENODEV,
+				     "Unsupported pwm device binding\n");
+
+	priv->regmap = syscon_node_to_regmap(np);
+	if (IS_ERR(priv->regmap))
+		return dev_err_probe(dev, PTR_ERR(priv->regmap),
+				     "Couldn't get regmap\n");
+
+	parent_dev = of_find_device_by_node(np);
+	priv->clk = devm_clk_get_enabled(&parent_dev->dev, NULL);
+	if (IS_ERR(priv->clk))
+		return dev_err_probe(dev, PTR_ERR(priv->clk),
+				     "Couldn't get clock\n");
+
+	priv->reset = devm_reset_control_get_shared(&parent_dev->dev, NULL);
+	if (IS_ERR(priv->reset))
+		return dev_err_probe(dev, PTR_ERR(priv->reset),
+				     "Couldn't get reset control\n");
+
+	ret = reset_control_deassert(priv->reset);
+	if (ret)
+		return dev_err_probe(dev, ret,
+				     "Couldn't deassert reset control\n");
+
+	ret = devm_add_action_or_reset(dev, aspeed_pwm_reset_assert,
+				       priv->reset);
+	if (ret)
+		return ret;
+
+	for_each_child_of_node(dev->of_node, child) {
+		ret = aspeed_pwm_extend_feature(dev, child, priv);
+		if (ret)
+			dev_warn(dev, "Set extend feature failed %d\n", ret);
+	}
+
+	priv->chip.dev = dev;
+	priv->chip.ops = &aspeed_pwm_ops;
+	priv->chip.npwm = PWM_ASPEED_NR_PWMS;
+
+	ret = pwmchip_add(&priv->chip);
+	if (ret < 0)
+		return dev_err_probe(dev, ret, "Failed to add PWM chip\n");
+	ret = devm_add_action_or_reset(dev, aspeed_pwm_chip_remove,
+				       &priv->chip);
+	if (ret)
+		return ret;
+	return 0;
+}
+
+static const struct of_device_id of_pwm_match_table[] = {
+	{
+		.compatible = "aspeed,ast2600-pwm",
+	},
+	{},
+};
+MODULE_DEVICE_TABLE(of, of_pwm_match_table);
+
+static struct platform_driver aspeed_pwm_driver = {
+	.probe = aspeed_pwm_probe,
+	.driver	= {
+		.name = "aspeed-pwm",
+		.of_match_table = of_pwm_match_table,
+	},
+};
+
+module_platform_driver(aspeed_pwm_driver);
+
+MODULE_AUTHOR("Billy Tsai <billy_tsai@aspeedtech.com>");
+MODULE_DESCRIPTION("Aspeed ast2600 PWM device driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/spi/Kconfig b/drivers/spi/Kconfig
index ee5f9e61cc28..2a879e70fd27 100644
--- a/drivers/spi/Kconfig
+++ b/drivers/spi/Kconfig
@@ -86,6 +86,14 @@ config SPI_AR934X
 	  This enables support for the SPI controller present on the
 	  Qualcomm Atheros AR934X/QCA95XX SoCs.
 
+config SPI_ASPEED
+	tristate "ASPEED SPI Controller"
+	depends on OF && HAS_IOMEM
+	help
+	  This is a driver for ASPEED SPI Controller. This driver
+	  is not a generic pure SPI driver, which is especially
+	  designed for spi-mem framework.
+
 config SPI_ATH79
 	tristate "Atheros AR71XX/AR724X/AR913X SPI controller driver"
 	depends on ATH79 || COMPILE_TEST
diff --git a/drivers/spi/Makefile b/drivers/spi/Makefile
index e30196d0a4cf..fcf3241017d6 100644
--- a/drivers/spi/Makefile
+++ b/drivers/spi/Makefile
@@ -19,6 +19,7 @@ obj-$(CONFIG_SPI_ALTERA_CORE)		+= spi-altera-core.o
 obj-$(CONFIG_SPI_ALTERA_DFL)		+= spi-altera-dfl.o
 obj-$(CONFIG_SPI_AR934X)		+= spi-ar934x.o
 obj-$(CONFIG_SPI_ARMADA_3700)		+= spi-armada-3700.o
+obj-$(CONFIG_SPI_ASPEED)		+= spi-aspeed.o
 obj-$(CONFIG_SPI_ASPEED_SMC)		+= spi-aspeed-smc.o
 obj-$(CONFIG_SPI_ATMEL)			+= spi-atmel.o
 obj-$(CONFIG_SPI_ATMEL_QUADSPI)		+= atmel-quadspi.o
diff --git a/drivers/spi/spi-aspeed.c b/drivers/spi/spi-aspeed.c
new file mode 100644
index 000000000000..adaa1d4c24bb
--- /dev/null
+++ b/drivers/spi/spi-aspeed.c
@@ -0,0 +1,1466 @@
+// SPDX-License-Identifier: GPL-2.0+
+
+/*
+ * ASPEED FMC/SPI Memory Controller Driver
+ *
+ * Copyright (c) 2020, ASPEED Corporation.
+ * Copyright (c) 2015-2016, IBM Corporation.
+ */
+
+#include <linux/bitops.h>
+#include <linux/clk.h>
+#include <linux/device.h>
+#include <linux/dma-mapping.h>
+#include <linux/err.h>
+#include <linux/errno.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+#include <linux/regmap.h>
+#include <linux/sizes.h>
+#include <linux/spi/spi.h>
+#include <linux/spi/spi-mem.h>
+
+/* ASPEED FMC/SPI memory control register related */
+#define OFFSET_CE_TYPE_SETTING		0x00
+#define OFFSET_CE_ADDR_MODE_CTRL	0x04
+#define OFFSET_INTR_CTRL_STATUS		0x08
+#define OFFSET_ADDR_DATA_MASK		0x0c
+#define OFFSET_CE0_CTRL_REG		0x10
+#define OFFSET_CE0_DECODE_RANGE_REG	0x30
+#define OFFSET_HOST_DIRECT_ACCESS_CMD_CTRL4	0x6c
+#define OFFSET_HOST_DIRECT_ACCESS_CMD_CTRL2	0x74
+#define OFFSET_DMA_CTRL			0x80
+#define OFFSET_DMA_FLASH_ADDR_REG	0x84
+#define OFFSET_DMA_RAM_ADDR_REG		0x88
+#define OFFSET_DMA_LEN_REG		0x8c
+#define OFFSET_DMA_CHECKSUM_RESULT	0x90
+#define OFFSET_CE0_TIMING_COMPENSATION	0x94
+
+#define CTRL_IO_SINGLE_DATA	0
+#define CTRL_IO_DUAL_DATA	BIT(29)
+#define CTRL_IO_QUAD_DATA	BIT(30)
+
+#define CTRL_IO_MODE_USER	GENMASK(1, 0)
+#define CTRL_IO_MODE_CMD_READ	BIT(0)
+#define CTRL_IO_MODE_CMD_WRITE	BIT(1)
+#define CTRL_STOP_ACTIVE	BIT(2)
+
+#define CALIBRATION_LEN		0x400
+#define SPI_DMA_IRQ_EN		BIT(3)
+#define SPI_DAM_REQUEST		BIT(31)
+#define SPI_DAM_GRANT		BIT(30)
+#define SPI_DMA_CALIB_MODE	BIT(3)
+#define SPI_DMA_CALC_CKSUM	BIT(2)
+#define SPI_DMA_ENABLE		BIT(0)
+#define SPI_DMA_STATUS		BIT(11)
+#define DMA_GET_REQ_MAGIC	0xaeed0000
+#define DMA_DISCARD_REQ_MAGIC	0xdeea0000
+#define FMC_SPI_DMA_BUF_LEN	0x3400
+
+enum aspeed_spi_ctl_reg_value {
+	ASPEED_SPI_BASE,
+	ASPEED_SPI_READ,
+	ASPEED_SPI_WRITE,
+	ASPEED_SPI_MAX,
+};
+
+#define ASPEED_SPI_MAX_CS 5
+
+/* definition for controller flag */
+#define SPI_MODE_USER	0x00000001
+#define SPI_FIXED_LOW_W_CLK 0x00000002
+#define SPI_DMA_WRITE	0x00000004
+#define SPI_DMA_READ	0x00000008
+
+#define MAX_READ_SZ_ONCE	0x3000 /* 12KB */
+#define FIXED_REMAPPED_MEM_SZ	0x1000
+
+static spinlock_t g_lock;
+
+struct aspeed_spi_controller;
+struct aspeed_spi_chip;
+
+struct aspeed_spi_info {
+	uint32_t cmd_io_ctrl_mask;
+	uint32_t max_data_bus_width;
+	uint32_t min_decode_sz;
+	void (*set_4byte)(struct aspeed_spi_controller *ast_ctrl, uint32_t cs);
+	int (*calibrate)(struct aspeed_spi_controller *ast_ctrl, uint32_t cs);
+	void (*adjust_decode_sz)(uint32_t decode_sz_arr[], int len);
+	uint32_t (*segment_start)(struct aspeed_spi_controller *ast_ctrl,
+				  uint32_t reg);
+	uint32_t (*segment_end)(struct aspeed_spi_controller *ast_ctrl,
+				uint32_t reg);
+	uint32_t (*segment_reg)(struct aspeed_spi_controller *ast_ctrl,
+				uint32_t start, uint32_t end);
+	void (*safs_support)(struct aspeed_spi_controller *ast_ctrl,
+		enum spi_mem_data_dir dir, uint8_t cmd, uint8_t addr_len,
+		uint8_t bus_width);
+};
+
+struct aspeed_spi_chip {
+	void __iomem *ahb_base;
+	void __iomem *ahb_base_phy;
+	uint32_t ahb_window_sz;
+	uint32_t ctrl_val[ASPEED_SPI_MAX];
+	uint32_t max_clk_freq;
+};
+
+struct aspeed_spi_controller {
+	struct device *dev;
+	const struct aspeed_spi_info *info; /* controller info */
+	void __iomem *regs; /* controller registers */
+	void __iomem *ahb_base;
+	uint8_t *op_buf;
+	dma_addr_t dma_addr_phy;
+	uint32_t ahb_base_phy; /* physical addr of AHB window */
+	uint32_t ahb_window_sz; /* AHB window size */
+	uint32_t num_cs;
+	uint64_t ahb_clk;
+	int irq; /* for dma write */
+	struct completion dma_done;
+	struct aspeed_spi_chip *chips; /* pointers to attached chips */
+	uint32_t flag;
+	bool disable_calib;
+	spinlock_t lock;
+};
+
+static uint32_t
+aspeed_2600_spi_segment_start(struct aspeed_spi_controller *ast_ctrl,
+			      uint32_t reg)
+{
+	uint32_t start_offset = (reg << 16) & 0x0ff00000;
+
+	return ast_ctrl->ahb_base_phy + start_offset;
+}
+
+static uint32_t
+aspeed_2600_spi_segment_end(struct aspeed_spi_controller *ast_ctrl,
+			    uint32_t reg)
+{
+	uint32_t end_offset = reg & 0x0ff00000;
+
+	/* no decode range, set to physical ahb base */
+	if (end_offset == 0)
+		return ast_ctrl->ahb_base_phy;
+
+	return ast_ctrl->ahb_base_phy + end_offset + 0x100000;
+}
+
+static uint32_t
+aspeed_2600_spi_segment_reg(struct aspeed_spi_controller *ast_ctrl,
+			    uint32_t start, uint32_t end)
+{
+	/* no decode range, assign zero value */
+	if (start == end)
+		return 0;
+
+	return ((start & 0x0ff00000) >> 16) | ((end - 0x100000) & 0x0ff00000);
+}
+
+static void aspeed_spi_chip_set_4byte(struct aspeed_spi_controller *ast_ctrl,
+				      uint32_t cs)
+{
+	uint32_t reg_val;
+
+	reg_val = readl(ast_ctrl->regs + OFFSET_CE_ADDR_MODE_CTRL);
+	reg_val |= 0x11 << cs;
+	writel(reg_val, ast_ctrl->regs + OFFSET_CE_ADDR_MODE_CTRL);
+}
+
+uint32_t aspeed_spi_get_io_mode(uint32_t bus_width)
+{
+	switch (bus_width) {
+	case 1:
+		return CTRL_IO_SINGLE_DATA;
+	case 2:
+		return CTRL_IO_DUAL_DATA;
+	case 4:
+		return CTRL_IO_QUAD_DATA;
+	default:
+		return CTRL_IO_SINGLE_DATA;
+	}
+}
+
+/*
+ * Check whether the data is not all 0 or 1 in order to
+ * avoid calibriate umount spi-flash.
+ */
+static bool aspeed_spi_calibriation_enable(const uint8_t *buf, uint32_t sz)
+{
+	const uint32_t *buf_32 = (const uint32_t *)buf;
+	uint32_t i;
+	uint32_t valid_count = 0;
+
+	for (i = 0; i < (sz / 4); i++) {
+		if (buf_32[i] != 0 && buf_32[i] != 0xffffffff)
+			valid_count++;
+		if (valid_count > 100)
+			return true;
+	}
+
+	return false;
+}
+
+static uint32_t
+aspeed_2600_spi_dma_checksum(struct aspeed_spi_controller *ast_ctrl,
+			     uint32_t cs, uint32_t div, uint32_t delay)
+{
+	uint32_t ctrl_val;
+	uint32_t checksum;
+
+	writel(DMA_GET_REQ_MAGIC, ast_ctrl->regs + OFFSET_DMA_CTRL);
+	if (readl(ast_ctrl->regs + OFFSET_DMA_CTRL) & SPI_DAM_REQUEST) {
+		while (!(readl(ast_ctrl->regs + OFFSET_DMA_CTRL) &
+			 SPI_DAM_GRANT))
+			;
+	}
+
+	writel((uint32_t)ast_ctrl->chips[cs].ahb_base_phy,
+	       ast_ctrl->regs + OFFSET_DMA_FLASH_ADDR_REG);
+	writel(CALIBRATION_LEN, ast_ctrl->regs + OFFSET_DMA_LEN_REG);
+
+	ctrl_val = SPI_DMA_ENABLE | SPI_DMA_CALC_CKSUM | SPI_DMA_CALIB_MODE |
+		   (delay << 8) | ((div & 0xf) << 16);
+	writel(ctrl_val, ast_ctrl->regs + OFFSET_DMA_CTRL);
+	while (!(readl(ast_ctrl->regs + OFFSET_INTR_CTRL_STATUS) &
+		 SPI_DMA_STATUS))
+		;
+
+	checksum = readl(ast_ctrl->regs + OFFSET_DMA_CHECKSUM_RESULT);
+
+	writel(0x0, ast_ctrl->regs + OFFSET_DMA_CTRL);
+	writel(DMA_DISCARD_REQ_MAGIC, ast_ctrl->regs + OFFSET_DMA_CTRL);
+
+	return checksum;
+}
+
+static int get_mid_point_of_longest_one(uint8_t *buf, uint32_t len)
+{
+	int i;
+	int start = 0, mid_point = 0;
+	int max_cnt = 0, cnt = 0;
+
+	for (i = 0; i < len; i++) {
+		if (buf[i] == 1) {
+			cnt++;
+		} else {
+			cnt = 0;
+			start = i;
+		}
+
+		if (max_cnt < cnt) {
+			max_cnt = cnt;
+			mid_point = start + (cnt / 2);
+		}
+	}
+
+	/*
+	 * In order to get a stable SPI read timing,
+	 * abandon the result if the length of longest
+	 * consecutive good points is too short.
+	 */
+	if (max_cnt < 4)
+		return -1;
+
+	return mid_point;
+}
+
+/* Transfer maximum clock frequency to register setting */
+static uint32_t
+aspeed_2600_spi_clk_basic_setting(struct aspeed_spi_controller *ast_ctrl,
+				  uint32_t *max_clk)
+{
+	struct device *dev = ast_ctrl->dev;
+	uint32_t hclk_clk = ast_ctrl->ahb_clk;
+	uint32_t hclk_div = 0x400; /* default value */
+	uint32_t i, j = 0;
+	bool found = false;
+	/* HCLK/1 ..	HCLK/16 */
+	uint32_t hclk_masks[] = { 15, 7, 14, 6, 13, 5, 12, 4,
+				  11, 3, 10, 2, 9,  1, 8,  0 };
+
+	/* FMC/SPIR10[27:24] */
+	for (j = 0; j < 0xf; i++) {
+		/* FMC/SPIR10[11:8] */
+		for (i = 0; i < ARRAY_SIZE(hclk_masks); i++) {
+			if (i == 0 && j == 0)
+				continue;
+
+			if (hclk_clk / (i + 1 + (j * 16)) <= *max_clk) {
+				found = 1;
+				*max_clk = hclk_clk / (i + 1 + (j * 16));
+				break;
+			}
+		}
+
+		if (found) {
+			hclk_div = ((j << 24) | hclk_masks[i] << 8);
+			break;
+		}
+	}
+
+	dev_dbg(dev, "found: %s, hclk: %d, max_clk: %d\n", found ? "yes" : "no",
+		hclk_clk, *max_clk);
+	dev_dbg(dev, "base_clk: %d, h_div: %d (mask %x), speed: %d\n", j, i + 1,
+		hclk_masks[i], hclk_clk / (i + 1 + j * 16));
+
+	return hclk_div;
+}
+
+/*
+ * If SPI frequency is too high, timing compensation is needed,
+ * otherwise, SPI controller will sample unready data. For AST2600
+ * SPI memory controller, only the first four frequency levels
+ * (HCLK/2, HCLK/3,..., HCKL/5) may need timing compensation.
+ * Here, for each frequency, we will get a sequence of reading
+ * result (pass or fail) compared to golden data. Then, getting the
+ * middle point of the maximum pass widow. Besides, if the flash's
+ * content is too monotonous, the frequency recorded in the device
+ * tree will be adopted.
+ */
+int aspeed_2600_spi_timing_calibration(struct aspeed_spi_controller *ast_ctrl,
+				       uint32_t cs)
+{
+	int ret = 0;
+	struct device *dev = ast_ctrl->dev;
+	struct aspeed_spi_chip *chip = &ast_ctrl->chips[cs];
+	uint32_t max_freq = chip->max_clk_freq;
+	/* HCLK/2, ..., HCKL/5 */
+	uint32_t hclk_masks[] = { 7, 14, 6, 13 };
+	uint8_t *calib_res = NULL;
+	uint8_t *check_buf = NULL;
+	uint32_t reg_val;
+	uint32_t checksum, gold_checksum;
+	uint32_t i, hcycle, delay_ns, final_delay = 0;
+	uint32_t hclk_div;
+	bool pass;
+	int calib_point;
+
+	reg_val =
+		readl(ast_ctrl->regs + OFFSET_CE0_TIMING_COMPENSATION + cs * 4);
+	if (reg_val != 0) {
+		dev_dbg(dev, "has executed calibration.\n");
+		goto no_calib;
+	}
+
+	dev_dbg(dev, "calculate timing compensation :\n");
+	/*
+	 * use the related low frequency to get check calibration data
+	 * and get golden data.
+	 */
+	reg_val = chip->ctrl_val[ASPEED_SPI_READ] & 0xf0fff0ff;
+	writel(reg_val, ast_ctrl->regs + OFFSET_CE0_CTRL_REG + cs * 4);
+
+	/*
+	 * timing calibration should be skipped when
+	 * "timing-calibration-disabled" property is configured
+	 * in the device tree.
+	 */
+	if (ast_ctrl->disable_calib)
+		goto no_calib;
+
+	check_buf = kzalloc(CALIBRATION_LEN, GFP_KERNEL);
+	if (!check_buf)
+		return -ENOMEM;
+
+	memcpy_fromio(check_buf, chip->ahb_base, CALIBRATION_LEN);
+	if (!aspeed_spi_calibriation_enable(check_buf, CALIBRATION_LEN)) {
+		dev_info(dev, "flash data is monotonous, skip calibration.");
+		goto no_calib;
+	}
+
+	gold_checksum = aspeed_2600_spi_dma_checksum(ast_ctrl, cs, 0, 0);
+
+	/*
+	 * allocate a space to record calibration result for
+	 * different timing compensation with fixed
+	 * HCLK division.
+	 */
+	calib_res = kzalloc(6 * 17, GFP_KERNEL);
+	if (!calib_res) {
+		ret = -ENOMEM;
+		goto no_calib;
+	}
+
+	/* From HCLK/2 to HCLK/5 */
+	for (i = 0; i < ARRAY_SIZE(hclk_masks); i++) {
+		if (max_freq < (uint32_t)ast_ctrl->ahb_clk / (i + 2)) {
+			dev_dbg(dev, "skipping freq %d\n",
+				(uint32_t)ast_ctrl->ahb_clk / (i + 2));
+			continue;
+		}
+
+		max_freq = (uint32_t)ast_ctrl->ahb_clk / (i + 2);
+
+		memset(calib_res, 0x0, 6 * 17);
+
+		for (hcycle = 0; hcycle <= 5; hcycle++) {
+			/* increase DI delay by the step of 0.5ns */
+			dev_dbg(dev, "Delay Enable : hcycle %x\n", hcycle);
+			for (delay_ns = 0; delay_ns <= 0xf; delay_ns++) {
+				checksum = aspeed_2600_spi_dma_checksum(
+					ast_ctrl, cs, hclk_masks[i],
+					BIT(3) | hcycle | (delay_ns << 4));
+				pass = (checksum == gold_checksum);
+				calib_res[hcycle * 17 + delay_ns] = pass;
+				dev_dbg(dev,
+					"HCLK/%d, %d HCLK cycle, %d delay_ns : %s\n",
+					i + 2, hcycle, delay_ns,
+					pass ? "PASS" : "FAIL");
+			}
+		}
+
+		calib_point = get_mid_point_of_longest_one(calib_res, 6 * 17);
+		if (calib_point < 0) {
+			dev_info(dev, "cannot get good calibration point.\n");
+			continue;
+		}
+
+		hcycle = calib_point / 17;
+		delay_ns = calib_point % 17;
+		dev_dbg(dev, "final hcycle: %d, delay_ns: %d\n", hcycle,
+			delay_ns);
+
+		final_delay = (BIT(3) | hcycle | (delay_ns << 4)) << (i * 8);
+		writel(final_delay, ast_ctrl->regs +
+					    OFFSET_CE0_TIMING_COMPENSATION +
+					    cs * 4);
+		break;
+	}
+
+no_calib:
+
+	hclk_div = aspeed_2600_spi_clk_basic_setting(ast_ctrl, &max_freq);
+
+	/* configure SPI clock frequency */
+	reg_val = readl(ast_ctrl->regs + OFFSET_CE0_CTRL_REG + cs * 4);
+	reg_val = (reg_val & 0xf0fff0ff) | hclk_div;
+	writel(reg_val, ast_ctrl->regs + OFFSET_CE0_CTRL_REG + cs * 4);
+
+	/* add clock setting info for CE ctrl setting */
+	for (i = 0; i < ASPEED_SPI_MAX; i++)
+		chip->ctrl_val[i] = (chip->ctrl_val[i] & 0xf0fff0ff) | hclk_div;
+
+	dev_info(dev, "freq: %dMHz\n", max_freq / 1000000);
+
+	kfree(check_buf);
+	kfree(calib_res);
+
+	return ret;
+}
+
+/*
+ * AST2600 SPI memory controllers support multiple chip selects.
+ * The start address of a decode range should be multiple
+ * of its related flash size. Namely, the total decoded size
+ * from flash 0 to flash N should be multiple of (N + 1) flash size.
+ */
+void aspeed_2600_adjust_decode_sz(uint32_t decode_sz_arr[], int len)
+{
+	int cs, j;
+	uint32_t sz;
+
+	for (cs = len - 1; cs >= 0; cs--) {
+		sz = 0;
+		for (j = 0; j < cs; j++)
+			sz += decode_sz_arr[j];
+
+		if (sz % decode_sz_arr[cs] != 0)
+			decode_sz_arr[0] += (sz % decode_sz_arr[cs]);
+	}
+}
+
+static int
+aspeed_spi_decode_range_config(struct aspeed_spi_controller *ast_ctrl,
+			       uint32_t decode_sz_arr[])
+{
+	struct aspeed_spi_chip *chip = ast_ctrl->chips;
+	uint32_t i;
+	uint32_t cs;
+	uint32_t decode_reg_val;
+	uint32_t start_addr_phy, end_addr_phy, pre_end_addr_phy = 0;
+	uint32_t total_decode_sz = 0;
+
+	/* decode range sanity */
+	for (cs = 0; cs < ast_ctrl->num_cs; cs++) {
+		total_decode_sz += decode_sz_arr[cs];
+		if (ast_ctrl->ahb_window_sz < total_decode_sz) {
+			dev_err(ast_ctrl->dev, "insufficient decode size\n");
+			for (i = 0; i <= cs; i++)
+				dev_err(ast_ctrl->dev, "cs:%d %x\n", i,
+					decode_sz_arr[i]);
+			return -ENOSPC;
+		}
+	}
+
+	for (cs = 0; cs < ast_ctrl->num_cs; cs++) {
+		if (chip[cs].ahb_base)
+			devm_iounmap(ast_ctrl->dev, chip[cs].ahb_base);
+	}
+
+	/* configure each CE's decode range */
+	for (cs = 0; cs < ast_ctrl->num_cs; cs++) {
+		if (cs == 0)
+			start_addr_phy = ast_ctrl->ahb_base_phy;
+		else
+			start_addr_phy = pre_end_addr_phy;
+
+		if ((ast_ctrl->flag & SPI_DMA_READ) == SPI_DMA_READ ||
+			(ast_ctrl->flag & SPI_MODE_USER) == SPI_MODE_USER) {
+			/* only small decoded range is needed for DMA and user mode */
+			chip[cs].ahb_base = devm_ioremap(ast_ctrl->dev, start_addr_phy,
+						 FIXED_REMAPPED_MEM_SZ);
+		} else {
+			chip[cs].ahb_base = devm_ioremap(ast_ctrl->dev, start_addr_phy,
+						 decode_sz_arr[cs]);
+		}
+		chip[cs].ahb_base_phy = (void __iomem *)start_addr_phy;
+
+		chip[cs].ahb_window_sz = decode_sz_arr[cs];
+		end_addr_phy = start_addr_phy + decode_sz_arr[cs];
+
+		decode_reg_val = ast_ctrl->info->segment_reg(
+			ast_ctrl, start_addr_phy, end_addr_phy);
+
+		writel(decode_reg_val,
+		       ast_ctrl->regs + OFFSET_CE0_DECODE_RANGE_REG + cs * 4);
+
+		pre_end_addr_phy = end_addr_phy;
+
+		dev_dbg(ast_ctrl->dev, "cs: %d, decode_reg: 0x%x\n", cs,
+			decode_reg_val);
+	}
+
+	return 0;
+}
+
+static const struct aspeed_spi_info ast2600_fmc_info = {
+	.max_data_bus_width = 4,
+	.cmd_io_ctrl_mask = 0xf0ff40c7,
+	/* for ast2600, the minimum decode size for each CE is 2MB */
+	.min_decode_sz = 0x200000,
+	.set_4byte = aspeed_spi_chip_set_4byte,
+	.calibrate = aspeed_2600_spi_timing_calibration,
+	.adjust_decode_sz = aspeed_2600_adjust_decode_sz,
+	.segment_start = aspeed_2600_spi_segment_start,
+	.segment_end = aspeed_2600_spi_segment_end,
+	.segment_reg = aspeed_2600_spi_segment_reg,
+};
+
+void aspeed_2600_spi_fill_safs_cmd(struct aspeed_spi_controller *ast_ctrl,
+		enum spi_mem_data_dir dir, uint8_t cmd,
+		uint8_t addr_len, uint8_t bus_width)
+{
+	uint32_t tmp_val;
+
+	if (dir == SPI_MEM_DATA_IN) {
+		tmp_val = readl(ast_ctrl->regs + OFFSET_HOST_DIRECT_ACCESS_CMD_CTRL4);
+		if (addr_len == 4)
+			tmp_val = (tmp_val & 0xffff00ff) | (cmd << 8);
+		else
+			tmp_val = (tmp_val & 0xffffff00) | cmd;
+
+		tmp_val = (tmp_val & 0x0fffffff) | aspeed_spi_get_io_mode(bus_width);
+
+		writel(tmp_val, ast_ctrl->regs + OFFSET_HOST_DIRECT_ACCESS_CMD_CTRL4);
+
+	} else if (dir == SPI_MEM_DATA_OUT) {
+		tmp_val = readl(ast_ctrl->regs + OFFSET_HOST_DIRECT_ACCESS_CMD_CTRL4);
+		tmp_val = (tmp_val & 0xf0ffffff) |
+				(aspeed_spi_get_io_mode(bus_width) >> 4);
+
+		writel(tmp_val, ast_ctrl->regs + OFFSET_HOST_DIRECT_ACCESS_CMD_CTRL4);
+
+		tmp_val = readl(ast_ctrl->regs + OFFSET_HOST_DIRECT_ACCESS_CMD_CTRL2);
+		if (addr_len == 4)
+			tmp_val = (tmp_val & 0xffff00ff) | (cmd << 8);
+		else
+			tmp_val = (tmp_val & 0xffffff00) | cmd;
+
+		writel(tmp_val, ast_ctrl->regs + OFFSET_HOST_DIRECT_ACCESS_CMD_CTRL2);
+	}
+}
+
+static const struct aspeed_spi_info ast2600_spi_info = {
+	.max_data_bus_width = 4,
+	.cmd_io_ctrl_mask = 0xf0ff40c7,
+	/* for ast2600, the minimum decode size for each CE is 2MB */
+	.min_decode_sz = 0x200000,
+	.set_4byte = aspeed_spi_chip_set_4byte,
+	.calibrate = aspeed_2600_spi_timing_calibration,
+	.adjust_decode_sz = aspeed_2600_adjust_decode_sz,
+	.segment_start = aspeed_2600_spi_segment_start,
+	.segment_end = aspeed_2600_spi_segment_end,
+	.segment_reg = aspeed_2600_spi_segment_reg,
+	.safs_support = aspeed_2600_spi_fill_safs_cmd,
+};
+
+/*
+ * If the slave device is SPI NOR flash, there are two types
+ * of command mode for ASPEED SPI memory controller used to
+ * transfer data. The first one is user mode and the other is
+ * command read/write mode. With user mode, SPI NOR flash
+ * command, address and data processes are all handled by CPU.
+ * But, when address filter is enabled to protect some flash
+ * regions from being written, user mode will be disabled.
+ * Thus, here, we use command read/write mode to issue SPI
+ * operations. After remapping flash space correctly, we can
+ * easily read/write data to flash by reading or writing
+ * related remapped address, then, SPI NOR flash command and
+ * address will be transferred to flash by controller
+ * automatically. Besides, ASPEED SPI memory controller can
+ * also block address or data bytes by configure FMC0C/SPIR0C
+ * address and data mask register in order to satisfy the
+ * following SPI flash operation sequences: (command) only,
+ * (command and address) only or (coommand and data) only.
+ */
+static int aspeed_spi_exec_op_cmd_mode(
+	struct spi_mem *mem,
+	const struct spi_mem_op *op)
+{
+	struct aspeed_spi_controller *ast_ctrl =
+		spi_controller_get_devdata(mem->spi->master);
+	struct device *dev = ast_ctrl->dev;
+	uint32_t cs = mem->spi->chip_select;
+	struct aspeed_spi_chip *chip = &ast_ctrl->chips[cs];
+	uint32_t ctrl_val;
+	uint32_t addr_mode_reg, addr_mode_reg_backup;
+	uint32_t addr_data_mask = 0;
+	void __iomem *op_addr;
+	const void *data_buf;
+	uint32_t data_byte = 0;
+	uint32_t dummy_data = 0;
+	unsigned long flags;
+
+	dev_dbg(dev, "cmd:%x(%d),addr:%llx(%d),dummy:%d(%d),data_len:%x(%d)\n",
+		op->cmd.opcode, op->cmd.buswidth, op->addr.val,
+		op->addr.buswidth, op->dummy.nbytes, op->dummy.buswidth,
+		op->data.nbytes, op->data.buswidth);
+
+	addr_mode_reg = addr_mode_reg_backup =
+		readl(ast_ctrl->regs + OFFSET_CE_ADDR_MODE_CTRL);
+	addr_data_mask = readl(ast_ctrl->regs + OFFSET_ADDR_DATA_MASK);
+
+	ctrl_val = chip->ctrl_val[ASPEED_SPI_BASE];
+	ctrl_val &= ~ast_ctrl->info->cmd_io_ctrl_mask;
+
+	/* configure opcode */
+	ctrl_val |= op->cmd.opcode << 16;
+
+	/* configure operation address, address length and address mask */
+	if (op->addr.nbytes != 0) {
+		if (op->addr.nbytes == 3)
+			addr_mode_reg &= ~(0x11 << cs);
+		else
+			addr_mode_reg |= (0x11 << cs);
+
+		addr_data_mask &= 0x0f;
+		op_addr = chip->ahb_base + op->addr.val;
+	} else {
+		addr_data_mask |= 0xf0;
+		op_addr = chip->ahb_base;
+	}
+
+	if (op->dummy.nbytes != 0) {
+		ctrl_val |= ((op->dummy.nbytes & 0x3) << 6 |
+			     (op->dummy.nbytes & 0x4) << 14);
+	}
+
+	/* configure data io mode and data mask */
+	if (op->data.nbytes != 0) {
+		addr_data_mask &= 0xF0;
+		data_byte = op->data.nbytes;
+		if (op->data.dir == SPI_MEM_DATA_OUT) {
+			if (data_byte % 4 != 0) {
+				memset(ast_ctrl->op_buf, 0xff, ((data_byte / 4) + 1) * 4);
+				memcpy(ast_ctrl->op_buf, op->data.buf.out, data_byte);
+				data_buf = ast_ctrl->op_buf;
+				data_byte = ((data_byte / 4) + 1) * 4;
+			} else {
+				data_buf = op->data.buf.out;
+			}
+		} else {
+			data_buf = op->data.buf.in;
+		}
+
+		if (op->data.buswidth)
+			ctrl_val |= aspeed_spi_get_io_mode(op->data.buswidth);
+
+	} else {
+		addr_data_mask |= 0x0f;
+		data_byte = 1;
+		data_buf = &dummy_data;
+	}
+
+	/* configure command mode */
+	if (op->data.dir == SPI_MEM_DATA_OUT)
+		ctrl_val |= CTRL_IO_MODE_CMD_WRITE;
+	else
+		ctrl_val |= CTRL_IO_MODE_CMD_READ;
+
+	/* set controller registers */
+	writel(ctrl_val, ast_ctrl->regs + OFFSET_CE0_CTRL_REG + cs * 4);
+	writel(addr_mode_reg, ast_ctrl->regs + OFFSET_CE_ADDR_MODE_CTRL);
+	writel(addr_data_mask, ast_ctrl->regs + OFFSET_ADDR_DATA_MASK);
+
+	dev_dbg(dev, "ctrl: 0x%08x, addr_mode: 0x%x, mask: 0x%x, addr:0x%08x\n",
+		ctrl_val, addr_mode_reg, addr_data_mask, (uint32_t)op_addr);
+
+	/* trigger spi transmission or reception sequence */
+	spin_lock_irqsave(&g_lock, flags);
+
+	if (op->data.dir == SPI_MEM_DATA_OUT)
+		memcpy_toio(op_addr, data_buf, data_byte);
+	else
+		memcpy_fromio((void *)data_buf, op_addr, data_byte);
+
+	spin_unlock_irqrestore(&g_lock, flags);
+
+	/* restore controller setting */
+	writel(chip->ctrl_val[ASPEED_SPI_READ],
+	       ast_ctrl->regs + OFFSET_CE0_CTRL_REG + cs * 4);
+	writel(addr_mode_reg_backup, ast_ctrl->regs + OFFSET_CE_ADDR_MODE_CTRL);
+	writel(0x0, ast_ctrl->regs + OFFSET_ADDR_DATA_MASK);
+
+	return 0;
+}
+
+
+static int aspeed_spi_read_from_ahb(void *buf, void __iomem *src, size_t len)
+{
+	size_t offset = 0;
+
+	if (IS_ALIGNED((uintptr_t)src, sizeof(uintptr_t)) &&
+	    IS_ALIGNED((uintptr_t)buf, sizeof(uintptr_t))) {
+		ioread32_rep(src, buf, len >> 2);
+		offset = len & ~0x3;
+		len -= offset;
+	}
+
+	ioread8_rep(src, (uint8_t *)buf + offset, len);
+
+	return 0;
+}
+
+static int aspeed_spi_write_to_ahb(void __iomem *dst, const void *buf,
+				   size_t len)
+{
+	size_t offset = 0;
+
+	if (IS_ALIGNED((uintptr_t)dst, sizeof(uintptr_t)) &&
+	    IS_ALIGNED((uintptr_t)buf, sizeof(uintptr_t))) {
+		iowrite32_rep(dst, buf, len >> 2);
+		offset = len & ~0x3;
+		len -= offset;
+	}
+
+	iowrite8_rep(dst, (const uint8_t *)buf + offset, len);
+
+	return 0;
+}
+
+static int aspeed_spi_exec_op_user_mode(
+	struct spi_mem *mem,
+	const struct spi_mem_op *op)
+{
+	struct aspeed_spi_controller *ast_ctrl =
+		spi_controller_get_devdata(mem->spi->master);
+	struct device *dev = ast_ctrl->dev;
+	uint32_t cs = mem->spi->chip_select;
+	struct aspeed_spi_chip *chip = &ast_ctrl->chips[cs];
+	uint32_t ctrl_val;
+	uint8_t dummy_data[16] = {0};
+	uint8_t addr[4] = {0};
+	int i;
+
+	dev_dbg(dev, "cmd:%x(%d),addr:%llx(%d),dummy:%d(%d),data_len:0x%x(%d)\n",
+		op->cmd.opcode, op->cmd.buswidth, op->addr.val,
+		op->addr.buswidth, op->dummy.nbytes, op->dummy.buswidth,
+		op->data.nbytes, op->data.buswidth);
+
+	/* start user mode */
+	ctrl_val = chip->ctrl_val[ASPEED_SPI_BASE];
+	writel(ctrl_val, ast_ctrl->regs + OFFSET_CE0_CTRL_REG + cs * 4);
+	ctrl_val &= (~CTRL_STOP_ACTIVE);
+	writel(ctrl_val, ast_ctrl->regs + OFFSET_CE0_CTRL_REG + cs * 4);
+
+	/* send command */
+	aspeed_spi_write_to_ahb(chip->ahb_base, &op->cmd.opcode, 1);
+
+	/* send address */
+	for (i = op->addr.nbytes; i > 0; i--) {
+		addr[op->addr.nbytes - i] =
+			((uint32_t)op->addr.val >> ((i - 1) * 8)) & 0xff;
+	}
+	aspeed_spi_write_to_ahb(chip->ahb_base, addr, op->addr.nbytes);
+
+	/* send dummy cycle */
+	aspeed_spi_write_to_ahb(chip->ahb_base, dummy_data, op->dummy.nbytes);
+
+	/* change io_mode */
+	ctrl_val |= aspeed_spi_get_io_mode(op->data.buswidth);
+	writel(ctrl_val, ast_ctrl->regs + OFFSET_CE0_CTRL_REG + cs * 4);
+
+	/* send data */
+	if (op->data.dir == SPI_MEM_DATA_OUT)
+		aspeed_spi_write_to_ahb(chip->ahb_base, op->data.buf.out, op->data.nbytes);
+	else
+		aspeed_spi_read_from_ahb(op->data.buf.in, chip->ahb_base, op->data.nbytes);
+
+	ctrl_val |= CTRL_STOP_ACTIVE;
+	writel(ctrl_val, ast_ctrl->regs + OFFSET_CE0_CTRL_REG + cs * 4);
+
+	/* restore controller setting */
+	writel(chip->ctrl_val[ASPEED_SPI_READ],
+	       ast_ctrl->regs + OFFSET_CE0_CTRL_REG + cs * 4);
+
+	return 0;
+}
+
+static ssize_t aspeed_spi_dirmap_cmd_read(struct spi_mem_dirmap_desc *desc,
+				  uint64_t offs, size_t len, void *buf)
+{
+	struct aspeed_spi_controller *ast_ctrl =
+		spi_controller_get_devdata(desc->mem->spi->master);
+	struct aspeed_spi_chip *chip =
+		&ast_ctrl->chips[desc->mem->spi->chip_select];
+	struct spi_mem_op op_tmpl = desc->info.op_tmpl;
+
+	if (chip->ahb_window_sz < offs + len) {
+		dev_info(ast_ctrl->dev,
+			 "read range exceeds flash remapping size\n");
+		return 0;
+	}
+
+	dev_dbg(ast_ctrl->dev, "read op:0x%x, addr:0x%llx, len:0x%x\n",
+		op_tmpl.cmd.opcode, offs, len);
+
+	memcpy_fromio(buf, chip->ahb_base + offs, len);
+
+	return len;
+}
+
+/*
+ * When DMA memory mode is enabled, there is a limitation for AST2600,
+ * both DMA source and destination address should be 4-byte aligned.
+ * Thus, a 4-byte aligned buffer should be allocated previously and
+ * CPU needs to copy data from it after DMA done.
+ */
+#if 0
+static ssize_t aspeed_spi_dirmap_dma_read(struct spi_mem_dirmap_desc *desc,
+				  uint64_t offs, size_t len, void *buf)
+{
+	int ret = 0;
+	uint32_t timeout = 0;
+	struct aspeed_spi_controller *ast_ctrl =
+		spi_controller_get_devdata(desc->mem->spi->master);
+	struct aspeed_spi_chip *chip =
+		&ast_ctrl->chips[desc->mem->spi->chip_select];
+	struct spi_mem_op op_tmpl = desc->info.op_tmpl;
+	struct device *dev = ast_ctrl->dev;
+	uint32_t reg_val;
+	uint32_t target_cs = desc->mem->spi->chip_select;
+	uint32_t extra;
+	uint32_t tb_read_len = len;
+	uint32_t read_len;
+	uint32_t buf_offs = 0;
+	uint32_t flash_offs = (uint32_t)offs;
+
+	if (chip->ahb_window_sz < offs + len) {
+		dev_info(ast_ctrl->dev,
+			 "read range exceeds flash remapping size\n");
+		return 0;
+	}
+
+	dev_dbg(ast_ctrl->dev, "read op:0x%x, addr:0x%llx, len:0x%x\n",
+		op_tmpl.cmd.opcode, offs, len);
+
+	while (tb_read_len > 0) {
+		/* read max 10KB bytes once */
+		read_len = MAX_READ_SZ_ONCE - (flash_offs % MAX_READ_SZ_ONCE);
+		if (tb_read_len < read_len)
+			read_len = tb_read_len;
+
+		/* For AST2600 SPI DMA, flash offset should be 4 byte aligned */
+		extra = flash_offs % 4;
+		if (extra != 0) {
+			flash_offs = (flash_offs / 4) * 4;
+			read_len += extra;
+		}
+
+		writel(DMA_GET_REQ_MAGIC, ast_ctrl->regs + OFFSET_DMA_CTRL);
+		if (readl(ast_ctrl->regs + OFFSET_DMA_CTRL) & SPI_DAM_REQUEST) {
+			while (!(readl(ast_ctrl->regs + OFFSET_DMA_CTRL) &
+				 SPI_DAM_GRANT))
+				;
+		}
+
+		reg_val = ast_ctrl->chips[target_cs].ctrl_val[ASPEED_SPI_READ];
+		writel(reg_val, ast_ctrl->regs + OFFSET_CE0_CTRL_REG + target_cs * 4);
+
+		/* don't use dma_map_single here, since we cannot make sure the buf's
+		 * start address is 4-byte-aligned.
+		 */
+		writel(0x0, ast_ctrl->regs + OFFSET_DMA_CTRL);
+		writel(ast_ctrl->dma_addr_phy, ast_ctrl->regs + OFFSET_DMA_RAM_ADDR_REG);
+		writel(chip->ahb_base_phy + flash_offs, ast_ctrl->regs + OFFSET_DMA_FLASH_ADDR_REG);
+		writel(read_len - 1, ast_ctrl->regs + OFFSET_DMA_LEN_REG);
+
+		/* enable DMA irq */
+		reg_val = readl(ast_ctrl->regs + OFFSET_INTR_CTRL_STATUS);
+		reg_val |= SPI_DMA_IRQ_EN;
+		writel(reg_val, ast_ctrl->regs + OFFSET_INTR_CTRL_STATUS);
+
+		reinit_completion(&ast_ctrl->dma_done);
+
+		/* enable read DMA */
+		writel(0x1, ast_ctrl->regs + OFFSET_DMA_CTRL);
+		timeout = wait_for_completion_timeout(&ast_ctrl->dma_done, msecs_to_jiffies(2000));
+		if (timeout == 0) {
+			writel(0x0, ast_ctrl->regs + OFFSET_DMA_CTRL);
+			writel(DMA_DISCARD_REQ_MAGIC, ast_ctrl->regs + OFFSET_DMA_CTRL);
+			dev_err(dev, "read data timeout %d\n", ret);
+			ret = -1;
+			goto end;
+		} else {
+			memcpy(buf + buf_offs, ast_ctrl->op_buf + extra, read_len - extra);
+		}
+
+		read_len -= extra;
+
+		buf_offs += read_len;
+		flash_offs += read_len;
+		tb_read_len -= read_len;
+	}
+
+end:
+	return ret ? 0 : len;
+}
+
+static ssize_t aspeed_spi_dirmap_cmd_write(struct spi_mem_dirmap_desc *desc,
+				   uint64_t offs, size_t len, const void *buf)
+{
+	struct aspeed_spi_controller *ast_ctrl =
+		spi_controller_get_devdata(desc->mem->spi->master);
+	struct aspeed_spi_chip *chip =
+		&ast_ctrl->chips[desc->mem->spi->chip_select];
+	uint32_t reg_val;
+	uint32_t target_cs = desc->mem->spi->chip_select;
+	struct spi_mem_op op_tmpl = desc->info.op_tmpl;
+	unsigned long flags;
+
+	if (chip->ahb_window_sz < offs + len) {
+		dev_info(ast_ctrl->dev,
+			 "write range exceeds flash remapping size\n");
+		return 0;
+	}
+
+	dev_dbg(ast_ctrl->dev, "write op:0x%x, addr:0x%llx, len:0x%x\n",
+		op_tmpl.cmd.opcode, offs, len);
+
+	reg_val = ast_ctrl->chips[target_cs].ctrl_val[ASPEED_SPI_WRITE];
+	writel(reg_val, ast_ctrl->regs + OFFSET_CE0_CTRL_REG + target_cs * 4);
+
+	/* Due to spi-flash's characteristic, write process couldn't be
+	 * interrupted. Otherwise, CS will be inactive and remain data
+	 * cannot be written into flash successfully even if CS is
+	 * active again.
+	 */
+	spin_lock_irqsave(&ast_ctrl->lock, flags);
+	memcpy_toio(chip->ahb_base + offs, buf, len);
+	spin_unlock_irqrestore(&ast_ctrl->lock, flags);
+
+	reg_val = ast_ctrl->chips[target_cs].ctrl_val[ASPEED_SPI_READ];
+	writel(reg_val, ast_ctrl->regs + OFFSET_CE0_CTRL_REG + target_cs * 4);
+
+	return len;
+}
+#endif
+
+static ssize_t aspeed_spi_dirmap_dma_write(struct spi_mem_dirmap_desc *desc,
+				   uint64_t offs, size_t len, const void *buf)
+{
+	int ret = 0;
+	uint32_t timeout = 0;
+	struct aspeed_spi_controller *ast_ctrl =
+		spi_controller_get_devdata(desc->mem->spi->master);
+	struct device *dev = ast_ctrl->dev;
+	struct aspeed_spi_chip *chip =
+		&ast_ctrl->chips[desc->mem->spi->chip_select];
+	uint32_t reg_val;
+	uint32_t target_cs = desc->mem->spi->chip_select;
+	struct spi_mem_op op_tmpl = desc->info.op_tmpl;
+
+	if (chip->ahb_window_sz < offs + len) {
+		dev_info(dev, "write range exceeds flash remapping size\n");
+		return 0;
+	}
+
+	if (len < 1)
+		return 0;
+
+	if (len > FMC_SPI_DMA_BUF_LEN) {
+		dev_info(dev,
+			 "written length exceeds expected value (0x%x)\n", len);
+		return 0;
+	}
+
+	dev_dbg(dev, "write op:0x%x, addr:0x%llx, len:0x%x\n",
+		op_tmpl.cmd.opcode, offs, len);
+
+	writel(DMA_GET_REQ_MAGIC, ast_ctrl->regs + OFFSET_DMA_CTRL);
+	if (readl(ast_ctrl->regs + OFFSET_DMA_CTRL) & SPI_DAM_REQUEST) {
+		while (!(readl(ast_ctrl->regs + OFFSET_DMA_CTRL) &
+			 SPI_DAM_GRANT))
+			;
+	}
+
+	reg_val = ast_ctrl->chips[target_cs].ctrl_val[ASPEED_SPI_WRITE];
+	writel(reg_val, ast_ctrl->regs + OFFSET_CE0_CTRL_REG + target_cs * 4);
+
+	/* don't use dma_map_single here, since we cannot make sure the buf's
+	 * start address is 4-byte-aligned.
+	 */
+	memcpy(ast_ctrl->op_buf, buf, len);
+
+	writel(0x0, ast_ctrl->regs + OFFSET_DMA_CTRL);
+	writel(ast_ctrl->dma_addr_phy, ast_ctrl->regs + OFFSET_DMA_RAM_ADDR_REG);
+	writel(chip->ahb_base_phy + offs, ast_ctrl->regs + OFFSET_DMA_FLASH_ADDR_REG);
+	writel(len - 1, ast_ctrl->regs + OFFSET_DMA_LEN_REG);
+
+	/* enable DMA irq */
+	reg_val = readl(ast_ctrl->regs + OFFSET_INTR_CTRL_STATUS);
+	reg_val |= SPI_DMA_IRQ_EN;
+	writel(reg_val, ast_ctrl->regs + OFFSET_INTR_CTRL_STATUS);
+
+	reinit_completion(&ast_ctrl->dma_done);
+
+	/* enable write DMA */
+	writel(0x3, ast_ctrl->regs + OFFSET_DMA_CTRL);
+	timeout = wait_for_completion_timeout(&ast_ctrl->dma_done, msecs_to_jiffies(2000));
+	if (timeout == 0) {
+		writel(0x0, ast_ctrl->regs + OFFSET_DMA_CTRL);
+		writel(DMA_DISCARD_REQ_MAGIC, ast_ctrl->regs + OFFSET_DMA_CTRL);
+		dev_err(dev, "write data timeout %d\n", ret);
+		ret = -1;
+	}
+
+	reg_val = ast_ctrl->chips[target_cs].ctrl_val[ASPEED_SPI_READ];
+	writel(reg_val, ast_ctrl->regs + OFFSET_CE0_CTRL_REG + target_cs * 4);
+
+	return ret ? 0 : len;
+}
+
+static irqreturn_t aspeed_spi_dma_isr(int irq, void *dev_id)
+{
+	struct aspeed_spi_controller *ast_ctrl =
+		(struct aspeed_spi_controller *)dev_id;
+	uint32_t reg_val;
+
+	if (!(readl(ast_ctrl->regs + OFFSET_INTR_CTRL_STATUS) & 0x800))
+		return IRQ_NONE;
+
+	reg_val = readl(ast_ctrl->regs + OFFSET_INTR_CTRL_STATUS);
+	reg_val &= ~SPI_DMA_IRQ_EN;
+	writel(reg_val, ast_ctrl->regs + OFFSET_INTR_CTRL_STATUS);
+
+	writel(0x0, ast_ctrl->regs + OFFSET_DMA_CTRL);
+	writel(DMA_DISCARD_REQ_MAGIC, ast_ctrl->regs + OFFSET_DMA_CTRL);
+
+	complete(&ast_ctrl->dma_done);
+
+	return IRQ_HANDLED;
+}
+
+static int aspeed_spi_dirmap_create(struct spi_mem_dirmap_desc *desc)
+{
+	int ret = 0;
+	struct aspeed_spi_controller *ast_ctrl =
+		spi_controller_get_devdata(desc->mem->spi->master);
+	struct device *dev = ast_ctrl->dev;
+	const struct aspeed_spi_info *info = ast_ctrl->info;
+	struct spi_mem_op op_tmpl = desc->info.op_tmpl;
+	uint32_t decode_sz_arr[5];
+	uint32_t cs, target_cs = desc->mem->spi->chip_select;
+	uint32_t reg_val;
+
+	if (desc->info.op_tmpl.data.dir == SPI_MEM_DATA_IN) {
+		/* record original decode size */
+		for (cs = 0; cs < ast_ctrl->num_cs; cs++) {
+			reg_val = readl(ast_ctrl->regs +
+					OFFSET_CE0_DECODE_RANGE_REG + cs * 4);
+			decode_sz_arr[cs] =
+				info->segment_end(ast_ctrl, reg_val) -
+				info->segment_start(ast_ctrl, reg_val);
+		}
+
+		decode_sz_arr[target_cs] = desc->info.length;
+
+		if (info->adjust_decode_sz)
+			info->adjust_decode_sz(decode_sz_arr, ast_ctrl->num_cs);
+
+		for (cs = 0; cs < ast_ctrl->num_cs; cs++) {
+			dev_dbg(dev, "cs: %d, sz: 0x%x\n", cs,
+				decode_sz_arr[cs]);
+		}
+
+		ret = aspeed_spi_decode_range_config(ast_ctrl, decode_sz_arr);
+		if (ret)
+			return ret;
+
+		reg_val = readl(ast_ctrl->regs + OFFSET_CE0_CTRL_REG +
+				target_cs * 4) &
+			  (~info->cmd_io_ctrl_mask);
+		reg_val |= aspeed_spi_get_io_mode(op_tmpl.data.buswidth) |
+			   op_tmpl.cmd.opcode << 16 |
+			   ((op_tmpl.dummy.nbytes) & 0x3) << 6 |
+			   ((op_tmpl.dummy.nbytes) & 0x4) << 14 |
+			   CTRL_IO_MODE_CMD_READ;
+
+		writel(reg_val,
+		       ast_ctrl->regs + OFFSET_CE0_CTRL_REG + target_cs * 4);
+		ast_ctrl->chips[target_cs].ctrl_val[ASPEED_SPI_READ] = reg_val;
+		ast_ctrl->chips[target_cs].max_clk_freq =
+			desc->mem->spi->max_speed_hz;
+
+		ast_ctrl->disable_calib = false;
+		if (!of_property_read_bool(ast_ctrl->dev->of_node,
+			"timing-calibration-disabled")) {
+			ast_ctrl->disable_calib = true;
+		}
+
+		ret = info->calibrate(ast_ctrl, target_cs);
+
+		dev_info(dev, "read bus width: %d [0x%08x]\n",
+			 op_tmpl.data.buswidth,
+			 ast_ctrl->chips[target_cs].ctrl_val[ASPEED_SPI_READ]);
+
+	} else if (desc->info.op_tmpl.data.dir == SPI_MEM_DATA_OUT) {
+		reg_val = readl(ast_ctrl->regs + OFFSET_CE0_CTRL_REG +
+					target_cs * 4) & (~info->cmd_io_ctrl_mask);
+
+		if ((ast_ctrl->flag & SPI_FIXED_LOW_W_CLK) == SPI_FIXED_LOW_W_CLK) {
+			/* adjust spi clk for write */
+			reg_val = (reg_val & (~0x0f000f00)) | 0x03000000;
+		}
+
+		reg_val |= aspeed_spi_get_io_mode(op_tmpl.data.buswidth) |
+			   op_tmpl.cmd.opcode << 16 | CTRL_IO_MODE_CMD_WRITE;
+
+		ast_ctrl->chips[target_cs].ctrl_val[ASPEED_SPI_WRITE] = reg_val;
+
+		dev_info(dev, "write bus width: %d [0x%08x]\n",
+			 op_tmpl.data.buswidth,
+			 ast_ctrl->chips[target_cs].ctrl_val[ASPEED_SPI_WRITE]);
+	}
+
+
+	if (info->safs_support) {
+		info->safs_support(ast_ctrl, desc->info.op_tmpl.data.dir,
+			op_tmpl.cmd.opcode, op_tmpl.addr.nbytes, op_tmpl.data.buswidth);
+	}
+
+	if (desc->info.op_tmpl.data.dir == SPI_MEM_DATA_OUT &&
+		desc->mem->spi->controller->mem_ops->dirmap_write == NULL)
+		return -EINVAL;
+
+	if (desc->info.op_tmpl.data.dir == SPI_MEM_DATA_IN &&
+		desc->mem->spi->controller->mem_ops->dirmap_read == NULL)
+		return -EINVAL;
+
+	return ret;
+}
+
+static const char *aspeed_spi_get_name(struct spi_mem *mem)
+{
+	struct device *dev = &mem->spi->master->dev;
+	const char *name;
+
+	name = devm_kasprintf(dev, GFP_KERNEL, "%s-%d", dev_name(dev),
+			      mem->spi->chip_select);
+
+	if (!name) {
+		dev_err(dev, "cannot get spi name\n");
+		return ERR_PTR(-ENOMEM);
+	}
+
+	return name;
+}
+
+/*
+ * Currently, only support 1-1-1, 1-1-2 or 1-1-4
+ * SPI NOR flash operation format.
+ */
+static bool aspeed_spi_support_op(struct spi_mem *mem,
+				  const struct spi_mem_op *op)
+{
+	struct aspeed_spi_controller *ast_ctrl =
+		spi_controller_get_devdata(mem->spi->master);
+
+	if (op->cmd.buswidth > 1)
+		return false;
+
+	if (op->addr.nbytes != 0) {
+		if (op->addr.buswidth > 1 || op->addr.nbytes > 4)
+			return false;
+	}
+
+	if (op->dummy.nbytes != 0) {
+		if (op->dummy.buswidth > 1 || op->dummy.nbytes > 7)
+			return false;
+	}
+
+	if (op->data.nbytes != 0 &&
+	    ast_ctrl->info->max_data_bus_width < op->data.buswidth)
+		return false;
+
+	if (!spi_mem_default_supports_op(mem, op))
+		return false;
+
+	if (op->addr.nbytes == 4)
+		ast_ctrl->info->set_4byte(ast_ctrl, mem->spi->chip_select);
+
+	return true;
+}
+
+/* AST2600-A3 */
+static const struct spi_controller_mem_ops aspeed_spi_ops_user_read_write = {
+	.exec_op = aspeed_spi_exec_op_user_mode,
+	.get_name = aspeed_spi_get_name,
+	.supports_op = aspeed_spi_support_op,
+	.dirmap_create = aspeed_spi_dirmap_create,
+};
+
+/* If CRTM feature is enabled on AST2600-A3, please use the following settings.
+ * static const struct spi_controller_mem_ops aspeed_spi_ops_user_read_write = {
+ *	.exec_op = aspeed_spi_exec_op_cmd_mode,
+ *	.get_name = aspeed_spi_get_name,
+ *	.supports_op = aspeed_spi_support_op,
+ *	.dirmap_create = aspeed_spi_dirmap_create,
+ *	.dirmap_read = aspeed_spi_dirmap_cmd_read,
+ * };
+ */
+
+/* AST2600-A1/A2 */
+static const struct spi_controller_mem_ops aspeed_spi_ops_cmd_read_dma_write = {
+	.exec_op = aspeed_spi_exec_op_cmd_mode,
+	.get_name = aspeed_spi_get_name,
+	.supports_op = aspeed_spi_support_op,
+	.dirmap_create = aspeed_spi_dirmap_create,
+	.dirmap_read = aspeed_spi_dirmap_cmd_read,
+	.dirmap_write = aspeed_spi_dirmap_dma_write,
+};
+
+/*
+ * Initialize SPI controller for each chip select.
+ * Here, only the minimum decode range is configured
+ * in order to get device (SPI NOR flash) information
+ * at the early stage.
+ */
+static int aspeed_spi_ctrl_init(struct aspeed_spi_controller *ast_ctrl)
+{
+	int ret;
+	uint32_t cs;
+	uint32_t val;
+	uint32_t decode_sz_arr[ASPEED_SPI_MAX_CS];
+
+	/* enable write capability for all CEs */
+	val = readl(ast_ctrl->regs + OFFSET_CE_TYPE_SETTING);
+	writel(val | (GENMASK(ast_ctrl->num_cs - 1, 0) << 16),
+	       ast_ctrl->regs + OFFSET_CE_TYPE_SETTING);
+
+	/* initial each CE's controller register */
+	for (cs = 0; cs < ast_ctrl->num_cs; cs++) {
+		val = CTRL_STOP_ACTIVE | CTRL_IO_MODE_USER;
+		writel(val, ast_ctrl->regs + OFFSET_CE0_CTRL_REG + cs * 4);
+		ast_ctrl->chips[cs].ctrl_val[ASPEED_SPI_BASE] = val;
+	}
+
+	for (cs = 0; cs < ast_ctrl->num_cs && cs < ASPEED_SPI_MAX_CS; cs++)
+		decode_sz_arr[cs] = ast_ctrl->info->min_decode_sz;
+
+	ret = aspeed_spi_decode_range_config(ast_ctrl, decode_sz_arr);
+
+	return ret;
+}
+
+static const struct of_device_id aspeed_spi_matches[] = {
+	{ .compatible = "aspeed,ast2600-fmc", .data = &ast2600_fmc_info },
+	{ .compatible = "aspeed,ast2600-spi", .data = &ast2600_spi_info },
+	{ /* sentinel */ }
+};
+MODULE_DEVICE_TABLE(of, aspeed_spi_matches);
+
+static int aspeed_spi_probe(struct platform_device *pdev)
+{
+	int ret;
+	struct device *dev = &pdev->dev;
+	struct spi_controller *spi_ctrl;
+	struct aspeed_spi_controller *ast_ctrl;
+	const struct of_device_id *match;
+	struct clk *clk;
+	struct resource *res;
+
+	spi_ctrl = spi_alloc_master(dev, sizeof(struct aspeed_spi_controller));
+	if (!spi_ctrl)
+		return -ENOMEM;
+
+	ast_ctrl = spi_controller_get_devdata(spi_ctrl);
+
+	match = of_match_device(aspeed_spi_matches, dev);
+	if (!match || !match->data) {
+		dev_err(dev, "no compatible OF match\n");
+		return -ENODEV;
+	}
+
+	ast_ctrl->info = match->data;
+
+	res = platform_get_resource_byname(pdev, IORESOURCE_MEM, "spi_ctrl_reg");
+	ast_ctrl->regs = devm_ioremap_resource(dev, res);
+	if (IS_ERR(ast_ctrl->regs))
+		return PTR_ERR(ast_ctrl->regs);
+
+	res = platform_get_resource_byname(pdev, IORESOURCE_MEM, "spi_mmap");
+	ast_ctrl->ahb_base_phy = res->start;
+	ast_ctrl->ahb_window_sz = resource_size(res);
+
+	ast_ctrl->dev = dev;
+
+	clk = devm_clk_get(&pdev->dev, NULL);
+	if (IS_ERR(clk))
+		return PTR_ERR(clk);
+	ast_ctrl->ahb_clk = clk_get_rate(clk);
+	devm_clk_put(&pdev->dev, clk);
+
+	if (of_property_read_u32(dev->of_node, "num-cs", &ast_ctrl->num_cs)) {
+		dev_err(dev, "fail to get chip number.\n");
+		goto end;
+	}
+
+	if (ast_ctrl->num_cs > ASPEED_SPI_MAX_CS) {
+		dev_err(dev, "chip number, %d, exceeds %d.\n", ast_ctrl->num_cs,
+			ASPEED_SPI_MAX_CS);
+		goto end;
+	}
+
+	ast_ctrl->flag = 0;
+	if (of_property_read_bool(dev->of_node, "fmc-spi-user-mode")) {
+		dev_info(dev, "adopt user mode\n");
+		ast_ctrl->flag |= SPI_MODE_USER;
+	}
+
+	/* Should be set on AST2600-A1/A2 for errata 65 */
+	if (of_property_read_bool(dev->of_node, "low-spi-clk-write")) {
+		dev_info(dev, "adopt low spi clk for write\n");
+		ast_ctrl->flag |= SPI_FIXED_LOW_W_CLK;
+	}
+
+	/*
+	 * "spi-dma-write" should be set on AST2600-A1/A2 for errata 65.
+	 * Should NOT be set on AST2600-A3 with high SPI clock
+	 */
+	if (of_property_read_bool(dev->of_node, "spi-dma-write")) {
+		dev_info(dev, "adopt dma write mode\n");
+		ast_ctrl->flag |= SPI_DMA_WRITE;
+	}
+
+	if (of_property_read_bool(dev->of_node, "spi-dma-read")) {
+		dev_info(dev, "adopt dma read mode\n");
+		ast_ctrl->flag |= SPI_DMA_READ;
+	}
+
+	if ((ast_ctrl->flag & SPI_MODE_USER) && (ast_ctrl->flag & SPI_DMA_WRITE)) {
+		dev_err(dev, "Invalid mode selection\n");
+		ret = -EINVAL;
+		goto end;
+	}
+
+	ast_ctrl->op_buf = dma_alloc_coherent(dev,
+		FMC_SPI_DMA_BUF_LEN, &ast_ctrl->dma_addr_phy, GFP_DMA | GFP_KERNEL);
+	if (!ast_ctrl->op_buf) {
+		ret = -ENOMEM;
+		goto end;
+	}
+
+	ast_ctrl->irq = platform_get_irq(pdev, 0);
+	if (ast_ctrl->irq < 0) {
+		dev_err(dev, "fail to get irq (%d)\n", ast_ctrl->irq);
+		return ast_ctrl->irq;
+	}
+
+	ret = devm_request_irq(dev, ast_ctrl->irq, aspeed_spi_dma_isr,
+					IRQF_SHARED, dev_name(dev), ast_ctrl);
+	if (ret < 0) {
+		dev_err(dev, "fail to request irq (%d)\n", ret);
+		return ret;
+	}
+
+	init_completion(&ast_ctrl->dma_done);
+
+	ast_ctrl->chips =
+		devm_kzalloc(dev,
+			     sizeof(struct aspeed_spi_chip) * ast_ctrl->num_cs,
+			     GFP_KERNEL);
+
+	platform_set_drvdata(pdev, ast_ctrl);
+
+	spi_ctrl->mode_bits =
+		SPI_RX_DUAL | SPI_RX_QUAD | SPI_TX_DUAL | SPI_TX_QUAD;
+
+	spi_ctrl->bus_num = -1;
+
+	if ((ast_ctrl->flag & SPI_DMA_WRITE) == SPI_DMA_WRITE) {
+		/* for AST2600-A1/A2 */
+		spi_ctrl->mem_ops = &aspeed_spi_ops_cmd_read_dma_write;
+	} else {
+		/* for AST2600-A3 */
+		spi_ctrl->mem_ops = &aspeed_spi_ops_user_read_write;
+	}
+
+	spi_ctrl->dev.of_node = dev->of_node;
+	spi_ctrl->num_chipselect = ast_ctrl->num_cs;
+
+	ret = aspeed_spi_ctrl_init(ast_ctrl);
+	if (ret)
+		goto end;
+
+	ret = devm_spi_register_master(dev, spi_ctrl);
+
+end:
+	return ret;
+}
+
+static int aspeed_spi_remove(struct platform_device *pdev)
+{
+	struct aspeed_spi_controller *ast_ctrl = platform_get_drvdata(pdev);
+	uint32_t val;
+
+	/* disable write capability for all CEs */
+	val = readl(ast_ctrl->regs + OFFSET_CE_TYPE_SETTING);
+	writel(val & ~(GENMASK(ast_ctrl->num_cs, 0) << 16),
+	       ast_ctrl->regs + OFFSET_CE_TYPE_SETTING);
+
+	return 0;
+}
+
+static struct platform_driver aspeed_spi_driver = {
+	.driver = {
+		.name = "ASPEED_FMC_SPI",
+		.bus = &platform_bus_type,
+		.of_match_table = aspeed_spi_matches,
+	},
+	.probe = aspeed_spi_probe,
+	.remove = aspeed_spi_remove,
+};
+module_platform_driver(aspeed_spi_driver);
+
+MODULE_DESCRIPTION("ASPEED FMC/SPI Memory Controller Driver");
+MODULE_AUTHOR("Chin-Ting Kuo <chin-ting_kuo@aspeedtech.com>");
+MODULE_AUTHOR("Cedric Le Goater <clg@kaod.org>");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/tty/serial/8250/8250_aspeed.c b/drivers/tty/serial/8250/8250_aspeed.c
new file mode 100644
index 000000000000..8eefd7260a46
--- /dev/null
+++ b/drivers/tty/serial/8250/8250_aspeed.c
@@ -0,0 +1,498 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) ASPEED Technology Inc.
+ */
+#include <linux/device.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/serial_8250.h>
+#include <linux/serial_reg.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/of_platform.h>
+#include <linux/platform_device.h>
+#include <linux/clk.h>
+#include <linux/reset.h>
+#include <linux/dma-mapping.h>
+#include <linux/circ_buf.h>
+#include <linux/tty_flip.h>
+#include <linux/pm_runtime.h>
+#include <linux/soc/aspeed/aspeed-udma.h>
+
+#include "8250.h"
+
+#define DEVICE_NAME "aspeed-uart"
+
+/* offsets for the aspeed virtual uart registers */
+#define VUART_GCRA	0x20
+#define   VUART_GCRA_VUART_EN			BIT(0)
+#define   VUART_GCRA_SIRQ_POLARITY		BIT(1)
+#define   VUART_GCRA_DISABLE_HOST_TX_DISCARD	BIT(5)
+#define VUART_GCRB	0x24
+#define   VUART_GCRB_HOST_SIRQ_MASK		GENMASK(7, 4)
+#define   VUART_GCRB_HOST_SIRQ_SHIFT		4
+#define VUART_ADDRL	0x28
+#define VUART_ADDRH	0x2c
+
+#define DMA_TX_BUFSZ	PAGE_SIZE
+#define DMA_RX_BUFSZ	(64 * 1024)
+
+struct uart_ops ast8250_pops;
+
+struct ast8250_vuart {
+	u32 port;
+	u32 sirq;
+	u32 sirq_pol;
+};
+
+struct ast8250_udma {
+	u32 ch;
+
+	u32 tx_rbsz;
+	u32 rx_rbsz;
+
+	dma_addr_t tx_addr;
+	dma_addr_t rx_addr;
+
+	struct circ_buf *tx_rb;
+	struct circ_buf *rx_rb;
+
+	bool tx_tmout_dis;
+	bool rx_tmout_dis;
+};
+
+struct ast8250_data {
+	int line;
+
+	u8 __iomem *regs;
+
+	bool is_vuart;
+	bool use_dma;
+
+	struct reset_control *rst;
+	struct clk *clk;
+
+	struct ast8250_vuart vuart;
+	struct ast8250_udma dma;
+};
+
+static void ast8250_dma_tx_complete(int tx_rb_rptr, void *id)
+{
+	u32 count;
+    unsigned long flags;
+	struct uart_port *port = (struct uart_port*)id;
+	struct ast8250_data *data = port->private_data;
+
+    spin_lock_irqsave(&port->lock, flags);
+
+	count = CIRC_CNT(tx_rb_rptr, port->state->xmit.tail, data->dma.tx_rbsz);
+	port->state->xmit.tail = tx_rb_rptr;
+	port->icount.tx += count;
+
+    if (uart_circ_chars_pending(&port->state->xmit) < WAKEUP_CHARS)
+        uart_write_wakeup(port);
+
+    spin_unlock_irqrestore(&port->lock, flags);
+}
+
+static void ast8250_dma_rx_complete(int rx_rb_wptr, void *id)
+{
+	unsigned long flags;
+	struct uart_port *up = (struct uart_port*)id;
+	struct tty_port *tp = &up->state->port;
+	struct ast8250_data *data = up->private_data;
+	struct ast8250_udma *dma = &data->dma;
+	struct circ_buf *rx_rb = dma->rx_rb;
+	u32 rx_rbsz = dma->rx_rbsz;
+	u32 count = 0;
+
+	spin_lock_irqsave(&up->lock, flags);
+
+	rx_rb->head = rx_rb_wptr;
+
+	dma_sync_single_for_cpu(up->dev,
+			dma->rx_addr, dma->rx_rbsz, DMA_FROM_DEVICE);
+
+	while (CIRC_CNT(rx_rb->head, rx_rb->tail, rx_rbsz)) {
+		count = CIRC_CNT_TO_END(rx_rb->head, rx_rb->tail, rx_rbsz);
+
+		tty_insert_flip_string(tp, rx_rb->buf + rx_rb->tail, count);
+
+		rx_rb->tail += count;
+		rx_rb->tail %= rx_rbsz;
+
+        up->icount.rx += count;
+	}
+
+	if (count) {
+		aspeed_udma_set_rx_rptr(data->dma.ch, rx_rb->tail);
+		tty_flip_buffer_push(tp);
+	}
+
+	spin_unlock_irqrestore(&up->lock, flags);
+}
+
+static void ast8250_dma_start_tx(struct uart_port *port)
+{
+	struct ast8250_data *data = port->private_data;
+	struct ast8250_udma *dma = &data->dma;
+	struct circ_buf *tx_rb = dma->tx_rb;
+
+	dma_sync_single_for_device(port->dev,
+			dma->tx_addr, dma->tx_rbsz, DMA_TO_DEVICE);
+
+	aspeed_udma_set_tx_wptr(dma->ch, tx_rb->head);
+}
+
+static void ast8250_dma_pops_hook(struct uart_port *port)
+{
+	static int first = 1;
+
+	if (first) {
+		ast8250_pops = *port->ops;
+		ast8250_pops.start_tx = ast8250_dma_start_tx;
+	}
+
+	first = 0;
+	port->ops = &ast8250_pops;
+}
+
+static void ast8250_vuart_init(struct ast8250_data *data)
+{
+	u8 reg;
+	struct ast8250_vuart *vuart = &data->vuart;
+
+	/* IO port address */
+	writeb((u8)(vuart->port >> 0), data->regs + VUART_ADDRL);
+	writeb((u8)(vuart->port >> 8), data->regs + VUART_ADDRH);
+
+	/* SIRQ number */
+	reg = readb(data->regs + VUART_GCRB);
+	reg &= ~VUART_GCRB_HOST_SIRQ_MASK;
+	reg |= ((vuart->sirq << VUART_GCRB_HOST_SIRQ_SHIFT) & VUART_GCRB_HOST_SIRQ_MASK);
+	writeb(reg, data->regs + VUART_GCRB);
+
+	/* SIRQ polarity */
+	reg = readb(data->regs + VUART_GCRA);
+	if (vuart->sirq_pol)
+		reg |= VUART_GCRA_SIRQ_POLARITY;
+	else
+		reg &= ~VUART_GCRA_SIRQ_POLARITY;
+	writeb(reg, data->regs + VUART_GCRA);
+}
+
+static void ast8250_vuart_set_host_tx_discard(struct ast8250_data *data, bool discard)
+{
+	u8 reg;
+
+	reg = readb(data->regs + VUART_GCRA);
+	if (discard)
+		reg &= ~VUART_GCRA_DISABLE_HOST_TX_DISCARD;
+	else
+		reg |= VUART_GCRA_DISABLE_HOST_TX_DISCARD;
+	writeb(reg, data->regs + VUART_GCRA);
+}
+
+static void ast8250_vuart_set_enable(struct ast8250_data *data, bool enable)
+{
+	u8 reg;
+
+	reg = readb(data->regs + VUART_GCRA);
+	if (enable)
+		reg |= VUART_GCRA_VUART_EN;
+	else
+		reg &= ~VUART_GCRA_VUART_EN;
+	writeb(reg, data->regs + VUART_GCRA);
+}
+
+static int ast8250_handle_irq(struct uart_port *port)
+{
+	u32 iir = port->serial_in(port, UART_IIR);
+	return serial8250_handle_irq(port, iir);
+}
+
+static int ast8250_startup(struct uart_port *port)
+{
+	int rc = 0;
+	struct ast8250_data *data = port->private_data;
+	struct ast8250_udma *dma;
+
+	if (data->is_vuart)
+		ast8250_vuart_set_host_tx_discard(data, false);
+
+	if (data->use_dma) {
+		dma = &data->dma;
+
+		dma->tx_rbsz = DMA_TX_BUFSZ;
+		dma->rx_rbsz = DMA_RX_BUFSZ;
+
+		/*
+		 * We take the xmit buffer passed from upper layers as
+		 * the DMA TX buffer and allocate a new buffer for the
+		 * RX use.
+		 *
+		 * To keep the TX/RX operation consistency, we use the
+		 * streaming DMA interface instead of the coherent one
+		 */
+		dma->tx_rb = &port->state->xmit;
+		dma->rx_rb->buf = kzalloc(data->dma.rx_rbsz, GFP_KERNEL);
+		if (IS_ERR_OR_NULL(dma->rx_rb->buf)) {
+			dev_err(port->dev, "failed to allcoate RX DMA buffer\n");
+			rc = -ENOMEM;
+			goto out;
+		}
+
+		dma->tx_addr = dma_map_single(port->dev, dma->tx_rb->buf,
+				dma->tx_rbsz, DMA_TO_DEVICE);
+		if (dma_mapping_error(port->dev, dma->tx_addr)) {
+			dev_err(port->dev, "failed to map streaming TX DMA region\n");
+			rc = -ENOMEM;
+			goto free_dma_n_out;
+		}
+
+		dma->rx_addr = dma_map_single(port->dev, dma->rx_rb->buf,
+				dma->rx_rbsz, DMA_FROM_DEVICE);
+		if (dma_mapping_error(port->dev, dma->rx_addr)) {
+			dev_err(port->dev, "failed to map streaming RX DMA region\n");
+			rc = -ENOMEM;
+			goto free_dma_n_out;
+		}
+
+		rc = aspeed_udma_request_tx_chan(dma->ch, dma->tx_addr,
+				dma->tx_rb, dma->tx_rbsz, ast8250_dma_tx_complete, port, dma->tx_tmout_dis);
+		if (rc) {
+			dev_err(port->dev, "failed to request DMA TX channel\n");
+			goto free_dma_n_out;
+		}
+
+		rc = aspeed_udma_request_rx_chan(dma->ch, dma->rx_addr,
+				dma->rx_rb, dma->rx_rbsz, ast8250_dma_rx_complete, port, dma->rx_tmout_dis);
+		if (rc) {
+			dev_err(port->dev, "failed to request DMA RX channel\n");
+			goto free_dma_n_out;
+		}
+
+		ast8250_dma_pops_hook(port);
+
+		aspeed_udma_tx_chan_ctrl(dma->ch, ASPEED_UDMA_OP_ENABLE);
+		aspeed_udma_rx_chan_ctrl(dma->ch, ASPEED_UDMA_OP_ENABLE);
+	}
+
+	memset(&port->icount, 0, sizeof(port->icount));
+	return serial8250_do_startup(port);
+
+free_dma_n_out:
+	kfree(dma->rx_rb->buf);
+out:
+	return rc;
+}
+
+static void ast8250_shutdown(struct uart_port *port)
+{
+	int rc;
+	struct ast8250_data *data = port->private_data;
+	struct ast8250_udma *dma;
+
+	if (data->use_dma) {
+		dma = &data->dma;
+
+		aspeed_udma_tx_chan_ctrl(dma->ch, ASPEED_UDMA_OP_RESET);
+		aspeed_udma_rx_chan_ctrl(dma->ch, ASPEED_UDMA_OP_RESET);
+
+		aspeed_udma_tx_chan_ctrl(dma->ch, ASPEED_UDMA_OP_DISABLE);
+		aspeed_udma_rx_chan_ctrl(dma->ch, ASPEED_UDMA_OP_DISABLE);
+
+		rc = aspeed_udma_free_tx_chan(dma->ch);
+		if (rc)
+			dev_err(port->dev, "failed to free DMA TX channel, rc=%d\n", rc);
+
+		rc = aspeed_udma_free_rx_chan(dma->ch);
+		if (rc)
+			dev_err(port->dev, "failed to free DMA TX channel, rc=%d\n", rc);
+
+		dma_unmap_single(port->dev, dma->tx_addr,
+				dma->tx_rbsz, DMA_TO_DEVICE);
+		dma_unmap_single(port->dev, dma->rx_addr,
+				dma->rx_rbsz, DMA_FROM_DEVICE);
+
+		if (dma->rx_rb->buf)
+			kfree(dma->rx_rb->buf);
+	}
+
+	if (data->is_vuart)
+		ast8250_vuart_set_host_tx_discard(data, true);
+
+	serial8250_do_shutdown(port);
+}
+
+static int __maybe_unused ast8250_suspend(struct device *dev)
+{
+	struct ast8250_data *data = dev_get_drvdata(dev);
+	serial8250_suspend_port(data->line);
+	return 0;
+}
+
+static int __maybe_unused ast8250_resume(struct device *dev)
+{
+	struct ast8250_data *data = dev_get_drvdata(dev);
+	serial8250_resume_port(data->line);
+	return 0;
+}
+
+static int ast8250_probe(struct platform_device *pdev)
+{
+	int rc;
+	struct uart_8250_port uart = {};
+	struct uart_port *port = &uart.port;
+	struct device *dev = &pdev->dev;
+	struct ast8250_data *data;
+
+	struct resource *res;
+	u32 irq;
+
+	data = devm_kzalloc(dev, sizeof(*data), GFP_KERNEL);
+	if (data == NULL)
+	    return -ENOMEM;
+
+	data->dma.rx_rb = devm_kzalloc(dev, sizeof(data->dma.rx_rb), GFP_KERNEL);
+	if (data->dma.rx_rb == NULL)
+		return -ENOMEM;
+
+	irq = platform_get_irq(pdev, 0);
+	if (irq < 0) {
+		if (irq != -EPROBE_DEFER)
+			dev_err(dev, "failed to get IRQ number\n");
+		return irq;
+	}
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (res == NULL) {
+		dev_err(dev, "failed to get register base\n");
+		return -ENODEV;
+	}
+
+	data->regs = devm_ioremap(dev, res->start, resource_size(res));
+	if (IS_ERR(data->regs)) {
+		dev_err(dev, "failed to map registers\n");
+		return PTR_ERR(data->regs);
+	}
+
+	data->clk = devm_clk_get(dev, NULL);
+	if (IS_ERR(data->clk)) {
+		dev_err(dev, "failed to get clocks\n");
+		return -ENODEV;
+	}
+
+	rc = clk_prepare_enable(data->clk);
+	if (rc) {
+		dev_err(dev, "failed to enable clock\n");
+		return rc;
+	}
+
+	data->rst = devm_reset_control_get_optional_exclusive(dev, NULL);
+	if (!IS_ERR(data->rst))
+		reset_control_deassert(data->rst);
+
+	data->is_vuart = of_property_read_bool(dev->of_node, "virtual");
+	if (data->is_vuart) {
+		rc = of_property_read_u32(dev->of_node, "port", &data->vuart.port);
+		if (rc) {
+			dev_err(dev, "failed to get VUART port address\n");
+			return -ENODEV;
+		}
+
+		rc = of_property_read_u32(dev->of_node, "sirq", &data->vuart.sirq);
+		if (rc) {
+			dev_err(dev, "failed to get VUART SIRQ number\n");
+			return -ENODEV;
+		}
+
+		rc = of_property_read_u32(dev->of_node, "sirq-polarity", &data->vuart.sirq_pol);
+		if (rc) {
+			dev_err(dev, "failed to get VUART SIRQ polarity\n");
+			return -ENODEV;
+		}
+
+		ast8250_vuart_init(data);
+		ast8250_vuart_set_host_tx_discard(data, true);
+		ast8250_vuart_set_enable(data, true);
+	}
+
+	data->use_dma = of_property_read_bool(dev->of_node, "dma-mode");
+	if (data->use_dma) {
+		rc = of_property_read_u32(dev->of_node, "dma-channel", &data->dma.ch);
+		if (rc) {
+			dev_err(dev, "failed to get DMA channel\n");
+			return -ENODEV;
+		}
+
+		data->dma.tx_tmout_dis = of_property_read_bool(dev->of_node, "dma-tx-timeout-disable");
+		data->dma.rx_tmout_dis = of_property_read_bool(dev->of_node, "dma-rx-timeout-disable");
+	}
+
+	spin_lock_init(&port->lock);
+	port->dev = dev;
+	port->type = PORT_16550A;
+	port->irq = irq;
+	port->line = of_alias_get_id(dev->of_node, "serial");
+	port->handle_irq = ast8250_handle_irq;
+	port->mapbase = res->start;
+	port->mapsize = resource_size(res);
+	port->membase = data->regs;
+	port->uartclk = clk_get_rate(data->clk);
+	port->regshift = 2;
+	port->iotype = UPIO_MEM32;
+	port->flags = UPF_FIXED_TYPE | UPF_FIXED_PORT | UPF_SHARE_IRQ;
+	port->startup = ast8250_startup;
+	port->shutdown = ast8250_shutdown;
+	port->private_data = data;
+	uart.bugs |= UART_BUG_TXRACE;
+
+	data->line = serial8250_register_8250_port(&uart);
+	if (data->line < 0) {
+		dev_err(dev, "failed to register 8250 port\n");
+		return data->line;
+	}
+
+	pm_runtime_set_active(&pdev->dev);
+	pm_runtime_enable(&pdev->dev);
+
+	platform_set_drvdata(pdev, data);
+	return 0;
+}
+
+static int ast8250_remove(struct platform_device *pdev)
+{
+    struct ast8250_data *data = platform_get_drvdata(pdev);
+
+	if (data->is_vuart)
+		ast8250_vuart_set_enable(data, false);
+
+    serial8250_unregister_port(data->line);
+	return 0;
+}
+
+static const struct dev_pm_ops ast8250_pm_ops = {
+	SET_SYSTEM_SLEEP_PM_OPS(ast8250_suspend, ast8250_resume)
+};
+
+static const struct of_device_id ast8250_of_match[] = {
+	{ .compatible = "aspeed,ast2500-uart" },
+	{ .compatible = "aspeed,ast2600-uart" },
+};
+
+static struct platform_driver ast8250_platform_driver = {
+	.driver = {
+		.name = DEVICE_NAME,
+		.pm = &ast8250_pm_ops,
+		.of_match_table = ast8250_of_match,
+	},
+	.probe = ast8250_probe,
+	.remove = ast8250_remove,
+};
+
+module_platform_driver(ast8250_platform_driver);
+
+MODULE_AUTHOR("Chia-Wei Wang <chiawei_wang@aspeedtech.com>");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Aspeed UART Driver");
diff --git a/drivers/tty/serial/8250/Kconfig b/drivers/tty/serial/8250/Kconfig
index b0f62345bc84..9de57c34b613 100644
--- a/drivers/tty/serial/8250/Kconfig
+++ b/drivers/tty/serial/8250/Kconfig
@@ -538,6 +538,14 @@ config SERIAL_8250_BCM7271
 	  including DMA support and high accuracy BAUD rates, say
 	  Y to this option. If unsure, say N.
 
+config SERIAL_8250_ASPEED
+	tristate "Aspeed serial port support"
+	depends on SERIAL_8250 && ARCH_ASPEED
+	select ASPEED_UDMA
+	help
+	  If you have a system using an Aspeed ASTXXXX SoCs and wish to make use
+	  of its UARTs, say Y to this option. If unsure, say N.
+
 config SERIAL_OF_PLATFORM
 	tristate "Devicetree based probing for 8250 ports"
 	depends on SERIAL_8250 && OF
diff --git a/drivers/tty/serial/8250/Makefile b/drivers/tty/serial/8250/Makefile
index 1615bfdde2a0..7f05b7ed422b 100644
--- a/drivers/tty/serial/8250/Makefile
+++ b/drivers/tty/serial/8250/Makefile
@@ -42,6 +42,7 @@ obj-$(CONFIG_SERIAL_8250_PERICOM)	+= 8250_pericom.o
 obj-$(CONFIG_SERIAL_8250_PXA)		+= 8250_pxa.o
 obj-$(CONFIG_SERIAL_8250_TEGRA)		+= 8250_tegra.o
 obj-$(CONFIG_SERIAL_8250_BCM7271)	+= 8250_bcm7271.o
+obj-$(CONFIG_SERIAL_8250_ASPEED)	+= 8250_aspeed.o
 obj-$(CONFIG_SERIAL_OF_PLATFORM)	+= 8250_of.o
 
 CFLAGS_8250_ingenic.o += -I$(srctree)/scripts/dtc/libfdt
diff --git a/drivers/usb/gadget/udc/aspeed-vhub/core.c b/drivers/usb/gadget/udc/aspeed-vhub/core.c
index 7a635c499777..a212fc3c0618 100644
--- a/drivers/usb/gadget/udc/aspeed-vhub/core.c
+++ b/drivers/usb/gadget/udc/aspeed-vhub/core.c
@@ -37,7 +37,7 @@ void ast_vhub_done(struct ast_vhub_ep *ep, struct ast_vhub_req *req,
 
 	list_del_init(&req->queue);
 
-	if (req->req.status == -EINPROGRESS)
+	if ((req->req.status == -EINPROGRESS) ||  (status == -EOVERFLOW))
 		req->req.status = status;
 
 	if (req->req.dma) {
@@ -240,6 +240,7 @@ void ast_vhub_init_hw(struct ast_vhub *vhub)
 	if (vhub->force_usb1)
 		ctrl |= VHUB_CTRL_FULL_SPEED_ONLY;
 
+	ctrl |= VHUB_CTRL_AUTO_REMOTE_WAKEUP;
 	ctrl |= VHUB_CTRL_UPSTREAM_CONNECT;
 	writel(ctrl, vhub->regs + AST_VHUB_CTRL);
 
diff --git a/drivers/usb/gadget/udc/aspeed-vhub/dev.c b/drivers/usb/gadget/udc/aspeed-vhub/dev.c
index 4f3bc27c1c62..c028c7a744d8 100644
--- a/drivers/usb/gadget/udc/aspeed-vhub/dev.c
+++ b/drivers/usb/gadget/udc/aspeed-vhub/dev.c
@@ -117,10 +117,14 @@ static int ast_vhub_dev_feature(struct ast_vhub_dev *d,
 
 	if (wValue == USB_DEVICE_REMOTE_WAKEUP) {
 		d->wakeup_en = is_set;
+		val = readl(d->vhub->regs + AST_VHUB_CTRL);
+		if (is_set)
+			writel(val | VHUB_CTRL_AUTO_REMOTE_WAKEUP,
+			       d->vhub->regs + AST_VHUB_CTRL);
+
 		return std_req_complete;
-	}
 
-	if (wValue == USB_DEVICE_TEST_MODE) {
+	} else if (wValue == USB_DEVICE_TEST_MODE) {
 		val = readl(d->vhub->regs + AST_VHUB_CTRL);
 		val &= ~GENMASK(10, 8);
 		val |= VHUB_CTRL_SET_TEST_MODE((wIndex >> 8) & 0x7);
@@ -240,7 +244,7 @@ int ast_vhub_std_dev_request(struct ast_vhub_ep *ep,
 		d->gadget.speed = ep->vhub->speed;
 		if (d->gadget.speed > d->driver->max_speed)
 			d->gadget.speed = d->driver->max_speed;
-		DDBG(d, "fist packet, captured speed %d\n",
+		DDBG(d, "first packet, captured speed %d\n",
 		     d->gadget.speed);
 	}
 
@@ -591,7 +595,6 @@ int ast_vhub_init_dev(struct ast_vhub *vhub, unsigned int idx)
 		d->gadget.max_speed = USB_SPEED_HIGH;
 	d->gadget.speed = USB_SPEED_UNKNOWN;
 	d->gadget.dev.of_node = vhub->pdev->dev.of_node;
-	d->gadget.dev.of_node_reused = true;
 
 	rc = usb_add_gadget_udc(d->port_dev, &d->gadget);
 	if (rc != 0)
diff --git a/drivers/usb/gadget/udc/aspeed-vhub/ep0.c b/drivers/usb/gadget/udc/aspeed-vhub/ep0.c
index b4cf46249fea..bea9cbb191a2 100644
--- a/drivers/usb/gadget/udc/aspeed-vhub/ep0.c
+++ b/drivers/usb/gadget/udc/aspeed-vhub/ep0.c
@@ -252,7 +252,7 @@ static void ast_vhub_ep0_do_receive(struct ast_vhub_ep *ep, struct ast_vhub_req
 		rc = -EOVERFLOW;
 	}
 
-	/* Hardware return wrong data len */
+	/* HW return wrong data len */
 	if (len < ep->ep.maxpacket && len != remain) {
 		EPDBG(ep, "using expected data len instead\n");
 		len = remain;
diff --git a/drivers/usb/gadget/udc/aspeed-vhub/epn.c b/drivers/usb/gadget/udc/aspeed-vhub/epn.c
index b5252880b389..cd6a9dfe6fdf 100644
--- a/drivers/usb/gadget/udc/aspeed-vhub/epn.c
+++ b/drivers/usb/gadget/udc/aspeed-vhub/epn.c
@@ -84,6 +84,7 @@ static void ast_vhub_epn_handle_ack(struct ast_vhub_ep *ep)
 {
 	struct ast_vhub_req *req;
 	unsigned int len;
+	int status = 0;
 	u32 stat;
 
 	/* Read EP status */
@@ -119,9 +120,15 @@ static void ast_vhub_epn_handle_ack(struct ast_vhub_ep *ep)
 	len = VHUB_EP_DMA_TX_SIZE(stat);
 
 	/* If not using DMA, copy data out if needed */
-	if (!req->req.dma && !ep->epn.is_in && len)
-		memcpy(req->req.buf + req->req.actual, ep->buf, len);
-
+	if (!req->req.dma && !ep->epn.is_in && len) {
+		if (req->req.actual + len > req->req.length) {
+			req->last_desc = 1;
+			status = -EOVERFLOW;
+			goto done;
+		} else {
+			memcpy(req->req.buf + req->req.actual, ep->buf, len);
+		}
+	}
 	/* Adjust size */
 	req->req.actual += len;
 
@@ -129,9 +136,10 @@ static void ast_vhub_epn_handle_ack(struct ast_vhub_ep *ep)
 	if (len < ep->ep.maxpacket)
 		req->last_desc = 1;
 
+done:
 	/* That's it ? complete the request and pick a new one */
 	if (req->last_desc >= 0) {
-		ast_vhub_done(ep, req, 0);
+		ast_vhub_done(ep, req, status);
 		req = list_first_entry_or_null(&ep->queue, struct ast_vhub_req,
 					       queue);
 
@@ -381,6 +389,11 @@ static int ast_vhub_epn_queue(struct usb_ep* u_ep, struct usb_request *u_req,
 	} else
 		u_req->dma = 0;
 
+	if (ep->dev->wakeup_en) {
+		EPVDBG(ep, "Wakeup host first\n");
+		ast_vhub_hub_wake_all(vhub);
+	}
+
 	EPVDBG(ep, "enqueue req @%p\n", req);
 	EPVDBG(ep, " l=%d dma=0x%x zero=%d noshort=%d noirq=%d is_in=%d\n",
 	       u_req->length, (u32)u_req->dma, u_req->zero,
@@ -466,21 +479,19 @@ static int ast_vhub_epn_dequeue(struct usb_ep* u_ep, struct usb_request *u_req)
 {
 	struct ast_vhub_ep *ep = to_ast_ep(u_ep);
 	struct ast_vhub *vhub = ep->vhub;
-	struct ast_vhub_req *req = NULL, *iter;
+	struct ast_vhub_req *req;
 	unsigned long flags;
 	int rc = -EINVAL;
 
 	spin_lock_irqsave(&vhub->lock, flags);
 
 	/* Make sure it's actually queued on this endpoint */
-	list_for_each_entry(iter, &ep->queue, queue) {
-		if (&iter->req != u_req)
-			continue;
-		req = iter;
-		break;
+	list_for_each_entry (req, &ep->queue, queue) {
+		if (&req->req == u_req)
+			break;
 	}
 
-	if (req) {
+	if (&req->req == u_req) {
 		EPVDBG(ep, "dequeue req @%p active=%d\n",
 		       req, req->active);
 		if (req->active)
@@ -779,6 +790,20 @@ static void ast_vhub_epn_dispose(struct usb_ep *u_ep)
 	ep->dev = NULL;
 }
 
+static void ast_vhub_epn_flush(struct usb_ep *u_ep)
+{
+	struct ast_vhub_ep *ep = to_ast_ep(u_ep);
+	struct ast_vhub *vhub = ep->vhub;
+	unsigned long flags;
+
+	EPDBG(ep, "flushing !\n");
+
+	spin_lock_irqsave(&vhub->lock, flags);
+	/* This will clear out all the request of the endpoint and send requests done messages. */
+	ast_vhub_nuke(ep, -EINVAL);
+	spin_unlock_irqrestore(&vhub->lock, flags);
+}
+
 static const struct usb_ep_ops ast_vhub_epn_ops = {
 	.enable		= ast_vhub_epn_enable,
 	.disable	= ast_vhub_epn_disable,
@@ -789,6 +814,7 @@ static const struct usb_ep_ops ast_vhub_epn_ops = {
 	.set_wedge	= ast_vhub_epn_set_wedge,
 	.alloc_request	= ast_vhub_alloc_request,
 	.free_request	= ast_vhub_free_request,
+	.fifo_flush	= ast_vhub_epn_flush,
 };
 
 struct ast_vhub_ep *ast_vhub_alloc_epn(struct ast_vhub_dev *d, u8 addr)
diff --git a/drivers/usb/gadget/udc/aspeed-vhub/hub.c b/drivers/usb/gadget/udc/aspeed-vhub/hub.c
index e2207d014620..7dd0e2dc9802 100644
--- a/drivers/usb/gadget/udc/aspeed-vhub/hub.c
+++ b/drivers/usb/gadget/udc/aspeed-vhub/hub.c
@@ -222,9 +222,8 @@ static int ast_vhub_hub_dev_feature(struct ast_vhub_ep *ep,
 		EPDBG(ep, "Hub remote wakeup %s\n",
 		      is_set ? "enabled" : "disabled");
 		return std_req_complete;
-	}
 
-	if (wValue == USB_DEVICE_TEST_MODE) {
+	} else if (wValue == USB_DEVICE_TEST_MODE) {
 		val = readl(ep->vhub->regs + AST_VHUB_CTRL);
 		val &= ~GENMASK(10, 8);
 		val |= VHUB_CTRL_SET_TEST_MODE((wIndex >> 8) & 0x7);
@@ -446,10 +445,9 @@ enum std_req_rc ast_vhub_std_hub_request(struct ast_vhub_ep *ep,
 
 		/* GET/SET_CONFIGURATION */
 	case DeviceRequest | USB_REQ_GET_CONFIGURATION:
-		return ast_vhub_simple_reply(ep, 1);
+		return ast_vhub_simple_reply(ep, vhub->current_config);
 	case DeviceOutRequest | USB_REQ_SET_CONFIGURATION:
-		if (wValue != 1)
-			return std_req_stall;
+		vhub->current_config = wValue;
 		return std_req_complete;
 
 		/* GET_DESCRIPTOR */
@@ -674,6 +672,9 @@ static enum std_req_rc ast_vhub_set_port_feature(struct ast_vhub_ep *ep,
 		ast_vhub_port_reset(vhub, port);
 		return std_req_complete;
 	case USB_PORT_FEAT_POWER:
+		ast_vhub_change_port_stat(vhub, port,
+					  0, USB_PORT_STAT_POWER,
+					  false);
 		/*
 		 * On Power-on, we mark the connected flag changed,
 		 * if there's a connected device, some hosts will
@@ -751,9 +752,6 @@ static enum std_req_rc ast_vhub_get_port_stat(struct ast_vhub_ep *ep,
 	stat = vhub->ports[port].status;
 	chg = vhub->ports[port].change;
 
-	/* We always have power */
-	stat |= USB_PORT_STAT_POWER;
-
 	EPDBG(ep, " port status=%04x change=%04x\n", stat, chg);
 
 	return ast_vhub_simple_reply(ep,
@@ -1059,10 +1057,8 @@ static int ast_vhub_init_desc(struct ast_vhub *vhub)
 	/* Initialize vhub String Descriptors. */
 	INIT_LIST_HEAD(&vhub->vhub_str_desc);
 	desc_np = of_get_child_by_name(vhub_np, "vhub-strings");
-	if (desc_np) {
+	if (desc_np)
 		ret = ast_vhub_of_parse_str_desc(vhub, desc_np);
-		of_node_put(desc_np);
-	}
 	else
 		ret = ast_vhub_str_alloc_add(vhub, &ast_vhub_strings);
 
diff --git a/drivers/usb/gadget/udc/aspeed-vhub/vhub.h b/drivers/usb/gadget/udc/aspeed-vhub/vhub.h
index 6b9dfa6e10eb..e6a11a22422a 100644
--- a/drivers/usb/gadget/udc/aspeed-vhub/vhub.h
+++ b/drivers/usb/gadget/udc/aspeed-vhub/vhub.h
@@ -419,6 +419,7 @@ struct ast_vhub {
 
 	/* Upstream bus speed captured at bus reset */
 	unsigned int			speed;
+	u8				current_config;
 
 	/* Standard USB Descriptors of the vhub. */
 	struct usb_device_descriptor	vhub_dev_desc;
diff --git a/include/linux/i3c/ccc.h b/include/linux/i3c/ccc.h
index ad59a4ae60d1..0e078d2fd1ca 100644
--- a/include/linux/i3c/ccc.h
+++ b/include/linux/i3c/ccc.h
@@ -32,6 +32,9 @@
 #define I3C_CCC_DEFSLVS			I3C_CCC_ID(0x8, true)
 #define I3C_CCC_ENTTM			I3C_CCC_ID(0xb, true)
 #define I3C_CCC_ENTHDR(x)		I3C_CCC_ID(0x20 + (x), true)
+#define I3C_CCC_SETAASA			I3C_CCC_ID(0x29, true)
+#define I3C_CCC_SETHID			I3C_CCC_ID(0x61, true)
+#define I3C_CCC_DEVCTRL			I3C_CCC_ID(0x62, true)
 
 /* Unicast-only commands */
 #define I3C_CCC_SETDASA			I3C_CCC_ID(0x7, false)
@@ -132,7 +135,7 @@ struct i3c_ccc_dev_desc {
 struct i3c_ccc_defslvs {
 	u8 count;
 	struct i3c_ccc_dev_desc master;
-	struct i3c_ccc_dev_desc slaves[];
+	struct i3c_ccc_dev_desc slaves[0];
 } __packed;
 
 /**
@@ -240,9 +243,18 @@ struct i3c_ccc_bridged_slave_desc {
  */
 struct i3c_ccc_setbrgtgt {
 	u8 count;
-	struct i3c_ccc_bridged_slave_desc bslaves[];
+	struct i3c_ccc_bridged_slave_desc bslaves[0];
 } __packed;
 
+
+/**
+ * struct i3c_ccc_sethid - payload passed to SETHID CCC
+ *
+ * @hid: 3-bit HID
+ */
+struct i3c_ccc_sethid {
+	u8 hid;
+};
 /**
  * enum i3c_sdr_max_data_rate - max data rate values for private SDR transfers
  */
@@ -318,7 +330,7 @@ enum i3c_ccc_setxtime_subcmd {
  */
 struct i3c_ccc_setxtime {
 	u8 subcmd;
-	u8 data[];
+	u8 data[0];
 } __packed;
 
 #define I3C_CCC_GETXTIME_SYNC_MODE	BIT(0)
@@ -369,6 +381,8 @@ struct i3c_ccc_cmd_dest {
  * @rnw: true if the CCC should retrieve data from the device. Only valid for
  *	 unicast commands
  * @id: CCC command id
+ * @dbp: true if the defining byte present
+ * @db: the defining byte
  * @ndests: number of destinations. Should always be one for broadcast commands
  * @dests: array of destinations and associated payload for this CCC. Most of
  *	   the time, only one destination is provided
@@ -377,6 +391,8 @@ struct i3c_ccc_cmd_dest {
 struct i3c_ccc_cmd {
 	u8 rnw;
 	u8 id;
+	u8 dbp;
+	u8 db;
 	unsigned int ndests;
 	struct i3c_ccc_cmd_dest *dests;
 	enum i3c_error_code err;
diff --git a/include/linux/i3c/device.h b/include/linux/i3c/device.h
index 8242e13e7b0b..6c39c403961c 100644
--- a/include/linux/i3c/device.h
+++ b/include/linux/i3c/device.h
@@ -49,6 +49,27 @@ enum i3c_hdr_mode {
 	I3C_HDR_TSL,
 };
 
+/**
+ * struct i3c_hdr_cmd - I3C HDR command
+ * @mode: HDR mode selected for this command
+ * @code: command opcode
+ * @addr: I3C dynamic address
+ * @ndatawords: number of data words (a word is 16bits wide)
+ * @data: input/output buffer
+ * @err: I3C error code
+ */
+struct i3c_hdr_cmd {
+	enum i3c_hdr_mode mode;
+	u8 code;
+	u8 addr;
+	int ndatawords;
+	union {
+		void *in;
+		const void *out;
+	} data;
+	enum i3c_error_code err;
+};
+
 /**
  * struct i3c_priv_xfer - I3C SDR private transfer
  * @rnw: encodes the transfer direction. true for a read, false for a write
@@ -71,11 +92,26 @@ struct i3c_priv_xfer {
 /**
  * enum i3c_dcr - I3C DCR values
  * @I3C_DCR_GENERIC_DEVICE: generic I3C device
+ * @I3C_DCR_HUB: I3C HUB device
  */
 enum i3c_dcr {
 	I3C_DCR_GENERIC_DEVICE = 0,
+	I3C_DCR_HUB = 194,
+	I3C_DCR_JESD403_BEGIN = 208,
+	I3C_DCR_THERMAL_SENSOR_FIRST = 210,
+	I3C_DCR_THERMAL_SENSOR_SECOND = 214,
+	I3C_DCR_PMIC_SECOND = 216,
+	I3C_DCR_PMIC_FIRST = 217,
+	I3C_DCR_SPD_HUB = 218,
+	I3C_DCR_RCD = 219,
+	I3C_DCR_PMIC_THIRD = 220,
+	I3C_DCR_JESD403_END = 223,
+	I3C_DCR_MAX = 228,
 };
 
+#define I3C_DCR_IS_JESD403_COMPLIANT(dcr)                                      \
+	(dcr >= I3C_DCR_JESD403_BEGIN && dcr <= I3C_DCR_JESD403_END)
+
 #define I3C_PID_MANUF_ID(pid)		(((pid) & GENMASK_ULL(47, 33)) >> 33)
 #define I3C_PID_RND_LOWER_32BITS(pid)	(!!((pid) & BIT_ULL(32)))
 #define I3C_PID_RND_VAL(pid)		((pid) & GENMASK_ULL(31, 0))
@@ -93,6 +129,22 @@ enum i3c_dcr {
 #define I3C_BCR_IBI_REQ_CAP		BIT(1)
 #define I3C_BCR_MAX_DATA_SPEED_LIM	BIT(0)
 
+/*
+ * MIPI I3C MDB definition
+ * see https://www.mipi.org/MIPI_I3C_mandatory_data_byte_values_public
+ */
+#define IBI_MDB_ID(grp, id)                                                    \
+	((((grp) << 5) & GENMASK(7, 5)) | ((id)&GENMASK(4, 0)))
+#define IBI_MDB_GET_GRP(m) (((m)&GENMASK(7, 5)) >> 5)
+#define IBI_MDB_GET_ID(m) ((m)&GENMASK(4, 0))
+
+#define IBI_MDB_GRP_PENDING_READ_NOTIF 0x5
+#define IS_MDB_PENDING_READ_NOTIFY(m)                                          \
+	(IBI_MDB_GET_GRP(m) == IBI_MDB_GRP_PENDING_READ_NOTIF)
+#define IBI_MDB_MIPI_DBGDATAREADY                                              \
+	IBI_MDB_ID(IBI_MDB_GRP_PENDING_READ_NOTIF, 0xd)
+#define IBI_MDB_MCTP IBI_MDB_ID(IBI_MDB_GRP_PENDING_READ_NOTIF, 0xe)
+
 /**
  * struct i3c_device_info - I3C device information
  * @pid: Provisional ID
@@ -107,6 +159,8 @@ enum i3c_dcr {
  * @max_read_turnaround: max read turn-around time in micro-seconds
  * @max_read_len: max private SDR read length in bytes
  * @max_write_len: max private SDR write length in bytes
+ * @pec: flag telling whether PEC (Packet Error Check) generation and verification for read
+ *       and write transaction is enabled
  *
  * These are all basic information that should be advertised by an I3C device.
  * Some of them are optional depending on the device type and device
@@ -128,6 +182,8 @@ struct i3c_device_info {
 	u32 max_read_turnaround;
 	u16 max_read_len;
 	u16 max_write_len;
+	u8 pec;
+	__be16 status;
 };
 
 /*
@@ -178,6 +234,7 @@ struct i3c_driver {
 	int (*probe)(struct i3c_device *dev);
 	void (*remove)(struct i3c_device *dev);
 	const struct i3c_device_id *id_table;
+	bool target;
 };
 
 static inline struct i3c_driver *drv_to_i3cdrv(struct device_driver *drv)
@@ -293,6 +350,12 @@ int i3c_device_do_priv_xfers(struct i3c_device *dev,
 			     struct i3c_priv_xfer *xfers,
 			     int nxfers);
 
+int i3c_device_send_hdr_cmds(struct i3c_device *dev,
+			     struct i3c_hdr_cmd *cmds,
+			     int ncmds);
+
+int i3c_device_generate_ibi(struct i3c_device *dev, const u8 *data, int len);
+
 void i3c_device_get_info(struct i3c_device *dev, struct i3c_device_info *info);
 
 struct i3c_ibi_payload {
@@ -331,5 +394,21 @@ int i3c_device_request_ibi(struct i3c_device *dev,
 void i3c_device_free_ibi(struct i3c_device *dev);
 int i3c_device_enable_ibi(struct i3c_device *dev);
 int i3c_device_disable_ibi(struct i3c_device *dev);
+int i3c_device_send_ccc_cmd(struct i3c_device *dev, u8 ccc_id);
+
+int i3c_device_getstatus_ccc(struct i3c_device *dev, struct i3c_device_info *info);
+int i3c_device_setmrl_ccc(struct i3c_device *dev, struct i3c_device_info *info, u16 read_len,
+			  u8 ibi_len);
+int i3c_device_setmwl_ccc(struct i3c_device *dev, struct i3c_device_info *info, u16 write_len);
+int i3c_device_getmrl_ccc(struct i3c_device *dev, struct i3c_device_info *info);
+int i3c_device_getmwl_ccc(struct i3c_device *dev, struct i3c_device_info *info);
+
+struct i3c_target_read_setup {
+	void (*handler)(struct i3c_device *dev, const u8 *data, size_t len);
+};
+
+int i3c_target_read_register(struct i3c_device *dev, const struct i3c_target_read_setup *setup);
+
+int i3c_device_control_pec(struct i3c_device *dev, bool pec);
 
 #endif /* I3C_DEV_H */
diff --git a/include/linux/i3c/master.h b/include/linux/i3c/master.h
index 604a126b78c8..03fe3377d993 100644
--- a/include/linux/i3c/master.h
+++ b/include/linux/i3c/master.h
@@ -22,10 +22,13 @@
 #define I3C_BROADCAST_ADDR		0x7e
 #define I3C_MAX_ADDR			GENMASK(6, 0)
 
+struct i3c_target_ops;
 struct i3c_master_controller;
 struct i3c_bus;
 struct i2c_device;
 struct i3c_device;
+struct i3c_slave_setup;
+struct i3c_slave_payload;
 
 /**
  * struct i3c_i2c_dev_desc - Common part of the I3C/I2C device descriptor
@@ -180,14 +183,26 @@ struct i3c_dev_boardinfo {
 	u8 init_dyn_addr;
 	u8 static_addr;
 	u64 pid;
+	u8 bcr;
+	u8 dcr;
 	struct device_node *of_node;
 };
 
+/**
+ * struct i3c_target_info - target information attached to a specific device
+ * @read handler: handler specified at i3c_target_read_register() call time.
+ */
+
+struct i3c_target_info {
+	void (*read_handler)(struct i3c_device *dev, const u8 *data, size_t len);
+};
+
 /**
  * struct i3c_dev_desc - I3C device descriptor
  * @common: common part of the I3C device descriptor
  * @info: I3C device information. Will be automatically filled when you create
  *	  your device with i3c_master_add_i3c_dev_locked()
+ * @target_info: I3C target information.
  * @ibi_lock: lock used to protect the &struct_i3c_device->ibi
  * @ibi: IBI info attached to a device. Should be NULL until
  *	 i3c_device_request_ibi() is called
@@ -206,6 +221,7 @@ struct i3c_dev_boardinfo {
 struct i3c_dev_desc {
 	struct i3c_i2c_dev_desc common;
 	struct i3c_device_info info;
+	struct i3c_target_info target_info;
 	struct mutex ibi_lock;
 	struct i3c_device_ibi_info *ibi;
 	struct i3c_device *dev;
@@ -383,6 +399,9 @@ struct i3c_bus {
  *		      all CCC commands are supported.
  * @send_ccc_cmd: send a CCC command
  *		  This method is mandatory.
+ * @send_hdr_cmds: send one or several HDR commands. If there is more than one
+ *		   command, they should ideally be sent in the same HDR
+ *		   transaction
  * @priv_xfers: do one or several private I3C SDR transfers
  *		This method is mandatory.
  * @attach_i2c_dev: called every time an I2C device is attached to the bus.
@@ -429,6 +448,7 @@ struct i3c_bus {
 struct i3c_master_controller_ops {
 	int (*bus_init)(struct i3c_master_controller *master);
 	void (*bus_cleanup)(struct i3c_master_controller *master);
+	void (*bus_reset)(struct i3c_master_controller *master);
 	int (*attach_i3c_dev)(struct i3c_dev_desc *dev);
 	int (*reattach_i3c_dev)(struct i3c_dev_desc *dev, u8 old_dyn_addr);
 	void (*detach_i3c_dev)(struct i3c_dev_desc *dev);
@@ -437,6 +457,9 @@ struct i3c_master_controller_ops {
 				 const struct i3c_ccc_cmd *cmd);
 	int (*send_ccc_cmd)(struct i3c_master_controller *master,
 			    struct i3c_ccc_cmd *cmd);
+	int (*send_hdr_cmds)(struct i3c_master_controller *master,
+			     struct i3c_hdr_cmd *cmds,
+			     int ncmds);
 	int (*priv_xfers)(struct i3c_dev_desc *dev,
 			  struct i3c_priv_xfer *xfers,
 			  int nxfers);
@@ -451,6 +474,14 @@ struct i3c_master_controller_ops {
 	int (*disable_ibi)(struct i3c_dev_desc *dev);
 	void (*recycle_ibi_slot)(struct i3c_dev_desc *dev,
 				 struct i3c_ibi_slot *slot);
+	int (*register_slave)(struct i3c_master_controller *master,
+			      const struct i3c_slave_setup *req);
+	int (*unregister_slave)(struct i3c_master_controller *master);
+	int (*send_sir)(struct i3c_master_controller *master,
+			struct i3c_slave_payload *payload);
+	int (*put_read_data)(struct i3c_master_controller *master,
+			     struct i3c_slave_payload *data,
+			     struct i3c_slave_payload *ibi_notify);
 };
 
 /**
@@ -462,6 +493,8 @@ struct i3c_master_controller_ops {
  *	 registered to the I2C subsystem to be as transparent as possible to
  *	 existing I2C drivers
  * @ops: master operations. See &struct i3c_master_controller_ops
+ * @target_ops: target operations. See &struct i3c_target_ops
+ * @target: true if the underlying I3C device acts as a target on I3C bus
  * @secondary: true if the master is a secondary master
  * @init_done: true when the bus initialization is done
  * @boardinfo.i3c: list of I3C  boardinfo objects
@@ -484,8 +517,12 @@ struct i3c_master_controller {
 	struct i3c_dev_desc *this;
 	struct i2c_adapter i2c;
 	const struct i3c_master_controller_ops *ops;
+	const struct i3c_target_ops *target_ops;
+	unsigned int pec_supported : 1;
+	unsigned int target : 1;
 	unsigned int secondary : 1;
 	unsigned int init_done : 1;
+	unsigned int jdec_spd : 1;
 	struct {
 		struct list_head i3c;
 		struct list_head i2c;
@@ -524,15 +561,20 @@ int i3c_master_disec_locked(struct i3c_master_controller *master, u8 addr,
 			    u8 evts);
 int i3c_master_enec_locked(struct i3c_master_controller *master, u8 addr,
 			   u8 evts);
+int i3c_master_setmrl_locked(struct i3c_master_controller *master,
+			     struct i3c_device_info *info, u16 read_len,
+			     u8 ibi_len);
 int i3c_master_entdaa_locked(struct i3c_master_controller *master);
 int i3c_master_defslvs_locked(struct i3c_master_controller *master);
-
+int i3c_master_rstdaa_locked(struct i3c_master_controller *master,
+				    u8 addr);
 int i3c_master_get_free_addr(struct i3c_master_controller *master,
 			     u8 start_addr);
 
 int i3c_master_add_i3c_dev_locked(struct i3c_master_controller *master,
 				  u8 addr);
 int i3c_master_do_daa(struct i3c_master_controller *master);
+int i3c_master_enable_hj(struct i3c_master_controller *master);
 
 int i3c_master_set_info(struct i3c_master_controller *master,
 			const struct i3c_device_info *info);
@@ -543,6 +585,13 @@ int i3c_master_register(struct i3c_master_controller *master,
 			bool secondary);
 int i3c_master_unregister(struct i3c_master_controller *master);
 
+int i3c_register(struct i3c_master_controller *master,
+		 struct device *parent,
+		 const struct i3c_master_controller_ops *master_ops,
+		 const struct i3c_target_ops *target_ops,
+		 bool secondary);
+int i3c_unregister(struct i3c_master_controller *master);
+
 /**
  * i3c_dev_get_master_data() - get master private data attached to an I3C
  *			       device descriptor
@@ -635,6 +684,18 @@ i3c_master_get_bus(struct i3c_master_controller *master)
 	return &master->bus;
 }
 
+struct i3c_slave_payload {
+	unsigned int len;
+	const void *data;
+};
+
+struct i3c_slave_setup {
+	unsigned int max_payload_len;
+	unsigned int num_slots;
+	void (*handler)(struct i3c_master_controller *m,
+			const struct i3c_slave_payload *payload);
+};
+
 struct i3c_generic_ibi_pool;
 
 struct i3c_generic_ibi_pool *
@@ -651,4 +712,32 @@ void i3c_master_queue_ibi(struct i3c_dev_desc *dev, struct i3c_ibi_slot *slot);
 
 struct i3c_ibi_slot *i3c_master_get_free_ibi_slot(struct i3c_dev_desc *dev);
 
+int i3c_master_register_slave(struct i3c_master_controller *master,
+			      const struct i3c_slave_setup *req);
+int i3c_master_unregister_slave(struct i3c_master_controller *master);
+int i3c_master_send_sir(struct i3c_master_controller *master,
+			struct i3c_slave_payload *payload);
+int i3c_master_send_hdr_cmds(struct i3c_master_controller *master,
+			     struct i3c_hdr_cmd *cmds, int ncmds);
+/**
+ * i3c_master_put_read_data() - put read data and optionally notify primary master
+ * @master: master object in slave mode
+ * @data: data structure to be read
+ * @ibi_notify: IBI data (including MDB) to notify primary master device
+ */
+int i3c_master_put_read_data(struct i3c_master_controller *master,
+			     struct i3c_slave_payload *data,
+			     struct i3c_slave_payload *ibi_notify);
+/*
+ * Slave message queue driver API
+ */
+#ifdef CONFIG_I3C_SLAVE_MQUEUE
+int i3c_slave_mqueue_probe(struct i3c_master_controller *master);
+int i3c_slave_mqueue_remove(struct i3c_master_controller *master);
+#endif
+
+#ifdef CONFIG_I3C_SLAVE_EEPROM
+int i3c_slave_eeprom_probe(struct i3c_master_controller *master);
+int i3c_slave_eeprom_remove(struct i3c_master_controller *master);
+#endif
 #endif /* I3C_MASTER_H */
diff --git a/include/linux/i3c/target.h b/include/linux/i3c/target.h
new file mode 100644
index 000000000000..9e71124b5325
--- /dev/null
+++ b/include/linux/i3c/target.h
@@ -0,0 +1,23 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (c) 2022, Intel Corporation */
+
+#ifndef I3C_TARGET_H
+#define I3C_TARGET_H
+
+#include <linux/device.h>
+#include <linux/i3c/device.h>
+
+struct i3c_master_controller;
+
+struct i3c_target_ops {
+	int (*bus_init)(struct i3c_master_controller *master);
+	void (*bus_cleanup)(struct i3c_master_controller *master);
+	int (*priv_xfers)(struct i3c_dev_desc *dev, struct i3c_priv_xfer *xfers, int nxfers);
+	int (*generate_ibi)(struct i3c_dev_desc *dev, const u8 *data, int len);
+};
+
+int i3c_target_register(struct i3c_master_controller *master, struct device *parent,
+			const struct i3c_target_ops *ops);
+int i3c_target_unregister(struct i3c_master_controller *master);
+
+#endif
diff --git a/include/linux/jtag.h b/include/linux/jtag.h
new file mode 100644
index 000000000000..3b7157df3e4c
--- /dev/null
+++ b/include/linux/jtag.h
@@ -0,0 +1,49 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (c) 2018 Mellanox Technologies. All rights reserved. */
+/* Copyright (c) 2018 Oleksandr Shamray <oleksandrs@mellanox.com> */
+/* Copyright (c) 2019 Intel Corporation */
+
+#ifndef __LINUX_JTAG_H
+#define __LINUX_JTAG_H
+
+#include <linux/types.h>
+#include <uapi/linux/jtag.h>
+
+#define JTAG_MAX_XFER_DATA_LEN (0xFFFFFFFF) //65535
+
+struct jtag;
+/**
+ * struct jtag_ops - callbacks for JTAG control functions:
+ *
+ * @freq_get: get frequency function. Filled by dev driver
+ * @freq_set: set frequency function. Filled by dev driver
+ * @status_get: get JTAG TAPC state function. Mandatory, Filled by dev driver
+ * @status_set: set JTAG TAPC state function. Mandatory, Filled by dev driver
+ * @xfer: send JTAG xfer function. Mandatory func. Filled by dev driver
+ * @mode_set: set specific work mode for JTAG. Filled by dev driver
+ * @trst_set: set TRST pin active(pull low) for JTAG. Filled by dev driver
+ * @bitbang: set low level bitbang operations. Filled by dev driver
+ * @enable: enables JTAG interface in master mode. Filled by dev driver
+ * @disable: disables JTAG interface master mode. Filled by dev driver
+ */
+struct jtag_ops {
+	int (*freq_get)(struct jtag *jtag, u32 *freq);
+	int (*freq_set)(struct jtag *jtag, u32 freq);
+	int (*status_get)(struct jtag *jtag, u32 *state);
+	int (*status_set)(struct jtag *jtag, struct jtag_tap_state *endst);
+	int (*xfer)(struct jtag *jtag, struct jtag_xfer *xfer, u8 *xfer_data);
+	int (*mode_set)(struct jtag *jtag, struct jtag_mode *jtag_mode);
+	int (*trst_set)(struct jtag *jtag, u32 active);
+	int (*bitbang)(struct jtag *jtag, struct bitbang_packet *bitbang,
+		       struct tck_bitbang *bitbang_data);
+	int (*enable)(struct jtag *jtag);
+	int (*disable)(struct jtag *jtag);
+};
+
+void *jtag_priv(struct jtag *jtag);
+int devm_jtag_register(struct device *dev, struct jtag *jtag);
+struct jtag *jtag_alloc(struct device *host, size_t priv_size,
+			const struct jtag_ops *ops);
+void jtag_free(struct jtag *jtag);
+
+#endif /* __LINUX_JTAG_H */
diff --git a/include/uapi/linux/i3c/i3cdev.h b/include/uapi/linux/i3c/i3cdev.h
new file mode 100644
index 000000000000..5adc1e3e7c4f
--- /dev/null
+++ b/include/uapi/linux/i3c/i3cdev.h
@@ -0,0 +1,37 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ * Copyright (c) 2020 Synopsys, Inc. and/or its affiliates.
+ *
+ * Author: Vitor Soares <vitor.soares@synopsys.com>
+ */
+
+#ifndef _UAPI_I3C_DEV_H_
+#define _UAPI_I3C_DEV_H_
+
+#include <linux/types.h>
+#include <linux/ioctl.h>
+
+/* IOCTL commands */
+#define I3C_DEV_IOC_MAGIC	0x07
+
+/**
+ * struct i3c_ioc_priv_xfer - I3C SDR ioctl private transfer
+ * @data: Holds pointer to userspace buffer with transmit data.
+ * @len: Length of data buffer buffers, in bytes.
+ * @rnw: encodes the transfer direction. true for a read, false for a write
+ */
+struct i3c_ioc_priv_xfer {
+	__u64 data;
+	__u16 len;
+	__u8 rnw;
+	__u8 pad[5];
+};
+
+#define I3C_PRIV_XFER_SIZE(N)	\
+	((((sizeof(struct i3c_ioc_priv_xfer)) * (N)) < (1 << _IOC_SIZEBITS)) \
+	? ((sizeof(struct i3c_ioc_priv_xfer)) * (N)) : 0)
+
+#define I3C_IOC_PRIV_XFER(N)	\
+	_IOC(_IOC_READ|_IOC_WRITE, I3C_DEV_IOC_MAGIC, 30, I3C_PRIV_XFER_SIZE(N))
+
+#endif
diff --git a/include/uapi/linux/jtag.h b/include/uapi/linux/jtag.h
new file mode 100644
index 000000000000..77d0b471efd3
--- /dev/null
+++ b/include/uapi/linux/jtag.h
@@ -0,0 +1,370 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/* Copyright (c) 2018 Mellanox Technologies. All rights reserved. */
+/* Copyright (c) 2018 Oleksandr Shamray <oleksandrs@mellanox.com> */
+/* Copyright (c) 2019 Intel Corporation */
+
+#ifndef __UAPI_LINUX_JTAG_H
+#define __UAPI_LINUX_JTAG_H
+
+#include <linux/types.h>
+#include <linux/ioctl.h>
+
+/*
+ * JTAG_XFER_MODE: JTAG transfer mode. Used to set JTAG controller transfer mode
+ * This is bitmask for feature param in jtag_mode for ioctl JTAG_SIOCMODE
+ */
+#define  JTAG_XFER_MODE 0
+/*
+ * JTAG_CONTROL_MODE: JTAG controller mode. Used to set JTAG controller mode
+ * This is bitmask for feature param in jtag_mode for ioctl JTAG_SIOCMODE
+ */
+#define  JTAG_CONTROL_MODE 1
+/*
+ * JTAG_MASTER_OUTPUT_DISABLE: JTAG master mode output disable, it is used to
+ * enable other devices to own the JTAG bus.
+ * This is bitmask for mode param in jtag_mode for ioctl JTAG_SIOCMODE
+ */
+#define  JTAG_MASTER_OUTPUT_DISABLE 0
+/*
+ * JTAG_MASTER_MODE: JTAG master mode. Used to set JTAG controller master mode
+ * This is bitmask for mode param in jtag_mode for ioctl JTAG_SIOCMODE
+ */
+#define  JTAG_MASTER_MODE 1
+/*
+ * JTAG_XFER_HW_MODE: JTAG hardware mode. Used to set HW drived or bitbang
+ * mode. This is bitmask for mode param in jtag_mode for ioctl JTAG_SIOCMODE
+ */
+#define  JTAG_XFER_HW_MODE 1
+/*
+ * JTAG_XFER_SW_MODE: JTAG software mode. Used to set SW drived or bitbang
+ * mode. This is bitmask for mode param in jtag_mode for ioctl JTAG_SIOCMODE
+ */
+#define  JTAG_XFER_SW_MODE 0
+
+/**
+ * enum jtag_tapstate:
+ *
+ * @JTAG_STATE_TLRESET: JTAG state machine Test Logic Reset state
+ * @JTAG_STATE_IDLE: JTAG state machine IDLE state
+ * @JTAG_STATE_SELECTDR: JTAG state machine SELECT_DR state
+ * @JTAG_STATE_CAPTUREDR: JTAG state machine CAPTURE_DR state
+ * @JTAG_STATE_SHIFTDR: JTAG state machine SHIFT_DR state
+ * @JTAG_STATE_EXIT1DR: JTAG state machine EXIT-1 DR state
+ * @JTAG_STATE_PAUSEDR: JTAG state machine PAUSE_DR state
+ * @JTAG_STATE_EXIT2DR: JTAG state machine EXIT-2 DR state
+ * @JTAG_STATE_UPDATEDR: JTAG state machine UPDATE DR state
+ * @JTAG_STATE_SELECTIR: JTAG state machine SELECT_IR state
+ * @JTAG_STATE_CAPTUREIR: JTAG state machine CAPTURE_IR state
+ * @JTAG_STATE_SHIFTIR: JTAG state machine SHIFT_IR state
+ * @JTAG_STATE_EXIT1IR: JTAG state machine EXIT-1 IR state
+ * @JTAG_STATE_PAUSEIR: JTAG state machine PAUSE_IR state
+ * @JTAG_STATE_EXIT2IR: JTAG state machine EXIT-2 IR state
+ * @JTAG_STATE_UPDATEIR: JTAG state machine UPDATE IR state
+ * @JTAG_STATE_CURRENT: JTAG current state, saved by driver
+ */
+enum jtag_tapstate {
+	JTAG_STATE_TLRESET,
+	JTAG_STATE_IDLE,
+	JTAG_STATE_SELECTDR,
+	JTAG_STATE_CAPTUREDR,
+	JTAG_STATE_SHIFTDR,
+	JTAG_STATE_EXIT1DR,
+	JTAG_STATE_PAUSEDR,
+	JTAG_STATE_EXIT2DR,
+	JTAG_STATE_UPDATEDR,
+	JTAG_STATE_SELECTIR,
+	JTAG_STATE_CAPTUREIR,
+	JTAG_STATE_SHIFTIR,
+	JTAG_STATE_EXIT1IR,
+	JTAG_STATE_PAUSEIR,
+	JTAG_STATE_EXIT2IR,
+	JTAG_STATE_UPDATEIR,
+	JTAG_STATE_CURRENT
+};
+
+/**
+ * enum jtag_reset:
+ *
+ * @JTAG_NO_RESET: JTAG run TAP from current state
+ * @JTAG_FORCE_RESET: JTAG force TAP to reset state
+ */
+enum jtag_reset {
+	JTAG_NO_RESET = 0,
+	JTAG_FORCE_RESET = 1,
+};
+
+/**
+ * enum jtag_xfer_type:
+ *
+ * @JTAG_SIR_XFER: SIR transfer
+ * @JTAG_SDR_XFER: SDR transfer
+ */
+enum jtag_xfer_type {
+	JTAG_SIR_XFER = 0,
+	JTAG_SDR_XFER = 1,
+};
+
+/**
+ * enum jtag_xfer_direction:
+ *
+ * @JTAG_READ_XFER: read transfer
+ * @JTAG_WRITE_XFER: write transfer
+ * @JTAG_READ_WRITE_XFER: read & write transfer
+ */
+enum jtag_xfer_direction {
+	JTAG_READ_XFER = 1,
+	JTAG_WRITE_XFER = 2,
+	JTAG_READ_WRITE_XFER = 3,
+};
+
+/**
+ * struct jtag_tap_state - forces JTAG state machine to go into a TAPC
+ * state
+ *
+ * @reset: 0 - run IDLE/PAUSE from current state
+ *         1 - go through TEST_LOGIC/RESET state before  IDLE/PAUSE
+ * @end: completion flag
+ * @tck: clock counter
+ *
+ * Structure provide interface to JTAG device for JTAG set state execution.
+ */
+struct jtag_tap_state {
+	__u8	reset;
+	__u8	from;
+	__u8	endstate;
+	__u8	tck;
+};
+
+/**
+ * union pad_config - Padding Configuration:
+ *
+ * @type: transfer type
+ * @pre_pad_number: Number of prepadding bits bit[11:0]
+ * @post_pad_number: Number of prepadding bits bit[23:12]
+ * @pad_data : Bit value to be used by pre and post padding bit[24]
+ * @int_value: unsigned int packed padding configuration value bit[32:0]
+ *
+ * Structure provide pre and post padding configuration in a single __u32
+ */
+union pad_config {
+	struct {
+		__u32 pre_pad_number	: 12;
+		__u32 post_pad_number	: 12;
+		__u32 pad_data		: 1;
+		__u32 rsvd		: 7;
+	};
+	__u32 int_value;
+};
+
+/**
+ * struct jtag_xfer - jtag xfer:
+ *
+ * @type: transfer type
+ * @direction: xfer direction
+ * @from: xfer current state
+ * @endstate: xfer end state
+ * @padding: xfer padding
+ * @length: xfer bits length
+ * @tdio : xfer data array
+ *
+ * Structure provide interface to JTAG device for JTAG SDR/SIR xfer execution.
+ */
+struct jtag_xfer {
+	__u8	type;
+	__u8	direction;
+	__u8	from;
+	__u8	endstate;
+	__u32	padding;
+	__u32	length;
+	__u64	tdio;
+};
+
+/**
+ * struct bitbang_packet - jtag bitbang array packet:
+ *
+ * @data:   JTAG Bitbang struct array pointer(input/output)
+ * @length: array size (input)
+ *
+ * Structure provide interface to JTAG device for JTAG bitbang bundle execution
+ */
+struct bitbang_packet {
+	struct tck_bitbang *data;
+	__u32	length;
+} __attribute__((__packed__));
+
+/**
+ * struct jtag_bitbang - jtag bitbang:
+ *
+ * @tms: JTAG TMS
+ * @tdi: JTAG TDI (input)
+ * @tdo: JTAG TDO (output)
+ *
+ * Structure provide interface to JTAG device for JTAG bitbang execution.
+ */
+struct tck_bitbang {
+	__u8	tms;
+	__u8	tdi;
+	__u8	tdo;
+} __attribute__((__packed__));
+
+/**
+ * struct jtag_mode - jtag mode:
+ *
+ * @feature: 0 - JTAG feature setting selector for JTAG controller HW/SW
+ *           1 - JTAG feature setting selector for controller bus master
+ *               mode output (enable / disable).
+ * @mode:    (0 - SW / 1 - HW) for JTAG_XFER_MODE feature(0)
+ *           (0 - output disable / 1 - output enable) for JTAG_CONTROL_MODE
+ *                                                    feature(1)
+ *
+ * Structure provide configuration modes to JTAG device.
+ */
+struct jtag_mode {
+	__u32	feature;
+	__u32	mode;
+};
+
+/* ioctl interface */
+#define __JTAG_IOCTL_MAGIC	0xb2
+
+#define JTAG_SIOCSTATE	_IOW(__JTAG_IOCTL_MAGIC, 0, struct jtag_tap_state)
+#define JTAG_SIOCFREQ	_IOW(__JTAG_IOCTL_MAGIC, 1, unsigned int)
+#define JTAG_GIOCFREQ	_IOR(__JTAG_IOCTL_MAGIC, 2, unsigned int)
+#define JTAG_IOCXFER	_IOWR(__JTAG_IOCTL_MAGIC, 3, struct jtag_xfer)
+#define JTAG_GIOCSTATUS _IOWR(__JTAG_IOCTL_MAGIC, 4, enum jtag_tapstate)
+#define JTAG_SIOCMODE	_IOW(__JTAG_IOCTL_MAGIC, 5, unsigned int)
+#define JTAG_IOCBITBANG	_IOW(__JTAG_IOCTL_MAGIC, 6, unsigned int)
+#define JTAG_SIOCTRST	_IOW(__JTAG_IOCTL_MAGIC, 7, unsigned int)
+
+/**
+ * struct tms_cycle - This structure represents a tms cycle state.
+ *
+ * @tmsbits: is the bitwise representation of the needed tms transitions to
+ *           move from one state to another.
+ * @count:   number of jumps needed to move to the needed state.
+ *
+ */
+struct tms_cycle {
+	unsigned char tmsbits;
+	unsigned char count;
+};
+
+/*
+ * This is the complete set TMS cycles for going from any TAP state to any
+ * other TAP state, following a "shortest path" rule.
+ */
+static const struct tms_cycle _tms_cycle_lookup[][16] = {
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* TLR  */{{0x00, 0}, {0x00, 1}, {0x02, 2}, {0x02, 3}, {0x02, 4}, {0x0a, 4},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x0a, 5}, {0x2a, 6}, {0x1a, 5}, {0x06, 3}, {0x06, 4}, {0x06, 5},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x16, 5}, {0x16, 6}, {0x56, 7}, {0x36, 6} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* RTI  */{{0x07, 3}, {0x00, 0}, {0x01, 1}, {0x01, 2}, {0x01, 3}, {0x05, 3},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x05, 4}, {0x15, 5}, {0x0d, 4}, {0x03, 2}, {0x03, 3}, {0x03, 4},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x0b, 4}, {0x0b, 5}, {0x2b, 6}, {0x1b, 5} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* SelDR*/{{0x03, 2}, {0x03, 3}, {0x00, 0}, {0x00, 1}, {0x00, 2}, {0x02, 2},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x02, 3}, {0x0a, 4}, {0x06, 3}, {0x01, 1}, {0x01, 2}, {0x01, 3},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x05, 3}, {0x05, 4}, {0x15, 5}, {0x0d, 4} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* CapDR*/{{0x1f, 5}, {0x03, 3}, {0x07, 3}, {0x00, 0}, {0x00, 1}, {0x01, 1},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x01, 2}, {0x05, 3}, {0x03, 2}, {0x0f, 4}, {0x0f, 5}, {0x0f, 6},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x2f, 6}, {0x2f, 7}, {0xaf, 8}, {0x6f, 7} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* SDR  */{{0x1f, 5}, {0x03, 3}, {0x07, 3}, {0x07, 4}, {0x00, 0}, {0x01, 1},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x01, 2}, {0x05, 3}, {0x03, 2}, {0x0f, 4}, {0x0f, 5}, {0x0f, 6},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x2f, 6}, {0x2f, 7}, {0xaf, 8}, {0x6f, 7} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* Ex1DR*/{{0x0f, 4}, {0x01, 2}, {0x03, 2}, {0x03, 3}, {0x02, 3}, {0x00, 0},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x00, 1}, {0x02, 2}, {0x01, 1}, {0x07, 3}, {0x07, 4}, {0x07, 5},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x17, 5}, {0x17, 6}, {0x57, 7}, {0x37, 6} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* PDR  */{{0x1f, 5}, {0x03, 3}, {0x07, 3}, {0x07, 4}, {0x01, 2}, {0x05, 3},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x00, 0}, {0x01, 1}, {0x03, 2}, {0x0f, 4}, {0x0f, 5}, {0x0f, 6},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x2f, 6}, {0x2f, 7}, {0xaf, 8}, {0x6f, 7} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* Ex2DR*/{{0x0f, 4}, {0x01, 2}, {0x03, 2}, {0x03, 3}, {0x00, 1}, {0x02, 2},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x02, 3}, {0x00, 0}, {0x01, 1}, {0x07, 3}, {0x07, 4}, {0x07, 5},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x17, 5}, {0x17, 6}, {0x57, 7}, {0x37, 6} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* UpdDR*/{{0x07, 3}, {0x00, 1}, {0x01, 1}, {0x01, 2}, {0x01, 3}, {0x05, 3},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x05, 4}, {0x15, 5}, {0x00, 0}, {0x03, 2}, {0x03, 3}, {0x03, 4},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x0b, 4}, {0x0b, 5}, {0x2b, 6}, {0x1b, 5} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* SelIR*/{{0x01, 1}, {0x01, 2}, {0x05, 3}, {0x05, 4}, {0x05, 5}, {0x15, 5},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x15, 6}, {0x55, 7}, {0x35, 6}, {0x00, 0}, {0x00, 1}, {0x00, 2},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x02, 2}, {0x02, 3}, {0x0a, 4}, {0x06, 3} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* CapIR*/{{0x1f, 5}, {0x03, 3}, {0x07, 3}, {0x07, 4}, {0x07, 5}, {0x17, 5},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x17, 6}, {0x57, 7}, {0x37, 6}, {0x0f, 4}, {0x00, 0}, {0x00, 1},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x01, 1}, {0x01, 2}, {0x05, 3}, {0x03, 2} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* SIR  */{{0x1f, 5}, {0x03, 3}, {0x07, 3}, {0x07, 4}, {0x07, 5}, {0x17, 5},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x17, 6}, {0x57, 7}, {0x37, 6}, {0x0f, 4}, {0x0f, 5}, {0x00, 0},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x01, 1}, {0x01, 2}, {0x05, 3}, {0x03, 2} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* Ex1IR*/{{0x0f, 4}, {0x01, 2}, {0x03, 2}, {0x03, 3}, {0x03, 4}, {0x0b, 4},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x0b, 5}, {0x2b, 6}, {0x1b, 5}, {0x07, 3}, {0x07, 4}, {0x02, 3},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x00, 0}, {0x00, 1}, {0x02, 2}, {0x01, 1} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* PIR  */{{0x1f, 5}, {0x03, 3}, {0x07, 3}, {0x07, 4}, {0x07, 5}, {0x17, 5},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x17, 6}, {0x57, 7}, {0x37, 6}, {0x0f, 4}, {0x0f, 5}, {0x01, 2},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x05, 3}, {0x00, 0}, {0x01, 1}, {0x03, 2} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* Ex2IR*/{{0x0f, 4}, {0x01, 2}, {0x03, 2}, {0x03, 3}, {0x03, 4}, {0x0b, 4},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x0b, 5}, {0x2b, 6}, {0x1b, 5}, {0x07, 3}, {0x07, 4}, {0x00, 1},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x02, 2}, {0x02, 3}, {0x00, 0}, {0x01, 1} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* UpdIR*/{{0x07, 3}, {0x00, 1}, {0x01, 1}, {0x01, 2}, {0x01, 3}, {0x05, 3},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x05, 4}, {0x15, 5}, {0x0d, 4}, {0x03, 2}, {0x03, 3}, {0x03, 4},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x0b, 4}, {0x0b, 5}, {0x2b, 6}, {0x00, 0} },
+};
+
+#endif /* __UAPI_LINUX_JTAG_H */
-- 
2.25.1

